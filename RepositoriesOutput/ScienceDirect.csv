Item type,Authors,Editors,Title,Journal,Publication year,Volume,Issue,Pages,Publisher,Book title,Date published,ISBN,ISSN,URLs,DOI,Abstract,Keywords,Notes,Series,Edition,Address
Journal Article,"Pecorelli F,Di Nucci D,De Roover C,De Lucia A",,A large empirical assessment of the role of data balancing in machine-learning-based code smell detection,Journal of Systems and Software,2020,169.0,,110693,,,2020,,0164-1212,https://www.sciencedirect.com/science/article/pii/S0164121220301448;http://dx.doi.org/10.1016/j.jss.2020.110693,10.1016/j.jss.2020.110693,"Code smells can compromise software quality in the long term by inducing technical debt. For this reason, many approaches aimed at identifying these design flaws have been proposed in the last decade. Most of them are based on heuristics in which a set of metrics is used to detect smelly code components. However, these techniques suffer from subjective interpretations, a low agreement between detectors, and threshold dependability. To overcome these limitations, previous work applied Machine-Learning that can learn from previous datasets without needing any threshold definition. However, more recent work has shown that Machine-Learning is not always suitable for code smell detection due to the highly imbalanced nature of the problem. In this study, we investigate five approaches to mitigate data imbalance issues to understand their impact on Machine Learning-based approaches for code smell detection in Object-Oriented systems and those implementing the Model-View-Controller pattern. Our findings show that avoiding balancing does not dramatically impact accuracy. Existing data balancing techniques are inadequate for code smell detection leading to poor accuracy for Machine-Learning-based approaches. Therefore, new metrics to exploit different software characteristics and new techniques to effectively combine them are needed.","Code smells, Machine learning, Data balancing, Object oriented, Model view controller",,,,
Journal Article,"Rebai S,Kessentini M,Alizadeh V,Sghaier OB,Kazman R",,Recommending refactorings via commit message analysis,Information and Software Technology,2020,126.0,,106332,,,2020,,0950-5849,https://www.sciencedirect.com/science/article/pii/S0950584920300914;http://dx.doi.org/10.1016/j.infsof.2020.106332,10.1016/j.infsof.2020.106332,"Context The purpose of software restructuring, or refactoring, is to improve software quality and developer productivity. Objective Prior studies have relied mainly on static and dynamic analysis of code to detect and recommend refactoring opportunities, such as code smells. Once identified, these smells are fixed by applying refactorings which then improve a set of quality metrics. While this approach has value and has shown promising results, many detected refactoring opportunities may not be related to a developer’s current context and intention. Recent studies have shown that while developers document their refactoring intentions, they may miss relevant refactorings aligned with their rationale. Method In this paper, we first identify refactoring opportunities by analyzing developer commit messages and check the quality improvements in the changed files, then we distill this knowledge into usable context-driven refactoring recommendations to complement static and dynamic analysis of code. Results The evaluation of our approach, based on six open source projects, shows that we outperform prior studies that apply refactorings based on static and dynamic analysis of code alone. Conclusion This study provides compelling evidence of the value of using the information contained in existing commit messages to recommend future refactorings.","Commit message, Refactoring recommendation, Quality attributes",,,,
Journal Article,"de Oliveira MC,Freitas D,Bonifácio R,Pinto G,Lo D",,Finding needles in a haystack: Leveraging co-change dependencies to recommend refactorings,Journal of Systems and Software,2019,158.0,,110420,,,2019,,0164-1212,https://www.sciencedirect.com/science/article/pii/S0164121219301943;http://dx.doi.org/10.1016/j.jss.2019.110420,10.1016/j.jss.2019.110420,"A fine-grained co-change dependency arises when two fine-grained source-code entities, e.g., a method, change frequently together. This kind of dependency is relevant when considering remodularization efforts (e.g., to keep methods that change together in the same class). However, existing approaches for recommending refactorings that change software decomposition (such as a move method) do not explore the use of fine-grained co-change dependencies. In this paper we present a novel approach for recommending move method and move field refactorings, which removes co-change dependencies and evolutionary smells, a particular type of dependency that arise when fine-grained entities that belong to different classes frequently change together. First we evaluate our approach using 49 open-source Java projects, finding 610 evolutionary smells. Our approach automatically computes 56 refactoring recommendations that remove these evolutionary smells, without introducing new static dependencies. We also evaluate our approach by submitting pull-requests with the recommendations of our technique, in the context of one large and two medium size proprietary Java systems. Quantitative results show that our approach outperforms existing approaches for recommending refactorings when dealing with co-change dependencies. Qualitative results show that our approach is promising, not only for recommending refactorings but also to reveal opportunities of design improvements.","Refactoring, Co-change dependencies, Remodularization, Software clustering, Design quality",,,,
Journal Article,"Szőke G,Antal G,Nagy C,Ferenc R,Gyimóthy T",,Empirical study on refactoring large-scale industrial systems and its effects on maintainability,Journal of Systems and Software,2017,129.0,,107-126,,,2017,,0164-1212,https://www.sciencedirect.com/science/article/pii/S0164121216301558;http://dx.doi.org/10.1016/j.jss.2016.08.071,10.1016/j.jss.2016.08.071,"Software evolves continuously, it gets modified, enhanced, and new requirements always arise. If we do not spend time occasionally on improving our source code, its maintainability will inevitably decrease. The literature tells us that we can improve the maintainability of a software system by regularly refactoring it. But does refactoring really increase software maintainability? Can it happen that refactoring decreases the maintainability? Empirical studies show contradicting answers to these questions and there have been only a few studies which were performed in a large-scale, industrial context. In our paper, we assess these questions in an in vivo context, where we analyzed the source code and measured the maintainability of 6 large-scale, proprietary software systems in their manual refactoring phase. We analyzed 2.5 million lines of code and studied the effects on maintainability of 315 refactoring commits which fixed 1273 coding issues. We found that single refactorings only make a very little difference (sometimes even decrease maintainability), but a whole refactoring period, in general, can significantly increase maintainability, which can result not only in the local, but also in the global improvement of the code.","Refactoring, Software quality, Maintainability, Coding issues, Antipatterns, ISO/IEC 25010",,,,
Journal Article,"Ouni A,Kessentini M,Sahraoui H,Inoue K,Hamdi MS",,Improving multi-objective code-smells correction using development history,Journal of Systems and Software,2015,105.0,,18-39,,,2015,,0164-1212,https://www.sciencedirect.com/science/article/pii/S0164121215000631;http://dx.doi.org/10.1016/j.jss.2015.03.040,10.1016/j.jss.2015.03.040,"One of the widely used techniques to improve the quality of software systems is refactoring. Software refactoring improves the internal structure of the system while preserving its external behavior. These two concerns drive the existing approaches to refactoring automation. However, recent studies demonstrated that these concerns are not enough to produce correct and consistent refactoring solutions. In addition to quality improvement and behavior preservation, studies consider, among others, construct semantics preservation and minimization of changes. From another perspective, development history was proven as a powerful source of knowledge in many maintenance tasks. Still, development history is not widely explored in the context of automated software refactoring. In this paper, we use the development history collected from existing software projects to propose new refactoring solutions taking into account context similarity with situations seen in the past. We propose a multi-objective optimization-based approach to find good refactoring sequences that (1) minimize the number of code-smells, and (2) maximize the use of development history while (3) preserving the construct semantics. To this end, we use the non-dominated sorting genetic algorithm (NSGA-II) to find the best trade-offs between these three objectives. We evaluate our approach using a benchmark composed of five medium and large-size open-source systems and four types of code-smells (Blob, spaghetti code, functional decomposition, and data class). Our experimental results show the effectiveness of our approach, compared to three different state-of-the-art approaches, with more than 85% of code-smells fixed and 86% of suggested refactorings semantically coherent when the change history is used.","Search-based software engineering, Refactoring, Code-smells",,,,
Journal Article,"Almuairfi S,Alenezi M",,Security controls in infrastructure as code,Computer Fraud & Security,2020,2020.0,10.0,13-19,,,2020,,1361-3723,https://www.sciencedirect.com/science/article/pii/S1361372320301093;http://dx.doi.org/10.1016/S1361-3723(20)30109-3,10.1016/S1361-3723(20)30109-3,"The development, deployment and management of software applications have shifted dramatically in the past 10 years. This fundamental shift is what we now know as development operations (DevOps). Infrastructure as Code (IaC) is one of the main tenets of DevOps. Previously, manual configuration via cloud providers’ UI consoles and physical hardware used to take place. But now, with the concept of IaC, the IT infrastructure can be automated by using blueprints that are easily readable by machines.",,,,,
Journal Article,"Nam J,Wang S,Xi Y,Tan L",,A bug finder refined by a large set of open-source projects,Information and Software Technology,2019,112.0,,164-175,,,2019,,0950-5849,https://www.sciencedirect.com/science/article/pii/S0950584919300977;http://dx.doi.org/10.1016/j.infsof.2019.04.014,10.1016/j.infsof.2019.04.014,"Context Static bug detection techniques are commonly used to automatically detect software bugs. The biggest obstacle to the wider adoption of static bug detection tools is false positives, i.e., reported bugs that developers do not have to act on. Objective The objective of this study is to reduce false positives resulting from static bug detection tools and to detect new bugs by exploring the effectiveness of a feedback-based bug detection rule design. Method We explored a large number of software projects and applied an iterative feedback-based process to design bug detection rules. The outcome of the process is a set of ten bug detection rules, which we used to build a feedback-based bug finder, FeeFin. Specifically, we manually examined 1622 patches to identify bugs and fix patterns, and implement bug detection rules. Then, we refined the rules by repeatedly using feedback from a large number of software projects. Results We applied FeeFin to the latest versions of the 1880 projects on GitHub to detect previously unknown bugs. FeeFin detected 98 new bugs, 63 of which have been reviewed by developers: 57 were confirmed as true bugs, and 9 were confirmed as false positives. In addition, we investigated the benefits of our FeeFin process in terms of new and improved bug patterns. We verified our bug patterns with four existing tools, namely PMD, FindBugs, Facebook Infer, and Google Error Prone, and found that our FeeFin process has the potential to identify new bug patterns and also to improve existing bug patterns. Conclusion Based on the results, we suggest that static bug detection tool designers identify new bug patterns by mining real-world patches from a large number of software projects. In addition, the FeeFin process is helpful in mitigating false positives generated from existing tools by refining their bug detection rules.","Static bug finder, bug detection rules, bug patterns",,,,
Journal Article,"Jaafar F,Lozano A,Guéhéneuc YG,Mens K",,Analyzing software evolution and quality by extracting Asynchrony change patterns,Journal of Systems and Software,2017,131.0,,311-322,,,2017,,0164-1212,https://www.sciencedirect.com/science/article/pii/S0164121217300948;http://dx.doi.org/10.1016/j.jss.2017.05.047,10.1016/j.jss.2017.05.047,"Change patterns describe two or more files were often changed together during the development or the maintenance of software systems. Several studies have been presented to detect change patterns and to analyze their types and their impact on software quality. In this context, we introduced the Asynchrony change pattern to describes a set of files that always change together in the same change periods, regardless developers who maintained them. In this paper, we investigate the impact of Asynchrony change pattern on design and code smells such as anti-patterns and code clones. Concretely, we conduct an empirical study by detecting Asynchrony change patterns, anti-patterns and code clones occurrences on 22 versions of four software systems and analyzing their fault-proneness. Results show that cloned files that follow the same Asynchrony change patterns have significantly increased fault-proneness with respect to other clones, and that anti-patterns following the same Asynchrony change pattern can be up to five times more risky in terms of fault-proneness as compared to other anti-patterns. Asynchrony change patterns thus seem to be strong indicators of fault-proneness for clones and anti-patterns.","Change patterns, Anti-patterns, Clones, Fault-proneness, Software quality",,,,
Journal Article,"Valdivia-Garcia H,Shihab E,Nagappan M",,Characterizing and predicting blocking bugs in open source projects,Journal of Systems and Software,2018,143.0,,44-58,,,2018,,0164-1212,https://www.sciencedirect.com/science/article/pii/S0164121218300530;http://dx.doi.org/10.1016/j.jss.2018.03.053,10.1016/j.jss.2018.03.053,"Software engineering researchers have studied specific types of issues such reopened bugs, performance bugs, dormant bugs, etc. However, one special type of severe bugs is blocking bugs. Blocking bugs are software bugs that prevent other bugs from being fixed. These bugs may increase maintenance costs, reduce overall quality and delay the release of the software systems. In this paper, we study blocking bugs in eight open source projects and propose a model to predict them early on. We extract 14 different factors (from the bug repositories) that are made available within 24 hours after the initial submission of the bug reports. Then, we build decision trees to predict whether a bug will be a blocking bugs or not. Our results show that our prediction models achieve F-measures of 21%–54%, which is a two-fold improvement over the baseline predictors. We also analyze the fixes of these blocking bugs to understand their negative impact. We find that fixing blocking bugs requires more lines of code to be touched compared to non-blocking bugs. In addition, our file-level analysis shows that files affected by blocking bugs are more negatively impacted in terms of cohesion, coupling complexity and size than files affected by non-blocking bugs.","Process metrics, Code metrics, Post-release defects",,,,
Journal Article,"Tappolet J,Kiefer C,Bernstein A",,Semantic web enabled software analysis,"Web Semantics: Science, Services and Agents on the World Wide Web",2010,8.0,2.0,225-240,,,2010,,1570-8268,https://www.sciencedirect.com/science/article/pii/S1570826810000338;http://dx.doi.org/10.1016/j.websem.2010.04.009,10.1016/j.websem.2010.04.009,"One of the most important decisions researchers face when analyzing software systems is the choice of a proper data analysis/exchange format. In this paper, we present EvoOnt, a set of software ontologies and data exchange formats based on OWL. EvoOnt models software design, release history information, and bug-tracking meta-data. Since OWL describes the semantics of the data, EvoOnt (1) is easily extendible, (2) can be processed with many existing tools, and (3) allows to derive assertions through its inherent Description Logic reasoning capabilities. The contribution of this paper is that it introduces a novel software evolution ontology that vastly simplifies typical software evolution analysis tasks. In detail, we show the usefulness of EvoOnt by repeating selected software evolution and analysis experiments from the 2004–2007 Mining Software Repositories Workshops (MSR). We demonstrate that if the data used for analysis were available in EvoOnt then the analyses in 75% of the papers at MSR could be reduced to one or at most two simple queries within off-the-shelf SPARQL tools. In addition, we present how the inherent capabilities of the Semantic Web have the potential of enabling new tasks that have not yet been addressed by software evolution researchers, e.g., due to the complexities of the data integration.","Software comprehension framework, Software release similarity, Bug prediction, Software evolution",Bridging the Gap—Data Mining and Social Network Analysis for Integrating Semantic Web and Web 2.0 The Future of Knowledge Dissemination: The Elsevier Grand Challenge for the Life Sciences,,,
Journal Article,"Lacerda G,Petrillo F,Pimenta M,Guéhéneuc YG",,Code smells and refactoring: A tertiary systematic review of challenges and observations,Journal of Systems and Software,2020,167.0,,110610,,,2020,,0164-1212,https://www.sciencedirect.com/science/article/pii/S0164121220300881;http://dx.doi.org/10.1016/j.jss.2020.110610,10.1016/j.jss.2020.110610,"Refactoring and smells have been well researched by the software-engineering research community these past decades. Several secondary studies have been published on code smells, discussing their implications on software quality, their impact on maintenance and evolution, and existing tools for their detection. Other secondary studies addressed refactoring, discussing refactoring techniques, opportunities for refactoring, impact on quality, and tools support. In this paper, we present a tertiary systematic literature review of previous surveys, secondary systematic literature reviews, and systematic mappings. We identify the main observations (what we know) and challenges (what we do not know) on code smells and refactoring. We perform this tertiary review using eight scientific databases, based on a set of five research questions, identifying 40 secondary studies between 1992 and 2018. We organize the main observations and challenges about code smell and their refactoring into: smells definitions, most common code-smell detection approaches, code-smell detection tools, most common refactoring, and refactoring tools. We show that code smells and refactoring have a strong relationship with quality attributes, i.e., with understandability, maintainability, testability, complexity, functionality, and reusability. We argue that code smells and refactoring could be considered as the two faces of a same coin. Besides, we identify how refactoring affects quality attributes, more than code smells. We also discuss the implications of this work for practitioners, researchers, and instructors. We identify 13 open issues that could guide future research work. Thus, we want to highlight the gap between code smells and refactoring in the current state of software-engineering research. We wish that this work could help the software-engineering research community in collaborating on future work on code smells and refactoring.","Code smells, Refactoring, Tertiary systematic review",,,,
Journal Article,"Marcilio D,Furia CA,Bonifácio R,Pinto G",,SpongeBugs: Automatically generating fix suggestions in response to static code analysis warnings,Journal of Systems and Software,2020,168.0,,110671,,,2020,,0164-1212,https://www.sciencedirect.com/science/article/pii/S016412122030128X;http://dx.doi.org/10.1016/j.jss.2020.110671,10.1016/j.jss.2020.110671,"Static code analysis tools such as FindBugs and SonarQube are widely used on open-source and industrial projects to detect a variety of issues that may negatively affect the quality of software. Despite these tools’ popularity and high level of automation, several empirical studies report that developers normally fix only a small fraction (typically, less than 10% (Marcilio et al., 2019) of the reported issues—so-called “warnings”. If these analysis tools could also automatically provide suggestions on how to fix the issues that trigger some of the warnings, their feedback would become more actionable and more directly useful to developers. In this work, we investigate whether it is feasible to automatically generate fix suggestions for common warnings issued by static code analysis tools, and to what extent developers are willing to accept such suggestions into the codebases they are maintaining. To this end, we implemented SpongeBugs, a Java program transformation technique that fixes 11 distinct rules checked by two well-known static code analysis tools (SonarQube and SpotBugs). Fix suggestions are generated automatically based on templates, which are instantiated in a way that removes the source of the warnings; templates for some rules are even capable of producing multi-line patches. Based on the suggestions provided by SpongeBugs, we submitted 38 pull requests, including 946 fixes generated automatically by our technique for various open-source Java projects, including Eclipse UI – a core component of the Eclipse IDE – and both SonarQube and SpotBugs. Project maintainers accepted 87% of our fix suggestions (97% of them without any modifications). We further evaluated the applicability of our technique on software written by students and on a curated collection of bugs. All results indicate that our approach to generating fix suggestions is feasible, flexible, and can help increase the applicability of static code analysis tools.","Static code analysis, Automatic fix suggestion",,,,
Journal Article,"Bavota G,De Lucia A,Di Penta M,Oliveto R,Palomba F",,An experimental investigation on the innate relationship between quality and refactoring,Journal of Systems and Software,2015,107.0,,1-14,,,2015,,0164-1212,https://www.sciencedirect.com/science/article/pii/S0164121215001053;http://dx.doi.org/10.1016/j.jss.2015.05.024,10.1016/j.jss.2015.05.024,"Previous studies have investigated the reasons behind refactoring operations performed by developers, and proposed methods and tools to recommend refactorings based on quality metric profiles, or on the presence of poor design and implementation choices, i.e., code smells. Nevertheless, the existing literature lacks observations about the relations between metrics/code smells and refactoring activities performed by developers. In other words, the characteristics of code components increasing/decreasing their chances of being object of refactoring operations are still unknown. This paper aims at bridging this gap. Specifically, we mined the evolution history of three Java open source projects to investigate whether refactoring activities occur on code components for which certain indicators—such as quality metrics or the presence of smells as detected by tools—suggest there might be need for refactoring operations. Results indicate that, more often than not, quality metrics do not show a clear relationship with refactoring. In other words, refactoring operations are generally focused on code components for which quality metrics do not suggest there might be need for refactoring operations. Finally, 42% of refactoring operations are performed on code entities affected by code smells. However, only 7% of the performed operations actually remove the code smells from the affected class.","Refactoring, Code smells, Empirical study",,,,
Journal Article,"Lenarduzzi V,Saarimäki N,Taibi D",,Some SonarQube issues have a significant but small effect on faults and changes. A large-scale empirical study,Journal of Systems and Software,2020,170.0,,110750,,,2020,,0164-1212,https://www.sciencedirect.com/science/article/pii/S0164121220301734;http://dx.doi.org/10.1016/j.jss.2020.110750,10.1016/j.jss.2020.110750,"Context: Companies frequently invest effort to remove technical issues believed to impact software qualities, such as removing anti-patterns or coding styles violations. Objective: We aim to analyze the diffuseness of SonarQube issues in software systems and to assess their impact on code changes and fault-proneness, considering also their different types and severities. Methods: We conducted a case study among 33 Java projects from the Apache Software Foundation repository. Results: We analyzed 726 commits containing 27K faults and 12M changes in Java files. The projects violated 173 SonarQube rules generating more than 95K SonarQube issues in more than 200K classes. Classes not affected by SonarQube issues are less change-prone than affected ones, but the difference between the groups is small. Non-affected classes are slightly more change-prone than classes affected by SonarQube issues of type Code Smell or Security Vulnerability. As for fault-proneness, there is no difference between non-affected and affected classes. Moreover, we found incongruities in the type and severity assigned by SonarQube. Conclusion: Our result can be useful for practitioners to understand which SonarQube issues should be refactored and for researchers to bridge the missing gaps. Moreover, results can also support companies and tool vendors in identifying SonarQube issues as accurately as possible.","Change-proneness, Fault-proneness, SonarQube, Empirical study",,,,
Journal Article,"Terra R,Valente MT,Miranda S,Sales V",,JMove: A novel heuristic and tool to detect move method refactoring opportunities,Journal of Systems and Software,2018,138.0,,19-36,,,2018,,0164-1212,https://www.sciencedirect.com/science/article/pii/S0164121217302960;http://dx.doi.org/10.1016/j.jss.2017.11.073,10.1016/j.jss.2017.11.073,"This paper presents a recommendation approach that suggests Move Method refactorings using the static dependencies established by methods. This approach, implemented in a publicly available tool called JMove, compares the similarity of the dependencies established by a method with the dependencies established by the methods in possible target classes. We first evaluate JMove using 195 Move Method refactoring opportunities, synthesized in 10 open-source systems. In this evaluation, JMove precision ranges from 21% (small methods) to 32% (large methods) and its median recall ranges from 21% (small methods) to 60% (large methods). In the same scenario, JDeodorant, which is a state-of-the-art Move Method recommender, has a maximal precision of 15% (large methods) and a maximal median recall of 40% (small methods). Therefore, we claim that JMove is specially useful to provide recommendations for large methods. We reinforce this claim by means of two other studies. First, by investigating the overlapping of the recommendations provided by JMove and three other recommenders (JDeodorant, inCode, and Methodbook). Second, by validating JMove and JDeodorant recommendations with experts in two industrial-strength systems.","Move method refactorings, Recommendation systems, Dependency sets, JMove, JDeodorant, Methodbook",,,,
Journal Article,"Nuñez-Varela AS,Pérez-Gonzalez HG,Martínez-Perez FE,Soubervielle-Montalvo C",,Source code metrics: A systematic mapping study,Journal of Systems and Software,2017,128.0,,164-197,,,2017,,0164-1212,https://www.sciencedirect.com/science/article/pii/S0164121217300663;http://dx.doi.org/10.1016/j.jss.2017.03.044,10.1016/j.jss.2017.03.044,"Context Source code metrics are essential components in the software measurement process. They are extracted from the source code of the software, and their values allow us to reach conclusions about the quality attributes measured by the metrics. Objectives This paper aims to collect source code metrics related studies, review them, and perform an analysis, while providing an overview on the current state of source code metrics and their current trends. Method A systematic mapping study was conducted. A total of 226 studies, published between the years 2010 and 2015, were selected and analyzed. Results Almost 300 source code metrics were found. Object oriented programming is the most commonly studied paradigm with the Chidamber and Kemerer metrics, lines of code, McCabe's cyclomatic complexity, and number of methods and attributes being the most used metrics. Research on aspect and feature oriented programming is growing, especially for the current interest in programming concerns and software product lines. Conclusions Object oriented metrics have gained much attention, but there is a current need for more studies on aspect and feature oriented metrics. Software fault prediction, complexity and quality assessment are recurrent topics, while concerns, big scale software and software product lines represent current trends.","Source code metrics, Software metrics, Object-oriented metrics, Aspect-oriented metrics, Feature-oriented metrics, Systematic mapping study",,,,
Journal Article,"Walter B,Alkhaeir T",,The relationship between design patterns and code smells: An exploratory study,Information and Software Technology,2016,74.0,,127-142,,,2016,,0950-5849,https://www.sciencedirect.com/science/article/pii/S0950584916300210;http://dx.doi.org/10.1016/j.infsof.2016.02.003,10.1016/j.infsof.2016.02.003,"Context—Design patterns represent recommended generic solutions to various design problems, whereas code smells are symptoms of design issues that could hinder further maintenance of a software system. We can intuitively expect that both concepts are mutually exclusive, and the presence of patterns is correlated with the absence of code smells. However, the existing experimental evidence supporting this claim is still insufficient, and studies separately analyzing the impact of smells and patterns on code quality deliver diverse results. Objective—The aim of the paper is threefold: (1) to determine if and how the presence of the design patterns is linked to the presence of code smells, (2) to investigate if and how these relationships change throughout evolution of code, and (3) to identify the relationships between individual patterns and code smells. Method—We analyze nine design patterns and seven code smells in two medium-size, long-evolving, open source Java systems. In particular, we explore how the presence of design patterns impacts the presence of code smells, analyze if this link evolves over time, and extract association rules that describe their individual relationships. Results—Classes participating in design patterns appear to display code smells less frequently than other classes. The observed effect is stronger for some patterns (e.g., Singleton, State-Strategy) and weaker for others (e.g., Composite). The ratio between the relative number of smells in the classes participating in patterns and the relative number of smells in other classes, is approximately stable or slightly decreasing in time. Conclusion—This observation could be used to anticipate the smell-proneness of individual classes, and improve code smell detectors. Overall, our findings indicate that the presence of design patterns is linked with a lower number of code smell instances. This could support programmers in a context-sensitive analysis of smells in code.","Design patterns, Code smells, Software evolution, Empirical study",,,,
Journal Article,"Fokaefs M,Tsantalis N,Stroulia E,Chatzigeorgiou A",,Identification and application of Extract Class refactorings in object-oriented systems,Journal of Systems and Software,2012,85.0,10.0,2241-2260,,,2012,,0164-1212,https://www.sciencedirect.com/science/article/pii/S0164121212001057;http://dx.doi.org/10.1016/j.jss.2012.04.013,10.1016/j.jss.2012.04.013,"Refactoring is recognized as an essential practice in the context of evolutionary and agile software development. Recognizing the importance of the practice, modern IDEs provide some support for low-level refactorings. A notable exception in the list of supported refactorings is the “Extract Class” refactoring, which is conceived to simplify large, complex, unwieldy and less cohesive classes. In this work, we describe a method and a tool, implemented as an Eclipse plugin, designed to fulfill exactly this need. Our method involves three steps: (a) recognition of Extract Class opportunities, (b) ranking of the identified opportunities in terms of the improvement each one is anticipated to bring about to the system design, and (c) fully automated application of the refactoring chosen by the developer. The first step relies on an agglomerative clustering algorithm, which identifies cohesive sets of class members within the system classes. The second step relies on the Entity Placement metric as a measure of design quality. Through a set of experiments we have shown that the tool is able to identify and extract new classes that developers recognize as “coherent concepts” and improve the design quality of the underlying system.","Refactoring, Software reengineering, Object-oriented programming, Clustering",Automated Software Evolution,,,
Journal Article,"Dalla Palma S,Di Nucci D,Palomba F,Tamburri DA",,Toward a catalog of software quality metrics for infrastructure code,Journal of Systems and Software,2020,170.0,,110726,,,2020,,0164-1212,https://www.sciencedirect.com/science/article/pii/S0164121220301618;http://dx.doi.org/10.1016/j.jss.2020.110726,10.1016/j.jss.2020.110726,"Infrastructure-as-code (IaC) is a practice to implement continuous deployment by allowing management and provisioning of infrastructure through the definition of machine-readable files and automation around them, rather than physical hardware configuration or interactive configuration tools. On the one hand, although IaC represents an ever-increasing widely adopted practice nowadays, still little is known concerning how to best maintain, speedily evolve, and continuously improve the code behind the IaC practice in a measurable fashion. On the other hand, source code measurements are often computed and analyzed to evaluate the different quality aspects of the software developed. However, unlike general-purpose programming languages (GPLs), IaC scripts use domain-specific languages, and metrics used for GPLs may not be applicable for IaC scripts. This article proposes a catalog consisting of 46 metrics to identify IaC properties focusing on Ansible, one of the most popular IaC language to date, and shows how they can be used to analyze IaC scripts.","Infrastructure as code, Software metrics, Software quality",,,,
Journal Article,"Wang Y,Wang Y",,Citrus ontology development based on the eight-point charter of agriculture,Computers and Electronics in Agriculture,2018,155.0,,359-370,,,2018,,0168-1699,https://www.sciencedirect.com/science/article/pii/S0168169917312012;http://dx.doi.org/10.1016/j.compag.2018.10.034,10.1016/j.compag.2018.10.034,"Developing large-scale agricultural ontologies is a challenging and error-prone task that requires substantial effort and collaboration between domain experts and ontology developers due to the complexity of agricultural knowledge. Inspired by the Chinese Eight-Point Charter of Agriculture, i.e., Soil, Fertilization, Water, Variety, Density, Protection, Management and Tool, this paper presents an approach to modeling and integrating citrus production knowledge. Citrus domain knowledge is classified into eight categories based on the Eight-Point Charter of Agriculture, and the relationships in each category and among categories are established. The eight categories and the relationships are defined as the citrus production knowledge framework. Then, we propose mechanisms to develop citrus ontology based on the citrus production knowledge framework. The Fertilization ontology is created as an illustration of our approach, which contains 866 ontology entities and 12,583 Resource Description Framework triples. The structural evaluation results of the eight metrics for the Fertilization ontology are considerably better than the average and median values of 1413 Web ontologies. In addition, four antipatterns were used to evaluate the ontology, and no occurrence of the antipatterns was detected for the 866 ontology entities. The accuracy of the ontology is ensured by the competency evaluation of the 110 questions with 88% accuracy. Our approach provides an effective solution for modeling complex agricultural knowledge and transforming the agriculture domain knowledge into computable resources.","Ontology, Knowledge modeling, Semantic web, Agriculture",,,,
Journal Article,"Morales R,Soh Z,Khomh F,Antoniol G,Chicano F",,On the use of developers’ context for automatic refactoring of software anti-patterns,Journal of Systems and Software,2017,128.0,,236-251,,,2017,,0164-1212,https://www.sciencedirect.com/science/article/pii/S0164121216300632;http://dx.doi.org/10.1016/j.jss.2016.05.042,10.1016/j.jss.2016.05.042,"Anti-patterns are poor solutions to design problems that make software systems hard to understand and extend. Entities involved in anti-patterns are reported to be consistently related to high change and fault rates. Refactorings, which are behavior preserving changes are often performed to remove anti-patterns from software systems. Developers are advised to interleave refactoring activities with their regular coding tasks to remove anti-patterns, and consequently improve software design quality. However, because the number of anti-patterns in a software system can be very large, and their interactions can require a solution in a set of conflicting objectives, the process of manual refactoring can be overwhelming. To automate this process, previous works have modeled anti-patterns refactoring as a batch process where a program provides a solution for the total number of classes in a system, and the developer has to examine a long list of refactorings, which is not feasible in most situations. Moreover, these proposed solutions often require that developers modify classes on which they never worked before (i.e., classes on which they have little or no knowledge). To improve on these limitations, this paper proposes an automated refactoring approach, ReCon (Refactoring approach based on task Context), that leverages information about a developer’s task (i.e., the list of code entities relevant to the developer’s task) and metaheuristics techniques to compute the best sequence of refactorings that affects only entities in the developer’s context. We mine 1705 task contexts (collected using the Eclipse plug-in Mylyn) and 1013 code snapshots from three open-source software projects (Mylyn, PDE, Eclipse Platform) to assess the performance of our proposed approach. Results show that ReCon can remove more than 50% of anti-patterns in a software system, using fewer resources than the traditional approaches from the literature.","Software maintenance, Automatic refactoring, Task context, Interaction traces, Anti-patterns, Metaheuristics",,,,
Journal Article,"Hamill M,Goseva-Popstojanova K",,Analyzing and predicting effort associated with finding and fixing software faults,Information and Software Technology,2017,87.0,,1-18,,,2017,,0950-5849,https://www.sciencedirect.com/science/article/pii/S0950584917300290;http://dx.doi.org/10.1016/j.infsof.2017.01.002,10.1016/j.infsof.2017.01.002,"Context: Software developers spend a significant amount of time fixing faults. However, not many papers have addressed the actual effort needed to fix software faults. Objective: The objective of this paper is twofold: (1) analysis of the effort needed to fix software faults and how it was affected by several factors and (2) prediction of the level of fix implementation effort based on the information provided in software change requests. Method: The work is based on data related to 1200 failures, extracted from the change tracking system of a large NASA mission. The analysis includes descriptive and inferential statistics. Predictions are made using three supervised machine learning algorithms and three sampling techniques aimed at addressing the imbalanced data problem. Results: Our results show that (1) 83% of the total fix implementation effort was associated with only 20% of failures. (2) Both post-release failures and safety-critical failures required more effort to fix than pre-release and non-critical counterparts, respectively; median values were two or more times higher. (3) Failures with fixes spread across multiple components or across multiple types of software artifacts required more effort. The spread across artifacts was more costly than spread across components. (4) Surprisingly, some types of faults associated with later life-cycle activities did not require significant effort. (5) The level of fix implementation effort was predicted with 73% overall accuracy using the original, imbalanced data. Oversampling techniques improved the overall accuracy up to 77% and, more importantly, significantly improved the prediction of the high level effort, from 31% to 85%. Conclusions: This paper shows the importance of tying software failures to changes made to fix all associated faults, in one or more software components and/or in one or more software artifacts, and the benefit of studying how the spread of faults and other factors affect the fix implementation effort.","Software faults and failures, Software fix implementation effort, Case study, Analysis, Prediction",,,,
Journal Article,"Singh S,Kaur S",,A systematic literature review: Refactoring for disclosing code smells in object oriented software,Ain Shams Engineering Journal,2018,9.0,4.0,2129-2151,,,2018,,2090-4479,https://www.sciencedirect.com/science/article/pii/S2090447917300412;http://dx.doi.org/10.1016/j.asej.2017.03.002,10.1016/j.asej.2017.03.002,"Context Reusing a design pattern is not always in the favor of developers. Thus, the code starts smelling. The presence of “Code Smells” leads to more difficulties for the developers. This racket of code smells is sometimes called Anti-Patterns. Objective The paper aimed at a systematic literature review of refactoring with respect to code smells. However the review of refactoring is done in general and the identification of code smells and anti-patterns is performed in depth. Method A systematic literature survey has been performed on 238 research items that includes articles from leading Conferences, Workshops and premier journals, theses of researchers and book chapters. Results Several data sets and tools for performing refactoring have been revealed under the specified research questions. Conclusion The work done in the paper is an addition to prior systematic literature surveys. With the study of paper the attentiveness of readers about code smells and anti-patterns will be enhanced.","Code smells, Anti-patterns, Refactoring",,,,
Journal Article,"Hönel S,Ericsson M,Löwe W,Wingkvist A",,Using source code density to improve the accuracy of automatic commit classification into maintenance activities,Journal of Systems and Software,2020,168.0,,110673,,,2020,,0164-1212,https://www.sciencedirect.com/science/article/pii/S0164121220301291;http://dx.doi.org/10.1016/j.jss.2020.110673,10.1016/j.jss.2020.110673,"Source code is changed for a reason, e.g., to adapt, correct, or adapt it. This reason can provide valuable insight into the development process but is rarely explicitly documented when the change is committed to a source code repository. Automatic commit classification uses features extracted from commits to estimate this reason. We introduce source code density, a measure of the net size of a commit, and show how it improves the accuracy of automatic commit classification compared to previous size-based classifications. We also investigate how preceding generations of commits affect the class of a commit, and whether taking the code density of previous commits into account can improve the accuracy further. We achieve up to 89% accuracy and a Kappa of 0.82 for the cross-project commit classification where the model is trained on one project and applied to other projects. Models trained on single projects yield accuracies of up to 93% with a Kappa approaching 0.90. The accuracy of the automatic commit classification has a direct impact on software (process) quality analyses that exploit the classification, so our improvements to the accuracy will also improve the confidence in such analyses.","Software quality, Commit classification, Source code density, Maintenance activities, Software evolution",,,,
Journal Article,"Tan Y,Xu S,Wang Z,Zhang T,Xu Z,Luo X",,Bug severity prediction using question-and-answer pairs from Stack Overflow,Journal of Systems and Software,2020,165.0,,110567,,,2020,,0164-1212,https://www.sciencedirect.com/science/article/pii/S0164121220300480;http://dx.doi.org/10.1016/j.jss.2020.110567,10.1016/j.jss.2020.110567,"Nowadays, bugs have been common in most software systems. For large-scale software projects, developers usually conduct software maintenance tasks by utilizing software artifacts (e.g., bug reports). The severity of bug reports describes the impact of the bugs and determines how quickly it needs to be fixed. Bug triagers often pay close attention to some features such as severity to determine the importance of bug reports and assign them to the correct developers. However, a large number of bug reports submitted every day increase the workload of developers who have to spend more time on fixing bugs. In this paper, we collect question-and-answer pairs from Stack Overflow and use logical regression to predict the severity of bug reports. In detail, we extract all the posts related to bug repositories from Stack Overflow and combine them with bug reports to obtain enhanced versions of bug reports. We achieve severity prediction on three popular open source projects (e,g., Mozilla, Ecplise, and GCC) with Naïve Bayesian, k-Nearest Neighbor algorithm (KNN), and Long Short-Term Memory (LSTM). The results of our experiments show that our model is more accurate than the previous studies for predicting the severity. Our approach improves by 23.03%, 21.86%, and 20.59% of the average F-measure for Mozilla, Eclipse, and GCC by comparing with the Naïve Bayesian based approach which performs the best among all baseline approaches.","Stack overflow, Severity prediction, Logistic regression, Bug reports",,,,
Journal Article,"Ampatzoglou A,Mittas N,Tsintzira AA,Ampatzoglou A,Arvanitou EM,Chatzigeorgiou A,Avgeriou P,Angelis L",,Exploring the Relation between Technical Debt Principal and Interest: An Empirical Approach,Information and Software Technology,2020,128.0,,106391,,,2020,,0950-5849,https://www.sciencedirect.com/science/article/pii/S0950584920301567;http://dx.doi.org/10.1016/j.infsof.2020.106391,10.1016/j.infsof.2020.106391,"Context The cornerstones of technical debt (TD) are two concepts borrowed from economics: principal and interest. Although in economics the two terms are related, in TD there is no study on this direction so as to validate the strength of the metaphor. Objective We study the relation between Principal and Interest, and subsequently dig further into the ‘ingredients’ of each concept (since they are multi-faceted). In particular, we investigate if artifacts with similar levels of TD Principal exhibit a similar amount of TD Interest, and vice-versa. Method To achieve this goal, we performed an empirical study, analyzing the dataset using the Mantel test. Through the Mantel test, we examined the relation between TD Principal and Interest, and identified aspects that are able to denote proximity of artifacts, with respect to TD. Next, through Linear Mixed Effects (LME) modelling we studied the generalizability of the results. Results The results of the study suggest that TD Principal and Interest are related, in the sense that classes with similar levels of TD Principal tend to have similar levels of Interest. Additionally, we have reached the conclusion that aggregated measures of TD Principal or Interest are more capable of identifying proximate artifacts, compared to isolated metrics. Finally, we have provided empirical evidence on the fact that improving certain quality properties (e.g., size and coupling) should be prioritized while ranking refactoring opportunities in the sense that high values of these properties are in most of the cases related to artifacts with higher levels of TD Principal. Conclusions The findings shed light on the relations between the two concepts, and can be useful for both researchers and practitioners: the former can get a deeper understanding of the concepts, whereas the latter can use our findings to guide their TD management processes such as prioritization and repayment.",,,,,
Book Chapter,"Ouni A,Kessentini M,Sahraoui H",Hurson A,Chapter Four - Multiobjective Optimization for Software Refactoring and Evolution,,2014,94.0,,103-167,Elsevier,,2014,,0065-2458,https://www.sciencedirect.com/science/article/pii/B9780128001615000049;http://dx.doi.org/10.1016/B978-0-12-800161-5.00004-9,10.1016/B978-0-12-800161-5.00004-9,"Many studies reported that software maintenance, traditionally defined as any modification made on a software system after its delivery, consumes up to 90% of the total cost of a typical software project. Adding new functionalities, correcting bugs, and modifying the code to improve its quality are major parts of those costs. To ease these maintenance activities, one of the most used techniques is the refactoring which improves design structure while preserving the external behavior. In general, refactoring is performed through two main steps: (1) detection of code fragments corresponding to design defects that need to be improved/fixed and (2) identification of refactoring solutions to achieve this goal. Our research project targets the automation of these two refactoring steps. Concretely, we consider the detection step as a search-based process to find the suitable detection rules for each type of design defect, by means of a genetic algorithm. To guide the rule-derivation process, real examples of design defects are used. For the refactoring identification step, a multiobjective search-based approach is also used. The process aims at finding the optimal sequence of refactoring operations that improve the software quality by minimizing the number of detected defects. In addition, we explore other objectives to optimize: the effort needed to apply refactorings, semantic preservation, and the similarity with good refactorings applied in the past to similar contexts. Hence, the effort corresponds to the code modification/adaptation score needed to apply the suggested refactoring solutions. On the other hand, the semantic preservation insures that the refactored program is semantically equivalent to the original one, and that it models correctly the domain semantics. Indeed, we use knowledge from historical code changes to propose new refactoring solutions in similar contexts to improve the automation of refactoring.",,,Advances in Computers,,
Journal Article,"Bafandeh Mayvan B,Rasoolzadegan A,Ebrahimi AM",,A new benchmark for evaluating pattern mining methods based on the automatic generation of testbeds,Information and Software Technology,2019,109.0,,60-79,,,2019,,0950-5849,https://www.sciencedirect.com/science/article/pii/S0950584919300102;http://dx.doi.org/10.1016/j.infsof.2019.01.007,10.1016/j.infsof.2019.01.007,"Context Mining patterns is one of the most attractive topics in the field of software design. Knowledge about the number, type, and location of pattern instances is crucial to understand the original design decisions. Several techniques and tools have been presented in the literature for mining patterns in a software system. However, evaluating the quality of the detection results is usually done manually or subjectively. This can significantly affect the evaluation results. Therefore, a fair comparison of the quality of the various mining methods is not possible. Objective This paper describes a new benchmark to evaluate pattern mining methods in source code or design. Our work aims at overcoming the challenges faced in benchmarking in pattern detection. The proposed benchmark is comprehensive, fair, and objective, with a repeatable evaluation process. Method Our proposed benchmark is based on automatic generation of testbeds using graph theory. The generated testbeds are Java source codes and their corresponding class diagrams in which various types of patterns and their variants are inserted in different locations. The generated testbeds differ in their levels of complexity and full information is available on the utilized patterns. Results The results show that our proposed benchmark is able to evaluate the pattern mining methods quantitatively and objectively. Also, it can be used to compare pattern mining methods in a fair and repeatable manner. Conclusions Based on our findings, it can be argued that benchmarking in the pattern mining field is significantly less mature than topics such as presenting a new detection method. Therefore, special attention is needed in the pattern evaluation topic. Our proposed benchmark is a step towards achieving a comparative understanding of the effectiveness of detection methods and demonstrating their strengths and weaknesses.","Benchmarking, Pattern mining, Design pattern detection, Automatic code generation",,,,
Journal Article,"Vassallo C,Grano G,Palomba F,Gall HC,Bacchelli A",,A large-scale empirical exploration on refactoring activities in open source software projects,Science of Computer Programming,2019,180.0,,1-15,,,2019,,0167-6423,https://www.sciencedirect.com/science/article/pii/S0167642318302557;http://dx.doi.org/10.1016/j.scico.2019.05.002,10.1016/j.scico.2019.05.002,"Refactoring is a well-established practice that aims at improving the internal structure of a software system without changing its external behavior. Existing literature provides evidence of how and why developers perform refactoring in practice. In this paper, we continue on this line of research by performing a large-scale empirical analysis of refactoring practices in 200 open source systems. Specifically, we analyze the change history of these systems at commit level to investigate: (i) whether developers perform refactoring operations and, if so, which are more diffused and (ii) when refactoring operations are applied, and (iii) which are the main developer-oriented factors leading to refactoring. Based on our results, future research can focus on enabling automatic support for less frequent refactorings and on recommending refactorings based on the developer's workload, project's maturity and developer's commitment to the project.","Refactoring, Software Evolution, Software Maintenance",,,,
Journal Article,"Christopoulou A,Giakoumakis EA,Zafeiris VE,Soukara V",,Automated refactoring to the Strategy design pattern,Information and Software Technology,2012,54.0,11.0,1202-1214,,,2012,,0950-5849,https://www.sciencedirect.com/science/article/pii/S0950584912001024;http://dx.doi.org/10.1016/j.infsof.2012.05.004,10.1016/j.infsof.2012.05.004,"Context The automated identification of code fragments characterized by common design flaws (or “code smells”) that can be handled through refactoring, fosters refactoring activities, especially in large code bases where multiple developers are engaged without a detailed view on the whole system. Automated refactoring to design patterns enables significant contributions to design quality even from developers with little experience on the use of the required patterns. Objective This work targets the automated identification of refactoring opportunities to the Strategy design pattern and the elimination through polymorphism of respective “code smells” that are related to extensive use of complex conditional statements. Method An algorithm is introduced for the automated identification of refactoring opportunities to the Strategy design pattern. Suggested refactorings comprise conditional statements that are characterized by analogies to the Strategy design pattern, in terms of the purpose and selection mode of strategies. Moreover, this work specifies the procedure for refactoring to Strategy the identified conditional statements. For special cases of these statements, a technique is proposed for total replacement of conditional logic with method calls of appropriate concrete Strategy instances. The identification algorithm and the refactoring procedure are implemented and integrated in the JDeodorant Eclipse plug-in. The method is evaluated on a set of Java projects, in terms of quality of the suggested refactorings and run-time efficiency. The relevance of the identified refactoring opportunities is verified by expert software engineers. Results The identification algorithm recalled, from the projects used during evaluation, many of the refactoring candidates that were identified by the expert software engineers. Its execution time on projects of varying size confirmed the run-time efficiency of this method. Conclusion The proposed method for automated refactoring to Strategy contributes to simplification of conditional statements. Moreover, it enhances system extensibility through the Strategy design pattern.","Refactoring, Design patterns, Strategy pattern, Polymorphism, Total replacement of conditional logic",,,,
Journal Article,"Rebai S,Kessentini M,Wang H,Maxim B",,Web service design defects detection: A bi-level multi-objective approach,Information and Software Technology,2020,121.0,,106255,,,2020,,0950-5849,https://www.sciencedirect.com/science/article/pii/S0950584919302733;http://dx.doi.org/10.1016/j.infsof.2019.106255,10.1016/j.infsof.2019.106255,"Context: Web services frequently evolve to integrate new features, update existing operations and fix errors to meet the new requirements of subscribers. While this evolution is critical, it may have a negative impact on the quality of services (QoS) such as reduced cohesion, increased coupling, poor response time and availability, etc. Thus, the design of services could become hard to maintain and extend in future releases. Recent studies addressed the problem of web service design antipatterns detection, also called design defects, by either manually defining detection rules, as combination of quality metrics, or generating them automatically from a set of defect examples. The manual definition of these rules is time-consuming and difficult due to the subjective nature of design issues, especially to find the right thresholds value. The efficiency of the generated rules, using automated approaches, will depend on the quality of the training set since examples of web services antipatterns are limited. Furthermore, the majority of existing studies for design defects detection for web services are limited to structural information (interface/code static metrics) and they ignore the use of quality of services (QoS) or performance metrics, such as response time and availability, for this detection process or understanding the impact of antipatterns on these QoS attributes. Objective: To address these challenges, we designed a bi-level multi-objective optimization approach to enable the generation of antipattern examples that can improve the efficiency of detection rules. Method: The upper-level generates a set of detection rules as a combination of quality metrics with their threshold values maximizing the coverage of defect examples extracted from several existing web services and artificial ones generated by a lower level. The lower level maximizes the number of generated artificial defects that cannot be detected by the rules of the upper level and minimizes the similarity to well-designed web service examples. The generated detection rules, by our approach, are based on a combination of dynamic QoS attributes and structural information of web service (static interface/code metrics). Results: The statistical analysis of our results, based on a data-set of 662 web services, confirms the efficiency of our approach in detecting web service antipatterns comparing to the current state of the art in terms of precision and recall. Conclusion: The multi-objective search formulation at both levels helped to diversify the generated artificial web service defects which produced better quality of detection rules. Furthermore, the combination of dynamic QoS attributes and structural information of web services improved the efficiency of the generated detection rules.","Search based software engineering, Quality of services, Services design",,,,
Journal Article,"Kaur S,Singh P",,How does object-oriented code refactoring influence software quality? Research landscape and challenges,Journal of Systems and Software,2019,157.0,,110394,,,2019,,0164-1212,https://www.sciencedirect.com/science/article/pii/S0164121219301694;http://dx.doi.org/10.1016/j.jss.2019.110394,10.1016/j.jss.2019.110394,"Context Software refactoring aims to improve software quality and developer productivity. Numerous empirical studies investigating the impact of refactoring activities on software quality have been conducted over the last two decades. Objective This study aims to perform a comprehensive systematic mapping study of existing empirical studies on evaluation of the effect of object-oriented code refactoring activities on software quality attributes. Method We followed a multi-stage scrutinizing process to select 142 primary studies published till December 2017. The selected primary studies were further classified based on several aspects to answer the research questions defined for this work. In addition, we applied vote-counting approach to combine the empirical results and their analysis reported in primary studies. Results The findings indicate that studies conducted in academic settings found more positive impact of refactoring on software quality than studies performed in industries. In general, refactoring activities caused all quality attributes to improve or degrade except for cohesion, complexity, inheritance, fault-proneness and power consumption attributes. Furthermore, individual refactoring activities have variable effects on most quality attributes explored in primary studies, indicating that refactoring does not always improve all quality attributes. Conclusions This study points out several open issues which require further investigation, e.g., lack of industrial validation, lesser coverage of refactoring activities, limited tool support, etc.","Software quality, Object-oriented software, Refactoring activity, Quality measures, Systematic mapping study",,,,
Journal Article,"Settas D,Cerone A,Fenz S",,Enhancing ontology-based antipattern detection using Bayesian networks,Expert Systems with Applications,2012,39.0,10.0,9041-9053,,,2012,,0957-4174,https://www.sciencedirect.com/science/article/pii/S095741741200293X;http://dx.doi.org/10.1016/j.eswa.2012.02.049,10.1016/j.eswa.2012.02.049,"Antipatterns provide information on commonly occurring solutions to problems that generate negative consequences. The antipattern ontology has been recently proposed as a knowledge base for SPARSE, an intelligent system that can detect the antipatterns that exist in a software project. However, apart from the plethora of antipatterns that are inherently informal and imprecise, the information used in the antipattern ontology itself is many times imprecise or vaguely defined. For example, the certainty in which a cause, symptom or consequence of an antipattern exists in a software project. Taking into account probabilistic information would yield more realistic, intelligent and effective ontology-based applications to support the technology of antipatterns. However, ontologies are not capable of representing uncertainty and the effective detection of antipatterns taking into account the uncertainty that exists in software project antipatterns still remains an open issue. Bayesian Networks (BNs) have been previously used in order to measure, illustrate and handle antipattern uncertainty in mathematical terms. In this paper, we explore the ways in which the antipattern ontology can be enhanced using Bayesian networks in order to reinforce the existing ontology-based detection process. This approach allows software developers to quantify the existence of an antipattern using Bayesian networks, based on probabilistic knowledge contained in the antipattern ontology regarding relationships of antipatterns through their causes, symptoms and consequences. The framework is exemplified using a Bayesian network model of 13 antipattern attributes, which is constructed using BNTab, a plug-in developed for the Protege ontology editor that generates BNs based on ontological information.","Bayesian networks, Ontology, Antipatterns, Antipattern detection",,,,
Journal Article,Kreimer J,,Adaptive Detection of Design Flaws,Electronic Notes in Theoretical Computer Science,2005,141.0,4.0,117-136,,,2005,,1571-0661,https://www.sciencedirect.com/science/article/pii/S1571066105051844;http://dx.doi.org/10.1016/j.entcs.2005.02.059,10.1016/j.entcs.2005.02.059,"Criteria for software quality measurement depend on the application area. In large software systems criteria like maintainability, comprehensibility and extensibility play an important role. My aim is to identify design flaws in software systems automatically and thus to avoid “bad” — incomprehensible, hardly expandable and changeable — program structures. Depending on the perception and experience of the searching engineer, design flaws are interpreted in a different way. I propose to combine known methods for finding design flaws on the basis of metrics with machine learning mechanisms, such that design flaw detection is adaptable to different views. This paper presents the underlying method, describes an analysis tool for Java programs and shows results of an initial case study.","Design flaw, code smell, object-oriented design, software quality, refactoring, program analysis, machine learning","Proceedings of the Fifth Workshop on Language Descriptions, Tools, and Applications (LDTA 2005)",,,
Journal Article,"Gómez-Martínez E,Linaje M,Sánchez-Figueroa F,Iglesias-Pérez A,Preciado JC,González-Cabero R,Merseguer J",,A semantic approach for designing Assistive Software Recommender systems,Journal of Systems and Software,2015,104.0,,166-178,,,2015,,0164-1212,https://www.sciencedirect.com/science/article/pii/S0164121215000606;http://dx.doi.org/10.1016/j.jss.2015.03.009,10.1016/j.jss.2015.03.009,"Assistive Software offers a solution for people with disabilities to manage specialized hardware, devices or services. However, these users may have difficulties in selecting and installing Assistive Software in their devices for managing smart environments. This paper addresses the requirements of these kinds of systems and their design in the context of interoperability architectures. Our solution follows a semantic approach, for which ontologies are a key. The paper also presents an implementation of our design proposal, i.e., a real and usable system which is evaluated according to a set of functional and non-functional requirements here proposed.","Software design, Assistive Software, Software non-functional evaluation",,,,
Journal Article,"Palomba F,Di Nucci D,Panichella A,Zaidman A,De Lucia A",,On the impact of code smells on the energy consumption of mobile applications,Information and Software Technology,2019,105.0,,43-55,,,2019,,0950-5849,https://www.sciencedirect.com/science/article/pii/S0950584918301678;http://dx.doi.org/10.1016/j.infsof.2018.08.004,10.1016/j.infsof.2018.08.004,"Context. The demand for green software design is steadily growing higher especially in the context of mobile devices, where the computation is often limited by battery life. Previous studies found how wrong programming solutions have a strong impact on the energy consumption. Objective. Despite the efforts spent so far, only a little knowledge on the influence of code smells, i.e.,symptoms of poor design or implementation choices, on the energy consumption of mobile applications is available. Method. To provide a wider overview on the relationship between smells and energy efficiency, in this paper we conducted a large-scale empirical study on the influence of 9 Android-specific code smells on the energy consumption of 60 Android apps. In particular, we focus our attention on the design flaws that are theoretically supposed to be related to non-functional attributes of source code, such as performance and energy consumption. Results. The results of the study highlight that methods affected by four code smell types, i.e.,Internal Setter, Leaking Thread, Member Ignoring Method, and Slow Loop, consume up to 87 times more than methods affected by other code smells. Moreover, we found that refactoring these code smells reduces energy consumption in all of the situations. Conclusions. Based on our findings, we argue that more research aimed at designing automatic refactoring approaches and tools for mobile apps is needed.","Code smells, Refactoring, Energy consumption, Mobile apps",,,,
Journal Article,"Saranya G,Khanna Nehemiah H,Kannan A,Nithya V",,Model level code smell detection using EGAPSO based on similarity measures,Alexandria Engineering Journal,2018,57.0,3.0,1631-1642,,,2018,,1110-0168,https://www.sciencedirect.com/science/article/pii/S111001681730234X;http://dx.doi.org/10.1016/j.aej.2017.07.006,10.1016/j.aej.2017.07.006,"Software maintenance is an essential part of any software that finds its use in the day-to-day activities of any organization. During the maintenance phase bugs detected must be corrected and the software must evolve with respect to changing requirements without ripple effects. Software maintenance is a cumbersome process if code smells exist in the software. The impact of poor design is code smells. In code smells detection, majority of the existing approaches are rule based, where a rule represents the combination of metrics and threshold. In rule based approach, defining the rules that detect the code smells are time consuming because identifying the correct threshold value is a tedious task, which can be fixed only through trial and error method. To address this issue, in this work Euclidean distance based Genetic Algorithm and Particle Swarm Optimization (EGAPSO) is used. Instead of relying on threshold value, this approach detects all code smells based on similarity between the system under study and the set of defect examples, where the former is the initial model and the latter is the base example. The approach is tested on the open source projects, namely Gantt Project and Log4j for identifying the five code smells namely Blob, Functional Decomposition, Spaghetti Code, Data Class and Feature Envy. Finally, the approach is compared with code smell detection using Genetic Algorithm (GA), DEtection and CORrection (DECOR), Parallel Evolutionary Algorithm (PEA) and Multi-Objective Genetic Programming (MOGP). The result of EGAPSO proves to be effective when compared to other code smell detection approaches.","Software maintenance, Code smell, Software metrics, Search based software engineering, Euclidean distance, Open source software",,,,
Journal Article,"Ordiales Coscia JL,Mateos C,Crasso M,Zunino A",,Refactoring code-first Web Services for early avoiding WSDL anti-patterns: Approach and comprehensive assessment,Science of Computer Programming,2014,89.0,,374-407,,,2014,,0167-6423,https://www.sciencedirect.com/science/article/pii/S0167642314001646;http://dx.doi.org/10.1016/j.scico.2014.03.015,10.1016/j.scico.2014.03.015,"Previous research of our own [34] has shown that by avoiding certain bad specification practices, or WSDL anti-patterns, contract-first Web Service descriptions expressed in WSDL can be greatly improved in terms of understandability and retrievability. The former means the capability of a human discoverer to effectively reason about a Web Service functionality just by inspecting its associated WSDL description. The latter means correctly retrieving a relevant Web Service by a syntactic service registry upon a meaningful user's query. However, code-first service construction dominates in the industry due to its simplicity. This paper proposes an approach to avoid WSDL anti-patterns in code-first Web Services. We also evaluate the approach in terms of services understandability and retrievability, deeply discuss the experimental results, and delineate some guidelines to help code-first Web Service developers in dealing with the trade-offs that arise between these two dimensions. Certainly, our approach allows services to be more understandable, due to anti-pattern remotion, and retrievable as measured by classical Information Retrieval metrics.","Web services, Code-first, WSDL anti-patterns, Service understandability, Service retrievability",,,,
Journal Article,"Garousi Yusifoğlu V,Amannejad Y,Betin Can A",,Software test-code engineering: A systematic mapping,Information and Software Technology,2015,58.0,,123-147,,,2015,,0950-5849,https://www.sciencedirect.com/science/article/pii/S0950584914001487;http://dx.doi.org/10.1016/j.infsof.2014.06.009,10.1016/j.infsof.2014.06.009,"Context As a result of automated software testing, large amounts of software test code (script) are usually developed by software teams. Automated test scripts provide many benefits, such as repeatable, predictable, and efficient test executions. However, just like any software development activity, development of test scripts is tedious and error prone. We refer, in this study, to all activities that should be conducted during the entire lifecycle of test-code as Software Test-Code Engineering (STCE). Objective As the STCE research area has matured and the number of related studies has increased, it is important to systematically categorize the current state-of-the-art and to provide an overview of the trends in this field. Such summarized and categorized results provide many benefits to the broader community. For example, they are valuable resources for new researchers (e.g., PhD students) aiming to conduct additional secondary studies. Method In this work, we systematically classify the body of knowledge related to STCE through a systematic mapping (SM) study. As part of this study, we pose a set of research questions, define selection and exclusion criteria, and systematically develop and refine a systematic map. Results Our study pool includes a set of 60 studies published in the area of STCE between 1999 and 2012. Our mapping data is available through an online publicly-accessible repository. We derive the trends for various aspects of STCE. Among our results are the following: (1) There is an acceptable mix of papers with respect to different contribution facets in the field of STCE and the top two leading facets are tool (68%) and method (65%). The studies that presented new processes, however, had a low rate (3%), which denotes the need for more process-related studies in this area. (2) Results of investigation about research facet of studies and comparing our result to other SM studies shows that, similar to other fields in software engineering, STCE is moving towards more rigorous validation approaches. (3) A good mixture of STCE activities has been presented in the primary studies. Among them, the two leading activities are quality assessment and co-maintenance of test-code with production code. The highest growth rate for co-maintenance activities in recent years shows the importance and challenges involved in this activity. (4) There are two main categories of quality assessment activity: detection of test smells and oracle assertion adequacy. (5) JUnit is the leading test framework which has been used in about 50% of the studies. (6) There is a good mixture of SUT types used in the studies: academic experimental systems (or simple code examples), real open-source and commercial systems. (7) Among 41 tools that are proposed for STCE, less than half of the tools (45%) were available for download. It is good to have this percentile of tools to be available, although not perfect, since the availability of tools can lead to higher impact on research community and industry. Conclusion We discuss the emerging trends in STCE, and discuss the implications for researchers and practitioners in this area. The results of our systematic mapping can help researchers to obtain an overview of existing STCE approaches and spot areas in the field that require more attention from the research community.","Systematic mapping, Survey, Study repository, Software test-code engineering, Development of test code, Quality assessment of test code",,,,
Journal Article,"Settas DL,Meditskos G,Stamelos IG,Bassiliades N",,SPARSE: A symptom-based antipattern retrieval knowledge-based system using Semantic Web technologies,Expert Systems with Applications,2011,38.0,6.0,7633-7646,,,2011,,0957-4174,https://www.sciencedirect.com/science/article/pii/S0957417410014600;http://dx.doi.org/10.1016/j.eswa.2010.12.097,10.1016/j.eswa.2010.12.097,"Antipatterns provide information on commonly occurring solutions to problems that generate negative consequences. The number of software project management antipatterns that appears in the literature and the Web increases to the extent that makes using antipatterns problematic. Furthermore, antipatterns are usually inter-related and rarely appear in isolation. As a result, detecting which antipatterns exist in a software project is a challenging task which requires expert knowledge. This paper proposes SPARSE, an OWL ontology based knowledge-based system that aims to assist software project managers in the antipattern detection process. The antipattern ontology documents antipatterns and how they are related with other antipatterns through their causes, symptoms and consequences. The semantic relationships that derive from the antipattern definitions are determined using the Pellet DL reasoner and they are transformed into the COOL language of the CLIPS production rule engine. The purpose of this transformation is to create a compact representation of the antipattern knowledge, enabling a set of object-oriented CLIPS production rules to run and retrieve antipatterns relevant to some initial symptoms. SPARSE is exemplified through 31 OWL ontology antipattern instances of software development antipatterns that appear on the Web.","Antipatterns, Symptom-based retrieval, OWL ontology, Production rules, Objects",,,,
Journal Article,"Vidal S,Oizumi W,Garcia A,Díaz Pace A,Marcos C",,Ranking architecturally critical agglomerations of code smells,Science of Computer Programming,2019,182.0,,64-85,,,2019,,0167-6423,https://www.sciencedirect.com/science/article/pii/S0167642318303514;http://dx.doi.org/10.1016/j.scico.2019.07.003,10.1016/j.scico.2019.07.003,"Code smells are symptoms in the source code that could help to identify architectural problems. However, developers may feel discouraged to analyze multiple smells if they are not able to focus their attention on a small set of source code locations. Unfortunately, current techniques fall short in assisting developers to prioritize smelly locations that are likely to indicate architectural problems. Furthermore, developers often have trouble analyzing interconnected smells that contribute together to realize an architectural problem. To deal with these issues, this work presents and evaluates a suite of five criteria for ranking groups of code smells as indicators of architectural problems in evolving systems. These criteria were implemented in a tool called JSpIRIT. In a first experiment, we have assessed the criteria in the context of 23 versions of 4 systems and analyzed their effectiveness for revealing architectural problem locations. In addition, we conducted a second experiment for analyzing similarities between the prioritization provided by developers and the prioritization provided by our best performing criterion. The results provide evidence that one of the proposed criteria helped to correctly prioritize more than 80 code locations of architectural problems, alleviating tedious manual inspection of the source code vis-a-vis with the architecture.","Code smells, Agglomerations, Software architecture",,,,
Journal Article,"Martinelli F,Mercaldo F,Nardone V,Santone A,Sangaiah AK,Cimitile A",,Evaluating model checking for cyber threats code obfuscation identification,Journal of Parallel and Distributed Computing,2018,119.0,,203-218,,,2018,,0743-7315,https://www.sciencedirect.com/science/article/pii/S0743731518302636;http://dx.doi.org/10.1016/j.jpdc.2018.04.008,10.1016/j.jpdc.2018.04.008,"Code obfuscation is a set of transformations that make code programs harder to understand. The goal of code obfuscation is to make reverse engineering of programs infeasible, while maintaining the logic on the program. Originally, it has been used to protect intellectual property. However, recently code obfuscation has been also used by malware writers in order to make cyber threats easily able to evade antimalware scanners. As a matter of fact, metamorphic and polymorphic viruses exhibit the ability to obfuscate their code as they propagate. In this paper we propose a model checking-based approach which is able to identify the most widespread obfuscating techniques, without making any assumptions about the nature of the obfuscations used. We evaluate the proposed method on a real-world dataset obtaining an accuracy equal to 0.9 in the identification of obfuscation techniques.","Obfuscation, Android, Model checking, Formal methods, Malware",,,,
Journal Article,"Saidani I,Ouni A,Chouchen M,Mkaouer MW",,Predicting continuous integration build failures using evolutionary search,Information and Software Technology,2020,128.0,,106392,,,2020,,0950-5849,https://www.sciencedirect.com/science/article/pii/S0950584920301579;http://dx.doi.org/10.1016/j.infsof.2020.106392,10.1016/j.infsof.2020.106392,"Context: Continuous Integration (CI) is a common practice in modern software development and it is increasingly adopted in the open-source as well as the software industry markets. CI aims at supporting developers in integrating code changes constantly and quickly through an automated build process. However, in such context, the build process is typically time and resource-consuming which requires a high maintenance effort to avoid build failure. Objective: The goal of this study is to introduce an automated approach to cut the expenses of CI build time and provide support tools to developers by predicting the CI build outcome. Method: In this paper, we address problem of CI build failure by introducing a novel search-based approach based on Multi-Objective Genetic Programming (MOGP) to build a CI build failure prediction model. Our approach aims at finding the best combination of CI built features and their appropriate threshold values, based on two conflicting objective functions to deal with both failed and passed builds. Results: We evaluated our approach on a benchmark of 56,019 builds from 10 large-scale and long-lived software projects that use the Travis CI build system. The statistical results reveal that our approach outperforms the state-of-the-art techniques based on machine learning by providing a better balance between both failed and passed builds. Furthermore, we use the generated prediction rules to investigate which factors impact the CI build results, and found that features related to (1) specific statistics about the project such as team size, (2) last build information in the current build and (3) the types of changed files are the most influential to indicate the potential failure of a given build. Conclusion: This paper proposes a multi-objective search-based approach for the problem of CI build failure prediction. The performances of the models developed using our MOGP approach were statistically better than models developed using machine learning techniques. The experimental results show that our approach can effectively reduce both false negative rate and false positive rate of CI build failures in highly imbalanced datasets.","Continuous integration, Build prediction, Multi-Objective optimization, Search-Based software engineering, Machine learning",,,,
Journal Article,"Arcelli Fontana F,Zanoni M",,A tool for design pattern detection and software architecture reconstruction,Information Sciences,2011,181.0,7.0,1306-1324,,,2011,,0020-0255,https://www.sciencedirect.com/science/article/pii/S0020025510005955;http://dx.doi.org/10.1016/j.ins.2010.12.002,10.1016/j.ins.2010.12.002,"It is well known that software maintenance and evolution are expensive activities, both in terms of invested time and money. Reverse engineering activities support the obtainment of abstractions and views from a target system that should help the engineers to maintain, evolve and eventually re-engineer it. Two important tasks pursued by reverse engineering are design pattern detection and software architecture reconstruction, whose main objectives are the identification of the design patterns that have been used in the implementation of a system as well as the generation of views placed at different levels of abstractions, which let the practitioners focus on the overall architecture of the system without worrying about the programming details it has been implemented with. In this context we propose an Eclipse plug-in called MARPLE (Metrics and Architecture Reconstruction Plug-in for Eclipse), which supports both the detection of design patterns and software architecture reconstruction activities through the use of basic elements and metrics that are mechanically extracted from the source code. The development of this platform is mainly based on the exploitation of the Eclipse framework and plug-ins as well as of different Java libraries for data access and graph management and visualization. In this paper we focus our attention on the design pattern detection process.","Reverse engineering, Software architecture reconstruction, Design pattern detection, Data mining techniques, Metrics computations",,,,
Journal Article,"Walter B,Fontana FA,Ferme V",,Code smells and their collocations: A large-scale experiment on open-source systems,Journal of Systems and Software,2018,144.0,,1-21,,,2018,,0164-1212,https://www.sciencedirect.com/science/article/pii/S0164121218301109;http://dx.doi.org/10.1016/j.jss.2018.05.057,10.1016/j.jss.2018.05.057,"Code smells indicate possible flaws in software design, that could negatively affect system’s maintainability. Interactions among smells located in the same classes (i.e., collocated smells) have even more detrimental effect on quality. Extracted frequent patterns of collocated smells could help to understand practical consequences of collocations. In this paper we identify and empirically validate frequent collocations of 14 code smells detected in 92 Java systems, using three approaches: pairwise correlation analysis, PCA and associative rules. To cross-validate the results, we used up to 6 detectors for each smell. Additionally, we examine and compare techniques used to extract the relationships. The contribution is three-fold: (1) we identify and empirically validate relationships among the examined code smells on a large dataset that we made publicly available, (2) we discuss how the choice of code smell detectors affects results, and (3) we analyze the impact of software domain on existence of the smell collocations. Additionally, we found that analytical methods we used to discover collocations, are complementary. Smells collocations display recurring patterns that could help prioritizing the classes affected by code smells to be refactored and developing or enhancing detectors exploiting information about collocations. They can also help the developers focusing on classes deserving more maintenance effort.","Code smells, Inter-smell relationships, Smell interaction, Collocated smells, Code smell detectors, Source code quality",,,,
Journal Article,"Sangeetha M,Chandrasekar C",,An empirical investigation into code smells rectifications through ADA_BOOSTER,Ain Shams Engineering Journal,2019,10.0,3.0,549-553,,,2019,,2090-4479,https://www.sciencedirect.com/science/article/pii/S2090447919300012;http://dx.doi.org/10.1016/j.asej.2018.10.005,10.1016/j.asej.2018.10.005,"Object Oriented Programming has become one of the most established paradigms. It offers us features like encapsulation, polymorphism, inheritance etc. By using these features we are able to develop good software’s that are easy to understand. But when the size of the software goes on increasing it poses problems with respect to maintaining the source code. Also we come across the problems like code scattering and code tangling. So object oriented programming has some limitations and this is where Aspect Oriented Programming comes into play. Refactoring is one of the most important activities in software development. It is done to improve the design of the software, to make the software easier and better to understand and to help us in writing programs faster. After the software is refactored, it is important to note the behaviour of that software. Here, we propose new refactorings technique. The refactorings are identified. They are applied on the Systems. The ADA_BOOSTER technique to measure the metrics. By using refactoring we can change the existing software without affecting the behaviour of the software.","Software refactoring, Code smell, Smell identifications, Smell rectifications, ADA-Booster",,,,
Journal Article,"Tahir A,Dietrich J,Counsell S,Licorish S,Yamashita A",,A large scale study on how developers discuss code smells and anti-pattern in Stack Exchange sites,Information and Software Technology,2020,125.0,,106333,,,2020,,0950-5849,https://www.sciencedirect.com/science/article/pii/S0950584920300926;http://dx.doi.org/10.1016/j.infsof.2020.106333,10.1016/j.infsof.2020.106333,"Context: In this paper, we investigate how developers discuss code smells and anti-patterns across three technical Stack Exchange sites. Understanding developers perceptions of these issues is important to inform and align future research efforts and direct tools vendors to design tailored tools that best suit developers. Method: we mined three Stack Exchange sites and used quantitative and qualitative methods to analyse more than 4000 posts that discuss code smells and anti-patterns.Results: results showed that developers often asked their peers to smell their code, thus utilising those sites as an informal, crowd-based code smell/anti-pattern detector. The majority of questions (556) asked were focused on smells like Duplicated Code, Spaghetti Code, God and Data Classes. In terms of languages, most of discussions centred around popular languages such as C# (772 posts), JavaScript (720) and Java (699), however greater support is available for Java compared to other languages (especially modern languages such as Swift and Kotlin). We also found that developers often discuss the downsides of implementing specific design patterns and ‘flag’ them as potential anti-patterns to be avoided. Some well-defined smells and anti-patterns are discussed as potentially being acceptable practice in certain scenarios. In general, developers actively seek to consider trade-offs to decide whether to use a design pattern, an anti-pattern or not.Conclusion: our results suggest that there is a need for: 1) more context and domain sensitive evaluations of code smells and anti-patterns, 2) better guidelines for making trade-offs when applying design patterns or eliminating smells/anti-patterns in industry, and 3) a unified, constantly updated, catalog of smells and anti-patterns. We conjecture that the crowd-based detection approach considers contextual factors and thus tend to be more trusted by developers than automated detection tools.","Code smells, Anti-patterns, Mining software repositories, Stack exchange",,,,
Journal Article,"Ferenc R,Gyimesi P,Gyimesi G,Tóth Z,Gyimóthy T",,An automatically created novel bug dataset and its validation in bug prediction,Journal of Systems and Software,2020,169.0,,110691,,,2020,,0164-1212,https://www.sciencedirect.com/science/article/pii/S0164121220301436;http://dx.doi.org/10.1016/j.jss.2020.110691,10.1016/j.jss.2020.110691,"Bugs are inescapable during software development due to frequent code changes, tight deadlines, etc.; therefore, it is important to have tools to find these errors. One way of performing bug identification is to analyze the characteristics of buggy source code elements from the past and predict the present ones based on the same characteristics, using e.g. machine learning models. To support model building tasks, code elements and their characteristics are collected in so-called bug datasets which serve as the input for learning. We present the BugHunter Dataset: a novel kind of automatically constructed and freely available bug dataset containing code elements (files, classes, methods) with a wide set of code metrics and bug information. Other available bug datasets follow the traditional approach of gathering the characteristics of all source code elements (buggy and non-buggy) at only one or more pre-selected release versions of the code. Our approach, on the other hand, captures the buggy and the fixed states of the same source code elements from the narrowest timeframe we can identify for a bug’s presence, regardless of release versions. To show the usefulness of the new dataset, we built and evaluated bug prediction models and achieved F-measure values over 0.74.","Bug dataset, Bug prediction, Static code analysis, Code metrics, Machine learning, GitHub",,,,
Journal Article,Al Dallal J,,Identifying refactoring opportunities in object-oriented code: A systematic literature review,Information and Software Technology,2015,58.0,,231-249,,,2015,,0950-5849,https://www.sciencedirect.com/science/article/pii/S0950584914001918;http://dx.doi.org/10.1016/j.infsof.2014.08.002,10.1016/j.infsof.2014.08.002,"Context Identifying refactoring opportunities in object-oriented code is an important stage that precedes the actual refactoring process. Several techniques have been proposed in the literature to identify opportunities for various refactoring activities. Objective This paper provides a systematic literature review of existing studies identifying opportunities for code refactoring activities. Method We performed an automatic search of the relevant digital libraries for potentially relevant studies published through the end of 2013, performed pilot and author-based searches, and selected 47 primary studies (PSs) based on inclusion and exclusion criteria. The PSs were analyzed based on a number of criteria, including the refactoring activities, the approaches to refactoring opportunity identification, the empirical evaluation approaches, and the data sets used. Results The results indicate that research in the area of identifying refactoring opportunities is highly active. Most of the studies have been performed by academic researchers using nonindustrial data sets. Extract Class and Move Method were found to be the most frequently considered refactoring activities. The results show that researchers use six primary existing approaches to identify refactoring opportunities and six approaches to empirically evaluate the identification techniques. Most of the systems used in the evaluation process were open-source, which helps to make the studies repeatable. However, a relatively high percentage of the data sets used in the empirical evaluations were small, which limits the generality of the results. Conclusions It would be beneficial to perform further studies that consider more refactoring activities, involve researchers from industry, and use large-scale and industrial-based systems.","Refactoring activity, Refactoring opportunity, Systematic literature review",,,,
Journal Article,"Ebert F,Castor F,Serebrenik A",,An exploratory study on exception handling bugs in Java programs,Journal of Systems and Software,2015,106.0,,82-101,,,2015,,0164-1212,https://www.sciencedirect.com/science/article/pii/S0164121215000862;http://dx.doi.org/10.1016/j.jss.2015.04.066,10.1016/j.jss.2015.04.066,"Most mainstream programming languages provide constructs to throw and to handle exceptions. However, several studies argue that exception handling code is usually of poor quality and that it is commonly neglected by developers. Moreover, it is said to be the least understood, documented, and tested part of the implementation of a system. Nevertheless, there are very few studies that analyze the actual exception handling bugs that occur in real software systems or that attempt to understand developers’ perceptions of these bugs. In this work we present an exploratory study on exception handling bugs that employs two complementary approaches: a survey of 154 developers and an analysis of 220 exception handling bugs from the repositories of Eclipse and Tomcat. Only 27% of the respondents claimed that policies and standards for the implementation of error handling are part of the culture of their organizations. Moreover, in 70% of the organizations there are no specific tests for the exception handling code. Also, 61% of the respondents stated that no to little importance is given to the documentation of exception handling in the design phase of the projects with which they are involved. In addition, about 40% of the respondents consider the quality of exception handling code to be either good or very good and only 14% of the respondents consider it to be bad or very bad. Furthermore, the repository analysis has shown (with statistical significance) that exception handling bugs are ignored by developers less often than other bugs. We have also observed that while overly general catch blocks are a well-known bad smell related to exceptions, bugs stemming from these catch blocks are rare, even though many overly general catch blocks occur in the code. Furthermore, while developers often mention empty catch blocks as causes of bugs they have fixed in the past, we found very few bug reports caused by them. On top of that, empty catch blocks are frequently used as part of bug fixes, including fixes for exception handling bugs. Based on our findings, we propose a classification of exception handling bugs and their causes. The proposed classification can be used to assist in the design and implementation of test suites, to guide code inspections, or as a basis for static analysis tools.","Exception handling, Bugs, Repository mining",,,,
Journal Article,"Hozano M,Garcia A,Fonseca B,Costa E",,Are you smelling it? Investigating how similar developers detect code smells,Information and Software Technology,2018,93.0,,130-146,,,2018,,0950-5849,https://www.sciencedirect.com/science/article/pii/S0950584916303901;http://dx.doi.org/10.1016/j.infsof.2017.09.002,10.1016/j.infsof.2017.09.002,"Context A code smell indicates a poor implementation choice that often worsens software quality. Thus, code smell detection is an elementary technique to identify refactoring opportunities in software systems. Unfortunately, there is limited knowledge on how similar two or more developers detect smells in code. In particular, few studies have investigated if developers agree or disagree when recognizing a smell and which factors can influence on such (dis)agreement. Objective We perform a broader study to investigate how similar the developers detect code smells. We also analyze whether certain factors related to the developers’ profiles concerning background and experience may influence such (dis)agreement. Moreover, we analyze if the heuristics adopted by developers on detecting code smells may influence on their (dis)agreement. Method We conducted an empirical study with 75 developers who evaluated instances of 15 different code smell types. For each smell type, we analyzed the agreement among the developers and we assessed the influence of 6 different factors on the developers’ evaluations. Altogether more than 2700 evaluations were collected, resulting in substantial quantitative and qualitative analyses. Results The results indicate that the developers presented a low agreement on detecting all 15 smell types analyzed in our study. The results also suggest that factors related to background and experience did not have a consistent influence on the agreement among the developers. On the other hand, the results show that the agreement was consistently influenced by specific heuristics employed by developers. Conclusions Our findings reveal that the developers detect code smells in significantly different ways. As a consequence, these findings introduce some questions concerning the results of previous studies that did not consider the different perceptions of developers on detecting code smells. Moreover, our findings shed light towards improving state-of-the-art techniques for accurate, customized detection of code smells.","Code smell, Detection, Software maintenance, Empirical study",,,,
Journal Article,"Pascarella L,Palomba F,Bacchelli A",,On the performance of method-level bug prediction: A negative result,Journal of Systems and Software,2020,161.0,,110493,,,2020,,0164-1212,https://www.sciencedirect.com/science/article/pii/S0164121219302675;http://dx.doi.org/10.1016/j.jss.2019.110493,10.1016/j.jss.2019.110493,"Bug prediction is aimed at identifying software artifacts that are more likely to be defective in the future. Most approaches defined so far target the prediction of bugs at class/file level. Nevertheless, past research has provided evidence that this granularity is too coarse-grained for its use in practice. As a consequence, researchers have started proposing defect prediction models targeting a finer granularity (particularly method-level granularity), providing promising evidence that it is possible to operate at this level. Particularly, models mixing product and process metrics provided the best results. We present a study in which we first replicate previous research on method-level bug-prediction, by using different systems and timespans. Afterwards, based on the limitations of existing research, we (1) re-evaluate method-level bug prediction models more realistically and (2) analyze whether alternative features based on textual aspects, code smells, and developer-related factors can be exploited to improve method-level bug prediction abilities. Key results of our study include that (1) the performance of the previously proposed models, tested using the same strategy but on different systems/timespans, is confirmed; but, (2) when evaluated with a more practical strategy, all the models show a dramatic drop in performance, with results close to that of a random classifier. Finally, we find that (3) the contribution of alternative features within such models is limited and unable to improve the prediction capabilities significantly. As a consequence, our replication and negative results indicate that method-level bug prediction is still an open challenge.","Defect prediction, Empirical software engineering, Mining software repositories",,,,
Journal Article,"Alsolai H,Roper M",,A systematic literature review of machine learning techniques for software maintainability prediction,Information and Software Technology,2020,119.0,,106214,,,2020,,0950-5849,https://www.sciencedirect.com/science/article/pii/S0950584919302228;http://dx.doi.org/10.1016/j.infsof.2019.106214,10.1016/j.infsof.2019.106214,"Context Software maintainability is one of the fundamental quality attributes of software engineering. The accurate prediction of software maintainability is a significant challenge for the effective management of the software maintenance process. Objective The major aim of this paper is to present a systematic review of studies related to the prediction of maintainability of object-oriented software systems using machine learning techniques. This review identifies and investigates a number of research questions to comprehensively summarize, analyse and discuss various viewpoints concerning software maintainability measurements, metrics, datasets, evaluation measures, individual models and ensemble models. Method The review uses the standard systematic literature review method applied to the most common computer science digital database libraries from January 1991 to July 2018. Results We survey 56 relevant studies in 35 journals and 21 conference proceedings. The results indicate that there is relatively little activity in the area of software maintainability prediction compared with other software quality attributes. CHANGE maintenance effort and the maintainability index were the most commonly used software measurements (dependent variables) employed in the selected primary studies, and most made use of class-level product metrics as the independent variables. Several private datasets were used in the selected studies, and there is a growing demand to publish datasets publicly. Most studies focused on regression problems and performed k-fold cross-validation. Individual prediction models were employed in the majority of studies, while ensemble models relatively rarely. Conclusion Based on the findings obtained in this systematic literature review, ensemble models demonstrated increased accuracy prediction over individual models, and have been shown to be useful models in predicting software maintainability. However, their application is relatively rare and there is a need to apply these, and other models to an extensive variety of datasets with the aim of improving the accuracy and consistency of results.","Systematic literature review, Software maintainability prediction, Machine learning, Metric, Dataset",,,,
Journal Article,"Palomba F,Bavota G,Di Penta M,Fasano F,Oliveto R,De Lucia A",,A large-scale empirical study on the lifecycle of code smell co-occurrences,Information and Software Technology,2018,99.0,,1-10,,,2018,,0950-5849,https://www.sciencedirect.com/science/article/pii/S0950584918300211;http://dx.doi.org/10.1016/j.infsof.2018.02.004,10.1016/j.infsof.2018.02.004,"Context Code smells are suboptimal design or implementation choices made by programmers during the development of a software system that possibly lead to low code maintainability and higher maintenance costs. Objective Previous research mainly studied the characteristics of code smell instances affecting a source code file, while only few studies analyzed the magnitude and effects of smell co-occurrence, i.e., the co-occurrence of different types of smells on the same code component. This paper aims at studying in details this phenomenon. Method We analyzed 13 code smell types detected in 395 releases of 30 software systems to firstly assess the extent to which code smells co-occur, and then we analyze (i) which code smells co-occur together, and (ii) how and why they are introduced and removed by developers. Results 59% of smelly classes are affected by more than one smell, and in particular there are six pairs of smell types (e.g., Message Chains and Spaghetti Code) that frequently co-occur. Furthermore, we observed that method-level code smells may be the root cause for the introduction of class-level smells. Finally, code smell co-occurrences are generally removed together as a consequence of other maintenance activities causing the deletion of the affected code components (with a consequent removal of the code smell instances) as well as the result of a major restructuring or scheduled refactoring actions. Conclusions Based on our findings, we argue that more research aimed at designing co-occurrence-aware code smell detectors and refactoring approaches is needed.","Code smells co-occurrences, Empirical study, Mining software repositories",,,,
Journal Article,"Rahman A,Williams L",,Source code properties of defective infrastructure as code scripts,Information and Software Technology,2019,112.0,,148-163,,,2019,,0950-5849,https://www.sciencedirect.com/science/article/pii/S0950584919300965;http://dx.doi.org/10.1016/j.infsof.2019.04.013,10.1016/j.infsof.2019.04.013,"Context In continuous deployment, software and services are rapidly deployed to end-users using an automated deployment pipeline. Defects in infrastructure as code (IaC) scripts can hinder the reliability of the automated deployment pipeline. We hypothesize that certain properties of IaC source code such as lines of code and hard-coded strings used as configuration values, show correlation with defective IaC scripts. Objective The objective of this paper is to help practitioners in increasing the quality of infrastructure as code (IaC) scripts through an empirical study that identifies source code properties of defective IaC scripts. Methodology We apply qualitative analysis on defect-related commits mined from open source software repositories to identify source code properties that correlate with defective IaC scripts. Next, we survey practitioners to assess the practitioner’s agreement level with the identified properties. We also construct defect prediction models using the identified properties for 2439 scripts collected from four datasets. Results We identify 10 source code properties that correlate with defective IaC scripts. Of the identified 10 properties we observe lines of code and hard-coded string i.e. putting strings as configuration values, to show the strongest correlation with defective IaC scripts. According to our survey analysis, majority of the practitioners show agreement for two properties: include, the property of executing external modules or scripts, and hard-coded string. Using the identified properties, our constructed defect prediction models show a precision of 0.70∼0.78, and a recall of 0.54∼0.67. Conclusion Based on our findings, we recommend practitioners to allocate sufficient inspection and testing efforts on IaC scripts that include any of the identified 10 source code properties of IaC scripts.","Configuration as code, Continuous deployment, Defect prediction, Devops, Empirical study, Infrastructure as code, Puppet",,,,
Journal Article,"Ng TH,Yu YT,Cheung SC,Chan WK",,Human and program factors affecting the maintenance of programs with deployed design patterns,Information and Software Technology,2012,54.0,1.0,99-118,,,2012,,0950-5849,https://www.sciencedirect.com/science/article/pii/S0950584911001790;http://dx.doi.org/10.1016/j.infsof.2011.08.002,10.1016/j.infsof.2011.08.002,"Context Practitioners may use design patterns to organize program code. Various empirical studies have investigated the effects of pattern deployment and work experience on the effectiveness and efficiency of program maintenance. However, results from these studies are not all consistent. Moreover, these studies have not considered some interesting factors, such as a maintainer’s prior exposure to the program under maintenance. Objective This paper aims at identifying what factors may contribute to the productivity of maintainers in the context of making correct software changes when they work on programs with deployed design patterns. Method We performed an empirical study involving 118 human subjects with three change tasks on a medium-sized program to explore the possible effects of a suite of six human and program factors on the productivity of maintainers, measured by the time taken to produce a correctly revised program in a course-based setting. The factors we studied include the deployment of design patterns and the presence of pattern-unaware solutions, as well as the maintainer’s prior exposure to design patterns, the subject program and the programming language, and prior work experience. Results Among the factors under examination, we find that the deployment of design patterns, prior exposure to the program and the presence of pattern-unaware solutions are strongly correlated with the time taken to correctly complete maintenance tasks. We also report some interesting observations from the experiment. Conclusion A new factor, namely, the presence of pattern-unaware solutions, contributes to the efficient completion of maintenance tasks of programs with deployed design patterns. Moreover, we conclude from the study that neither prior exposure to design patterns nor prior exposure to the programming language is supported by sufficient evidences to be significant factors, whereas the subjects’ exposure to the program under maintenance is notably more important.","Design patterns, Empirical study, Human factors, Pattern-deployed software, Program factors, Software maintenance",,,,
Journal Article,"Chen Z,Chen L,Ma W,Zhou X,Zhou Y,Xu B",,Understanding metric-based detectable smells in Python software: A comparative study,Information and Software Technology,2018,94.0,,14-29,,,2018,,0950-5849,https://www.sciencedirect.com/science/article/pii/S0950584916301690;http://dx.doi.org/10.1016/j.infsof.2017.09.011,10.1016/j.infsof.2017.09.011,"Context Code smells are supposed to cause potential comprehension and maintenance problems in software development. Although code smells are studied in many languages, e.g. Java and C#, there is a lack of technique or tool support addressing code smells in Python. Objective Due to the great differences between Python and static languages, the goal of this study is to define and detect code smells in Python programs and to explore the effects of Python smells on software maintainability. Method In this paper, we introduced ten code smells and established a metric-based detection method with three different filtering strategies to specify metric thresholds (Experience-Based Strategy, Statistics-Based Strategy, and Tuning Machine Strategy). Then, we performed a comparative study to investigate how three detection strategies perform in detecting Python smells and how these smells affect software maintainability with different detection strategies. This study utilized a corpus of 106 Python projects with most stars on GitHub. Results The results showed that: (1) the metric-based detection approach performs well in detecting Python smells and Tuning Machine Strategy achieves the best accuracy; (2) the three detection strategies discover some different smell occurrences, and Long Parameter List and Long Method are more prevalent than other smells; (3) several kinds of code smells are more significantly related to changes or faults in Python modules. Conclusion These findings reveal the key features of Python smells and also provide a guideline for the choice of detection strategy in detecting and analyzing Python smells.","Python, Code smell, Detection strategy, Software maintainability",,,,
Journal Article,"Cai H,Fu X,Hamou-Lhadj A",,A study of run-time behavioral evolution of benign versus malicious apps in android,Information and Software Technology,2020,122.0,,106291,,,2020,,0950-5849,https://www.sciencedirect.com/science/article/pii/S0950584920300410;http://dx.doi.org/10.1016/j.infsof.2020.106291,10.1016/j.infsof.2020.106291,"Context The constant evolution of the Android platform and its applications have imposed significant challenges both to understanding and securing the Android ecosystem. Yet, despite the growing body of relevant research, it remains unclear how Android apps evolve in terms of their run-time behaviors in ways that impede our gaining consistent empirical knowledge about the workings of the ecosystem and developing effective technical solutions to defending it against security threats. Intuitively, an essential step towards addressing these challenges is to first understand the evolution itself. Among others, one avenue to examining a program’s run-time behavior is to dissect the program’s execution in terms of its syntactic and semantic structure. Objective In this paper, we study how benign Android apps execute differently from malware over time, in terms of their execution structures measured by the distribution and interaction among functionality scopes, app components, and callbacks. In doing so, we attempt to reveal how relevant app execution structure is to app security orientation (i.e., benign or malicious). Method By tracing the method calls and inter-component communications (ICCs) of 15,451 benign apps and 15,183 malware developed during eight years (2010–2017), we systematically characterized the execution structure of malware versus benign apps and revealed similarities and disparities between them that are not previously known. Results Our results show, among other findings, that (1) despite their similarity in execution distribution over functionality scopes, malware accessed framework functionalities mainly through third-party libraries, while benign apps were dominated by calls within the framework; (2) use of Activity component had been rising in malware while benign apps saw continuous drop in such uses; (3) malware invoked significantly more Services but less Content Providers than benign apps during the evolution of both groups; (4) malware carried ICC data significantly less often via standard data fields than benign apps, albeit both groups did not carry any data in most ICCs; and (5) newer malware tended to have more even distribution of callbacks among event-handler categories, while the distribution remained constant in benign apps over time. Conclusion We discussed how these findings inform understanding app behaviors, optimizing static and dynamic code analysis of Android apps, and developing sustainable app security defense solutions.","Android apps, Security, Execution, Evolution, Longitudinal study",,,,
Journal Article,"Couto C,Pires P,Valente MT,Bigonha RS,Anquetil N",,Predicting software defects with causality tests,Journal of Systems and Software,2014,93.0,,24-41,,,2014,,0164-1212,https://www.sciencedirect.com/science/article/pii/S0164121214000351;http://dx.doi.org/10.1016/j.jss.2014.01.033,10.1016/j.jss.2014.01.033,"In this paper, we propose a defect prediction approach centered on more robust evidences towards causality between source code metrics (as predictors) and the occurrence of defects. More specifically, we rely on the Granger causality test to evaluate whether past variations in source code metrics values can be used to forecast changes in time series of defects. Our approach triggers alarms when changes made to the source code of a target system have a high chance of producing defects. We evaluated our approach in several life stages of four Java-based systems. We reached an average precision greater than 50% in three out of the four systems we evaluated. Moreover, by comparing our approach with baselines that are not based on causality tests, it achieved a better precision.","Defect prediction, Causality, Granger test",,,,
Journal Article,"Abid C,Kessentini M,Wang H",,"Early prediction of quality of service using interface-level metrics, code-level metrics, and antipatterns",Information and Software Technology,2020,126.0,,106313,,,2020,,0950-5849,https://www.sciencedirect.com/science/article/pii/S0950584920300653;http://dx.doi.org/10.1016/j.infsof.2020.106313,10.1016/j.infsof.2020.106313,"Context: With the current high trends of deploying and using web services in practice, effective techniques for maintaining high quality of Service are becoming critical for both service providers and subscribers/users. Service providers want to predict the quality of service during early stages of development before releasing them to customers. Service clients consider the quality of service when selecting the best one satisfying their preferences in terms of price/budget and quality between the services offering the same features. The majority of existing studies for the prediction of quality of service are based on clustering algorithms to classify a set of services based on their collected quality attributes. Then, the user can select the best service based on his expectations both in terms of quality and features. However, this assumption requires the deployment of the services before being able to make the prediction and it can be time-consuming to collect the required data of running web services during a period of time. Furthermore, the clustering is only based on well-known quality attributes related to the services performance after deployment. Objective: In this paper, we start from the hypothesis that the quality of the source code and interface design can be used as indicators to predict the quality of service attributes without the need to deploy or run the services by the subscribers. Method: We collected training data of 707 web services and we used machine learning to generate association rules that predict the quality of service based on the interface and code quality metrics, and antipatterns. Results: The empirical validation of our prediction techniques shows that the generated association rules have strong support and high confidence which confirms our hypothesis that source code and interface quality metrics/antipatterns are correlated with web service quality attributes which are response time, availability, throughput, successability, reliability, compliance, best practices, latency, and documentation. Conclusion: To the best of our knowledge, this paper represents the first study to validate the correlation between interface metrics, source code metrics, antipatterns and quality of service. Another contribution of our work consists of generating association rules between the code/interface metrics and quality of service that can be used for prediction purposes before deploying new releases.","Quality of service, Web services, Interface metrics, Code quality, Performance prediction, Anti-patterns",,,,
Journal Article,"Perkusich M,Chaves e Silva L,Costa A,Ramos F,Saraiva R,Freire A,Dilorenzo E,Dantas E,Santos D,Gorgônio K,Almeida H,Perkusich A",,Intelligent software engineering in the context of agile software development: A systematic literature review,Information and Software Technology,2020,119.0,,106241,,,2020,,0950-5849,https://www.sciencedirect.com/science/article/pii/S0950584919302587;http://dx.doi.org/10.1016/j.infsof.2019.106241,10.1016/j.infsof.2019.106241,"CONTEXT: Intelligent Software Engineering (ISE) refers to the application of intelligent techniques to software engineering. We define an “intelligent technique” as a technique that explores data (from digital artifacts or domain experts) for knowledge discovery, reasoning, learning, planning, natural language processing, perception or supporting decision-making. OBJECTIVE: The purpose of this study is to synthesize and analyze the state of the art of the field of applying intelligent techniques to Agile Software Development (ASD). Furthermore, we assess its maturity and identify adoption risks. METHOD: Using a systematic literature review, we identified 104 primary studies, resulting in 93 unique studies. RESULTS: We identified that there is a positive trend in the number of studies applying intelligent techniques to ASD. Also, we determined that reasoning under uncertainty (mainly, Bayesian network), search-based solutions, and machine learning are the most popular intelligent techniques in the context of ASD. In terms of purposes, the most popular ones are effort estimation, requirements prioritization, resource allocation, requirements selection, and requirements management. Furthermore, we discovered that the primary goal of applying intelligent techniques is to support decision making. As a consequence, the adoption risks in terms of the safety of the current solutions are low. Finally, we highlight the trend of using explainable intelligent techniques. CONCLUSION: Overall, although the topic area is up-and-coming, for many areas of application, it is still in its infancy. So, this means that there is a need for more empirical studies, and there are a plethora of new opportunities for researchers.","Intelligent software engineering, Agile software development, Search-based software engineering, Machine learning, Bayesian networks, Artificial intelligence",,,,
Journal Article,"Bafandeh Mayvan B,Rasoolzadegan A,Ghavidel Yazdi Z",,The state of the art on design patterns: A systematic mapping of the literature,Journal of Systems and Software,2017,125.0,,93-118,,,2017,,0164-1212,https://www.sciencedirect.com/science/article/pii/S0164121216302321;http://dx.doi.org/10.1016/j.jss.2016.11.030,10.1016/j.jss.2016.11.030,"Design patterns are widely used by software developers to build complex systems. Hence, they have been investigated by many researchers in recent decades. This leads to the emergence of various topics in the design patterns field. The objective of this paper is to present an overview of the research efforts on design patterns for those researchers who seek to enter this area. The main contributions are as follows: (a) identifying research topics in design patterns, (b) quantifying the research emphasis on each topic, and (c) describing the demographics of design patterns research. The last secondary study with similar goals in the design patterns field considers the Gang of Four design patterns only. However, the scope of the current study is all of the design patterns. Moreover, our review covers about six additional years and a larger number of publications and venues. In this systematic mapping study, a total of 2775 papers were identified as relevant, and 637 of them were included. According to the results, design patterns can be classified into six different research topics. As a consequence, it is concluded that Pattern Development, Pattern Mining, and Pattern Usage are the most active topics in the field of design patterns.","Design patterns, Systematic mapping study, Systematic review",,,,
Journal Article,"Mensah S,Keung J,Svajlenko J,Bennin KE,Mi Q",,On the value of a prioritization scheme for resolving Self-admitted technical debt,Journal of Systems and Software,2018,135.0,,37-54,,,2018,,0164-1212,https://www.sciencedirect.com/science/article/pii/S0164121217302133;http://dx.doi.org/10.1016/j.jss.2017.09.026,10.1016/j.jss.2017.09.026,"Programmers tend to leave incomplete, temporary workarounds and buggy codes that require rework in software development and such pitfall is referred to as Self-admitted Technical Debt (SATD). Previous studies have shown that SATD negatively affects software project and incurs high maintenance overheads. In this study, we introduce a prioritization scheme comprising mainly of identification, examination and rework effort estimation of prioritized tasks in order to make a final decision prior to software release. Using the proposed prioritization scheme, we perform an exploratory analysis on four open source projects to investigate how SATD can be minimized. Four prominent causes of SATD are identified, namely code smells (23.2%), complicated and complex tasks (22.0%), inadequate code testing (21.2%) and unexpected code performance (17.4%). Results show that, among all the types of SATD, design debts on average are highly prone to software bugs across the four projects analysed. Our findings show that a rework effort of approximately 10 to 25 commented LOC per SATD source file is needed to address the highly prioritized SATD (vital few) tasks. The proposed prioritization scheme is a novel technique that will aid in decision making prior to software release in an attempt to minimize high maintenance overheads.","Self-admitted technical debt, Prioritization scheme, Textual indicators, Source code comment, Open source projects",,,,
Journal Article,"Garousi V,Küçük B",,Smells in software test code: A survey of knowledge in industry and academia,Journal of Systems and Software,2018,138.0,,52-81,,,2018,,0164-1212,https://www.sciencedirect.com/science/article/pii/S0164121217303060;http://dx.doi.org/10.1016/j.jss.2017.12.013,10.1016/j.jss.2017.12.013,"As a type of anti-pattern, test smells are defined as poorly designed tests and their presence may negatively affect the quality of test suites and production code. Test smells are the subject of active discussions among practitioners and researchers, and various guidelines to handle smells are constantly offered for smell prevention, smell detection, and smell correction. Since there is a vast grey literature as well as a large body of research studies in this domain, it is not practical for practitioners and researchers to locate and synthesize such a large literature. Motivated by the above need and to find out what we, as the community, know about smells in test code, we conducted a ‘multivocal’ literature mapping (classification) on both the scientific literature and also practitioners’ grey literature. By surveying all the sources on test smells in both industry (120 sources) and academia (46 sources), 166 sources in total, our review presents the largest catalogue of test smells, along with the summary of guidelines/techniques and the tools to deal with those smells. This article aims to benefit the readers (both practitioners and researchers) by serving as an “index” to the vast body of knowledge in this important area, and by helping them develop high-quality test scripts, and minimize occurrences of test smells and their negative consequences in large test automation projects.","Software testing, Automated testing, Test automation, Test scripts, Test smells, Test anti-patterns, Multivocal literature mapping, Survey, Systematic mapping",,,,
Journal Article,"Pandey SK,Mishra RB,Tripathi AK",,BPDET: An effective software bug prediction model using deep representation and ensemble learning techniques,Expert Systems with Applications,2020,144.0,,113085,,,2020,,0957-4174,https://www.sciencedirect.com/science/article/pii/S0957417419308024;http://dx.doi.org/10.1016/j.eswa.2019.113085,10.1016/j.eswa.2019.113085,"In software fault prediction systems, there are many hindrances for detecting faulty modules, such as missing values or samples, data redundancy, irrelevance features, and correlation. Many researchers have built a software bug prediction (SBP) model, which classify faulty and non-faulty module which are associated with software metrics. Till now very few works has been done which addresses the class imbalance problem in SBP. The main objective of this paper is to reveal the favorable result by feature selection and machine learning methods to detect defective and non-defective software modules. We propose a rudimentary classification based framework Bug Prediction using Deep representation and Ensemble learning (BPDET) techniques for SBP. It combinedly applies by ensemble learning (EL) and deep representation(DR). The software metrics which are used for SBP are mostly conventional. Staked denoising auto-encoder (SDA) is used for the deep representation of software metrics, which is a robust feature learning method. Propose model is mainly divided into two stages: deep learning stage and two layers of EL stage (TEL). The extraction of the feature from SDA in the very first step of the model then applied TEL in the second stage. TEL is also dealing with the class imbalance problem. The experiment mainly performed NASA (12) datasets, to reveal the efficiency of DR, SDA, and TEL. The performance is analyzed in terms of Mathew co-relation coefficient (MCC), the area under the curve (AUC), precision-recall area (PRC), F-measure and Time. Out of 12 dataset MCC values over 11 datasets, ROC values over 6 datasets, PRC values overall 12 datasets and F-measure over 8 datasets surpass the existing state of the art bug prediction methods. We have tested BPDET using Wilcoxon rank sum test which rejects the null hypothesis at α = 0.025. We have also tested the stability of the model over 5, 8, 10, 12, and 15 fold cross-validation and got similar results. Finally, we conclude that BPDET is a stable and outperformed on most of the datasets compared with EL and another state of the art techniques.","Software bug prediction, Classification technique, Software metrics, Deep representation, Boosting, Staked denoising auto-encoder, Heterogeneous Ensemble learning technique",,,,
Journal Article,"Barbez A,Khomh F,Guéhéneuc YG",,A machine-learning based ensemble method for anti-patterns detection,Journal of Systems and Software,2020,161.0,,110486,,,2020,,0164-1212,https://www.sciencedirect.com/science/article/pii/S0164121219302602;http://dx.doi.org/10.1016/j.jss.2019.110486,10.1016/j.jss.2019.110486,"Anti-patterns are poor solutions to recurring design problems. Several empirical studies have highlighted their negative impact on program comprehension, maintainability, as well as fault-proneness. A variety of detection approaches have been proposed to identify their occurrences in source code. However, these approaches can identify only a subset of the occurrences and report large numbers of false positives and misses. Furthermore, a low agreement is generally observed among different approaches. Recent studies have shown the potential of machine-learning models to improve this situation. However, such algorithms require large sets of manually-produced training-data, which often limits their application in practice. In this paper, we present SMAD (SMart Aggregation of Anti-patterns Detectors), a machine-learning based ensemble method to aggregate various anti-patterns detection approaches on the basis of their internal detection rules. Thus, our method uses several detection tools to produce an improved prediction from a reasonable number of training examples. We implemented SMAD for the detection of two well known anti-patterns: God Class and Feature Envy. With the results of our experiments conducted on eight java projects, we show that: (1) Our method clearly improves the so aggregated tools; (2) SMAD significantly outperforms other ensemble methods.","Software quality, Anti-patterns, Machine learning, Ensemble methods",,,,
Journal Article,"Azeem MI,Palomba F,Shi L,Wang Q",,Machine learning techniques for code smell detection: A systematic literature review and meta-analysis,Information and Software Technology,2019,108.0,,115-138,,,2019,,0950-5849,https://www.sciencedirect.com/science/article/pii/S0950584918302623;http://dx.doi.org/10.1016/j.infsof.2018.12.009,10.1016/j.infsof.2018.12.009,"Background: Code smells indicate suboptimal design or implementation choices in the source code that often lead it to be more change- and fault-prone. Researchers defined dozens of code smell detectors, which exploit different sources of information to support developers when diagnosing design flaws. Despite their good accuracy, previous work pointed out three important limitations that might preclude the use of code smell detectors in practice: (i) subjectiveness of developers with respect to code smells detected by such tools, (ii) scarce agreement between different detectors, and (iii) difficulties in finding good thresholds to be used for detection. To overcome these limitations, the use of machine learning techniques represents an ever increasing research area. Objective: While the research community carefully studied the methodologies applied by researchers when defining heuristic-based code smell detectors, there is still a noticeable lack of knowledge on how machine learning approaches have been adopted for code smell detection and whether there are points of improvement to allow a better detection of code smells. Our goal is to provide an overview and discuss the usage of machine learning approaches in the field of code smells. Method: This paper presents a Systematic Literature Review (SLR) on Machine Learning Techniques for Code Smell Detection. Our work considers papers published between 2000 and 2017. Starting from an initial set of 2456 papers, we found that 15 of them actually adopted machine learning approaches. We studied them under four different perspectives: (i) code smells considered, (ii) setup of machine learning approaches, (iii) design of the evaluation strategies, and (iv) a meta-analysis on the performance achieved by the models proposed so far. Results: The analyses performed show that God Class, Long Method, Functional Decomposition, and Spaghetti Code have been heavily considered in the literature. Decision Trees and Support Vector Machines are the most commonly used machine learning algorithms for code smell detection. Models based on a large set of independent variables have performed well. JRip and Random Forest are the most effective classifiers in terms of performance. The analyses also reveal the existence of several open issues and challenges that the research community should focus on in the future. Conclusion: Based on our findings, we argue that there is still room for the improvement of machine learning techniques in the context of code smell detection. The open issues emerged in this study can represent the input for researchers interested in developing more powerful techniques.","Code smells, Machine learning, Systematic literature review",,,,
Journal Article,"Politowski C,Khomh F,Romano S,Scanniello G,Petrillo F,Guéhéneuc YG,Maiga A",,A large scale empirical study of the impact of Spaghetti Code and Blob anti-patterns on program comprehension,Information and Software Technology,2020,122.0,,106278,,,2020,,0950-5849,https://www.sciencedirect.com/science/article/pii/S0950584920300288;http://dx.doi.org/10.1016/j.infsof.2020.106278,10.1016/j.infsof.2020.106278,"Context Several studies investigated the impact of anti-patterns (i.e., “poor” solutions to recurring design problems) during maintenance activities and reported that anti-patterns significantly affect the developers’ effort required to edit files. However, before developers edit files, they must understand the source code of the systems. This source code must be easy to understand by developers. Objective In this work, we provide a complete assessment of the impact of two instances of two anti-patterns, Blob or Spaghetti Code, on program comprehension. Method We analyze the impact of these two anti-patterns through three empirical studies conducted at Polytechnique Montré al (Canada) with 24 participants; at Carlton University (Canada) with 30 participants; and at University Basilicata (Italy) with 79 participants. Results We collect data from 372 tasks obtained thanks to 133 different participants from the three universities. We use three metrics to assess the developers’ comprehension of the source code: (1) the duration to complete each task; (2) their percentage of correct answers; and, (3) the NASA task load index for their effort. Conclusions We report that, although single occurrences of Blob or Spaghetti code anti-patterns have little effect on code comprehension, two occurrences of either Blob or Spaghetti Code significantly increases the developers’ time spent in their tasks, reduce their percentage of correct answers, and increase their effort. Hence, we recommend that developers act on both anti-patterns, which should be refactored out of the source code whenever possible. We also recommend further studies on combinations of anti-patterns rather than on single anti-patterns one at a time.","Anti-patterns, Blob, Spaghetti Code, Program comprehension, Java",,,,
Journal Article,"de Freitas Farias MA,de Mendonça Neto MG,Kalinowski M,Spínola RO",,Identifying self-admitted technical debt through code comment analysis with a contextualized vocabulary,Information and Software Technology,2020,121.0,,106270,,,2020,,0950-5849,https://www.sciencedirect.com/science/article/pii/S0950584920300203;http://dx.doi.org/10.1016/j.infsof.2020.106270,10.1016/j.infsof.2020.106270,"Context Previous work has shown that one can explore code comments to detect Self-Admitted Technical Debt (SATD) using a contextualized vocabulary. However, current detection strategies still return a large number of false positives items. Moreover, those strategies do not allow the automatic identification of the type of debt of the identified items. Objective This work applies, evaluates, and improves a set of contextualized patterns we built to detect self-admitted technical debt using code comment analysis. We refer to this set of patterns as the self-admitted technical debt identification vocabulary. Method We carry out three empirical studies. Firstly, 23 participants analyze the patterns of a previously defined contextualized vocabulary and register their level of importance in identifying SATD items. Secondly, we perform a qualitative analysis to investigate the relation between each pattern and types of debt. Finally, we perform a feasibility study using a new vocabulary, improved based on the results of the previous empirical studies, to automatically identify self-admitted technical debt items, and types of debt, that exist in three open source projects. Results More than half of the new patterns were considered decisive or very decisive to detect technical debt items. The new vocabulary was able to find items associated to code, design, defect, documentation, and requirement debt. Thus, the result of the work is an improved vocabulary that considers the level of importance of each pattern and the relationship between patterns and debt types to support the identification and classification of SATD items. Conclusion The studies allowed us to improve a vocabulary to identify self-admitted technical debt items through code comments analysis. The results show that the use of pattern-based code comment analysis can contribute to improve existing methods, or create new ones, for automatically identifying and classifying technical debt items.","Technical debt, Self-admitted technical debt, Technical debt identification, Code comment analysis",,,,
Journal Article,"Arcelli Fontana F,Zanoni M",,Code smell severity classification using machine learning techniques,Knowledge-Based Systems,2017,128.0,,43-58,,,2017,,0950-7051,https://www.sciencedirect.com/science/article/pii/S0950705117301880;http://dx.doi.org/10.1016/j.knosys.2017.04.014,10.1016/j.knosys.2017.04.014,"Several code smells detection tools have been developed providing different results, because smells can be subjectively interpreted and hence detected in different ways. Machine learning techniques have been used for different topics in software engineering, e.g., design pattern detection, code smell detection, bug prediction, recommending systems. In this paper, we focus our attention on the classification of code smell severity through the use of machine learning techniques in different experiments. The severity of code smells is an important factor to take into consideration when reporting code smell detection results, since it allows the prioritization of refactoring efforts. In fact, code smells with high severity can be particularly large and complex, and create larger issues to the maintainability of software a system. In our experiments, we apply several machine learning models, spanning from multinomial classification to regression, plus a method to apply binary classifiers for ordinal classification. In fact, we model code smell severity as an ordinal variable. We take the baseline models from previous work, where we applied binary classification models for code smell detection with good results. We report and compare the performance of the models according to their accuracy and four different performance measures used for the evaluation of ordinal classification techniques. From our results, while the accuracy of the classification of severity is not high as in the binary classification of absence or presence of code smells, the ranking correlation of the actual and predicted severity for the best models reaches 0.88–0.96, measured through Spearman’s ρ.","Code smells detection, Machine learning, Code smell severity, Ordinal classification, Refactoring prioritization",,,,
Journal Article,"Lenarduzzi V,Lomio F,Saarimäki N,Taibi D",,Does migrating a monolithic system to microservices decrease the technical debt?,Journal of Systems and Software,2020,169.0,,110710,,,2020,,0164-1212,https://www.sciencedirect.com/science/article/pii/S0164121220301539;http://dx.doi.org/10.1016/j.jss.2020.110710,10.1016/j.jss.2020.110710,"Background: The migration from a monolithic system to microservices requires a deep refactoring of the system. Therefore, such a migration usually has a big economic impact and companies tend to postpone several activities during this process, mainly to speed up the migration itself, but also because of the demand for releasing new features. Objective: We monitored the technical debt of an SME while it migrated from a legacy monolithic system to an ecosystem of microservices. Our goal was to analyze changes in the code technical debt before and after the migration to microservices. Method: We conducted a case study analyzing more than four years of the history of a twelve-year-old project (280K Lines of Code) where two teams extracted five business processes from the monolithic system as microservices. For the study, we first analyzed the technical debt with SonarQube and then performed a qualitative study with company members to understand the perceived quality of the system and the motivation for possibly postponed activities. Results: The migration to microservices helped to reduce the technical debt in the long run. Despite an initial spike in the technical debt due to the development of the new microservice, after a relatively short period of time the technical debt tended to grow slower than in the monolithic system.","Technical debt, Architectural debt, Code quality, Microservices, Refactoring",,,,
Journal Article,"Sotiropolos P,Vassilakis C",,Detection of intermittent faults in software programs through identification of suspicious shared variable access patterns,Journal of Systems and Software,2020,159.0,,110455,,,2020,,0164-1212,https://www.sciencedirect.com/science/article/pii/S0164121219302298;http://dx.doi.org/10.1016/j.jss.2019.110455,10.1016/j.jss.2019.110455,"Intermittent faults are a very common problem in the software world, while difficult to be debugged. Most of the existing approaches though assume that suitable instrumentation has been provided in the program, typically in the form of assertions that dictate which program states are considered to be erroneous. In this paper we propose a method that can be used to detect probable sources of intermittent faults within a program. Our method proposes certain points in the code, whose data interdependencies combined with their execution interweaving indicate that they could be the cause of intermittent faults. It is the responsibility of the user to accept or reject these proposals. An advantage of this method is that it removes the need for having predefined assertion points in the code, being able to detect potential sources of intermittent faults in the whole bulk of the code, with no instrumentation requirements on the side of the programmer. The proposed approach exploits information from the dynamic behavior of the program. In comparison with parser-based approaches which analyze only the program structure, our approach is immutable to language term changes and in general is not depending on any user-provided assertions or configuration.","Intermittent faults, Fault detection, Shared variables, Model-based checking",,,,
Journal Article,"Grano G,Palomba F,Di Nucci D,De Lucia A,Gall HC",,Scented since the beginning: On the diffuseness of test smells in automatically generated test code,Journal of Systems and Software,2019,156.0,,312-327,,,2019,,0164-1212,https://www.sciencedirect.com/science/article/pii/S0164121219301487;http://dx.doi.org/10.1016/j.jss.2019.07.016,10.1016/j.jss.2019.07.016,"Software testing represents a key software engineering practice to ensure source code quality and reliability. To support developers in this activity and reduce testing effort, several automated unit test generation tools have been proposed. Most of these approaches have the main goal of covering as more branches as possible. While these approaches have good performance, little is still known on the maintainability of the test code they produce, i.e.,whether the generated tests have a good code quality and if they do not possibly introduce issues threatening their effectiveness. To bridge this gap, in this paper we study to what extent existing automated test case generation tools produce potentially problematic test code. We consider seven test smells, i.e.,suboptimal design choices applied by programmers during the development of test cases, as measure of code quality of the generated tests, and evaluate their diffuseness in the unit test classes automatically generated by three state-of-the-art tools such as Randoop, JTExpert, and Evosuite. Moreover, we investigate whether there are characteristics of test and production code influencing the generation of smelly tests. Our study shows that all the considered tools tend to generate a high quantity of two specific test smell types, i.e.,Assertion Roulette and Eager Test, which are those that previous studies showed to negatively impact the reliability of production code. We also discover that test size is correlated with the generation of smelly tests. Based on our findings, we argue that more effective automated generation algorithms that explicitly take into account test code quality should be further investigated and devised.","Test smells, Test case generation, Software quality, Empirical studies",,,,
Book Chapter,Baker DJ,"Lambert CG,Baker DJ,Patrinos GP","Chapter 11 - Artificial Intelligence: The Future Landscape of Genomic Medical Diagnosis: Dataset, In Silico Artificial Intelligent Clinical Information, and Machine Learning Systems",,2018,,,223-267,Academic Press,Human Genome Informatics,2018,9780128094143.0,,https://www.sciencedirect.com/science/article/pii/B9780128094143000115;http://dx.doi.org/10.1016/B978-0-12-809414-3.00011-5,10.1016/B978-0-12-809414-3.00011-5,"Integrated complex systems play a vital role in modern medicine and are providing formidable tools for clinical decision making. Artificial intelligence or AI is a vast undertaking combining learning explored by nonfiction, fiction, myth, and philosophy since time immemorial. This chapter will look at the real-time integration of complex systems using state-of-the-art techniques in data/metadata analysis, machine learning, and artificial intelligence for predictive medical responses within the clinical genomic space. In particular, the Magrathea platform will be used as a paradigm for artificial intelligence solutions for genomics. The author will also look at the impact these new technologies are having on the ability to collaborate and share findings within the wider global research community, the walling of these important global resources, and how this will dramatically shift future prospects for international disease prevention.","AI, Artificial intelligences, Algorithms, Complex systems, Diagnostics, Emergence, Epoch, Genomic databases, Genomic variants, ICD, Metadata, Machine learning, Next-generation sequencing, Rare disease",,,,
Journal Article,"Herremans D,Weisser S,Sörensen K,Conklin D",,Generating structured music for bagana using quality metrics based on Markov models,Expert Systems with Applications,2015,42.0,21.0,7424-7435,,,2015,,0957-4174,https://www.sciencedirect.com/science/article/pii/S0957417415003796;http://dx.doi.org/10.1016/j.eswa.2015.05.043,10.1016/j.eswa.2015.05.043,"In this research, a system is built that generates bagana music, a traditional lyre from Ethiopia, based on a first order Markov model. Due to the size of many datasets it is often only possible to get rich and reliable statistics for low order models, yet these do not handle structure very well and their output is often very repetitive. A first contribution of this paper is to propose a method that allows the enforcement of structure and repetition within music, thus handling long term coherence with a first order model. The second goal of this research is to explain and propose different ways in which low order Markov models can be used to build quality assessment metrics for an optimization algorithm. These are then implemented in a variable neighborhood search algorithm that generates bagana music. The results are examined and thoroughly evaluated.","Markov models, Markov processes, Metaheuristics, Music, Bagana, Computer aided composition (CAC), Variable neighborhood search (VNS), Combinatorial optimization",,,,
Journal Article,"Khomh F,Vaucher S,Guéhéneuc YG,Sahraoui H",,BDTEX: A GQM-based Bayesian approach for the detection of antipatterns,Journal of Systems and Software,2011,84.0,4.0,559-572,,,2011,,0164-1212,https://www.sciencedirect.com/science/article/pii/S0164121210003225;http://dx.doi.org/10.1016/j.jss.2010.11.921,10.1016/j.jss.2010.11.921,"The presence of antipatterns can have a negative impact on the quality of a program. Consequently, their efficient detection has drawn the attention of both researchers and practitioners. However, most aspects of antipatterns are loosely specified because quality assessment is ultimately a human-centric process that requires contextual data. Consequently, there is always a degree of uncertainty on whether a class in a program is an antipattern or not. None of the existing automatic detection approaches handle the inherent uncertainty of the detection process. First, we present BDTEX (Bayesian Detection Expert), a Goal Question Metric (GQM) based approach to build Bayesian Belief Networks (BBNs) from the definitions of antipatterns. We discuss the advantages of BBNs over rule-based models and illustrate BDTEX on the Blob antipattern. Second, we validate BDTEX with three antipatterns: Blob, Functional Decomposition, and Spaghetti code, and two open-source programs: GanttProject v1.10.2 and Xerces v2.7.0. We also compare the results of BDTEX with those of another approach, DECOR, in terms of precision, recall, and utility. Finally, we also show the applicability of our approach in an industrial context using Eclipse JDT and JHotDraw and introduce a novel classification of antipatterns depending on the effort needed to map their definitions to automatic detection approaches.","Code smells, Antipatterns, Detection",The Ninth International Conference on Quality Software,,,
Journal Article,"Tsoukalas D,Kehagias D,Siavvas M,Chatzigeorgiou A",,Technical debt forecasting: An empirical study on open-source repositories,Journal of Systems and Software,2020,170.0,,110777,,,2020,,0164-1212,https://www.sciencedirect.com/science/article/pii/S0164121220301904;http://dx.doi.org/10.1016/j.jss.2020.110777,10.1016/j.jss.2020.110777,"Technical debt (TD) is commonly used to indicate additional costs caused by quality compromises that can yield short-term benefits in the software development process, but may negatively affect the long-term quality of software products. Predicting the future value of TD could facilitate decision-making tasks regarding software maintenance and assist developers and project managers in taking proactive actions regarding TD repayment. However, no notable contributions exist in the field of TD forecasting, indicating that it is a scarcely investigated field. To this end, in the present paper, we empirically evaluate the ability of machine learning (ML) methods to model and predict TD evolution. More specifically, an extensive study is conducted, based on a dataset that we constructed by obtaining weekly snapshots of fifteen open source software projects over three years and using two popular static analysis tools to extract software-related metrics that can act as TD predictors. Subsequently, based on the identified TD predictors, a set of TD forecasting models are produced using popular ML algorithms and validated for various forecasting horizons. The results of our analysis indicate that linear Regularization models are able to fit and provide meaningful forecasts of TD evolution for shorter forecasting horizons, while the non-linear Random Forest regression performs better than the linear models for longer forecasting horizons. In most of the cases, the future TD value is captured with a sufficient level of accuracy. These models can be used to facilitate planning for software evolution budget and time allocation. The approach presented in this paper provides a basis for predictive TD analysis, suitable for projects with a relatively long history. To the best of our knowledge, this is the first study that investigates the feasibility of using ML models for forecasting TD.","Technical debt, Technical debt forecasting, Machine learning, Empirical study",,,,
Journal Article,"Trubiani C,Bran A,van Hoorn A,Avritzer A,Knoche H",,Exploiting load testing and profiling for Performance Antipattern Detection,Information and Software Technology,2018,95.0,,329-345,,,2018,,0950-5849,https://www.sciencedirect.com/science/article/pii/S0950584917302276;http://dx.doi.org/10.1016/j.infsof.2017.11.016,10.1016/j.infsof.2017.11.016,"Context: The performance assessment of complex software systems is not a trivial task since it depends on the design, code, and execution environment. All these factors may affect the system quality and generate negative consequences, such as delays and system failures. The identification of bad practices leading to performance flaws is of key relevance to avoid expensive rework in redesign, reimplementation, and redeployment. Objective: The goal of this manuscript is to provide a systematic process, based on load testing and profiling data, to identify performance issues with runtime data. These performance issues represent an important source of knowledge as they are used to trigger the software refactoring process. Software characteristics and performance measurements are matched with well-known performance antipatterns to document common performance issues and their solutions. Method: We execute load testing based on the characteristics of collected operational profile, thus to produce representative workloads. Performance data from the system under test is collected using a profiler tool to create profiler snapshots and get performance hotspot reports. From such data, performance issues are identified and matched with the specification of antipatterns. Software refactorings are then applied to solve these performance antipatterns. Results: The approach has been applied to a real-world industrial case study and to a representative laboratory study. Experimental results demonstrate the effectiveness of our tool-supported approach that is able to automatically detect two performance antipatterns by exploiting the knowledge of domain experts. In addition, the software refactoring process achieves a significant performance gain at the operational stage in both case studies. Conclusion: Performance antipatterns can be used to effectively support the identification of performance issues from load testing and profiling data. The detection process triggers an antipattern-based software refactoring that in our two case studies results in a substantial performance improvement.","Software performance engineering, Software performance antipatterns, Empirical data, Load testing and profiling",,,,
Journal Article,"Almarimi N,Ouni A,Mkaouer MW",,Learning to detect community smells in open source software projects,Knowledge-Based Systems,2020,204.0,,106201,,,2020,,0950-7051,https://www.sciencedirect.com/science/article/pii/S0950705120304226;http://dx.doi.org/10.1016/j.knosys.2020.106201,10.1016/j.knosys.2020.106201,"Community smells are symptoms of organizational and social issues within the software development community that often lead to additional project costs. Recent studies identified a variety of community smells and defined them as sub-optimal patterns connected to organizational-social structures in the software development community. To early detect and discover existence of potential community smells in a software project, we introduce, in this paper, a novel machine learning-based detection approach, named csDetector, that learns from various existing bad community development practices to provide automated support in detecting such community smells. In particular, our approach learns from a set of organizational-social symptoms that characterize the existence of potential instances of community smells in a software project. We built a detection model using Decision Tree by adopting the C4.5 classifier to identify eight commonly occurring community smells in software projects. To evaluate the performance of our approach, we conduct an empirical study on a benchmark of 74 open source projects from Github. Our statistical results show a high performance of csDetector, achieving an average accuracy of 96% and AUC of 0.94. Moreover, our results indicate that the csDetector outperforms two recent state-of-the-art techniques in terms of detection accuracy. Finally, we investigate the most influential community-related metrics to identify each community smell type. We found that the number of commits and developers per time zone, the number of developers per community, and the social network betweenness and closeness centrality are the most influential community characteristics.","Community smells detection, Social debt, Socio-technical metrics, Machine learning",,,,
Journal Article,"Fregnan E,Baum T,Palomba F,Bacchelli A",,A survey on software coupling relations and tools,Information and Software Technology,2019,107.0,,159-178,,,2019,,0950-5849,https://www.sciencedirect.com/science/article/pii/S0950584918302441;http://dx.doi.org/10.1016/j.infsof.2018.11.008,10.1016/j.infsof.2018.11.008,"Context Coupling relations reflect the dependencies between software entities and can be used to assess the quality of a program. For this reason, a vast amount of them has been developed, together with tools to compute their related metrics. However, this makes the coupling measures suitable for a given application challenging to find. Goals The first objective of this work is to provide a classification of the different kinds of coupling relations, together with the metrics to measure them. The second consists in presenting an overview of the tools proposed until now by the software engineering academic community to extract these metrics. Method This work constitutes a systematic literature review in software engineering. To retrieve the referenced publications, publicly available scientific research databases were used. These sources were queried using keywords inherent to software coupling. We included publications from the period 2002 to 2017 and highly cited earlier publications. A snowballing technique was used to retrieve further related material. Results Four groups of coupling relations were found: structural, dynamic, semantic and logical. A fifth set of coupling relations includes approaches too recent to be considered an independent group and measures developed for specific environments. The investigation also retrieved tools that extract the metrics belonging to each coupling group. Conclusion This study shows the directions followed by the research on software coupling: e.g., developing metrics for specific environments. Concerning the metric tools, three trends have emerged in recent years: use of visualization techniques, extensibility and scalability. Finally, some coupling metrics applications were presented (e.g., code smell detection), indicating possible future research directions. Public preprint [https://doi.org/10.5281/zenodo.2002001].","Software engineering, Coupling relations, Software metrics",,,,
Journal Article,"Arcelli Fontana F,Lenarduzzi V,Roveda R,Taibi D",,Are architectural smells independent from code smells? An empirical study,Journal of Systems and Software,2019,154.0,,139-156,,,2019,,0164-1212,https://www.sciencedirect.com/science/article/pii/S0164121219301013;http://dx.doi.org/10.1016/j.jss.2019.04.066,10.1016/j.jss.2019.04.066,"Background. Architectural smells and code smells are symptoms of bad code or design that can cause different quality problems, such as faults, technical debt, or difficulties with maintenance and evolution. Some studies show that code smells and architectural smells often appear together in the same file. The correlation between code smells and architectural smells, however, is not clear yet; some studies on a limited set of projects have claimed that architectural smells can be derived from code smells, while other studies claim the opposite. Objective. The goal of this work is to understand whether architectural smells are independent from code smells or can be derived from a code smell or from one category of them. Method. We conducted a case study analyzing the correlations among 19 code smells, six categories of code smells, and four architectural smells. Results. The results show that architectural smells are correlated with code smells only in a very low number of occurrences and therefore cannot be derived from code smells. Conclusion. Architectural smells are independent from code smells, and therefore deserve special attention by researchers, who should investigate their actual harmfulness, and practitioners, who should consider whether and when to remove them.","Code smells, Architectural smells, Technical debt, Empirical analysis",,,,
Journal Article,"Hegedűs P,Kádár I,Ferenc R,Gyimóthy T",,Empirical evaluation of software maintainability based on a manually validated refactoring dataset,Information and Software Technology,2018,95.0,,313-327,,,2018,,0950-5849,https://www.sciencedirect.com/science/article/pii/S0950584916303561;http://dx.doi.org/10.1016/j.infsof.2017.11.012,10.1016/j.infsof.2017.11.012,"Context Refactoring is a technique for improving the internal structure of software systems. It has a solid theoretical background while being used in development practice also. However, we lack empirical research results on the real effect of code refactoring and its application. Objective This paper presents a manually validated subset of a previously published dataset containing the refactorings extracted by the RefFinder tool, code metrics, and maintainability of 7 open-source systems. We found that RefFinder had around 27% overall average precision on the subject systems, thus our manually validated subset has substantial added value. Using the dataset, we studied several aspects of the refactored and non-refactored source code elements (classes and methods), like the differences in their maintainability and source code metrics. Method We divided the source code elements into a group containing the refactored elements and a group with non-refactored elements. We analyzed the elements’ characteristics in these groups using correlation analysis, Mann–Whitney U test and effect size measures. Results Source code elements subjected to refactorings had significantly lower maintainability than elements not affected by refactorings. Moreover, refactored elements had significantly higher size related metrics, complexity, and coupling. Also these metrics changed more significantly in the refactored elements. The results are mostly in line with our previous findings on the not validated dataset, with the difference that clone metrics had no strong connection with refactoring. Conclusions Compared to the preliminary analysis using a not validated dataset, the manually validated dataset led to more significant results, which suggests that developers find targets for refactorings based on some internal quality properties of the source code, like their size, complexity or coupling, but not clone related metrics as reported in our previous studies. They do not just use these properties for identifying targets, but also control them with refactorings.","Code refactoring, Manually validated empirical dataset, Source code metrics, Software maintainability, Empirical study",,,,
Journal Article,"Santos JA,Rocha-Junior JB,Prates LC,do Nascimento RS,Freitas MF,de Mendonça MG",,A systematic review on the code smell effect,Journal of Systems and Software,2018,144.0,,450-477,,,2018,,0164-1212,https://www.sciencedirect.com/science/article/pii/S0164121218301444;http://dx.doi.org/10.1016/j.jss.2018.07.035,10.1016/j.jss.2018.07.035,"Context: Code smell is a term commonly used to describe potential problems in the design of software. The concept is well accepted by the software engineering community. However, some studies have presented divergent findings about the usefulness of the smell concept as a tool to support software development tasks. The reasons of these divergences have not been considered because the studies are presented independently. Objective: To synthesize current knowledge related to the usefulness of the smell concept. We focused on empirical studies investigating how smells impact the software development, the code smell effect. Method: A systematic review about the smell effect is carried out. We grouped the primary studies findings in a thematic map. Result: The smell concept does not support the evaluation of quality design in practice activities of software development. There is no strong evidence correlating smells and some important software development attributes, such as effort in maintenance. Moreover, the studies point out that human agreement on smell detection is low. Conclusion: In order to improve analysis on the subject, the area needs to better outline: (i) factors affecting human evaluation of smells; and (ii) a classification of types of smells, grouping them according to relevant characteristics.","Code smell, Systematic review, Thematic synthesis",,,,
Journal Article,"Yang W,Pan M,Zhou Y,Huang Z",,Developer portraying: A quick approach to understanding developers on OSS platforms,Information and Software Technology,2020,125.0,,106336,,,2020,,0950-5849,https://www.sciencedirect.com/science/article/pii/S0950584920301038;http://dx.doi.org/10.1016/j.infsof.2020.106336,10.1016/j.infsof.2020.106336,"Context Millions of software developers are using open-source software (OSS) platforms to host their code and collaborate with each other. They possess different programming skills, styles, and preferences, etc., and it is important to understand them for making collaborative decisions such as programming task assignment. Existing OSS platforms do not provide sufficient information about developers, and we need to spend significant effort in searching the OSS platforms for such information. Objective Different than the basic developer information displayed on OSS platforms, we propose portraying developers as a quick approach for characterizing and understanding them. We discuss how to build developer portraits to make them concise yet informative. Method We propose a multi-dimensional developer portrait model to specify the attributes of various aspects concerning software development about developers. Then, a method that leverages text analysis, web data analysis, and code analysis techniques is presented to analyze a developer’s various sources of data on OSS platforms for constructing the portrait. Results The constructed portraits can be vividly displayed on the web to help people quickly understand developers and make better decisions during collaborative software development. Case studies on two representative problems in the software engineering area—code recommendation and programming task assignment—are conducted, and the results show the improvement in recommendation and the potential for proper assignments when using our portraits. Conclusion The developer portrait is an effective form to characterize developers. It can help people quickly understand the developers and can be applied to various applications in the software development process.","Developer portraits, OSS Platforms, Developer characterization, Collaborative software development, Code analysis",,,,
Journal Article,"Conejero JM,Rodríguez-Echeverría R,Hernández J,Clemente PJ,Ortiz-Caraballo C,Jurado E,Sánchez-Figueroa F",,Early evaluation of technical debt impact on maintainability,Journal of Systems and Software,2018,142.0,,92-114,,,2018,,0164-1212,https://www.sciencedirect.com/science/article/pii/S0164121218300736;http://dx.doi.org/10.1016/j.jss.2018.04.035,10.1016/j.jss.2018.04.035,"It is widely claimed that Technical Debt is related to quality problems being often produced by poor processes, lack of verification or basic incompetence. Several techniques have been proposed to detect Technical Debt in source code, as identification of modularity violations, code smells or grime buildups. These approaches have been used to empirically demonstrate the relation among Technical Debt indicators and quality harms. However, these works are mainly focused on programming level, when the system has already been implemented. There may also be sources of Technical Debt in non-code artifacts, e.g. requirements, and its identification may provide important information to move refactoring efforts to previous stages and reduce future Technical Debt interest. This paper presents an empirical study to evaluate whether modularity anomalies at requirements level are directly related to maintainability attributes affecting systems quality and increasing, thus, system's interest. The study relies on a framework that allows the identification of modularity anomalies and its quantification by using modularity metrics. Maintainability metrics are also used to assess dynamic maintainability properties. The results obtained by both sets of metrics are pairwise compared to check whether the more modularity anomalies the system presents, the less stable and more difficult to maintain it is.","Technical Debt indicator, Requirements, Modularity anomalies, Maintainability, Empirical evaluation",,,,
Journal Article,"Garg R,Singh RK",,Analysis and Prioritization of Design Metrics,Procedia Computer Science,2020,167.0,,1495-1504,,,2020,,1877-0509,https://www.sciencedirect.com/science/article/pii/S1877050920308267;http://dx.doi.org/10.1016/j.procs.2020.03.360,10.1016/j.procs.2020.03.360,"Clones at domain level during the design phase plays an important role to identify the risk at an early stage. The authors have analyzed the Design Metrics (DM) for its contribution towards the structural properties in domain models of the software. Then prioritization of these DM leads to classification of the majorly contributing DM elements. These Design Metrics relates to classes within the class diagram at design level that is useful for the identification of cross cutting concerns, origin analysis of software, software quality prediction and maintenance at the class level granularity. The authors have empirically evaluated the results for structural model clone detection.","Software Clones, Model clones, Model Driven Development, Software Maintenance",International Conference on Computational Intelligence and Data Science,,,
Journal Article,"Bezerra CI,Andrade RM,Monteiro JM",,Exploring quality measures for the evaluation of feature models: a case study,Journal of Systems and Software,2017,131.0,,366-385,,,2017,,0164-1212,https://www.sciencedirect.com/science/article/pii/S0164121216301340;http://dx.doi.org/10.1016/j.jss.2016.07.040,10.1016/j.jss.2016.07.040,"Evaluating the quality of a feature model is essential to ensure that errors in the early stages do not spread throughout the Software Product Line (SPL). One way to evaluate the feature model is to use measures that could be associated with the feature model quality characteristics and their quality attributes. In this paper, we aim at investigating how measures can be applied to the quality assessment of SPL feature models. We performed an exploratory case study using the COfFEE maintainability measures catalog and the S.P.L.O.T. feature models repository. In order to support this case study, we built a dataset (denoted by MAcchiATO) containing the values of 32 measures from COfFEE for 218 software feature models, extracted from S.P.L.O.T. This research approach allowed us to explore three different data analysis techniques. First, we applied the Spearman’s rank correlation coefficient in order to identify relationships between the measures. This analysis showed that not all 32 measures in COfFEE are necessary to reveal the quality of a feature model and just 15 measures could be used. Next, the 32 measures in COfFEE were grouped by applying the Principal Component Analysis and a set of 9 new grouped measures were defined. Finally, we used the Tolerance Interval technique to define statistical thresholds for these 9 new grouped measures. So, our findings suggest that measures can be effectively used to support the quality evaluation of SPL feature models.","Software product line, Quality evaluation, Measures, Feature models",,,,
Journal Article,"Sharma T,Spinellis D",,A survey on software smells,Journal of Systems and Software,2018,138.0,,158-173,,,2018,,0164-1212,https://www.sciencedirect.com/science/article/pii/S0164121217303114;http://dx.doi.org/10.1016/j.jss.2017.12.034,10.1016/j.jss.2017.12.034,"Context Smells in software systems impair software quality and make them hard to maintain and evolve. The software engineering community has explored various dimensions concerning smells and produced extensive research related to smells. The plethora of information poses challenges to the community to comprehend the state-of-the-art tools and techniques. Objective We aim to present the current knowledge related to software smells and identify challenges as well as opportunities in the current practices. Method We explore the definitions of smells, their causes as well as effects, and their detection mechanisms presented in the current literature. We studied 445 primary studies in detail, synthesized the information, and documented our observations. Results The study reveals five possible defining characteristics of smells — indicator, poor solution, violates best-practices, impacts quality, and recurrence. We curate ten common factors that cause smells to occur including lack of skill or awareness and priority to features over quality. We classify existing smell detection methods into five groups — metrics, rules/heuristics, history, machine learning, and optimization-based detection. Challenges in the smells detection include the tools’ proneness to false-positives and poor coverage of smells detectable by existing tools.","Code smells, Software smells, Antipatterns, Software quality, Maintainability, Smell detection tools, Technical debt",,,,
Journal Article,"Krishna R,Menzies T,Layman L",,Less is more: Minimizing code reorganization using XTREE,Information and Software Technology,2017,88.0,,53-66,,,2017,,0950-5849,https://www.sciencedirect.com/science/article/pii/S0950584916301641;http://dx.doi.org/10.1016/j.infsof.2017.03.012,10.1016/j.infsof.2017.03.012,"Context: Developers use bad code smells to guide code reorganization. Yet developers, textbooks, tools, and researchers disagree on which bad smells are important. How can we offer reliable advice to developers about which bad smells to fix? Objective: To evaluate the likelihood that a code reorganization to address bad code smells will yield improvement in the defect-proneness of the code. Method: We introduce XTREE, a framework that analyzes a historical log of defects seen previously in the code and generates a set of useful code changes. Any bad smell that requires changes outside of that set can be deprioritized (since there is no historical evidence that the bad smell causes any problems). Evaluation: We evaluate XTREE’s recommendations for bad smell improvement against recommendations from previous work (Shatnawi, Alves, and Borges) using multiple data sets of code metrics and defect counts. Results: Code modules that are changed in response to XTREE’s recommendations contain significantly fewer defects than recommendations from previous studies. Further, XTREE endorses changes to very few code metrics, so XTREE requires programmers to do less work. Further, XTREE’s recommendations are more responsive to the particulars of different data sets. Finally XTREE’s recommendations may be generalized to identify the most crucial factors affecting multiple datasets (see the last figure in paper). Conclusion: Before undertaking a code reorganization based on a bad smell report, use a framework like XTREE to check and ignore any such operations that are useless; i.e. ones which lack evidence in the historical record that it is useful to make that change. Note that this use case applies to both manual code reorganizations proposed by developers as well as those conducted by automatic methods.","Bad smells, Performance prediction, Decision trees",,,,
Journal Article,"Xuan J,Cornu B,Martinez M,Baudry B,Seinturier L,Monperrus M",,B-Refactoring: Automatic test code refactoring to improve dynamic analysis,Information and Software Technology,2016,76.0,,65-80,,,2016,,0950-5849,https://www.sciencedirect.com/science/article/pii/S0950584916300714;http://dx.doi.org/10.1016/j.infsof.2016.04.016,10.1016/j.infsof.2016.04.016,"Context: Developers design test suites to verify that software meets its expected behaviors. Many dynamic analysis techniques are performed on the exploitation of execution traces from test cases. In practice, one test case may imply various behaviors. However, the execution of a test case only yields one trace, which can hide the others. Objective: In this article, we propose a new technique of test code refactoring, called B-Refactoring. The idea behind B-Refactoring is to split a test case into small test fragments, which cover a simpler part of the control flow to provide better support for dynamic analysis. Method: For a given dynamic analysis technique, B-Refactoring monitors the execution of test cases and constructs small test cases without loss of the testability. We apply B-Refactoring to assist two existing analysis tasks: automatic repair of if-condition bugs and automatic analysis of exception contracts. Results: Experimental results show that B-Refactoring can effectively improve the execution traces of the test suite. Real-world bugs that could not be previously fixed with the original test suites are fixed after applying B-Refactoring; meanwhile, exception contracts are better verified via applying B-Refactoring to original test suites. Conclusions: We conclude that applying B-Refactoring improves the execution traces of test cases for dynamic analysis. This improvement can enhance existing dynamic analysis tasks.",,,,,
Book Chapter,"Palomba F,De Lucia A,Bavota G,Oliveto R",Memon A,"Chapter Four - Anti-Pattern Detection: Methods, Challenges, and Open Issues",,2014,95.0,,201-238,Elsevier,,2014,,0065-2458,https://www.sciencedirect.com/science/article/pii/B9780128001608000048;http://dx.doi.org/10.1016/B978-0-12-800160-8.00004-8,10.1016/B978-0-12-800160-8.00004-8,"Anti-patterns are poor solutions to recurring design problems. They occur in object-oriented systems when developers unwillingly introduce them while designing and implementing the classes of their systems. Several empirical studies have highlighted that anti-patterns have a negative impact on the comprehension and maintainability of a software systems. Consequently, their identification has received recently more attention from both researchers and practitioners who have proposed various approaches to detect them. This chapter discusses on the approaches proposed in the literature. In addition, from the analysis of the state-of-the-art, we will (i) derive a set of guidelines for building and evaluating recommendation systems supporting the detection of anti-patterns; and (ii) discuss some problems that are still open, to trace future research directions in the field. For this reason, the chapter provides a support to both researchers, who are interested in comprehending the results achieved so far in the identification of anti-patterns, and practitioner, who are interested in adopting a tool to identify anti-patterns in their software systems.","Anti-pattern, Code bad smells, Linguistic anti-pattern, Software metrics",,Advances in Computers,,
Journal Article,"Koch P,Hofer B,Wotawa F",,On the refinement of spreadsheet smells by means of structure information,Journal of Systems and Software,2019,147.0,,64-85,,,2019,,0164-1212,https://www.sciencedirect.com/science/article/pii/S016412121830219X;http://dx.doi.org/10.1016/j.jss.2018.09.092,10.1016/j.jss.2018.09.092,"Spreadsheet users are often unaware of the risks imposed by poorly designed spreadsheets. One way to assess spreadsheet quality is to detect smells which attempt to identify parts of spreadsheets that are hard to comprehend or maintain and which are more likely to be the root source of bugs. Unfortunately, current spreadsheet smell detection techniques suffer from a number of drawbacks that lead to incorrect or redundant smell reports. For example, the same quality issue is often reported for every copy of a cell, which may overwhelm users. To deal with these issues, we propose to refine spreadsheet smells by exploiting inferred structural information for smell detection. We therefore first provide a detailed description of our static analysis approach to infer clusters and blocks of related cells. We then elaborate on how to improve existing smells by providing three example refinements of existing smells that incorporate information about cell groups and computation blocks. Furthermore, we propose three novel smell detection techniques that make use of the inferred spreadsheet structures. Empirical evaluation of the proposed techniques suggests that the refinements successfully reduce the number of incorrectly and redundantly reported smells, and novel deficits are revealed by the newly introduced smells.","Spreadsheets, Code smells, Static analysis",,,,
Journal Article,"de Oliveira Barros M,de Almeida Farzat F,Travassos GH",,Learning from optimization: A case study with Apache Ant,Information and Software Technology,2015,57.0,,684-704,,,2015,,0950-5849,https://www.sciencedirect.com/science/article/pii/S0950584914001839;http://dx.doi.org/10.1016/j.infsof.2014.07.015,10.1016/j.infsof.2014.07.015,"Context Software architecture degrades when changes violating the design-time architectural intents are imposed on the software throughout its life cycle. Such phenomenon is called architecture erosion. When changes are not controlled, erosion makes maintenance harder and negatively affects software evolution. Objective To study the effects of architecture erosion on a large software project and determine whether search-based module clustering might reduce the conceptual distance between the current architecture and the design-time one. Method To run an exploratory study with Apache Ant. First, we characterize Ant’s evolution in terms of size, change dispersion, cohesion, and coupling metrics, highlighting the potential introduction of architecture and code-level problems that might affect the cost of changing the system. Then, we reorganize the distribution of Ant’s classes using a heuristic search approach, intending to re-emerge its design-time architecture. Results In characterizing the system, we observed that its original, simple design was lost due to maintenance and the addition of new features. In optimizing its architecture, we found that current models used to drive search-based software module clustering produce complex designs, which maximize the characteristics driving optimization while producing class distributions that would hardly be acceptable to developers maintaining Ant. Conclusion The structural perspective promoted by the coupling and cohesion metrics precludes observing the adequate software module clustering from the perspective of software engineers when considering a large open source system. Our analysis adds evidence to the criticism of the dogma of driving design towards high cohesion and low coupling, at the same time observing the need for better models to drive design decisions. Apart from that, we see SBSE as a learning tool, allowing researchers to test Software Engineering models in extreme situations that would not be easily found in software projects.","Apache Ant, Heuristic search, Software module clustering, Experimental Software Engineering",,,,
Journal Article,"Blouin A,Lelli V,Baudry B,Coulon F",,User interface design smell: Automatic detection and refactoring of Blob listeners,Information and Software Technology,2018,102.0,,49-64,,,2018,,0950-5849,https://www.sciencedirect.com/science/article/pii/S0950584918300892;http://dx.doi.org/10.1016/j.infsof.2018.05.005,10.1016/j.infsof.2018.05.005,"Context. User Interfaces (UIs) intensively rely on event-driven programming: interactive objects send UI events, which capture users’ interactions, to dedicated objects called controllers. Controllers use several UI listeners that handle these events to produce UI commands. Objective. First, we reveal the presence of design smells in the code that describes and controls UIs. Second, we demonstrate that specific code analyses are necessary to analyze and refactor UI code, because of its coupling with the rest of the code. Method. We conducted an empirical study on four large Java software systems. We studied to what extent the number of UI commands that a UI listener can produce has an impact on the change- and fault-proneness of the UI listener code. We developed a static code analysis for detecting UI commands in the code. Results. We identified a new type of design smell, called Blob listener, that characterizes UI listeners that can produce more than two UI commands. We proposed a systematic static code analysis procedure that searches for Blob listener that we implement in InspectorGuidget. We conducted experiments on the four software systems for which we manually identified 53 instances of Blob listener. InspectorGuidget successfully detected 52 Blob listeners out of 53. The results exhibit a precision of 81.25% and a recall of 98.11%. We then developed a semi-automatically and behavior-preserving refactoring process to remove Blob listeners. 49.06% of the 53 Blob listeners were automatically refactored. Patches have been accepted and merged. Discussions with developers of the four software systems assess the relevance of the Blob listener. Conclusion. This work shows that UI code also suffers from design smells that have to be identified and characterized. We argue that studies have to be conducted to find other UI design smells and tools that analyze UI code must be developed.","User interface, Event handling, Design smell, Software maintenance, Code refactoring, Empirical software engineering",,,,
Journal Article,"Sierra G,Shihab E,Kamei Y",,A survey of self-admitted technical debt,Journal of Systems and Software,2019,152.0,,70-82,,,2019,,0164-1212,https://www.sciencedirect.com/science/article/pii/S0164121219300457;http://dx.doi.org/10.1016/j.jss.2019.02.056,10.1016/j.jss.2019.02.056,"Technical Debt is a metaphor used to express sub-optimal source code implementations that are introduced for short-term benefits that often need to be paid back later, at an increased cost. In recent years, various empirical studies have focused on investigating source code comments that indicate Technical Debt often referred to as Self-Admitted Technical Debt (SATD). Since the introduction of SATD as a concept, an increasing number of studies have examined various aspects pertaining to SATD. Therefore, in this paper we survey research work on SATD, analyzing the characteristics of current approaches and techniques for SATD detection, comprehension, and repayment. To motivate the submission of novel and improved work, we compile tools, resources, and data sets made available to replicate or extend current SATD research. To set the stage for future work, we identify open challenges in the study of SATD, areas that are missing investigation, and discuss potential future research avenues.","Self admitted technical debt, Software maintenance, Literature survey, Source code comments",,,,
Journal Article,"Islam S,Krinke J,Binkley D,Harman M",,Coherent clusters in source code,Journal of Systems and Software,2014,88.0,,1-24,,,2014,,0164-1212,https://www.sciencedirect.com/science/article/pii/S016412121300188X;http://dx.doi.org/10.1016/j.jss.2013.07.040,10.1016/j.jss.2013.07.040,"This paper presents the results of a large scale empirical study of coherent dependence clusters. All statements in a coherent dependence cluster depend upon the same set of statements and affect the same set of statements; a coherent cluster's statements have ‘coherent’ shared backward and forward dependence. We introduce an approximation to efficiently locate coherent clusters and show that it has a minimum precision of 97.76%. Our empirical study also finds that, despite their tight coherence constraints, coherent dependence clusters are in abundance: 23 of the 30 programs studied have coherent clusters that contain at least 10% of the whole program. Studying patterns of clustering in these programs reveals that most programs contain multiple substantial coherent clusters. A series of subsequent case studies uncover that all clusters of significant size map to a logical functionality and correspond to a program structure. For example, we show that for the program acct, the top five coherent clusters all map to specific, yet otherwise non-obvious, functionality. Cluster visualization also brings out subtle deficiencies in program structure and identifies potential refactoring candidates. A study of inter-cluster dependence is used to highlight how coherent clusters are connected to each other, revealing higher-level structures, which can be used in reverse engineering. Finally, studies are presented to illustrate how clusters are not correlated with program faults as they remain stable during most system evolution.","Dependence analysis, Program comprehension, Software clustering",,,,
Journal Article,"Islam MR,Zibran MF",,SentiStrength-SE: Exploiting domain specificity for improved sentiment analysis in software engineering text,Journal of Systems and Software,2018,145.0,,125-146,,,2018,,0164-1212,https://www.sciencedirect.com/science/article/pii/S0164121218301675;http://dx.doi.org/10.1016/j.jss.2018.08.030,10.1016/j.jss.2018.08.030,"Automated sentiment analysis in software engineering textual artifacts has long been suffering from inaccuracies in those few tools available for the purpose. We conduct an in-depth qualitative study to identify the difficulties responsible for such low accuracy. Majority of the exposed difficulties are then carefully addressed through building a domain dictionary and appropriate heuristics. These domain-specific techniques are then realized in SentiStrength-SE, a tool we have developed for improved sentiment analysis in text especially designed for application in the software engineering domain. Using a benchmark dataset consisting of 5,600 manually annotated JIRA issue comments, we carry out both qualitative and quantitative evaluations of our tool. We also separately evaluate the contributions of individual major components (i.e., domain dictionary and heuristics) of SentiStrength-SE. The empirical evaluations confirm that the domain specificity exploited in our SentiStrength-SE enables it to substantially outperform the existing domain-independent tools/toolkits (SentiStrength, NLTK, and Stanford NLP) in detecting sentiments in software engineering text.","Sentiments, Emotions, Software engineering, Empirical study, Automation, Domain dictionary",,,,
Journal Article,"Majd A,Vahidi-Asl M,Khalilian A,Poorsarvi-Tehrani P,Haghighi H",,SLDeep: Statement-level software defect prediction using deep-learning model on static code features,Expert Systems with Applications,2020,147.0,,113156,,,2020,,0957-4174,https://www.sciencedirect.com/science/article/pii/S0957417419308735;http://dx.doi.org/10.1016/j.eswa.2019.113156,10.1016/j.eswa.2019.113156,"Software defect prediction (SDP) seeks to estimate fault-prone areas of the code to focus testing activities on more suspicious portions. Consequently, high-quality software is released with less time and effort. The current SDP techniques however work at coarse-grained units, such as a module or a class, putting some burden on the developers to locate the fault. To address this issue, we propose a new technique called as Statement-Level software defect prediction using Deep-learning model (SLDeep). The significance of SLDeep for intelligent and expert systems is that it demonstrates a novel use of deep-learning models to the solution of a practical problem faced by software developers. To reify our proposal, we defined a suite of 32 statement-level metrics, such as the number of binary and unary operators used in a statement. Then, we applied as learning model, long short-term memory (LSTM). We conducted experiments using 119,989 C/C++ programs within Code4Bench. The programs comprise 2,356,458 lines of code of which 292,064 lines are faulty. The benchmark comprises a diverse set of programs and versions, written by thousands of developers. Therefore, it tends to give a model that can be used for cross-project SDP. In the experiments, our trained model could successfully classify the unseen data (that is, fault-proneness of new statements) with average performance measures 0.979, 0.570, and 0.702 in terms of recall, precision, and accuracy, respectively. These experimental results suggest that SLDeep is effective for statement-level SDP. The impact of this work is twofold. Working at statement-level further alleviates developer's burden in pinpointing the fault locations. Second, cross-project feature of SLDeep helps defect prediction research become more industrially-viable.","Defect, Software fault proneness, Machine learning, Fault prediction model, Software metric",,,,
Journal Article,"Yamashita A,Counsell S",,Code smells as system-level indicators of maintainability: An empirical study,Journal of Systems and Software,2013,86.0,10.0,2639-2653,,,2013,,0164-1212,https://www.sciencedirect.com/science/article/pii/S0164121213001258;http://dx.doi.org/10.1016/j.jss.2013.05.007,10.1016/j.jss.2013.05.007,"Context Code smells are manifestations of design flaws that can degrade code maintainability. So far, no research has investigated if these indicators are useful for conducting system-level maintainability evaluations. Aim The research in this paper investigates the potential of code smells to reflect system-level indicators of maintainability. Method We evaluated four medium-sized Java systems using code smells and compared the results against previous evaluations on the same systems based on expert judgment and the Chidamber and Kemerer suite of metrics. The systems were maintained over a period of up to 4 weeks. During maintenance, effort (person-hours) and number of defects were measured to validate the different evaluation approaches. Results Most code smells are strongly influenced by size; consequently code smells are not good indicators for comparing the maintainability of systems differing greatly in size. Also, from the comparison of the different evaluation approaches, expert judgment was found as the most accurate and flexible since it considered effects due to the system's size and complexity and could adapt to different maintenance scenarios. Conclusion Code smell approaches show promise as indicators of the need for maintenance in a way that other purely metric-based approaches lack.","Code smells, Maintainability, Empirical study, System evaluation",,,,
Journal Article,"Sudhamani M,Rangarajan L",,Code similarity detection through control statement and program features,Expert Systems with Applications,2019,132.0,,63-75,,,2019,,0957-4174,https://www.sciencedirect.com/science/article/pii/S0957417419302751;http://dx.doi.org/10.1016/j.eswa.2019.04.045,10.1016/j.eswa.2019.04.045,"Software clone detection is an emerging research area in the field of software engineering. Software systems are subjected to continuous modifications in source code to improve the performance of the software, which may lead to code redundancy. Duplicate code/code clone is a piece of code reworked several times in software programs due to copy paste activity or reusability of existing software. Code clone is a prime subject in software evolution. Detection of software clones at the time of software evolution may improve the performance of software and reduce the maintenance cost and effort. This paper proposes metric based methods to detect code clones, as software clone is a universal problem in large scale programming environment. This paper introduces two metric based approaches to detect code clones by comparing (i) Control Statement Features (ii) Program Features like different types of statements, operators and operands. In order to demonstrate the effectiveness of the proposed approaches, extensive experiments are conducted on two datasets, C projects of Bellon's benchmark dataset and student lab programs (SLP).The methods efficiently identify similar functional clones. Proposed models only find similarity of whole programs but intelligent enough to highlight similar code segments across program files.","Code duplication, Control Statement features, Program features, and Functional clones",,,,
Journal Article,"Mäkinen S,Leppänen M,Kilamo T,Mattila AL,Laukkanen E,Pagels M,Männistö T",,Improving the delivery cycle: A multiple-case study of the toolchains in Finnish software intensive enterprises,Information and Software Technology,2016,80.0,,175-194,,,2016,,0950-5849,https://www.sciencedirect.com/science/article/pii/S0950584916301434;http://dx.doi.org/10.1016/j.infsof.2016.09.001,10.1016/j.infsof.2016.09.001,"Context: Software companies seek to gain benefit from agile development approaches in order to meet evolving market needs without losing their innovative edge. Agile practices emphasize frequent releases with the help of an automated toolchain from code to delivery. Objective: We investigate, which tools are used in software delivery, what are the reasons omitting certain parts of the toolchain and what implications toolchains have on how rapidly software gets delivered to customers. Method: We present a multiple-case study of the toolchains currently in use in Finnish software-intensive organizations interested in improving their delivery frequency. We conducted qualitative semi-structured interviews in 18 case organizations from various software domains. The interviewees were key representatives of their organization, considering delivery activities. Results: Commodity tools, such as version control and continuous integration, were used in almost every organization. Modestly used tools, such as UI testing and performance testing, were more distinctly missing from some organizations. Uncommon tools, such as artifact repository and acceptance testing, were used only in a minority of the organizations. Tool usage is affected by the state of current workflows, manual work and relevancy of tools. Organizations whose toolchains were more automated and contained fewer manual steps were able to deploy software more rapidly. Conclusions: There is variety in the need for tool support in different development steps as there are domain-specific differences in the goals of the case organizations. Still, a well-founded toolchain supports speedy delivery of new software.","Continuous deployment, Continuous delivery, Software development tools, Deployment pipeline, Agile software development",,,,
Journal Article,"Shang W,Adams B,Hassan AE",,Using Pig as a data preparation language for large-scale mining software repositories studies: An experience report,Journal of Systems and Software,2012,85.0,10.0,2195-2204,,,2012,,0164-1212,https://www.sciencedirect.com/science/article/pii/S0164121211002007;http://dx.doi.org/10.1016/j.jss.2011.07.034,10.1016/j.jss.2011.07.034,"The Mining Software Repositories (MSR) field analyzes software repository data to uncover knowledge and assist development of ever growing, complex systems. However, existing approaches and platforms for MSR analysis face many challenges when performing large-scale MSR studies. Such approaches and platforms rarely scale easily out of the box. Instead, they often require custom scaling tricks and designs that are costly to maintain and that are not reusable for other types of analysis. We believe that the web community has faced many of these software engineering scaling challenges before, as web analyses have to cope with the enormous growth of web data. In this paper, we report on our experience in using a web-scale platform (i.e., Pig) as a data preparation language to aid large-scale MSR studies. Through three case studies, we carefully validate the use of this web platform to prepare (i.e., Extract, Transform, and Load, ETL) data for further analysis. Despite several limitations, we still encourage MSR researchers to leverage Pig in their large-scale studies because of Pig's scalability and flexibility. Our experience report will help other researchers who want to scale their analyses.","Software engineering, Mining Software Repositories, Pig, MapReduce",Automated Software Evolution,,,
Journal Article,"Fernández-López M,Poveda-Villalón M,Suárez-Figueroa MC,Gómez-Pérez A",,Why are ontologies not reused across the same domain?,Journal of Web Semantics,2019,57.0,,100492,,,2019,,1570-8268,https://www.sciencedirect.com/science/article/pii/S1570826818300726;http://dx.doi.org/10.1016/j.websem.2018.12.010,10.1016/j.websem.2018.12.010,"Even though one of the main characteristics of ontologies has always been claimed to be their reusability, throughout this paper it will be shown that ontology reuse across a given domain is not a consolidated practice. We have carried out a statistical study on ontology reuse in the ontologies collected in Linked Open Vocabularies (LOV), in addition to a particular analysis of a use case. The results of the present work show that, when building an ontology, the heterogeneity between the needed conceptualization and that of available ontologies, as well as the deficiencies in some of such ontologies (concerning documentation, licensing, etc.) are important obstacles for reusing ontologies of the same domain of the ontology under development. A possible approach to lessen these problems could be the creation of communities similar to open software ones in charge of developing and maintaining ontologies.","Ontology reuse, Ontology reliability, Ontology heterogeneity, Percentage of reuse, Reuse patterns, Drawbacks for reuse",,,,
Journal Article,"Chambers C,Scaffidi C",,Impact and utility of smell-driven performance tuning for end-user programmers,Journal of Visual Languages & Computing,2015,28.0,,176-194,,,2015,,1045-926X,https://www.sciencedirect.com/science/article/pii/S1045926X15000038;http://dx.doi.org/10.1016/j.jvlc.2015.01.002,10.1016/j.jvlc.2015.01.002,"This paper proposes a technique, called Smell-driven performance tuning (SDPT), which semi-automatically assists end-user programmers with fixing performance problems in visual dataflow programming languages. A within-subjects laboratory experiment showed SDPT increased end-user programmers’ success rate and decreased the time they required. Another study, based on using SDPT to analyze a corpus of example end-user programs, demonstrated that applying all available SDPT transformations achieved an execution time improvement of 42% and a memory usage improvement of 20%, comparable to improvements that expert programmers historically had manually achieved on the same programs. These results indicate that SDPT is an effective method for helping end-user programmers to fix performance problems.","End-user programming, Performance, Visual language",,,,
Book Chapter,,Betz CT,Appendix A - Extended Definitions for the IT Architectural Catalogs,,2011,,,315-408,Morgan Kaufmann,"Architecture and Patterns for IT Service Management, Resource Planning, and Governance: Making Shoes for the Cobbler's Children (Second Edition)",2011,9780123850171.0,,https://www.sciencedirect.com/science/article/pii/B9780123850171000055;http://dx.doi.org/10.1016/B978-0-12-385017-1.00005-5,10.1016/B978-0-12-385017-1.00005-5,,,,,Second Edition,Boston
Journal Article,"Azpeitia I,Iturrioz J,Díaz O",,Volunteering for Linked Data Wrapper maintenance: A platform perspective,Information Systems,2020,89.0,,101468,,,2020,,0306-4379,https://www.sciencedirect.com/science/article/pii/S0306437919305204;http://dx.doi.org/10.1016/j.is.2019.101468,10.1016/j.is.2019.101468,"Linked Data Wrappers (LDWs) turn Web APIs into RDF end-points, leveraging the Linked Open Data cloud with current data. Unfortunately, LDWs are fragile upon upgrades on the underlying APIs, compromising LDW stability. Hence, for API-based LDWs to become a sustainable foundation for the Web of Data, we should recognize LDW maintenance as a continuous effort that outlives their breakout projects. This is not new in Software Engineering. Other projects in the past faced similar issues. The strategy: becoming open source and turning towards dedicated platforms. By making LDWs open, we permit others not only to inspect (hence, increasing trust and consumption), but also to maintain (to cope with API upgrades) and reuse (to adapt for their own purposes). Promoting consumption, adaptation and reuse might all help to increase the user base, and in so doing, might provide the critical mass of volunteers, current LDW projects lack. Drawing upon the Helping Theory, we investigate three enablers of volunteering applied to LDW maintenance: impetus to respond, positive evaluation of contributing and increasing awareness. Insights are fleshed out through SYQL, a LDW platform on top of Yahoo’s YQL. Specifically, SYQL capitalizes on the YQL community (i.e. impetus to respond), providesannotation overlays to easy participation (i.e. positive evaluation of contributing), and introduces aHealth Checker (i.e. increasing awareness). Evaluation is conducted for 12 subjects involved in maintaining someone else’s LDWs. Results indicate that both the Health Checker and the annotation overlays provide utility as enablers of awareness and contribution.","Linked Data Wrappers, Maintenance, Open platforms, Volunteering, YQL",,,,
Journal Article,Shu Y,,A practical approach to modelling and validating integrity constraints in the Semantic Web,Knowledge-Based Systems,2018,153.0,,29-39,,,2018,,0950-7051,https://www.sciencedirect.com/science/article/pii/S0950705118301862;http://dx.doi.org/10.1016/j.knosys.2018.04.021,10.1016/j.knosys.2018.04.021,"Efforts have been made in the Semantic Web to combine rules with ontologies. One result of these efforts is the development of the Semantic Web Rule Language (SWRL) which is designed to integrate closely with the Web Ontology Language (OWL). Both SWRL and OWL adhere to the open-world semantics of first-order logic, and thus are not suitable for modelling integrity constraints in applications where complete knowledge about some parts of the domain can be assumed. In this paper, we investigate this problem and present a practical approach to modelling and validating constraints in the Semantic Web. Building on existing work, we show that by employing a constraint semantics for both OWL and SWRL, we can model common constraints as OWL axioms or SWRL rules. We also show that by using a query reduction technique, we can validate constraints using existing OWL/SWRL reasoners. Finally, we demonstrate the usefulness of our approach via a real-world case study.","Semantic Web, Data validation, Integrity constraint, OWL, SWRL",,,,
Journal Article,Al Dallal J,,Constructing models for predicting extract subclass refactoring opportunities using object-oriented quality metrics,Information and Software Technology,2012,54.0,10.0,1125-1141,,,2012,,0950-5849,https://www.sciencedirect.com/science/article/pii/S0950584912000754;http://dx.doi.org/10.1016/j.infsof.2012.04.004,10.1016/j.infsof.2012.04.004,"Context Refactoring is a maintenance task that refers to the process of restructuring software source code to enhance its quality without affecting its external behavior. Inspecting and analyzing the source code of the system under consideration to identify the classes in need of extract subclass refactoring (ESR) is a time consuming and costly process. Objective This paper explores the abilities of several quality metrics considered individually and in combination to predict the classes in need of ESR. Method For a given a class, this paper empirically investigates, using univariate logistic regression analysis, the abilities of 25 existing size, cohesion, and coupling metrics to predict whether the class is in need of restructuring by extracting a subclass from it. In addition, models of combined metrics based on multivariate logistic regression analysis were constructed and validated to predict the classes in need of ESR, and the best model is justifiably recommended. We explored the statistical relations between the values of the selected metrics and the decisions of the developers of six open source Java systems with respect to whether the classes require ESR. Results The results indicate that there was a strong statistical relation between some of the quality metrics and the decision of whether ESR activity was required. From a statistical point of view, the recommended model of metrics has practical thresholds that lead to an outstanding classification of the classes into those that require ESR and those that do not. Conclusion The proposed model can be applied to automatically predict the classes in need of ESR and present them as suggestions to developers working to enhance the system during the maintenance phase. In addition, the model is capable of ranking the classes of the system under consideration according to their degree of need of ESR.","Object-oriented design, Extract subclass refactoring, Class quality, Class cohesion, Class coupling, Logistic regression analysis",,,,
Book Chapter,"Farcas E,Menarini M,Farcas C,Griswold WG,Patrick K,Krueger I,Demchak B,Raab F,Yan Y,Ziftci C","Mistrik I,Soley R,Ali N,Grundy J,Tekinerdogan B",Chapter 13 - Influences of architectural and implementation choices on CyberInfrastructure quality—a case study,,2016,,,279-332,Morgan Kaufmann,Software Quality Assurance,2016,9780128023013.0,,https://www.sciencedirect.com/science/article/pii/B9780128023013000132;http://dx.doi.org/10.1016/B978-0-12-802301-3.00013-2,10.1016/B978-0-12-802301-3.00013-2,"CyberInfrastructures (CIs) are complex socio-technical-economical systems that are difficult to describe, design, analyze, and evaluate. As one example, E-Health CIs are patient-centric community-serving systems that have particular regulations for information security, governance, resource management, scalability, and maintainability. We have built multiple successful E-Health interdisciplinary projects at the University of California, San Diego, including CYberinfrastructure for COmparative effectiveness REsearch (CYCORE) for cancer clinical trials, Personal Activity Location Measurement System (PALMS) for studying human activity patterns, CitiSense for sensing city pollution, and Integrating Data for Analysis, Anonymization, and SHaring (iDASH), which is a National Center for Biomedical Computing. These CIs have served their user communities well with respect to their projects’ goals and durations. Due to their shared requirements, these systems were all designed around the service bus-based Rich Services software architecture. However, there are also significant differences in how these CIs were designed and implemented, which might be informative to the community of CI developers. In this chapter, we detail our experience with developing E-Health CIs from the software quality perspective. We start from generic CI requirements along with domain and application-specific requirements, and we analyze the tradeoffs of development and deployment choices and propose a blueprint for next generation CIs. The main question that we answer is: How can we improve the quality of the next CIs by capitalizing on lessons learned, the best architectural and implementation choices, and reusing elements from previously existing systems? We present three contributions. First, we identify a set of activities and attributes that enable us to assess the quality of a CI with regard to important maintenance activities that are common to all CIs. These activities and attributes and the way we perform the evaluation are generic and can be applied to other projects and domains. Second, we analyze multiple projects and identify a set of common requirements for CIs and applications in the E-Health domain. We separate these requirements into common requirements that apply to all CIs, E-Health specific requirements that apply to CIs and most applications in the Health domain, and application-specific requirements. This separation makes many requirements reusable, and we expect that they will inform the development of future E-Health CIs and applications. Finally, we identify a set of architectural and implementation recommendations that will help developers create higher quality CIs for the E-Health domain.","Maintainability, maintenance activities, quality models, system architecture, architecture evaluation, service-oriented architecture, design methodology, cyberinfrastructure, requirements, distributed systems, large-scale systems, e-health, agile processes",,,,Boston
Journal Article,"Mao K,Capra L,Harman M,Jia Y",,A survey of the use of crowdsourcing in software engineering,Journal of Systems and Software,2017,126.0,,57-84,,,2017,,0164-1212,https://www.sciencedirect.com/science/article/pii/S0164121216301832;http://dx.doi.org/10.1016/j.jss.2016.09.015,10.1016/j.jss.2016.09.015,"The term ‘crowdsourcing’ was initially introduced in 2006 to describe an emerging distributed problem-solving model by online workers. Since then it has been widely studied and practiced to support software engineering. In this paper we provide a comprehensive survey of the use of crowdsourcing in software engineering, seeking to cover all literature on this topic. We first review the definitions of crowdsourcing and derive our definition of Crowdsourcing Software Engineering together with its taxonomy. Then we summarise industrial crowdsourcing practice in software engineering and corresponding case studies. We further analyse the software engineering domains, tasks and applications for crowdsourcing and the platforms and stakeholders involved in realising Crowdsourced Software Engineering solutions. We conclude by exposing trends, open issues and opportunities for future research on Crowdsourced Software Engineering.","Crowdsourced software engineering, Software crowdsourcing, Crowdsourcing, Literature survey",,,,
Journal Article,"Danenas P,Skersys T,Butleris R",,Natural language processing-enhanced extraction of SBVR business vocabularies and business rules from UML use case diagrams,Data & Knowledge Engineering,2020,128.0,,101822,,,2020,,0169-023X,https://www.sciencedirect.com/science/article/pii/S0169023X1930299X;http://dx.doi.org/10.1016/j.datak.2020.101822,10.1016/j.datak.2020.101822,"Discovery, specification and proper representation of various aspects of business knowledge plays crucial part in model-driven information systems engineering, especially when it comes to the early stages of systems development. Being among the most applicable and advanced features of model-driven development, model transformation could help improving one of the most time- and resource-consuming efforts in this process, namely, discovery and specification of business vocabularies and business rules within the problem domain. One of our latest developments in this area was the solution for the automatic extraction of SBVR business vocabularies and business rules from UML use case diagrams, which was arguably one of the most comprehensive developments of this kind currently available in public. In this paper, we present an enhancement to our previous development by introducing a novel natural language processing component to it. This enhancement provides more advanced extraction capabilities (such as recognition of entities, entire noun and verb phrases, multinary associations) and better quality of the extraction results compared to our previous solution. The main contributions presented in this paper are pre- and post-processing algorithms, and two extraction algorithms using custom-trained POS tagger. Based on the related work findings, it is safe to state that the presented solution is novel and original in its approach of combining together M2M transformation of UML and SBVR models with natural language processing techniques in the field of model-driven information systems engineering.","SBVR business vocabulary and rules, UML use case diagram, Model-to-model transformation, Controlled natural language, Natural language processing, Information extraction",,,,
Journal Article,"Mansoor U,Kessentini M,Langer P,Wimmer M,Bechikh S,Deb K",,MOMM: Multi-objective model merging,Journal of Systems and Software,2015,103.0,,423-439,,,2015,,0164-1212,https://www.sciencedirect.com/science/article/pii/S016412121400274X;http://dx.doi.org/10.1016/j.jss.2014.11.043,10.1016/j.jss.2014.11.043,"Nowadays, software systems are complex and large. To cope with this situation, teams of developers have to cooperate and work in parallel on software models. Thus, techniques to support the collaborative development of models are a must. To this end, several approaches exist to identify the change operations applied in parallel, to detect conflicts among them, as well as to construct a merged model by incorporating all non-conflicting operations. Conflicts often denote situations where the application of one operation disables the applicability of another one. Consequently, one operation has to be omitted to construct a valid merged model in such scenarios. When having to decide which operation to omit, the importance of its application has to be taken into account depending on the operation type and the application context. However, existing works treat the operations to merge with equal importance. We introduce in this paper, for the first time, a multi-objective formulation of the problem of model merging, based on NSGA-II, that aims to find the best trade-off between minimizing the number of omitted operations and maximizing the number of successfully applied important operations. We evaluated our approach using seven open source systems and compared it with different existing model merging approaches. The merging solutions obtained with our approach were found in all of the scenarios of our experiments to be comparable in terms of minimizing the number of conflicts to those suggested by existing approaches and to carry a high importance score of merged operations. Our results also revealed an interesting feature concerning the trade-off between the two conflicting objectives that demonstrates the practical value of taking the importance of operations into account in model merging tasks. In fact, the shape of the Pareto front represents an interesting guidance for developers to select best solutions based on their preferences.","Search-based software engineering, Merging, Multi-objective optimization",,,,
Journal Article,"Mohan M,Greer D,McMullan P",,Technical debt reduction using search based automated refactoring,Journal of Systems and Software,2016,120.0,,183-194,,,2016,,0164-1212,https://www.sciencedirect.com/science/article/pii/S0164121216300541;http://dx.doi.org/10.1016/j.jss.2016.05.019,10.1016/j.jss.2016.05.019,"Software refactoring has been recognized as a valuable process during software development and is often aimed at repaying technical debt. Technical debt arises when a software product has been built or amended without full care for structure and extensibility. Refactoring is useful to keep technical debt low and if it can be automated there are obvious efficiency benefits. Using a combination of automated refactoring techniques, software metrics and metaheuristic searches, an automated refactoring tool can improve the structure of a software system without affecting its functionality. In this paper, four different refactoring approaches are compared using an automated software refactoring tool. Weighted sums of metrics are used to form different fitness functions that drive the search process towards certain aspects of software quality. Metrics are combined to measure coupling, abstraction and inheritance and a fourth fitness function is proposed to measure reduction in technical debt. The 4 functions are compared against each other using 3 different searches on 6 different open source programs. Four out of the 6 programs show a larger improvement in the technical debt function after the search based refactoring process. The results show that the technical debt function is useful for assessing improvement in quality.","Search based software engineering, Automated refactoring, Refactoring tools, Technical debt, Software metrics, Simulated annealing",,,,
Journal Article,"Tong H,Liu B,Wang S",,Software defect prediction using stacked denoising autoencoders and two-stage ensemble learning,Information and Software Technology,2018,96.0,,94-111,,,2018,,0950-5849,https://www.sciencedirect.com/science/article/pii/S0950584917300113;http://dx.doi.org/10.1016/j.infsof.2017.11.008,10.1016/j.infsof.2017.11.008,"Context Software defect prediction (SDP) plays an important role in allocating testing resources reasonably, reducing testing costs, and ensuring software quality. However, software metrics used for SDP are almost entirely traditional features compared with deep representations (DPs) from deep learning. Although stacked denoising autoencoders (SDAEs) are powerful for feature learning and have been successfully applied in other fields, to the best of our knowledge, it has not been investigated in the field of SDP. Meanwhile, class-imbalance is still a pressing problem needing to be addressed. Objective In this paper, we propose a novel SDP approach, SDAEsTSE, which takes advantages of SDAEs and ensemble learning, namely the proposed two-stage ensemble (TSE). Method Our method mainly includes two phases: the deep learning phase and two-stage ensemble (TSE) phase. We first use SDAEs to extract the DPs from the traditional software metrics, and then a novel ensemble learning approach, TSE, is proposed to address the class-imbalance problem. Results Experiments are performed on 12 NASA datasets to demonstrate the effectiveness of DPs, the proposed TSE, and SDAEsTSE, respectively. The performance is evaluated in terms of F-measure, the area under the curve (AUC), and Matthews correlation coefficient (MCC). Generally, DPs, TSE, and SDAEsTSE contribute to significantly higher performance compared with corresponding traditional metrics, classic ensemble methods, and benchmark SDP models. Conclusions It can be concluded that (1) deep representations are promising for SDP compared with traditional software metrics, (2) TSE is more effective for addressing the class-imbalance problem in SDP compared with classic ensemble learning methods, and (3) the proposed SDAEsTSE is significantly effective for SDP.","Software defect prediction, Stacked denoising autoencoders, Ensemble learning, Software metrics, Deep learning",,,,
Journal Article,"Sowe SK,Cerone A,Settas D",,An empirical study of FOSS developers patterns of contribution: Challenges for data linkage and analysis,Science of Computer Programming,2014,91.0,,249-265,,,2014,,0167-6423,https://www.sciencedirect.com/science/article/pii/S0167642313003262;http://dx.doi.org/10.1016/j.scico.2013.11.033,10.1016/j.scico.2013.11.033,"The majority of Free and Open Source Software (FOSS) developers are mobile and often use different identities in the projects or communities they participate in. These characteristics pose challenges for researchers studying the presence and contributions of developers across multiple repositories. In this paper, we present a methodology, employ various statistical measures, and leverage Bayesian networks to study the patterns of contribution of 502 developers in both Version Control System (VCS) and mailing list repositories in 20 GNOME projects. Our findings shows that only a small percentage of developers are contributing to both repositories and this cohort is making more commits than they are posting messages to mailing lists. The implications of these findings for understanding the patterns of contribution in FOSS projects and on the quality of the final product are discussed.","Open Source Software developers, Open Source Software projects, Software repositories, Concurrent Versions System, Mailing lists",Special Issue on Selected Contributions from the Open Source Software Certification (OpenCert) Workshops,,,
Journal Article,"Rattan D,Bhatia R,Singh M",,Software clone detection: A systematic review,Information and Software Technology,2013,55.0,7.0,1165-1199,,,2013,,0950-5849,https://www.sciencedirect.com/science/article/pii/S0950584913000323;http://dx.doi.org/10.1016/j.infsof.2013.01.008,10.1016/j.infsof.2013.01.008,"Context Reusing software by means of copy and paste is a frequent activity in software development. The duplicated code is known as a software clone and the activity is known as code cloning. Software clones may lead to bug propagation and serious maintenance problems. Objective This study reports an extensive systematic literature review of software clones in general and software clone detection in particular. Method We used the standard systematic literature review method based on a comprehensive set of 213 articles from a total of 2039 articles published in 11 leading journals and 37 premier conferences and workshops. Results Existing literature about software clones is classified broadly into different categories. The importance of semantic clone detection and model based clone detection led to different classifications. Empirical evaluation of clone detection tools/techniques is presented. Clone management, its benefits and cross cutting nature is reported. Number of studies pertaining to nine different types of clones is reported. Thirteen intermediate representations and 24 match detection techniques are reported. Conclusion We call for an increased awareness of the potential benefits of software clone management, and identify the need to develop semantic and model clone detection techniques. Recommendations are given for future research.","Software clone, Clone detection, Systematic literature review, Semantic clones, Model based clone",,,,
Journal Article,"Estefo P,Simmonds J,Robbes R,Fabry J",,The Robot Operating System: Package reuse and community dynamics,Journal of Systems and Software,2019,151.0,,226-242,,,2019,,0164-1212,https://www.sciencedirect.com/science/article/pii/S0164121219300342;http://dx.doi.org/10.1016/j.jss.2019.02.024,10.1016/j.jss.2019.02.024,"ROS, the Robot Operating System, offers a core set of software for operating robots that can be extended by creating or using existing packages, making it possible to write robotic software that can be reused on different hardware platforms. With thousands of packages available per stable distribution, encapsulating algorithms, sensor drivers, etc., it is the de facto middleware for robotics. Like any software ecosystem, ROS must evolve in order to keep meeting the requirements of its users. In practice, packages may end up being abandoned between releases: no one may be available to update a package, or newer packages offer similar functionality. As such, we wanted to identify and understand the evolution challenges faced by the ROS ecosystem. In this article, we report our findings after interviewing 19 ROS developers in depth, followed by a focus group (4 participants) and an online survey of 119 ROS community members. We specifically focused on the issues surrounding package reuse and how to contribute to existing packages. To conclude, we discuss the implications of our findings, and propose five recommendations for overcoming the identified issues, with the goal of improving the health of the ROS ecosystem.","Robot Operating System, Package management, Software ecosystems",,,,
Journal Article,"Jannach D,Schmitz T,Hofer B,Wotawa F",,"Avoiding, finding and fixing spreadsheet errors – A survey of automated approaches for spreadsheet QA",Journal of Systems and Software,2014,94.0,,129-150,,,2014,,0164-1212,https://www.sciencedirect.com/science/article/pii/S0164121214000788;http://dx.doi.org/10.1016/j.jss.2014.03.058,10.1016/j.jss.2014.03.058,"Spreadsheet programs can be found everywhere in organizations and they are used for a variety of purposes, including financial calculations, planning, data aggregation and decision making tasks. A number of research surveys have however shown that such programs are particularly prone to errors. Some reasons for the error-proneness of spreadsheets are that spreadsheets are developed by end users and that standard software quality assurance processes are mostly not applied. Correspondingly, during the last two decades, researchers have proposed a number of techniques and automated tools aimed at supporting the end user in the development of error-free spreadsheets. In this paper, we provide a review of the research literature and develop a classification of automated spreadsheet quality assurance (QA) approaches, which range from spreadsheet visualization, static analysis and quality reports, over testing and support to model-based spreadsheet development. Based on this review, we outline possible opportunities for future work in the area of automated spreadsheet QA.","Spreadsheet, Quality assurance, Tool support",,,,
Journal Article,"Cheong MW,Chong ZS,Liu SQ,Zhou W,Curran P,Bin Yu",,"Characterisation of calamansi (Citrus microcarpa). Part I: Volatiles, aromatic profiles and phenolic acids in the peel",Food Chemistry,2012,134.0,2.0,686-695,,,2012,,0308-8146,https://www.sciencedirect.com/science/article/pii/S0308814612003779;http://dx.doi.org/10.1016/j.foodchem.2012.02.162,10.1016/j.foodchem.2012.02.162,"Volatile compounds in the peel of calamansi (Citrus microcarpa) from Malaysia, the Philippines and Vietnam were extracted with dichloromethane and hexane, and then analysed by gas chromatography–mass spectroscopy/flame ionisation detector. Seventy-nine compounds representing >98% of the volatiles were identified. Across the three geographical sources, a relatively small proportion of potent oxygenated compounds was significantly different, exemplified by the highest amount of methyl N-methylanthranilate in Malaysian calamansi peel. Principal component analysis and canonical discriminant analysis were applied to interpret the complex volatile compounds in the calamansi peel extracts, and to verify the discrimination among the different origins. In addition, four common hydroxycinnamic acids (caffeic, p-coumaric, ferulic and sinapic acids) were determined in the methanolic extracts of calamansi peel using ultra-fast liquid chromatography coupled to photodiode array detector. The Philippines calamansi peel contained the highest amount of total phenolic acids. In addition, p-Coumaric acid was the dominant free phenolic acids, whereas ferulic acid was the main bound phenolic acid.","Calamansi (), Peel, Volatiles, PCA, CDA, Sensory evaluation",,,,
Journal Article,"Menzies T,Shepperd M",,“Bad smells” in software analytics papers,Information and Software Technology,2019,112.0,,35-47,,,2019,,0950-5849,https://www.sciencedirect.com/science/article/pii/S095058491930076X;http://dx.doi.org/10.1016/j.infsof.2019.04.005,10.1016/j.infsof.2019.04.005,"Context There has been a rapid growth in the use of data analytics to underpin evidence-based software engineering. However the combination of complex techniques, diverse reporting standards and poorly understood underlying phenomena are causing some concern as to the reliability of studies. Objective Our goal is to provide guidance for producers and consumers of software analytics studies (computational experiments and correlation studies). Method We propose using “bad smells”, i.e., surface indications of deeper problems and popular in the agile software community and consider how they may be manifest in software analytics studies. Results We list 12 “bad smells” in software analytics papers (and show their impact by examples). Conclusions We believe the metaphor of bad smell is a useful device. Therefore we encourage more debate on what contributes to the validity of software analytics studies (so we expect our list will mature over time).",,,,,
Journal Article,"Rago A,Diaz-Pace JA,Marcos C",,Do concern mining tools really help requirements analysts? An empirical study of the vetting process,Journal of Systems and Software,2019,156.0,,181-203,,,2019,,0164-1212,https://www.sciencedirect.com/science/article/pii/S0164121219301359;http://dx.doi.org/10.1016/j.jss.2019.06.073,10.1016/j.jss.2019.06.073,"Software requirements are often described in natural language because they are useful to communicate and validate. Due to their focus on particular facets of a system, this kind of specifications tends to keep relevant concerns (also known as early aspects) from the analysts’ view. These concerns are known as crosscutting concerns because they appear scattered among documents. Concern mining tools can help analysts to uncover concerns latent in the text and bring them to their attention. Nonetheless, analysts are responsible for vetting tool-generated solutions, because the detection of concerns is currently far from perfect. In this article, we empirically investigate the role of analysts in the concern vetting process, which has been little studied in the literature. In particular, we report on the behavior and performance of 55 subjects in three case-studies working with solutions produced by two different tools, assessed in terms of binary classification measures. We discovered that analysts can improve “bad” solutions to a great extent, but performed significantly better with “good” solutions. We also noticed that the vetting time is not a decisive factor to their final accuracy. Finally, we observed that subjects working with solutions substantially different from those of existing tools (better recall) can also achieve a good performance.","Crosscutting concern, Requirements engineering, Use case specifications, Human behavior, Empirical study, Tool support",,,,
Book Chapter,Betz CT,Betz CT,Chapter 5 - Patterns for IT Enablement,,2007,,,307-364,Morgan Kaufmann,"Architecture and Patterns for IT Service Management, Resource Planning, and Governance",2007,9780123705938.0,,https://www.sciencedirect.com/science/article/pii/B9780123705938500320;http://dx.doi.org/10.1016/B978-012370593-8/50032-0,10.1016/B978-012370593-8/50032-0,"Publisher Summary The concept of patterns, originating in building architecture and city planning, is applied in computing for almost two decades. This chapter also uses a pattern language, but it does so at a higher level than the software engineering pattern literature; patterns of process, organization, data sourcing and flow, and human motivation are discussed. These approaches are refined with great precision in modern BPM methodology. Many of the patterns here are proposed in support of such objectives. Achieving even one of these objectives in the context of a particular work flow or intraorganizational relationship can represent significant progress. The objective of the pattern analysis is to tie the systems architecture, data, and processes together across the functional barriers so that the value chain (and its governance) is enabled. Each pattern generally discusses the problem it is intended to address, the overall concept of the pattern, and implementation issues, including possible risks and difficulties.",,,,,Burlington
Journal Article,"Palomba F,Linares-Vásquez M,Bavota G,Oliveto R,Penta MD,Poshyvanyk D,Lucia A",,Crowdsourcing user reviews to support the evolution of mobile apps,Journal of Systems and Software,2018,137.0,,143-162,,,2018,,0164-1212,https://www.sciencedirect.com/science/article/pii/S0164121217302807;http://dx.doi.org/10.1016/j.jss.2017.11.043,10.1016/j.jss.2017.11.043,"In recent software development and distribution scenarios, app stores are playing a major role, especially for mobile apps. On one hand, app stores allow continuous releases of app updates. On the other hand, they have become the premier point of interaction between app providers and users. After installing/updating apps, users can post reviews and provide ratings, expressing their level of satisfaction with apps, and possibly pointing out bugs or desired features. In this paper we empirically investigate—by performing a study on the evolution of 100 open source Android apps and by surveying 73 developers—to what extent app developers take user reviews into account, and whether addressing them contributes to apps’ success in terms of ratings. In order to perform the study, as well as to provide a monitoring mechanism for developers and project managers, we devised an approach, named CRISTAL, for tracing informative crowd reviews onto source code changes, and for monitoring the extent to which developers accommodate crowd requests and follow-up user reactions as reflected in their ratings. The results of our study indicate that (i) on average, half of the informative reviews are addressed, and over 75% of the interviewed developers claimed to take them into account often or very often, and that (ii) developers implementing user reviews are rewarded in terms of significantly increased user ratings.","Mobile app evolution, User reviews, Mining app stores, Empirical study",,,,
Journal Article,"Linares-Vásquez M,Vendome C,Tufano M,Poshyvanyk D",,How developers micro-optimize Android apps,Journal of Systems and Software,2017,130.0,,1-23,,,2017,,0164-1212,https://www.sciencedirect.com/science/article/pii/S016412121730081X;http://dx.doi.org/10.1016/j.jss.2017.04.018,10.1016/j.jss.2017.04.018,"Optimizing mobile apps early on in the development cycle is supposed to be a key strategy for obtaining higher user rankings, more downloads, and higher retention. In fact, mobile platform designers publish specific guidelines, and tools aimed at optimizing apps. However, little research has been done with respect to identifying and understanding actual optimization practices performed by developers. In this paper, we present the results of three empirical studies aimed at investigating practices of Android developers towards improving the performance of their apps, by means of micro-optimizations. We mined change histories of 3513 apps to identify the most frequent micro-optimization opportunities in 297K+ snapshots and to understand if (and when) developers implement these optimizations. Then, we performed an in-depth analysis into whether implementing micro-optimizations can help reduce memory/CPU usage. Finally, we conducted a survey with 389 open-source developers to understand how they use micro-optimizations to improve the performance of Android apps. Surprisingly, our results indicate that developers rarely implement micro-optimizations. Also, the impact of the analyzed micro-optimization on CPU/memory consumption is negligible in most of the cases. Finally, the results from the survey shed some light into why this happens as well as upon which practices developers rely upon.","Optimizations, Mining software repositories, Empirical studies, Android, Measurement",,,,
Journal Article,"Feitosa D,Ampatzoglou A,Avgeriou P,Chatzigeorgiou A,Nakagawa EY",,What can violations of good practices tell about the relationship between GoF patterns and run-time quality attributes?,Information and Software Technology,2019,105.0,,1-16,,,2019,,0950-5849,https://www.sciencedirect.com/science/article/pii/S0950584918301617;http://dx.doi.org/10.1016/j.infsof.2018.07.014,10.1016/j.infsof.2018.07.014,"Context GoF patterns have been extensively studied with respect to the benefit they provide as problem-solving, communication and quality improvement mechanisms. The latter has been mostly investigated through empirical studies, but some aspects of quality (esp. run-time ones) are still under-investigated. Objective In this paper, we study if the presence of patterns enforces the conformance to good coding practices. To achieve this goal, we explore the relationship between the presence of GoF design patterns and violations of good practices related to source code correctness, performance and security, via static analysis. Method Specifically, we exploit static analysis so as to investigate whether the number of violations of good coding practices identified on classes is related to: (a) their participation in pattern occurrences, (b) the pattern category, (c) the pattern in which they participate, and (d) their role within the pattern occurrence. To answer these questions, we performed a case study on approximately 13,000 classes retrieved from five open-source projects. Results The obtained results suggest that classes not participating in patterns are more probable to violate good coding practices for correctness, performance and security. In a more fine-grained level of analysis, by focusing on specific patterns, we observed that patterns with more complex structure (e.g., Decorator) and pattern roles that are more change-prone (e.g., Subclasses) are more likely to be associated with a higher number of violations (up to 50 times more violations). Conclusion This finding implies that investing in a well-thought architecture based on best practices, such as patterns, is often accompanied with cleaner code with fewer violations.","Software architecture, GoF patterns, Design, Quality analysis, Evaluation",,,,
Journal Article,"Hammad M,Basit HA,Jarzabek S,Koschke R",,A systematic mapping study of clone visualization,Computer Science Review,2020,37.0,,100266,,,2020,,1574-0137,https://www.sciencedirect.com/science/article/pii/S1574013719302679;http://dx.doi.org/10.1016/j.cosrev.2020.100266,10.1016/j.cosrev.2020.100266,"Knowing code clones (similar code fragments) is helpful in software maintenance and re-engineering. As clone detectors return huge numbers of clones, visualization techniques have been proposed to make cloning information more comprehensible and useful for programmers. We present a mapping study of clone visualization techniques, classifying visualizations in respect to the user goals to be achieved by means of clone visualizations and relevant clone-related information needs. Our mapping study will aid tool users in selecting clone visualization tools suitable for the task at hand, tool vendors in improving capabilities of their tools, and researchers in identifying open problems in clone visualization research.","Clone, Visualization techniques, Feature analysis, User goals, Information needs, Human–computer interaction",,,,
Journal Article,"Pérez-Castillo R,Fernández-Ropero M,Piattini M",,Business process model refactoring applying IBUPROFEN. An industrial evaluation,Journal of Systems and Software,2019,147.0,,86-103,,,2019,,0164-1212,https://www.sciencedirect.com/science/article/pii/S016412121830222X;http://dx.doi.org/10.1016/j.jss.2018.10.012,10.1016/j.jss.2018.10.012,"Business process models are recognized as being important assets for companies, since appropriate management of them provides companies with a competitive advantage. Quality assurance of business process models has become a critical issue, especially when companies carry out reverse engineering techniques to retrieve their business process models. Thus, companies have to deal with several quality faults, such as unmeaningful elements, fine-grained granularity or incompleteness, which seriously affect understandability and modifiability of business process models. The most widely-used method to reduce these faults is refactoring. Although several refactoring operators exist in the literature, there are no refactoring techniques specially developed for business process models obtained by process mining and other reverse engineering techniques. Therefore, this paper presents the use of IBUPROFEN, a business process model refactoring technique for those models obtained by reverse engineering. IBUPROFEN is applied in an in-depth case study with a real-life information system belonging to a European bank company. The goal of this industrial evaluation is to prove that the refactoring operators improve the understandability and modifiability of the business process model after being refactored. In addition, the scalability of the technique is assessed to demonstrate the feasibility of its application.","Business process model, Refactoring, Understandability, Modifiability, Case study",,,,
Journal Article,"Colanzi TE,Assunção WK,Vergilio SR,Farah PR,Guizzo G",,"The Symposium on Search-Based Software Engineering: Past, Present and Future",Information and Software Technology,2020,127.0,,106372,,,2020,,0950-5849,https://www.sciencedirect.com/science/article/pii/S0950584919301740;http://dx.doi.org/10.1016/j.infsof.2020.106372,10.1016/j.infsof.2020.106372,"Context Search-Based Software Engineering (SBSE) is the research field where Software Engineering (SE) problems are modelled as search problems to be solved by search-based techniques. The Symposium on Search Based Software Engineering (SSBSE) is the premier event on SBSE, which had its 11th edition in 2019. Objective In order to better understand the characteristics and evolution of papers published at SSBSE, this work reports results from a mapping study targeting the proceedings of all SSBSE editions. Despite the existing mapping studies on SBSE, our contribution in this work is to provide information to researchers and practitioners willing to enter the SBSE field, being a source of information to strengthen the symposium, guide new studies, and motivate new collaboration among research groups. Method A systematic mapping study was conducted with a set of four research questions, in which 134 studies published in all editions of SSBSE, dated from 2009 to 2019, were evaluated. In a fifth question, 32 papers published in the challenge track were summarised. Results Throughout the years, 290 authors from 25 countries have contributed to the main track of the symposium, with the collaboration of at least two institutions in 46.3% of the papers. SSBSE papers have got substantial external visibility, as most citations are from different venues. The SE tasks addressed by SSBSE are mostly related to software testing, software debugging, software design, and maintenance. Evolutionary algorithms are present in 75% of the papers, being the most common search technique. The evaluation of the SBSE approaches usually includes industrial systems. Conclusions SSBSE has helped increase the popularity of SBSE in the SE research community and has played an important role on making SBSE mature. There are still problems and challenges to be addressed in the SBSE field, which can be tackled by SSBSE authors in further studies.","Systematic mapping, Search-based software engineering, Bibliometric analysis",,,,
Journal Article,"Jernigan W,Horvath A,Lee M,Burnett M,Cuilty T,Kuttal S,Peters A,Kwan I,Bahmani F,Ko A,Mendez CJ,Oleson A",,General principles for a Generalized Idea GardenImage 1,Journal of Visual Languages & Computing,2017,39.0,,51-65,,,2017,,1045-926X,https://www.sciencedirect.com/science/article/pii/S1045926X17300708;http://dx.doi.org/10.1016/j.jvlc.2017.04.005,10.1016/j.jvlc.2017.04.005,"Many systems are designed to help novices who want to learn programming, but few support those who are not necessarily interested in learning programming. This paper targets the subset of end-user programmers (EUPs) in this category. We present a set of principles on how to help EUPs like this learn just a little when they need to overcome a barrier. We then instantiate the principles in a prototype and empirically investigate them in three studies: a formative think-aloud study, a pair of summer camps attended by 42 teens, and a third summer camp study featuring a different environment attended by 48 teens. Finally, we present a generalized architecture to facilitate the inclusion of Idea Gardens into other systems, illustrating with examples from Idea Garden prototypes. Results have been very encouraging. For example, under our principles, Study #2’s camp participants required significantly less in-person help than in a previous camp to learn the same amount of material in the same amount of time.",,Special Issue on Programming and Modelling Tools,,,
Journal Article,"Dias K,Borba P,Barreto M",,Understanding predictive factors for merge conflicts,Information and Software Technology,2020,121.0,,106256,,,2020,,0950-5849,https://www.sciencedirect.com/science/article/pii/S095058492030001X;http://dx.doi.org/10.1016/j.infsof.2020.106256,10.1016/j.infsof.2020.106256,"Context: Merge conflicts often occur when developers change the same code artifacts. Such conflicts might be frequent in practice, and resolving them might be costly and is an error-prone activity. Objective: To minimize these problems by reducing merge conflicts, it is important to better understand how conflict occurrence is affected by technical and organizational factors. Method: With that aim, we investigate seven factors related to modularity, size, and timing of developers contributions. To do so, we reproduce and analyze 73504 merge scenarios in GitHub repositories of Ruby and Python MVC projects. Results: We find evidence that the likelihood of merge conflict occurrence significantly increases when contributions to be merged are not modular in the sense that they involve files from the same MVC slice (related model, view, and controller files). We also find bigger contributions involving more developers, commits, and changed files are more likely associated with merge conflicts. Regarding the timing factors, we observe contributions developed over longer periods of time are more likely associated with conflicts. No evaluated factor shows predictive power concerning both the number of merge conflicts and the number of files with conflicts. Conclusion: Our results could be used to derive recommendations for development teams and merge conflict prediction models. Project management and assistive tools could benefit from these models.","code integration, merge conflict, modularity, collaborative development, empirical study",,,,
Book Chapter,"Stephan M,Rapos EJ","Tekinerdogan B,Babur Ö,Cleophas L,van den Brand M,Akşit M",Chapter 3 - Model clone detection and its role in emergent model pattern mining: Towards using model clone detectors as emergent pattern miners – Potential and challenges,,2020,,,37-65,Academic Press,Model Management and Analytics for Large Scale Systems,2020,9780128166499.0,,https://www.sciencedirect.com/science/article/pii/B9780128166499000119;http://dx.doi.org/10.1016/B978-0-12-816649-9.00011-9,10.1016/B978-0-12-816649-9.00011-9,"Model-based software engineering approaches continue to gain traction in both industry and research. Accordingly the size, complexity, and prevalence of the models themselves are increasing. Model analysis and management thus becomes an essential task within any model-based process. One form of analysis that can support model-based approaches during the software engineering life cycle is pattern extraction, whereby tooling identifies emergent model patterns. These patterns can be used by analysts to ensure adherence to standards, software quality assurance, and library generation and optimization. In this chapter, we discuss the plausibility of using model clone detection as a form of emergent pattern mining for model-based systems. After a brief primer on the field of model clone detection and model pattern detection, we propose a conceptual framework, MCPM, centered on model clone detection that analysts can employ to detect emergent patterns in their models. In describing our framework concept, we illustrate our ideas using Simulink, and our Simulink model clone detector, Simone, as an example. However, we also consider other model clone detectors' potential within the MCPM framework. This includes how existing research and tooling can be applied to each step within the framework. We identify open challenges for researchers in realizing model clone detection as a model pattern mining tool, as well as the potential benefit that can be experienced by practitioners in the application of MCPM.","model clone detection, model clones, model pattern mining, model patterns, model-driven engineering",,,,
Journal Article,"Shigarov A,Khristyuk V,Mikhailov A",,TabbyXL: Software platform for rule-based spreadsheet data extraction and transformation,SoftwareX,2019,10.0,,100270,,,2019,,2352-7110,https://www.sciencedirect.com/science/article/pii/S2352711018302966;http://dx.doi.org/10.1016/j.softx.2019.100270,10.1016/j.softx.2019.100270,"Spreadsheets are widely used in science, engineering, business, and other activities. Overall, they conceal a large volume of data in a form intended to be interpreted by humans. We present a novel software platform facilitated for liberating such data. It provides rule-based spreadsheet data extraction and transformation to a structured form. Its core consists of a flexible table object model and a domain-specific rule language for table analysis. They serve to represent knowledge of table layout and content features, as well as their interpretation depending on transformation goals. This enables processing arbitrary tables originating from various domains. Our empirical results demonstrate that one ruleset can be applied to process arbitrary tables having the same features of layout, style, or content. The paper also describes two applications using the software platform to develop programs for rule-based converting data from arbitrary spreadsheet tables.","Table understanding, Information extraction, Unstructured data management, Rule-based programming, Spreadsheet data, Software development",,,,
Book Chapter,Lambrix P,Brahme A,"6.05 - Semantic Web, Ontologies, and Linked Data",,2014,,,67-76,Elsevier,Comprehensive Biomedical Physics,2014,9780444536334.0,,https://www.sciencedirect.com/science/article/pii/B9780444536327011278;http://dx.doi.org/10.1016/B978-0-444-53632-7.01127-8,10.1016/B978-0-444-53632-7.01127-8,"Researchers in various areas in the life sciences use biomedical data sources and tools for their research. However, with the explosion of the amount of available data sources and tools, researchers also face the difficulties of finding and retrieving relevant information and tools as well as integrating information from different sources. The vision of the Semantic Web alleviates these difficulties. In this chapter, we introduce the Semantic Web and discuss steps that have been taken toward this vision. We discuss ontologies as a key technology as well as the recent development of Linked Data. Further, for each of these, we list issues for future research.","Bioinformatics, Knowledge representation, Life sciences, Linked Data, Linked open data, Ontologies, Ontology alignment, Ontology debugging, Ontology engineering, Semantic Web",,,,Oxford
Journal Article,"Torrents-Barrena J,López-Velazco R,Piella G,Masoller N,Valenzuela-Alcaraz B,Gratacós E,Eixarch E,Ceresa M,Ángel González Ballester M",,TTTS-GPS: Patient-specific preoperative planning and simulation platform for twin-to-twin transfusion syndrome fetal surgery,Computer Methods and Programs in Biomedicine,2019,179.0,,104993,,,2019,,0169-2607,https://www.sciencedirect.com/science/article/pii/S0169260719301944;http://dx.doi.org/10.1016/j.cmpb.2019.104993,10.1016/j.cmpb.2019.104993,"Twin-to-twin transfusion syndrome (TTTS) is a serious condition that may occur in pregnancies when two or more fetuses share the same placenta. It is characterized by abnormal vascular connections in the placenta that cause blood to flow unevenly between the babies. If left untreated, perinatal mortality occurs in 90% of cases, whilst neurological injuries are still present in TTTS survivors. Minimally invasive fetoscopic laser surgery is the standard and optimal treatment for this condition, but is technically challenging and can lead to complications. Acquiring and maintaining the required surgical skills need consistent practice, and a steep learning curve. An accurate preoperative planning is thus vital for complex TTTS cases. To this end, we propose the first TTTS fetal surgery planning and simulation platform. The soft tissue of the mother, the uterus, the umbilical cords, the placenta and its vascular tree are segmented and registered automatically from magnetic resonance imaging and 3D ultrasound using computer vision and deep learning techniques. The proposed state-of-the-art technology is integrated into a flexible C++ and MITK-based application to provide a full exploration of the intrauterine environment by simulating the fetoscope camera as well as the laser ablation, determining the correct entry point, training doctors’ movements and trajectory ahead of operation, which allows improving upon current practice. A comprehensive usability study is reported. Experienced surgeons rated highly our TTTS planner and simulator, thus being a potential tool to be implemented in real and complex TTTS surgeries.","Twin-to-twin transfusion syndrome, Fetal surgery, Surgical planning and simulation, MITK, Computer vision, Deep learning",,,,
Book Chapter,"Moilanen K,Pulman S","Pozzi FA,Fersini E,Messina E,Liu B",Chapter 15 - Price-Sensitive Ripples and Chain Reactions: Tracking the Impact of Corporate Announcements With Real-Time Multidimensional Opinion Streaming,,2017,,,223-237,Morgan Kaufmann,Sentiment Analysis in Social Networks,2017,9780128044124.0,,https://www.sciencedirect.com/science/article/pii/B9780128044124000152;http://dx.doi.org/10.1016/B978-0-12-804412-4.00015-2,10.1016/B978-0-12-804412-4.00015-2,"Publicly quoted companies make official announcements and release potentially price-sensitive information at regular intervals. While the rich financial performance cues present in corporate announcements are of critical intrinsic importance to the market, it is equally important for companies that make announcements to be able to monitor the impact, ripple effects, and chain events triggered by downstream public reaction—subjective discussions, analyses, recommendations, predictions, and general speculation—among analysts, traders, investors, other companies, the press, and the wider public. In this chapter, we describe how deep, multidimensional opinion streaming, powered by a large-scale custom natural language processing pipeline for sentiment, affect, and irrealis analysis, is used to monitor, quantify, and estimate the impact of corporate announcements and other related events in real time across financial feeds, social media firehoses, blogs, forums, and news to provide rich price-sensitive feedback and insights for corporations and the wider market audience alike.","Real-time opinion streaming, Corporate announcements, Financial sentiment, Market sentiment, Compositional sentiment analysis, Affect analysis, Speculation analysis, Irrealis mood",,,,Boston
Journal Article,"Catolino G,Palomba F,De Lucia A,Ferrucci F,Zaidman A",,Enhancing change prediction models using developer-related factors,Journal of Systems and Software,2018,143.0,,14-28,,,2018,,0164-1212,https://www.sciencedirect.com/science/article/pii/S0164121218300918;http://dx.doi.org/10.1016/j.jss.2018.05.003,10.1016/j.jss.2018.05.003,"Continuous changes applied during software maintenance risk to deteriorate the structure of a system and are a threat to its maintainability. In this context, predicting the portions of source code where specific maintenance operations should be focused on may be crucial for developers to prevent maintainability issues. Previous work proposed change prediction models relying on product and process metrics as predictors of change-prone source code classes. However, we believe that existing approaches still miss an important piece of information, i.e., developer-related factors that are able to capture the complexity of the development process under different perspectives. In this paper, we firstly investigate three change prediction models that exploit developer-related factors (e.g., number of developers working on a class) as predictors of change-proneness of classes and then we compare them with existing models. Our findings reveal that these factors improve the capabilities of change prediction models. Moreover, we observed interesting complementarities among the prediction models. For this reason, we devised a novel change prediction model exploiting the combination of developer-related factors and product and evolution metrics. The results show that such a combined model is up to 22% more effective than the single models in the identification of change-prone classes.","Change prediction, Mining software repositories, Empirical study",,,,
Journal Article,"Silva LL,Valente MT,Maia MA",,Co-change patterns: A large scale empirical study,Journal of Systems and Software,2019,152.0,,196-214,,,2019,,0164-1212,https://www.sciencedirect.com/science/article/pii/S0164121219300597;http://dx.doi.org/10.1016/j.jss.2019.03.014,10.1016/j.jss.2019.03.014,"Co-Change Clustering is a modularity assessment technique that reveals how often changes are localized in modules and whether a change propagation represents design problems. This technique is centered on co-change clusters, which are highly inter-related source code files considering co-change relations. In this paper, we conduct a series of empirical analysis in a large corpus of 133 popular software projects on GitHub. We describe six co-change patterns by projecting them over the directory structure. We mine 1802 co-change clusters and 1719 co-change clusters (95%) are covered by the six co-change patterns. In this study, we aim to answer two central questions: (i) Are co-change patterns detected in different programming languages? (ii) How do different co-change patterns relate to rippling, activity density, ownership, and team diversity on clusters? We conclude that Encapsulated and Well-Confined clusters (Wrapped) implement well-defined and confined concerns. Octopus clusters are proportionally numerous regarding to other patterns. They relate significantly with ripple effect, activity, ownership, and diversity in development teams. Although Crosscutting are scattered over directories, they implement well-defined concerns. Despite they present higher activity compared to Wrapped clusters, it is not necessarily easy to get rid of them, suggesting that support tools may play a crucial role.","Modularity, Co-change clusters, Co-change patterns",,,,
Journal Article,El-Attar M,,Evaluating and empirically improving the visual syntax of use case diagrams,Journal of Systems and Software,2019,156.0,,136-163,,,2019,,0164-1212,https://www.sciencedirect.com/science/article/pii/S0164121219301402;http://dx.doi.org/10.1016/j.jss.2019.06.096,10.1016/j.jss.2019.06.096,"Use case modeling is a forefront technique to specify functional requirements of a system. Many research works related to use case modeling have been devoted to improving various aspects of use case modeling and its utilization in software development processes. One key aspect of use case models that has thus far been overlooked by the research community is the visual perception of use case diagrams by its readers. Any model is used transfer a mental idea by a modeler to a model reader. Even if a use case diagram is constructed flawlessly, if it is misread or misinterpreted by its reader then the intrinsic purpose of modeling has failed. This paper provides a two-fold contribution. Firstly, this paper presents an evaluation of the cognitive effectiveness of use case diagrams notation. The evaluation is based on theory principles and empirical evidence mainly from the cognitive science field. Secondly, it provides empirically validated improvements to the use case diagram notation that enhances its cognitive effectiveness. Empirical validation of the improvements is drawn by conducting an industrial survey using business analyst professionals. Empirical validation is also drawn by conducting an experiment using software engineering professionals as subjects.","Use case notation, Visual syntax, Cognitive effectiveness, UML",,,,
Journal Article,"Peiris M,Hill JH",,Adapting system execution traces to support analysis of software system performance properties,Journal of Systems and Software,2013,86.0,11.0,2849-2862,,,2013,,0164-1212,https://www.sciencedirect.com/science/article/pii/S0164121213001635;http://dx.doi.org/10.1016/j.jss.2013.06.060,10.1016/j.jss.2013.06.060,"UNITE is a method and tool that analyzes software system performance properties, e.g., end-to-end response time, throughput, and service time, via system execution traces. UNITE, however, assumes that a system execution trace contains properties (e.g., identifiable keywords, unique message instances, and enough variation among the same event types) to support performance analysis. With proper planning, it is possible to ensure that properties required to support such analysis are incorporated in the generated system execution trace. It, however, is not safe to assume this to be the case with many existing software systems. This article therefore presents a method and a tool called the System Execution Trace Adaptation Framework (SETAF), which is built atop of UNITE and adapts system execution traces to support performance analysis of software systems. It also presents examples and results of applying SETAF to different open-source projects. The results show that SETAF enables proper performance analysis via system execution traces without requiring developers to make modifications to the originating software system's source code, which can be a expensive and time-consuming task.","SETAF, System execution traces, Adaptation",,,,
Journal Article,"Devitt C,O'Neill E,Waldron R",,Drivers and barriers among householders to managing domestic wastewater treatment systems in the Republic of Ireland; implications for risk prevention behaviour,Journal of Hydrology,2016,535.0,,534-546,,,2016,,0022-1694,https://www.sciencedirect.com/science/article/pii/S0022169416300403;http://dx.doi.org/10.1016/j.jhydrol.2016.02.015,10.1016/j.jhydrol.2016.02.015,"Summary Septic systems that are malfunctioning, improperly sited or designed, present a contamination risk to drinking water sources, and subsequently, to human health. However, the international literature identifies gaps in householder knowledge regarding the function and maintenance requirements of septic systems, and also the potential health and environmental risk implications. Allied with householder fears related to the financial cost of risk management, these factors tend to reduce concern to recognise a malfunctioning system. In the Republic of Ireland, three-quarters of households in rural areas utilise an individual domestic wastewater treatment system (or septic system). Consequently, a significant portion of rural households that rely on groundwater sources via private-well use are at risk. Ireland reports one of the highest crude incidence rates of Verotoxigenic Escherichia coli (VTEC) infection in the European Union, and waterborne transmission related to contact with untreated or poorly treated water from private water sources is a factor in its transmission. Following recent Irish legislative change that places a duty of care on individual householders to ensure a proper system functioning, this exploratory study examines perceptions towards the risk management of septic systems among Irish householders. Using qualitative research methods, four focus groups selected on the basis of geographical variation, and two semi-structured interviews were conducted. While most householders agreed that poorly maintained septic systems represented a threat to the environment and to public health, none reported to having a regular maintenance routine in place. Thematic analysis revealed the drivers and barriers to septic system maintenance, and preferences of householders pertaining to communication on septic systems. The Health Belief Model is employed to help understand results. Results suggest that householder capacity to engage in regular risk management is reduced by limited perceptions of risk susceptibility and severity, impeding cues to action and barrier concerns. Understanding societal perceptions is central to effectively engaging with the public, and informing an improved approach to future pro-environmental engagement and behaviour.","Domestic wastewater treatment systems, Septic systems, Thematic analysis, Risk perception, Health Belief Model",,,,
Book Chapter,"Allemang D,Hendler J","Allemang D,Hendler J",Chapter 14 - Good and bad modeling practices,,2011,,,307-324,Morgan Kaufmann,Semantic Web for the Working Ontologist (Second Edition),2011,9780123859655.0,,https://www.sciencedirect.com/science/article/pii/B9780123859655100147;http://dx.doi.org/10.1016/B978-0-12-385965-5.10014-7,10.1016/B978-0-12-385965-5.10014-7,"Publisher Summary The basic assumptions behind the Semantic Web—the AAA, Open World, and Nonunique Naming assumptions—place very specific restrictions on the modeling language. The structure of RDF is in the form of statements with familiar grammatical constructs like subject, predicate, and object. The structure of Web Ontology Language (OWL) includes familiar concepts like class, subClassOf, and property. But the meaning of a model is given by the inference rules of OWL, which incorporate the assumptions of the Semantic Web. One can tell if one has built a useful model, one that conforms to these assumptions, by making sure that the inferences it supports are useful and meaningful. According to the AAA slogan, one cannot say that any of the practices in the chapter are “errors” because Anyone can say Anything about Any topic. All of these models are valid expressions in RDF/OWL. However, they are erroneous in the sense that they do not accomplish what the modeler intended by creating them. In each case, the mismatch can be revealed through careful examination of the inferences that the model entails. There are occasions where these modeling constructs are insufficient, and more advanced capabilities are required. Many of these have been included in version 2 of the OWL standard. This chapter outlines some fundamental new capabilities. It provides some of the background one will need to search through the OWL standard documents to explore its rich landscape.",,,,Second Edition,Boston
Journal Article,"Oyetoyan TD,Cruzes DS,Conradi R",,A study of cyclic dependencies on defect profile of software components,Journal of Systems and Software,2013,86.0,12.0,3162-3182,,,2013,,0164-1212,https://www.sciencedirect.com/science/article/pii/S0164121213001878;http://dx.doi.org/10.1016/j.jss.2013.07.039,10.1016/j.jss.2013.07.039,"Background Empirical evidence shows that dependency cycles among software components are pervasive in real-life software systems, although such cycles are known to be detrimental to software quality attributes such as understandability, testability, reusability, build-ability and maintainability. Research goals Can the use of extended object-oriented metrics make us better understand the relationships among cyclic related components and their defect-proneness? Approach First, we extend such metrics to mine and classify software components into two groups – the cyclic and the non-cyclic ones. Next, we have performed an empirical study of six software applications. Using standard statistical tests on four different hypotheses, we have determined the significance of the defect profiles of both groups. Results Our results show that most defects and defective components are concentrated in cyclic-dependent components, either directly or indirectly. Discussion and conclusion These results have important implications for software maintenance and system testing. By identifying the most defect-prone set in a software system, it is possible to effectively allocate testing resources in a cost efficient manner. Based on these results, we demonstrate how additional structural properties could be collected to understand component's defect proneness and aid decision process in refactoring defect-prone cyclic related components.","Dependency cycle, Defects, Defect-prone components",,,,
Journal Article,"Eghan EE,Moslehi P,Rilling J,Adams B",,The missing link – A semantic web based approach for integrating screencasts with security advisories,Information and Software Technology,2020,117.0,,106197,,,2020,,0950-5849,https://www.sciencedirect.com/science/article/pii/S0950584919302046;http://dx.doi.org/10.1016/j.infsof.2019.106197,10.1016/j.infsof.2019.106197,"Context Collaborative tools and repositories have been introduced to facilitate open source software development, allowing projects, developers, and users to share their knowledge and expertise through formal and informal channels such as repositories, Q&A websites, blogs and screencasts. While significant progress has been made in mining and cross-linking traditional software repositories, limited work exists in making multimedia content in the form of screencasts or audio recordings an integrated part of software engineering processes. Objective The objective of this research is to provide a standardized ontological representation that allows for a seamless knowledge integration of screencasts with other software artifacts across knowledge resource boundaries. Method In this paper, we propose a modeling approach that takes advantage of the Semantic Web and its inference services to capture and establish traceability links between knowledge extracted from different resources such as vulnerability information in NVD, project dependency information from Maven Central, and YouTube screencasts. Results We performed a case study on 48 videos that illustrate attacks on vulnerable systems and show that our approach can successfully link relevant vulnerabilities and screencasts with an average precision of 98% and an average recall of 54% when vulnerability identifiers (CVE ID) are explicitly mentioned in the metadata (title and description) of videos. When no CVE ID is present, our initial results show that for a reduced search space (for one vulnerability), using only the textual content of the image frames, our approach is still able to link video-vulnerability pairs and rank the correct result within the top two positions of the result set. Conclusion Our approach not only establishes bi-directional, direct, and indirect traceability links from screencasts to these other software artifacts; these links can also be used to guide practitioners in comprehending the potential security impact of vulnerable components in their projects.","Crowd-based documentation, Mining video content, Software security vulnerabilities, Software dependencies, Software traceability, Semantic knowledge modeling, Semantic web",,,,
Journal Article,"Shen L,Chen H,Yu Z,Kang W,Zhang B,Li H,Yang B,Liu D",,Evolving support vector machines using fruit fly optimization for medical data classification,Knowledge-Based Systems,2016,96.0,,61-75,,,2016,,0950-7051,https://www.sciencedirect.com/science/article/pii/S0950705116000125;http://dx.doi.org/10.1016/j.knosys.2016.01.002,10.1016/j.knosys.2016.01.002,"In this paper, a new support vector machines (SVM) parameter tuning scheme that uses the fruit fly optimization algorithm (FOA) is proposed. Termed as FOA-SVM, the scheme is successfully applied to medical diagnosis. In the proposed FOA-SVM, the FOA technique effectively and efficiently addresses the parameter set in SVM. Additionally, the effectiveness and efficiency of FOA-SVM is rigorously evaluated against four well-known medical datasets, including the Wisconsin breast cancer dataset, the Pima Indians diabetes dataset, the Parkinson dataset, and the thyroid disease dataset, in terms of classification accuracy, sensitivity, specificity, AUC (the area under the receiver operating characteristic (ROC) curve) criterion, and processing time. Four competitive counterparts are employed for comparison purposes, including the particle swarm optimization algorithm-based SVM (PSO-SVM), genetic algorithm-based SVM (GA-SVM), bacterial forging optimization-based SVM (BFO-SVM), and grid search technique-based SVM (Grid-SVM). The empirical results demonstrate that the proposed FOA-SVM method can obtain much more appropriate model parameters as well as significantly reduce the computational time, which generates a high classification accuracy. Promisingly, the proposed method can be regarded as a useful clinical tool for medical decision making.","Support vector machine, Parameter optimization, Fruit fly optimization, Medical diagnosis",,,,
Journal Article,"Silva B,Sant'Anna C,Rocha N,Chavez C",,The effect of automatic concern mapping strategies on conceptual cohesion measurement,Information and Software Technology,2016,75.0,,56-70,,,2016,,0950-5849,https://www.sciencedirect.com/science/article/pii/S0950584916300520;http://dx.doi.org/10.1016/j.infsof.2016.03.006,10.1016/j.infsof.2016.03.006,"Context: Cohesion has been recognized as an important quality attribute of software design across decades. It can be defined as the degree to which a module is focused on a single concern of software. A concern is any concept, feature, requirement or property of the problem or solution domain. Conceptual cohesion is an alternative way of cohesion measurement based on what concerns each module addresses. Therefore, adopting a strategy to map concerns to source code elements is challenging but necessary. Objective: We aim at providing empirical evidence about whether automatic concern mapping strategies are already ready to be used effectively for conceptual cohesion measurement. Method: We carried out an empirical study to assess the ability of conceptual cohesion measurement using different automatic concern mapping strategies in selecting the least cohesive modules. Results: Conceptual cohesion measurements over the two analyzed mapping strategies performed weakly in the ability of selecting the least cohesive modules. We then provide a discussion to explain the reasons. Conclusion: Concern mapping strategies should be carefully chosen for conceptual cohesion measurement, specially if automatic mapping is under consideration. Manual mapping is still the most reliable way for computing conceptual cohesion. We pointed out limitations in automatic mapping strategies that go beyond conceptual cohesion measurement purposes and which should be considered in future research or applications in industry.","Module cohesion, Cohesion metrics, Concern mapping, Comparative empirical study",,,,
Book Chapter,"Allemang D,Hendler J","Allemang D,Hendler J",Chapter 12 - Good and Bad Modeling Practices,,2008,,,271-291,Morgan Kaufmann,Semantic Web for the Working Ontologist,2008,9780123735560.0,,https://www.sciencedirect.com/science/article/pii/B9780123735560000125;http://dx.doi.org/10.1016/B978-0-12-373556-0.00012-5,10.1016/B978-0-12-373556-0.00012-5,"Publisher Summary The basic assumptions behind the Semantic Web—the AAA (Anyone can say Anything about Any topic), Open World, and Nonunique Naming assumptions—place very specific restrictions on the modeling language. The structure of RDF (Resource Description Framework) is in the form of statements with familiar grammatical constructs like subject, predicate, and object. The structure of OWL (Web Ontology Language) includes familiar concepts like class and property. But the meaning of a model is given by the inference rules of OWL, which incorporate the assumptions of the Semantic Web. All of these models are valid expressions in RDF/OWL, but they are erroneous in the sense that they do not accomplish what the modeler intended by creating them. In each case, the mismatch can be revealed through careful examination of the inferences that the model entails. In some cases (like the objectification error), the requirements themselves are inconsistent with the Semantic Web assumptions. In other cases (like the exclusivity error), the requirements are quite consistent with the Semantic Web assumptions and can be modeled easily with a simple pattern. The support that a model provides for question answering is given formally by the inferences that the model entails. As far as an inference engine is concerned, entities in the model could have any name at all, like G0001 or Node97. But names of this sort are of little help when perusing a model to determine whether it can satisfy one's own goals.",,,,,San Francisco
Journal Article,"Bipp T,Lepper A,Schmedding D",,Pair programming in software development teams – An empirical study of its benefits,Information and Software Technology,2008,50.0,3.0,231-240,,,2008,,0950-5849,https://www.sciencedirect.com/science/article/pii/S0950584907000596;http://dx.doi.org/10.1016/j.infsof.2007.05.006,10.1016/j.infsof.2007.05.006,"We present the results of an extensive and substantial case study on pair programming, which was carried out in courses for software development at the University of Dortmund, Germany. Thirteen software development teams with about 100 students took part in the experiments. The groups were divided into two sets with different working conditions. In one set, the group members worked on their projects in pairs. Even though the paired teams could only use half of the workstations the teams of individual workers could use, the paired teams produced nearly as much code as the teams of individual workers at the same time. In addition, the code produced by the paired teams was easier to read and to understand. This facilitates finding errors and maintenance.","Pair programming, Empirical software engineering, Quality of software",,,,
Journal Article,"van Deursen A,Mesbah A,Nederlof A",,Crawl-based analysis of web applications: Prospects and challenges,Science of Computer Programming,2015,97.0,,173-180,,,2015,,0167-6423,https://www.sciencedirect.com/science/article/pii/S0167642314003815;http://dx.doi.org/10.1016/j.scico.2014.09.005,10.1016/j.scico.2014.09.005,"In this paper we review five years of research in the field of automated crawling and testing of web applications. We describe the open source Crawljax tool, and the various extensions that have been proposed in order to address such issues as cross-browser compatibility testing, web application regression testing, and style sheet usage analysis. Based on that we identify the main challenges and future directions of crawl-based testing of web applications. In particular, we explore ways to reduce the exponential growth of the state space, as well as ways to involve the human tester in the loop, thus reconciling manual exploratory testing and automated test input generation. Finally, we sketch the future of crawl-based testing in the light of upcoming developments, such as the pervasive use of touch devices and mobile computing, and the increasing importance of cyber-security.","Test automation, Web crawling, Software evolution",Special Issue on New Ideas and Emerging Results in Understanding Software,,,
Book Chapter,Betz CT,Betz CT,Chapter 3 - Patterns for the IT Processes,,2011,,,151-241,Morgan Kaufmann,"Architecture and Patterns for IT Service Management, Resource Planning, and Governance: Making Shoes for the Cobbler's Children (Second Edition)",2011,9780123850171.0,,https://www.sciencedirect.com/science/article/pii/B9780123850171000031;http://dx.doi.org/10.1016/B978-0-12-385017-1.00003-1,10.1016/B978-0-12-385017-1.00003-1,"Publisher Summary This chapter discusses the major IT processes and some nonobvious, interesting patterns for implementing them, especially across functional boundaries. Such integrations represent a clear maturation of any large IT organization, as these processes are larger grained and deliver higher-order value. But they are not the highest order of value, which is to be found in the longest lived value streams of technology product, asset, infrastructure service, and application service. The pervasive effect of these lifecycles on the IT capability as a whole indicates the value to be found in managing them. The objective of the pattern analysis is to tie the system's architecture, data, and processes together across the functional barriers, so that the value chain (and its governance) is enabled. The patterns focus especially on breaking down the functional boundaries between IT planning, solution development, and service management, and enabling the accuracy of the core information store at the heart of well-managed IT. The chapter focuses on patterns for the nine major IT processes: accept demand, execute project, deliver release, complete change, fulfill service request, deliver transactional service, restore service, improve service, and retire service. Because processes are known for crossing functions and sharing data, describing specifically how these coordinations may take place is the primary goal of this discussion. A “Lifecycle implications” section at the end of each process discussion examines how that process affects the longer lived IT portfolio lifecycles (application service, infrastructure service, asset, and technology product).",,,,Second Edition,Boston
Journal Article,"Sampaio L,Garcia A",,Exploring context-sensitive data flow analysis for early vulnerability detection,Journal of Systems and Software,2016,113.0,,337-361,,,2016,,0164-1212,https://www.sciencedirect.com/science/article/pii/S0164121215002873;http://dx.doi.org/10.1016/j.jss.2015.12.021,10.1016/j.jss.2015.12.021,"Secure programming is the practice of writing programs that are resistant to attacks by malicious people or programs. Programmers of secure software have to be continuously aware of security vulnerabilities when writing their program statements. In order to improve programmers’ awareness, static analysis techniques have been devised to find vulnerabilities in the source code. However, most of these techniques are built to encourage vulnerability detection a posteriori, only when developers have already fully produced (and compiled) one or more modules of a program. Therefore, this approach, also known as late detection, does not support secure programming but rather encourages posterior security analysis. The lateness of vulnerability detection is also influenced by the high rate of false positives yielded by pattern matching, the underlying mechanism used by existing static analysis techniques. The goal of this paper is twofold. First, we propose to perform continuous detection of security vulnerabilities while the developer is editing each program statement, also known as early detection. Early detection can leverage his knowledge on the context of the code being created, contrary to late detection when developers struggle to recall and fix the intricacies of the vulnerable code they produced from hours to weeks ago. Second, we explore context-sensitive data flow analysis (DFA) for improving vulnerability detection and mitigate the limitations of pattern matching. DFA might be suitable for finding if an object has a vulnerable path. To this end, we have implemented a proof-of-concept Eclipse plugin for continuous DFA-based detection of vulnerabilities in Java programs. We also performed two empirical studies based on several industry-strength systems to evaluate if the code security can be improved through DFA and early vulnerability detection. Our studies confirmed that: (i) the use of context-sensitive DFA significantly reduces the rate of false positives when compared to existing techniques, without being detrimental to the detector performance, and (ii) early detection improves the awareness among developers and encourages programmers to fix security vulnerabilities promptly.","Early detection, Data flow analysis, Secure programming",,,,
Journal Article,"Alves NS,Mendes TS,de Mendonça MG,Spínola RO,Shull F,Seaman C",,Identification and management of technical debt: A systematic mapping study,Information and Software Technology,2016,70.0,,100-121,,,2016,,0950-5849,https://www.sciencedirect.com/science/article/pii/S0950584915001743;http://dx.doi.org/10.1016/j.infsof.2015.10.008,10.1016/j.infsof.2015.10.008,"Context The technical debt metaphor describes the effect of immature artifacts on software maintenance that bring a short-term benefit to the project in terms of increased productivity and lower cost, but that may have to be paid off with interest later. Much research has been performed to propose mechanisms to identify debt and decide the most appropriate moment to pay it off. It is important to investigate the current state of the art in order to provide both researchers and practitioners with information that enables further research activities as well as technical debt management in practice. Objective This paper has the following goals: to characterize the types of technical debt, identify indicators that can be used to find technical debt, identify management strategies, understand the maturity level of each proposal, and identify what visualization techniques have been proposed to support technical debt identification and management activities. Method A systematic mapping study was performed based on a set of three research questions. In total, 100 studies, dated from 2010 to 2014, were evaluated. Results We proposed an initial taxonomy of technical debt types, created a list of indicators that have been proposed to identify technical debt, identified the existing management strategies, and analyzed the current state of art on technical debt, identifying topics where new research efforts can be invested. Conclusion The results of this mapping study can help to identify points that still require further investigation in technical debt research.","Technical debt, Software maintenance, Software engineering, Systematic mapping",,,,
Journal Article,"Horridge M,Bail S,Parsia B,Sattler U",,Toward cognitive support for OWL justifications,Knowledge-Based Systems,2013,53.0,,66-79,,,2013,,0950-7051,https://www.sciencedirect.com/science/article/pii/S0950705113002578;http://dx.doi.org/10.1016/j.knosys.2013.08.021,10.1016/j.knosys.2013.08.021,"Justifications are the dominant form of explanation for entailments of OWL ontologies, with popular OWL ontology editors, such as Protégé 4, providing justification-based explanation facilities. A justification is a minimal subset of an ontology which is sufficient for an entailment to hold; they correspond to the premises of a proof. Unlike proofs, however, justifications do not articulate how their axioms support the entailment. We frequently observe that ontology developers find certain justifications difficult to work with; and while in some cases the sources of difficulty are obvious (such as a large number of axioms), we do not have a good general understanding of what makes justifications easy or difficult for ontology users. In this paper, we present an approach to determining the cognitive complexity of justifications for entailments of OWL ontologies. We describe an exploratory study which forms the basis for a cognitive complexity model that predicts the complexity of OWL justifications, and present the results of validating that model via experiments involving OWL users. This is concluded by an investigation into strategies OWL users apply to support them in understanding justifications. Our contributions include an evaluation of the cognitive complexity model, new insights into the complexity of justifications for entailments of OWL ontologies, a significant corpus with novel analyses of justifications suitable for experimentation, and an experimental protocol suitable for model validation and refinement.","Semantic Web, Description Logics, Web Ontology Language, Ontology debugging, Explanation",,,,
Journal Article,"Romano S,Fucci D,Scanniello G,Turhan B,Juristo N",,Findings from a multi-method study on test-driven development,Information and Software Technology,2017,89.0,,64-77,,,2017,,0950-5849,https://www.sciencedirect.com/science/article/pii/S0950584917302550;http://dx.doi.org/10.1016/j.infsof.2017.03.010,10.1016/j.infsof.2017.03.010,"Context Test-driven development (TDD) is an iterative software development practice where unit tests are defined before production code. A number of quantitative empirical investigations have been conducted about this practice. The results are contrasting and inconclusive. In addition, previous studies fail to analyze the values, beliefs, and assumptions that inform and shape TDD. Objective We present a study designed, and conducted to understand the values, beliefs, and assumptions about TDD. Participants were novice and professional software developers. Method We conducted an ethnographically-informed study with 14 novice software developers, i.e., graduate students in Computer Science at the University of Basilicata, and six professional software developers (with one to 10 years work experience). The participants worked on the implementation of a new feature for an existing software written in Java. We immersed ourselves in the context of our study. We collected qualitative information by means of audio recordings, contemporaneous field notes, and other kinds of artifacts. We collected quantitative data from the integrated development environment to support or refute the ethnography results. Results The main insights of our study can be summarized as follows: (i) refactoring (one of the phases of TDD) is not performed as often as the process requires and it is considered less important than other phases, (ii) the most important phase is implementation, (iii) unit tests are almost never up-to-date, and (iv) participants first build in their mind a sort of model of the source code to be implemented and only then write test cases. The analysis of the quantitative data supported the following qualitative findings: (i), (iii), and (iv). Conclusions Developers write quick-and-dirty production code to pass the tests, do not update their tests often, and ignore refactoring.","Ethnographically-informed study, Qualitative study, Test driven development",,,,
Book Chapter,Shivakumar SK,Shivakumar SK,1 - Architecting Scalable Enterprise Web Applications,,2015,,,1-57,Morgan Kaufmann,"Architecting High Performing, Scalable and Available Enterprise Web Applications",2015,9780128022580.0,,https://www.sciencedirect.com/science/article/pii/B9780128022580000019;http://dx.doi.org/10.1016/B978-0-12-802258-0.00001-9,10.1016/B978-0-12-802258-0.00001-9,"Scalability of an enterprise application is critical to the success of its online strategy and business. Business needs its online platform to be scalable to sustain the growth, and this requires the enterprise web applications to efficiently handle the increased user traffic and input data volume with acceptable performance. In this chapter, we will look at various aspects of scalability such as scalability dimensions, key tenets, scalability challenges, scalability best practices, scalability patterns, and a comprehensive process to achieve scalability. A deep-dive analysis of scalability is elaborated, which includes layer-wise scalability, fault tolerance, distributed computing, sizing and capacity planning, scalable software design, load distribution, services scalability, database scalability, storage scalability, and virtualization techniques. The patterns, challenges, and best practices are all analyzed from hardware and software perspectives. The chapter also discusses various scalability testing methods and scalability-related anti-patterns. We then examine a case study at the end of this chapter where we look at a business scenario close to real world and apply the patterns and best practices discussed in the chapter to design a scalable system.","Scalability dimensions, scalability best practices, scalability patterns, scalability challenges, fault tolerance, failover, redundancy, clustered deployment, horizontal clustering, vertical clustering, stateless session, lightweight design, on-demand data loading, resource pooling, replication, service-oriented architecture, virtualization, sizing and capacity planning, scalability testing, scalable software design, load distribution",,,,Boston
Journal Article,"Garousi V,Felderer M,Kılıçaslan FN",,A survey on software testability,Information and Software Technology,2019,108.0,,35-64,,,2019,,0950-5849,https://www.sciencedirect.com/science/article/pii/S0950584918302490;http://dx.doi.org/10.1016/j.infsof.2018.12.003,10.1016/j.infsof.2018.12.003,"Context Software testability is the degree to which a software system or a unit under test supports its own testing. To predict and improve software testability, a large number of techniques and metrics have been proposed by both practitioners and researchers in the last several decades. Reviewing and getting an overview of the entire state-of-the-art and state-of-the-practice in this area is often challenging for a practitioner or a new researcher. Objective Our objective is to summarize the body of knowledge in this area and to benefit the readers (both practitioners and researchers) in preparing, measuring and improving software testability. Method To address the above need, the authors conducted a survey in the form of a systematic literature mapping (classification) to find out what we as a community know about this topic. After compiling an initial pool of 303 papers, and applying a set of inclusion/exclusion criteria, our final pool included 208 papers (published between 1982 and 2017). Results The area of software testability has been comprehensively studied by researchers and practitioners. Approaches for measurement of testability and improvement of testability are the most-frequently addressed in the papers. The two most often mentioned factors affecting testability are observability and controllability. Common ways to improve testability are testability transformation, improving observability, adding assertions, and improving controllability. Conclusion This paper serves for both researchers and practitioners as an “index” to the vast body of knowledge in the area of testability. The results could help practitioners measure and improve software testability in their projects. To assess potential benefits of this review paper, we shared its draft version with two of our industrial collaborators. They stated that they found the review useful and beneficial in their testing activities. Our results can also benefit researchers in observing the trends in this area and identify the topics that require further investigation.","Software testing, Software testability, Survey, Systematic literature mapping, Systematic literature review, Systematic mapping",,,,
Book Chapter,Hughes R,Hughes R,Chapter 8 - Adapting Agile for Data Warehousing,,2013,,,251-302,Morgan Kaufmann,Agile Data Warehousing Project Management,2013,9780123964632.0,,https://www.sciencedirect.com/science/article/pii/B9780123964632000089;http://dx.doi.org/10.1016/B978-0-12-396463-2.00008-9,10.1016/B978-0-12-396463-2.00008-9,,,,,,Boston
Journal Article,"Alkhazi B,Abid C,Kessentini M,Wimmer M",,On the value of quality attributes for refactoring ATL model transformations: A multi-objective approach,Information and Software Technology,2020,120.0,,106243,,,2020,,0950-5849,https://www.sciencedirect.com/science/article/pii/S0950584919302617;http://dx.doi.org/10.1016/j.infsof.2019.106243,10.1016/j.infsof.2019.106243,"Context Model transformations play a fundamental role in Model-Driven Engineering (MDE) as they are used to manipulate models and to transform them between source and target metamodels. However, model transformation programs lack significant support to maintain good quality which is in contrast to established programming paradigms such as object-oriented programming. In order to improve the quality of model transformations, the majority of existing studies suggest manual support for the developers to execute a number of refactoring types on model transformation programs. Other recent studies aimed to automate the refactoring of model transformation programs, mostly focusing on the ATLAS Transformation Language (ATL), by improving mainly few quality metrics using a number of refactoring types. Objective In this paper, we propose a novel set of quality attributes to evaluate refactored ATL programs based on the hierarchical quality model QMOOD. Method We used the proposed quality attributes to guide the selection of the best refactorings to improve ATL programs using multi-objective search. Results We validate our approach on a comprehensive dataset of model transformations. The statistical analysis of our experiments on 30 runs shows that our automated approach recommended useful refactorings based on a benchmark of ATL transformations and compared to random search, mono-objective search formulation, a previous work based on a different formulation of multi-objective search with few quality metrics, and a semi-automated refactoring approach not based on heuristic search. Conclusion All these existing studies did not use our QMOOD adaptation for ATL which confirms the relevance of our quality attributes to guide the search for good refactoring suggestions.","Search based software engineering, Model transformations, Quality attributes, Refactoring",,,,
Journal Article,"Vogel-Heuser B,Fay A,Schaefer I,Tichy M",,Evolution of software in automated production systems: Challenges and research directions,Journal of Systems and Software,2015,110.0,,54-84,,,2015,,0164-1212,https://www.sciencedirect.com/science/article/pii/S0164121215001818;http://dx.doi.org/10.1016/j.jss.2015.08.026,10.1016/j.jss.2015.08.026,"Abstract Coping with evolution in automated production systems implies a cross-disciplinary challenge along the system's life-cycle for variant-rich systems of high complexity. The authors from computer science and automation provide an interdisciplinary survey on challenges and state of the art in evolution of automated production systems. Selected challenges are illustrated on the case of a simple pick and place unit. In the first part of the paper, we discuss the development process of automated production systems as well as the different type of evolutions during the system's life-cycle on the case of a pick and place unit. In the second part, we survey the challenges associated with evolution in the different development phases and a couple of cross-cutting areas and review existing approaches addressing the challenges. We close with summarizing future research directions to address the challenges of evolution in automated production systems.","Evolution, Automation, Automated production systems, Software engineering",,,,
Journal Article,"Kolahdouz-Rahimi S,Lano K,Sharbaf M,Karimi M,Alfraihi H",,A comparison of quality flaws and technical debt in model transformation specifications,Journal of Systems and Software,2020,169.0,,110684,,,2020,,0164-1212,https://www.sciencedirect.com/science/article/pii/S0164121220301382;http://dx.doi.org/10.1016/j.jss.2020.110684,10.1016/j.jss.2020.110684,"The quality of model transformations (MT) has high impact on model-driven engineering (MDE) software development approaches, because of the central role played by transformations in MDE for refining, migrating, refactoring and other operations on models. For programming languages, a popular paradigm for code quality is the concept of technical debt (TD), which uses the analogy that quality flaws in code are a debt burden carried by the software, which must either be ‘redeemed’ by expending specific effort to remove its flaws, or be tolerated, with ongoing additional costs to maintenance due to the flaws. Whilst the analysis and management of quality flaws and TD in programming languages has been investigated in depth over several years, less research on the topic has been carried out for model transformations. In this paper we investigate the characteristics of quality flaws and technical debt in model transformation languages, based upon systematic analysis of over 100 transformation cases in four leading MT languages. Based on quality flaw indicators for TD, we identify significant differences in the level and kinds of technical debt in different MT languages, and we propose ways in which TD in MT can be reduced and managed.","Model transformations, Technical debt, Software quality",,,,
Book Chapter,"Ricca F,Leotta M,Stocco A",Memon AM,Chapter Three - Three Open Problems in the Context of E2E Web Testing and a Vision: NEONATE,,2019,113.0,,89-133,Elsevier,,2019,,0065-2458,https://www.sciencedirect.com/science/article/pii/S0065245818300652;http://dx.doi.org/10.1016/bs.adcom.2018.10.005,10.1016/bs.adcom.2018.10.005,"Web applications are critical assets of our society and thus assuring their quality is of undeniable importance. Despite the advances in software testing, the ever-increasing technological complexity of these applications makes it difficult to prevent errors. In this work, we provide a thorough description of the three open problems hindering web test automation: fragility problem, strong coupling and low cohesion problem, and incompleteness problem. We conjecture that a major breakthrough in test automation is needed, because the problems are closely correlated, and hence need to be attacked together rather than separately. To this aim, we describe Neonate, a novel integrated testing environment specifically designed to empower the web tester. Our utmost purpose is to make the research community aware of the existence of the three problems and their correlation, so that more research effort can be directed in providing solutions and tools to advance the state of the art of web test automation.","Web testing, End-to-end testing, Page object pattern, Robust locators, Reverse engineering, Selenium, Development effort, Maintenance effort",,Advances in Computers,,
Journal Article,"Berlato S,Ceccato M",,A large-scale study on the adoption of anti-debugging and anti-tampering protections in android apps,Journal of Information Security and Applications,2020,52.0,,102463,,,2020,,2214-2126,https://www.sciencedirect.com/science/article/pii/S2214212619305976;http://dx.doi.org/10.1016/j.jisa.2020.102463,10.1016/j.jisa.2020.102463,"Android apps are subject to malicious reverse engineering and code tampering for many reasons, like premium features unlocking and malware piggybacking. Scientific literature and practitioners proposed several Anti-Debugging and Anti-Tampering protections, readily implementable by app developers, to empower Android apps to react against malicious reverse engineering actively. However, the extent to which Android app developers deploy these protections is not known. In this paper, we describe a large-scale study on Android apps to quantify the practical adoption of Anti-Debugging and Anti-Tampering protections. We analyzed 14,173 apps from 2015 and 23,610 apps from 2019 from the Google Play Store. Our analysis shows that 59% of these apps implement neither Anti-Debugging nor Anti-Tampering protections. Moreover, half of the remaining apps deploy only one protection, not exploiting the variety of available protections. We also observe that app developers prefer Java to Native protections by a ratio of 99 to 1. Finally, we note that apps in 2019 employ more protections against reverse engineering than apps in 2015.","Anti-debugging, Anti-tampering, Android apps, Static analysis",,,,
Journal Article,"Mo R,Yin Z",,Exploring software bug-proneness based on evolutionary clique modeling and analysis,Information and Software Technology,2020,128.0,,106380,,,2020,,0950-5849,https://www.sciencedirect.com/science/article/pii/S0950584920301476;http://dx.doi.org/10.1016/j.infsof.2020.106380,10.1016/j.infsof.2020.106380,"Context: Even if evolutionary coupling between files has been widely used for various studies, such as change impact analysis, defect prediction, and software design analysis etc., there has little work focusing on studying the linkage among evolutionary coupled files. Objective: In this paper, we propose a novel model, evolutionary clique (EClique), to characterize evolutionary coupled files as maintainable groups for bug fixes, analyze their bug-proneness and examine the possible causes of the bug-proneness. Methods: To identify ECliques from a project, we propose two history measures to reason about the evolutionary coupling between files, and create a novel clustering algorithm. Given the evolutionary coupling information, our clustering algorithm will automatically identify ECliques in a project. Results: We conduct analyses on 33,099 commits of ten open source projects to evaluate the usefulness of our EClique modeling and analysis approach: (1) The results show that files involved in an EClique are more likely to share similar design characteristics and change together for resolving bugs; (2) The results also show that the identified ECliques significantly contribute to a project’s bug-proneness. Meanwhile, the majority of a project’s bug-proneness can be captured by just a few ECliques which only contain a small portion of files; (3) Finally, we qualitatively demonstrate that bug-prone ECliques often exhibit design problems that propagate changes among files and can potentially be the causes of bug-proneness. Conclusion: To reduce the bug-proneness of a software project, practitioners should pay attention to the identified ECliques, and resolve design problems embedded in these ECliques.","Software design, Software bug-proneness, Mining repository, Co-change analysis",,,,
Book Chapter,"dos Santos PS,Travassos GH",Zelkowitz MV,Chapter 5 - Action Research Can Swing the Balance in Experimental Software Engineering,,2011,83.0,,205-276,Elsevier,,2011,,0065-2458,https://www.sciencedirect.com/science/article/pii/B9780123855107000059;http://dx.doi.org/10.1016/B978-0-12-385510-7.00005-9,10.1016/B978-0-12-385510-7.00005-9,"In general, professionals still ignore scientific evidence in place of expert opinions in most of their decision making. For this reason, it is still common to see the adoption of new software technologies in the field without any scientific basis or well-grounded criteria, but on the opinions of experts. Experimental Software Engineering is of paramount importance to provide the foundations to understand the limits and applicability of software technologies. The need to better observe and understand the practice of Software Engineering leads us to look for alternative experimental approaches to support our studies. Different research strategies can be used to explore different Software Engineering practices. Action Research can be seen as one alternative to intensify the conducting of important experimental studies with results of great value while investigating the Software Engineering practices in depth. In this chapter, a discussion on the use of Action Research in Software Engineering is presented. As indicated by a technical literature survey, along the years a growing tendency for addressing different research topics in Software Engineering through Action Research studies has been seen. This behavior can indicate the great potential of its applicability in our scientific field. Despite their clear benefits and diversity of application, the initial findings also revealed that the rigor and control of such studies should improve in Software Engineering. Aiming at better explaining the application of Action Research, an experimental study (in vivo) on the investigation of the subjective decisions of software developers, concerned with the refactoring of source code to improve source code quality in a distributed software development context is depicted. A Software Engineering theory regarding refactoring and some guidance on how to accomplish an Action Research study in Software Engineering supplement the discussions in this chapter.","Action Research, Refactoring, study, Software Engineering Theory, Scientific Knowledge Management, Experimental Software Engineering",,Advances in Computers,,
Journal Article,"Morales R,Chicano F,Khomh F,Antoniol G",,Efficient refactoring scheduling based on partial order reduction,Journal of Systems and Software,2018,145.0,,25-51,,,2018,,0164-1212,https://www.sciencedirect.com/science/article/pii/S0164121218301523;http://dx.doi.org/10.1016/j.jss.2018.07.076,10.1016/j.jss.2018.07.076,"Anti-patterns are poor solutions to design problems that make software systems hard to understand and to extend. Components involved in anti-patterns are reported to be consistently related to high changes and faults rates. Developers are advised to perform refactoring to remove anti-patterns, and consequently improve software design quality and reliability. However, since the number of anti-patterns in a system can be very large, the process of manual refactoring can be overwhelming. To assist a software engineer who has to perform this task, we propose a novel approach RePOR (Refactoring approach based on Partial Order Reduction). We perform a case study with five open source systems to assess the performance of RePOR against two well-known metaheuristics (Genetic Algorithm, and Ant Colony Optimization), one conflict-aware refactoring approach and, a new approach based on sampling (Sway). Results show that RePOR can correct a median of 73% of anti-patterns (10% more than existing approaches) with a significant reduction in effort (measured by the number of refactorings applied) ranging from 69% to 85%, and a reduction of execution time ranging between 50% and 87%, in comparison to existing approaches.","Software refactoring, Refactoring schedule, Anti-patterns, Design quality, Ant colony optimization, Genetic algorithm",,,,
Journal Article,"Chacón-Luna AE,Gutiérrez AM,Galindo JA,Benavides D",,Empirical software product line engineering: A systematic literature review,Information and Software Technology,2020,128.0,,106389,,,2020,,0950-5849,https://www.sciencedirect.com/science/article/pii/S0950584920301555;http://dx.doi.org/10.1016/j.infsof.2020.106389,10.1016/j.infsof.2020.106389,"Context: The adoption of Software Product Line Engineering (SPLE) is usually only based on its theoretical benefits instead of empirical evidences. In fact, there is no work that synthesizes the empirical studies on SPLE. This makes it difficult for researchers to base their contributions on previous works validated with an empirical strategy. Objective: The objective of this work is to discover and summarize the studies that have used empirical evidences in SPLE limited to those ones with the intervention of humans. This will allow evaluating the quality and to know the scope of these studies over time. Doing so, research opportunities can arise Methods: A systematic literature review was conducted. The scope of the work focuses on those studies in which there is human intervention and were published between 2000 and 2018. We considered peer-reviewed papers from journals and top software engineering conferences. Results: Out of a total of 1880 studies in the initial set, a total of 62 primary studies were selected after applying a series of inclusion and exclusion criteria. We found that, approximately 56% of the studies used the empirical case study strategy while the rest used experimental strategies. Around 86% of the case studies were performed in an industrial environment showing the penetration of SPLE in industry. Conclusion: The interest of empirical studies has been growing since 2008. Around 95.16% of the studies address aspects related to domain engineering while application engineering received less attention. Most of the experiments and case study evaluated showed an acceptable level of quality. The first study found dates from 2005 and since then, the interest in the empirical SPLE has increased.","Software product lines, Empirical strategies, Case study, Experiment, Systematic literature review",,,,
Journal Article,"Ramírez A,Romero JR,Ventura S",,A survey of many-objective optimisation in search-based software engineering,Journal of Systems and Software,2019,149.0,,382-395,,,2019,,0164-1212,https://www.sciencedirect.com/science/article/pii/S0164121218302759;http://dx.doi.org/10.1016/j.jss.2018.12.015,10.1016/j.jss.2018.12.015,"Search-based software engineering (SBSE) is changing the way traditional software engineering (SE) activities are carried out by reformulating them as optimisation problems. The natural evolution of SBSE is bringing new challenges, such as the need of a large number of objectives to formally represent the many decision criteria involved in the resolution of SE tasks. This suggests that SBSE is moving towards many-objective optimisation, an emerging area that provides advanced techniques to cope with high-dimensional optimisation problems. To analyse this phenomenon, this paper surveys relevant SBSE literature focused on the resolution of many-objective problems. From the gathered knowledge, current limitations regarding problem formulation, algorithm selection, experimental design and industrial applicability are discussed. Through the analysis of observed trends, this survey provides a historical perspective and future lines of research concerning the adoption of many-objective optimisation within SBSE.","Search-based software engineering, Many-objective optimisation, Multi-objective optimisation, Evolutionary algorithms, Literature survey",,,,
Book Chapter,Hughes R,Hughes R,Chapter 9 - Starting and Scaling Agile Data Warehousing,,2013,,,303-344,Morgan Kaufmann,Agile Data Warehousing Project Management,2013,9780123964632.0,,https://www.sciencedirect.com/science/article/pii/B9780123964632000090;http://dx.doi.org/10.1016/B978-0-12-396463-2.00009-0,10.1016/B978-0-12-396463-2.00009-0,,,,,,Boston
Journal Article,"Cheong MW,Tong KH,Ong JJ,Liu SQ,Curran P,Yu B",,Volatile composition and antioxidant capacity of Arabica coffee,Food Research International,2013,51.0,1.0,388-396,,,2013,,0963-9969,https://www.sciencedirect.com/science/article/pii/S0963996913000185;http://dx.doi.org/10.1016/j.foodres.2012.12.058,10.1016/j.foodres.2012.12.058,"This study compared the volatile and phenolic constituents of four Asian coffee varieties. The volatile components of Arabica coffee beans from Thailand (Doi Chang), Indonesia (Sidikalang and Sidikalang Kopi Luwak) and China (Yunnan) were extracted using dichloromethane and correlated with sensory data. The total concentrations of volatile compounds identified were 1239.04ppm (Yunnan), 1084.18ppm (Doi Chang), 1016.17ppm (Sidikalang) and 845.53 (Sidikalang Kopi Luwak). It was found that the Sidikalang Kopi Luwak coffee was most favourable with a well-balanced aromatic profile, though its overall profile was similar to that of Sidikalang. The Doi Chang and Yunnan coffees were perceived with distinctive sulphury notes mainly attributed to the sulphur-containing compounds of 8.32ppm and 12.63ppm, respectively. The phenolic compositions of the respective green and roasted coffee beans were also determined to assess their potential antioxidant activities. The total polyphenol content of the Sidikalang beans was the highest among the four coffee varieties. The antioxidant activities were measured using diphenyl-1-picrylhydrazyl (DPPH) and ferric reducing antioxidant power assays (FRAP). Between the green and roasted coffee beans, the radical scavenging activity was similar, whereas the Arabica Sidikalang variety registered the highest ferric reducing capacity (p<0.05). Knowledge of the volatile composition and antioxidant capacities would facilitate a better understanding of Asian coffee quality.","Coffee, Volatiles, Phenolic acids, Antioxidant capacity",,,,
Journal Article,"Tao C,Guo H,Huang Z",,Identifying security issues for mobile applications based on user review summarization,Information and Software Technology,2020,122.0,,106290,,,2020,,0950-5849,https://www.sciencedirect.com/science/article/pii/S0950584920300409;http://dx.doi.org/10.1016/j.infsof.2020.106290,10.1016/j.infsof.2020.106290,"Context With the development of mobile apps, public concerns about security issues are continually rising. From the user’s perspective, it is crucial to be aware of the security issues of apps. Reviews serve as an important channel for users to discover the diverse issues of apps. However, previous works rarely rely on existing reviews to provide a detailed summarization of the app’s security issues. Objective To provide a detailed overview of apps’ security issues for users, this paper introduces SRR-Miner, a novel review summarization approach that automatically summarizes security issues and users’ sentiments. Method SRR-Miner follows a keyword-based approach to extracting security-related review sentences. It summarizes security issues and users’ sentiments with triples, which makes full use of the deep analysis of sentence structures. SRR-Miner also provides visualized review summarization through a radar chart. Results The evaluation on 17 mobile apps shows that SRR-Miner achieves higher F1-score and MCC than Machine Learning-based classification approaches in extracting security-related review sentences. It also accurately identifies misbehaviors, aspects and opinions from review sentences. A qualitative study shows that SRR-Miner outperforms two state-of-the-art approaches (AR-Miner and SUR-Miner) in terms of summarizing security issues and users’ sentiments. A further user survey indicates the usefulness of the summarization of SRR-Miner. Conclusion SRR-Miner is capable of automatically extracting security-related review sentences based on keywords, and summarizing misbehaviors, aspects and opinions of review sentences with a deep analysis of the sentence structures.","Mobile app review summarization, Natural language processing, Security and privacy",,,,
Journal Article,"Astromskis S,Bavota G,Janes A,Russo B,Di Penta M",,Patterns of developers behaviour: A 1000-hour industrial study,Journal of Systems and Software,2017,132.0,,85-97,,,2017,,0164-1212,https://www.sciencedirect.com/science/article/pii/S016412121730136X;http://dx.doi.org/10.1016/j.jss.2017.06.072,10.1016/j.jss.2017.06.072,"Monitoring developers’ activity in the Integrated Development Environment (IDE) and, in general, in their working environment, can be useful to provide context to recommender systems, and, in perspective, to develop smarter IDEs. This paper reports results of a long (about 1000 h) observational study conducted in an industrial environment, in which we captured developers’ interaction with the IDE, with various applications available in their workstation, and related them with activities performed on source code files. Specifically, the study involved six developers working on three software systems and investigated (i) how much time developers spent on various activities and how they shift from one activity to another (ii) how developers navigate through the software architecture during their task, and (iii) how the complexity and readability of source code may trigger further actions, such as requests for help or browsing/changing other files. Results of our study suggest that: (i) not surprisingly, developers spend most or their time (∼ 61%) in development activities while the usage of online help is limited (2%) but intensive in specific development sessions; (ii) developers often execute the system under development after working on code, likely to verify the effect of applied changes on the system’s behaviour; (iii) while working on files having a high complexity, developers tend to more frequently execute the system as well as to use more online help websites.","Monitoring developers’ activities, Case study",,,,
Journal Article,"Brunnert A,Krcmar H",,Continuous performance evaluation and capacity planning using resource profiles for enterprise applications,Journal of Systems and Software,2017,123.0,,239-262,,,2017,,0164-1212,https://www.sciencedirect.com/science/article/pii/S0164121215001831;http://dx.doi.org/10.1016/j.jss.2015.08.030,10.1016/j.jss.2015.08.030,"Continuous delivery (CD) is a software release process that helps to make features and bug fixes rapidly available in new enterprise application (EA) versions. Evaluating the performance of each EA version in a CD process requires a test environment comparable to a production system. Maintaining such systems is labor intensive and expensive. If multiple deployments of the same EA exist, it is often not feasible to maintain test instances for all of these systems. Furthermore, not all deployments are known at the time of a release (e.g., for off-the-shelf products). To address these challenges, this work proposes the use of resource profiles which describe the resource demand per transaction for each component of an EA and allow for performance predictions for different hardware environments and workloads without the need to own corresponding test environments. Within a CD process, resource profiles can be used to detect performance changes in EA versions. Once a version is released, resource profiles can be distributed along with the application binaries to support capacity planning for new deployments. Three integrated experiments for a representative EA provide validation for these capabilities.","Performance evaluation, Capacity planning, Resource profile",,,,
Book Chapter,"Babur Ö,Suresh A,Alberts W,Cleophas L,Schiffelers R,van den Brand M","Tekinerdogan B,Babur Ö,Cleophas L,van den Brand M,Akşit M",Chapter 11 - Model analytics for industrial MDE ecosystems,,2020,,,273-316,Academic Press,Model Management and Analytics for Large Scale Systems,2020,9780128166499.0,,https://www.sciencedirect.com/science/article/pii/B9780128166499000211;http://dx.doi.org/10.1016/B978-0-12-816649-9.00021-1,10.1016/B978-0-12-816649-9.00021-1,"Widespread adoption of Model-Driven Engineering (MDE) in industrial contexts, especially in large companies, leads to an abundance of MDE artifacts such as Domain-Specific Languages and models. ASML is an example of such a company where multidisciplinary teams work on various ecosystems with many languages and models. Automated analyses of those artifacts, e.g., for detecting duplication and cloning, can potentially aid the maintenance and evolution of those ecosystems. In this chapter, we explore a variety of model analytics approaches using our framework SAMOS in the industrial context of ASML ecosystems. We have performed case studies involving clone detection on ASML's data and control models within the ASOME ecosystem, cross-language conceptual analysis and language-level clone detection on three ecosystems, and finally architectural analysis and reconstruction on the CARM2G ecosystem. We discuss how model analytics can be used to discover insights in MDE ecosystems (e.g., via model clone detection and architectural analysis) and opportunities such as refactoring to improve them.","Model-Driven Engineering, Domain-Specific Languages, model analytics, model clone detection, clustering, topic modeling, architectural analysis, software ecosystems, software maintenance",,,,
Journal Article,"Meditskos G,Bassiliades N",,CLIPS–OWL: A framework for providing object-oriented extensional ontology queries in a production rule engine,Data & Knowledge Engineering,2011,70.0,7.0,661-681,,,2011,,0169-023X,https://www.sciencedirect.com/science/article/pii/S0169023X11000577;http://dx.doi.org/10.1016/j.datak.2011.04.001,10.1016/j.datak.2011.04.001,"In this paper, we define a framework, namely CLIPS–OWL, for enabling the CLIPS production rule engine to represent the extensional results of DL reasoning on OWL ontologies in the form of Object-Oriented (OO) models. The purpose of this transformation is to allow CLIPS to use these OO models as static query models that are able to answer extensional ontology queries directly by the RETE reasoning engine during the development of custom CLIPS production rule programs, without interfacing at runtime the external DL reasoner. In that way, any CLIPS-based application may enhance its functionality by incorporating ontological knowledge without modifying the architecture of the CLIPS rule engine. CLIPS–OWL has been implemented using the Pellet DL reasoner and the CLIPS Object-Oriented Language (COOL).","Production rules, Ontologies, Object-oriented model, CLIPS, OWL",,,,
Book Chapter,Mesbah A,Memon AM,Chapter Five - Advances in Testing JavaScript-Based Web Applications,,2015,97.0,,201-235,Elsevier,,2015,,0065-2458,https://www.sciencedirect.com/science/article/pii/S0065245814000114;http://dx.doi.org/10.1016/bs.adcom.2014.12.003,10.1016/bs.adcom.2014.12.003,"JavaScript is a flexible and expressive prototype-based scripting language that is used by developers to create interactive web applications. The language is interpreted, dynamic, weakly typed, and has first-class functions. It also interacts extensively with other web languages such as CSS and HTML at runtime. All these characteristics make JavaScript code particularly error-prone and challenging to analyze and test. In this chapter, we explore recent advances made in analysis and testing techniques geared toward JavaScript-based web applications. In particular, we look at recent empirical studies, testing techniques, test oracle automation approaches, test adequacy assessment methods, fault localization and repair, and Integrated Development Environment support to help programmers write better JavaScript code.","JavaScript, Testing, Dynamic analysis, Web applications, Test adequacy, Oracles",,Advances in Computers,,
Journal Article,"González-Torres A,García-Peñalvo FJ,Therón-Sánchez R,Colomo-Palacios R",,Knowledge discovery in software teams by means of evolutionary visual software analytics,Science of Computer Programming,2016,121.0,,55-74,,,2016,,0167-6423,https://www.sciencedirect.com/science/article/pii/S0167642315002658;http://dx.doi.org/10.1016/j.scico.2015.09.005,10.1016/j.scico.2015.09.005,"The day-to-day management of human resources that occurs during the development and maintenance process of software systems is a responsibility of project leads and managers, who usually perform such a task empirically. Moreover, rotation and distributed software development affect the establishment of long-term relationships between project managers and software projects, as well as between project managers and companies. It is also common for project leads and managers to face decision-making on human resources without the necessary prior knowledge. In this context, the application of visual analytics to software evolution supports software project leads and managers using analysis methods and a shared knowledge space for decision-making by means of visualization and interaction techniques. This approach offers the possibility of determining which programmer has led a project or contributed more to the development and maintenance of a software system in terms of revisions. Moreover, this approach helps to elucidate both the software items1 that have been changed in common by a group of programmers and who has changed what software items. With this information, software project leads and managers can make decisions regarding task assignment to developers and staff substitutions due to unexpected situations or staff turnover. Consequently, this research is aimed at supporting software practitioners in tasks related to human resources management through the application of Visual Analytics to Software Evolution.","Visual Analytics, Software Evolution, Software Evolution Visualization, Visual Software Analytics, Evolutionary Visual Software Analytics",Special Issue on Knowledge-based Software Engineering,,,
Journal Article,"Brabra H,Mtibaa A,Petrillo F,Merle P,Sliman L,Moha N,Gaaloul W,Guéhéneuc YG,Benatallah B,Gargouri F",,On semantic detection of cloud API (anti)patterns,Information and Software Technology,2019,107.0,,65-82,,,2019,,0950-5849,https://www.sciencedirect.com/science/article/pii/S095058491830226X;http://dx.doi.org/10.1016/j.infsof.2018.10.012,10.1016/j.infsof.2018.10.012,"Context Open standards are urgently needed for enabling software interoperability in Cloud Computing. Open Cloud Computing Interface (OCCI) provides a set of best design principles to create interoperable REST management APIs. Although OCCI is the only standard addressing the management of any kind of cloud resources, it does not support a range of best principles related to REST design. This often worsens REST API quality by decreasing their understandability and reusability. Objective We aim at assisting cloud developers to enhance their REST management APIs by providing a compliance evaluation of OCCI and REST best principles and a recommendation support to comply with these principles. Method First, we leverage patterns and anti-patterns to drive respectively the good and poor practices of OCCI and REST best principles. Then, we propose a semantic-based approach for defining and detecting REST and OCCI (anti)patterns and providing a set of correction recommendations to comply with both REST and OCCI best principles. We validated this approach by applying it on cloud REST APIs and evaluating its accuracy, usefulness and extensibility. Results We found that our approach accurately detects OCCI and REST(anti)patterns and provides useful recommendations. According to the compliance results, we reveal that there is no widespread adoption of OCCI principles in existing APIs. In contrast, these APIs have reached an acceptable level of maturity regarding REST principles. Conclusion Our approach provides an effective and extensible technique for defining and detecting OCCI and REST (anti)patterns in Cloud REST APIs. Cloud software developers can benefit from our approach and defined principles to accurately evaluate their APIs from OCCI and REST perspectives. This contributes in designing interoperable, understandable, and reusable Cloud management APIs. Thank to the compliance analysis and the recommendation support, we also contribute to improving these APIs, which make them more straightforward.","Cloud computing, REST, OCCI, Pattern, Anti-pattern, Analysis, Specification, Detection, Ontology",,,,
Book Chapter,Hughes R,Hughes R,Chapter 1 - What Is Agile Data Warehousing?,,2013,,,3-32,Morgan Kaufmann,Agile Data Warehousing Project Management,2013,9780123964632.0,,https://www.sciencedirect.com/science/article/pii/B9780123964632000016;http://dx.doi.org/10.1016/B978-0-12-396463-2.00001-6,10.1016/B978-0-12-396463-2.00001-6,,,,,,Boston
Journal Article,"Fernandes E,Chávez A,Garcia A,Ferreira I,Cedrim D,Sousa L,Oizumi W",,Refactoring effect on internal quality attributes: What haven’t they told you yet?,Information and Software Technology,2020,126.0,,106347,,,2020,,0950-5849,https://www.sciencedirect.com/science/article/pii/S0950584920301142;http://dx.doi.org/10.1016/j.infsof.2020.106347,10.1016/j.infsof.2020.106347,"Context Code refactoring was conceived for enhancing code structures, often in terms of internal quality attributes such as cohesion and coupling. Developers may have to apply multiple refactoring operations to achieve the expected enhancement. Re-refactoring occurs whenever one or more refactoring operations are performed on a previously refactored code element. The literature often assumes each single refactoring improves rather than worsens internal quality attributes, while re-refactoring implies further improvements. Unfortunately, quantitative evidence on this matter is scarce if not nonexistent. Objective This paper extends a large quantitative study about the refactoring effect on internal quality attributes with new insights, plus an unprecedented re-refactoring effect analysis. We particularly investigate if re-refactoring operations are more effective in improving attributes when compared to single operations. Method We analyzed 23 open software projects with 29,303 refactoring operations, from which nearly 50% constitute re-refactorings. We assessed five attributes: cohesion, complexity, coupling, inheritance, and size. We combined descriptive analysis and statistical tests to deeply understand the effect of both refactoring and re-refactoring on each attribute. Results Contrary to current knowledge, our study revealed that 90% of refactoring operations, and 100% of re-refactoring operations, were applied to code elements with at least one critical attribute. Critical attribute is an attribute whose metrics used for computing it have anomalous values, e.g. high coupling. Most operations (65%) improve attributes presumably associated with the refactoring type applied; the other operations (35%) keep those attributes unaffected. Whenever refactoring and re-refactoring operations are applied without additional changes, i.e., root-canal refactoring, attributes tend to improve or at least not worsen. Surprisingly, if these operations occur with additional changes such as feature additions, i.e., floss refactoring, they mostly improve rather than worsen attributes. Conclusions Besides revealing the effect of refactoring and re-refactoring on each attribute, we derived insights on leveraging the current refactoring practices.","Code refactoring, Internal quality attribute, Software metric, Quantitative study, Mining software repository",,,,
Journal Article,"Bigonha MA,Ferreira K,Souza P,Sousa B,Januário M,Lima D",,The usefulness of software metric thresholds for detection of bad smells and fault prediction,Information and Software Technology,2019,115.0,,79-92,,,2019,,0950-5849,https://www.sciencedirect.com/science/article/pii/S0950584919301697;http://dx.doi.org/10.1016/j.infsof.2019.08.005,10.1016/j.infsof.2019.08.005,"Context Software metrics may be an effective tool to assess the quality of software, but to guide their use it is important to define their thresholds. Bad smells and fault also impact the quality of software. Extracting metrics from software systems is relatively low cost since there are tools widely used for this purpose, which makes feasible applying software metrics to identify bad smells and to predict faults. Objective To inspect whether thresholds of object-oriented metrics may be used to aid bad smells detection and fault predictions. Method To direct this research, we have defined three research questions (RQ), two related to identification of bad smells, and one for identifying fault in software systems. To answer these RQs, we have proposed detection strategies for the bad smells: Large Class, Long Method, Data Class, Feature Envy, and Refused Bequest, based on metrics and their thresholds. To assess the quality of the derived thresholds, we have made two studies. The first one was conducted to evaluate their efficacy on detecting these bad smells on 12 systems. A second study was conducted to investigate for each of the class level software metrics: DIT, LCOM, NOF, NOM, NORM, NSC, NSF, NSM, SIX, and WMC, if the ranges of values determined by thresholds are useful to identify fault in software systems. Results Both studies confirm that metric thresholds may support the prediction of faults in software and are significantly and effective in the detection of bad smells. Conclusion The results of this work suggest practical applications of metric thresholds to identify bad smells and predict faults and hence, support software quality assurance activities.Their use may help developers to focus their efforts on classes that tend to fail, thereby minimizing the occurrence of future problems.","Software metrics, Software quality, Thresholds, Detection strategies, Bad smell, Fault prediction",,,,
Journal Article,"Merino L,Ghafari M,Anslow C,Nierstrasz O",,A systematic literature review of software visualization evaluation,Journal of Systems and Software,2018,144.0,,165-180,,,2018,,0164-1212,https://www.sciencedirect.com/science/article/pii/S0164121218301237;http://dx.doi.org/10.1016/j.jss.2018.06.027,10.1016/j.jss.2018.06.027,"Context:Software visualizations can help developers to analyze multiple aspects of complex software systems, but their effectiveness is often uncertain due to the lack of evaluation guidelines. Objective: We identify common problems in the evaluation of software visualizations with the goal of formulating guidelines to improve future evaluations. Method:We review the complete literature body of 387 full papers published in the SOFTVIS/VISSOFT conferences, and study 181 of those from which we could extract evaluation strategies, data collection methods, and other aspects of the evaluation. Results:Of the proposed software visualization approaches, 62% lack a strong evaluation. We argue that an effective software visualization should not only boost time and correctness but also recollection, usability, engagement, and other emotions. Conclusion:We call on researchers proposing new software visualizations to provide evidence of their effectiveness by conducting thorough (i) case studies for approaches that must be studied in situ, and when variables can be controlled, (ii) experiments with randomly selected participants of the target audience and real-world open source software systems to promote reproducibility and replicability. We present guidelines to increase the evidence of the effectiveness of software visualization approaches, thus improving their adoption rate.","Software visualisation, Evaluation, Literature review",,,,
