Item type,Authors,Title,Journal,Publication year,Volume,Issue,Pages,Publisher,Address,Proceedings title,Conference location,Date published,ISBN,ISSN,URLs,DOI,Abstract,Keywords,Series,Sub-type,Book title,Edition
Conference Paper,"Maiga A,Ali N,Bhattacharya N,Sabané A,Guéhéneuc YG,Antoniol G,Aïmeur E",Support Vector Machines for Anti-Pattern Detection,,2012,,,278–281,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 27th IEEE/ACM International Conference on Automated Software Engineering,"Essen, Germany",2012,9781450312042.0,,https://doi.org/10.1145/2351676.2351723;http://dx.doi.org/10.1145/2351676.2351723,10.1145/2351676.2351723,"Developers may introduce anti-patterns in their software systems because of time pressure, lack of understanding, communication, and--or skills. Anti-patterns impede development and maintenance activities by making the source code more difficult to understand. Detecting anti-patterns in a whole software system may be infeasible because of the required parsing time and of the subsequent needed manual validation. Detecting anti-patterns on subsets of a system could reduce costs, effort, and resources. Researchers have proposed approaches to detect occurrences of anti-patterns but these approaches have currently some limitations: they require extensive knowledge of anti-patterns, they have limited precision and recall, and they cannot be applied on subsets of systems. To overcome these limitations, we introduce SVMDetect, a novel approach to detect anti-patterns, based on a machine learning technique---support vector machines. Indeed, through an empirical study involving three subject systems and four anti-patterns, we showed that the accuracy of SVMDetect is greater than of DETEX when detecting anti-patterns occurrences on a set of classes. Concerning, the whole system, SVMDetect is able to find more anti-patterns occurrences than DETEX.","Anti-pattern, empirical software engineering, program comprehension, program maintenance",ASE 2012,,,
Conference Paper,"Peiris M,Hill JH","Automatically Detecting ""Excessive Dynamic Memory Allocations"" Software Performance Anti-Pattern",,2016,,,237–248,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 7th ACM/SPEC on International Conference on Performance Engineering,"Delft, The Netherlands",2016,9781450340809.0,,https://doi.org/10.1145/2851553.2851563;http://dx.doi.org/10.1145/2851553.2851563,10.1145/2851553.2851563,"This paper presents a methodology for automatically detecting the excessive dynamic memory allocation software performance anti-pattern, which is implemented in a tool named Excessive Memory Allocation Detector (EMAD). To the best of author's knowledge, EMAD is the first attempt to detect excessive dynamic memory allocation anti-pattern without human intervention. EMAD uses dynamic binary instrumentation and exploratory data analysis to determine if an application (or middleware) exhibits excessive dynamic memory allocations. Unlike traditional approaches, EMAD's technique does not rely on source code analysis. Results of applying EMAD to several open-source projects show that EMAD can detect the excessive dynamic memory allocations anti-pattern correctly. The results also show that application performance improves when the detected excessive dynamic memory allocations are resolved.","software performance anti-pattern, excessive dynamic memory allocation, dynamic binary instrumentation, detection",ICPE '16,,,
Conference Paper,"Pecorelli F,Palomba F,Di Nucci D,De Lucia A",Comparing Heuristic and Machine Learning Approaches for Metric-Based Code Smell Detection,,2019,,,93–104,IEEE Press,"Montreal, Quebec, Canada",Proceedings of the 27th International Conference on Program Comprehension,,2019,,,https://doi.org/10.1109/ICPC.2019.00023;http://dx.doi.org/10.1109/ICPC.2019.00023,10.1109/ICPC.2019.00023,"Code smells represent poor implementation choices performed by developers when enhancing source code. Their negative impact on source code maintainability and comprehensibility has been widely shown in the past and several techniques to automatically detect them have been devised. Most of these techniques are based on heuristics, namely they compute a set of code metrics and combine them by creating detection rules; while they have a reasonable accuracy, a recent trend is represented by the use of machine learning where code metrics are used as predictors of the smelliness of code artefacts. Despite the recent advances in the field, there is still a noticeable lack of knowledge of whether machine learning can actually be more accurate than traditional heuristic-based approaches. To fill this gap, in this paper we propose a large-scale study to empirically compare the performance of heuristic-based and machine-learning-based techniques for metric-based code smell detection. We consider five code smell types and compare machine learning models with Decor, a state-of-the-art heuristic-based approach. Key findings emphasize the need of further research aimed at improving the effectiveness of both machine learning and heuristic approaches for code smell detection: while Decor generally achieves better performance than a machine learning baseline, its precision is still too low to make it usable in practice.","empirical study, heuristics, machine learning, code smells detection",ICPC '19,,,
Conference Paper,"Pecorelli F,Di Nucci D,De Roover C,De Lucia A",On the Role of Data Balancing for Machine Learning-Based Code Smell Detection,,2019,,,19–24,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 3rd ACM SIGSOFT International Workshop on Machine Learning Techniques for Software Quality Evaluation,"Tallinn, Estonia",2019,9781450368551.0,,https://doi.org/10.1145/3340482.3342744;http://dx.doi.org/10.1145/3340482.3342744,10.1145/3340482.3342744,"Code smells can compromise software quality in the long term by inducing technical debt. For this reason, many approaches aimed at identifying these design flaws have been proposed in the last decade. Most of them are based on heuristics in which a set of metrics (e.g., code metrics, process metrics) is used to detect smelly code components. However, these techniques suffer of subjective interpretation, low agreement between detectors, and threshold dependability. To overcome these limitations, previous work applied Machine Learning techniques that can learn from previous datasets without needing any threshold definition. However, more recent work has shown that Machine Learning is not always suitable for code smell detection due to the highly unbalanced nature of the problem. In this study we investigate several approaches able to mitigate data unbalancing issues to understand their impact on ML-based approaches for code smell detection. Our findings highlight a number of limitations and open issues with respect to the usage of data balancing in ML-based code smell detection.","Code Smells, Machine Learning, Data Balancing",MaLTeSQuE 2019,,,
Conference Paper,"Borovits N,Kumara I,Krishnan P,Palma SD,Di Nucci D,Palomba F,Tamburri DA,van den Heuvel WJ",DeepIaC: Deep Learning-Based Linguistic Anti-Pattern Detection in IaC,,2020,,,7–12,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 4th ACM SIGSOFT International Workshop on Machine-Learning Techniques for Software-Quality Evaluation,"Virtual, USA",2020,9781450381246.0,,https://doi.org/10.1145/3416505.3423564;http://dx.doi.org/10.1145/3416505.3423564,10.1145/3416505.3423564,"Linguistic anti-patterns are recurring poor practices concerning inconsistencies among the naming, documentation, and implementation of an entity. They impede readability, understandability, and maintainability of source code. This paper attempts to detect linguistic anti-patterns in infrastructure as code (IaC) scripts used to provision and manage computing environments. In particular, we consider inconsistencies between the logic/body of IaC code units and their names. To this end, we propose a novel automated approach that employs word embeddings and deep learning techniques. We build and use the abstract syntax tree of IaC code units to create their code embedments. Our experiments with a dataset systematically extracted from open source repositories show that our approach yields an accuracy between 0.785 and 0.915 in detecting inconsistencies.","Word2Vec, IaC, Code Embedding, Linguistic Anti-patterns, Infrastructure Code, Deep Learning, Defects",MaLTeSQuE 2020,,,
Conference Paper,"Picha P,Brada P",Software Process Anti-Pattern Detection in Project Data,,2019,,,,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 24th European Conference on Pattern Languages of Programs,"Irsee, Germany",2019,9781450362061.0,,https://doi.org/10.1145/3361149.3361169;http://dx.doi.org/10.1145/3361149.3361169,10.1145/3361149.3361169,"There is a significant amount of guidance on Project Management (PM) including software development methodologies, best practices and anti-patterns (APs). There is, however, a lack of automated way of applying this knowledge by analyzing readily available data from tools aiding in software PM, such as Application Lifecycle Management (ALM) tools. We propose a method of detecting process and PM anti-patterns in project data which can be used to warn software development teams about a potential threat to the project, or to conduct more general studies on the impact of AP occurrence on project success and product quality. We previously published a concept for the data mining and analysis toolset distinct from other research approaches and related work. Based on this toolset, we devised a formalized basis for our detection method in the form of standardized AP description template and a model for pattern operationalization over project data extracted from ALM tools. The main contribution of this paper is the general method for AP operationalization taking the description template as a starting point, discussed together with its potential limitations. We performed an initial validation of the method on data from student projects, using an AP we encountered in practice called ""Collective Procrastination"" which we also describe in this paper together with its detailed formal operationalization.","project management anti-patterns, pattern detection, software process anti-patterns, ALM tools",EuroPLop '19,,,
Conference Paper,"Azadi U,Fontana FA,Zanoni M",Machine Learning Based Code Smell Detection through WekaNose,,2018,,,288–289,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 40th International Conference on Software Engineering: Companion Proceeedings,"Gothenburg, Sweden",2018,9781450356633.0,,https://doi.org/10.1145/3183440.3194974;http://dx.doi.org/10.1145/3183440.3194974,10.1145/3183440.3194974,"Code smells can be subjectively interpreted, the results provided by detectors are usually different, the agreement in the results is scarce, and a benchmark for the comparison of these results is not yet available. The main approaches used to detect code smells are based on the computation of a set of metrics. However code smell detectors often use different metrics and/or different thresholds, according to their detection rules. As result of this inconsistency the number of detected smells can increase or decrease accordingly, and this makes hard to understand when, for a specific software, a certain characteristic identifies a code smell or not. In this work, we introduce WekaNose, a tool that allows to perform an experiment to study code smell detection through machine learning techniques. The experiment's purpose is to select rules, and/or obtain trained algorithms, that can classify an instance (method or class) as affected or not by a code smell. These rules have the main advantage of being extracted through an example-based approach, rather then a heuristic-based one.",,ICSE '18,,,
Conference Paper,"Madeyski L,Lewowski T",MLCQ: Industry-Relevant Code Smell Data Set,,2020,,,342–347,Association for Computing Machinery,"New York, NY, USA",Proceedings of the Evaluation and Assessment in Software Engineering,"Trondheim, Norway",2020,9781450377317.0,,https://doi.org/10.1145/3383219.3383264;http://dx.doi.org/10.1145/3383219.3383264,10.1145/3383219.3383264,"Context Research on code smells accelerates and there are many studies that discuss them in the machine learning context. However, while data sets used by researchers vary in quality, all which we encountered share visible shortcomings---data sets are gathered from a rather small number of often outdated projects by single individuals whose professional experience is unknown.Aim This study aims to provide a new data set that addresses the aforementioned issues and, additionally, opens new research opportunities.Method We collaborate with professional software developers (including the code quest company behind the codebeat automated code review platform integrated with GitHub) to review code samples with respect to bad smells. We do not provide additional hints as to what do we mean by a given smell, because our goal is to extract professional developers' contemporary understanding of code smells instead of imposing thresholds from the legacy literature. We gather samples from active open source projects manually verified for industry-relevance and provide repository links and revisions. Records in our MLCQ data set contain the type of smell, its severity and the exact location in source code, but do not contain any source code metrics which can be calculated using various tools. To open new research opportunities, we provide results of an extensive survey of developers involved in the study including a wide range of details concerning their professional experience in software development and many other characteristics. This allows us to track each code review to the developer's background. To the best of our knowledge, this is a unique trait of the presented data set.Conclusions The MLCQ data set with nearly 15000 code samples was created by software developers with professional experience who reviewed industry-relevant, contemporary Java open source projects. We expect that this data set should stay relevant for a longer time than data sets that base on code released years ago and, additionally, will enable researchers to investigate the relationship between developers' background and code smells' perception.","data set, code smells, software quality, software development, bad code smells",EASE '20,,,
Conference Paper,"Shcherban S,Liang P,Tahir A,Li X",Automatic Identification of Code Smell Discussions on Stack Overflow: A Preliminary Investigation,,2020,,,,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 14th ACM / IEEE International Symposium on Empirical Software Engineering and Measurement (ESEM),"Bari, Italy",2020,9781450375801.0,,https://doi.org/10.1145/3382494.3422161;http://dx.doi.org/10.1145/3382494.3422161,10.1145/3382494.3422161,"Background: Code smells indicate potential design or implementation problems that may have a negative impact on programs. Similar to other software artefacts, developers use Stack Overflow (SO) to ask questions about code smells. However, given the high number of questions asked on the platform, and the limitations of the default tagging system, it takes significant effort to extract knowledge about code smells by means of manual approaches. Aim: We utilized supervised machine learning techniques to automatically identify code-smell discussions from SO posts. Method: We conducted an experiment using a manually labeled dataset that contains 3000 code-smell and 3000 non-code-smell posts to evaluate the performance of different classifiers when automatically identifying code smell discussions. Results: Our results show that Logistic Regression (LR) with parameter C=20 (inverse of regularization strength) and Bag of Words (BoW) feature extraction technique achieved the best performance amongst the algorithms we evaluated with a precision of 0.978, a recall of 0.965, and an F1-score of 0.971. Conclusion: Our results show that machine learning approach can effectively locate code-smell posts even if posts' title and/or tags cannot be of help. The technique can be used to extract code smell discussions from other textual artefacts (e.g., code reviews), and promisingly to extract SO discussions of other topics.","Discussion, Stack Overflow, Automatic Classification, Code Smell",ESEM '20,,,
Conference Paper,"Pecorelli F,Palomba F,Khomh F,De Lucia A",Developer-Driven Code Smell Prioritization,,2020,,,220–231,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 17th International Conference on Mining Software Repositories,"Seoul, Republic of Korea",2020,9781450375177.0,,https://doi.org/10.1145/3379597.3387457;http://dx.doi.org/10.1145/3379597.3387457,10.1145/3379597.3387457,"Code smells are symptoms of poor implementation choices applied during software evolution. While previous research has devoted effort in the definition of automated solutions to detect them, still little is known on how to support developers when prioritizing them. Some works attempted to deliver solutions that can rank smell instances based on their severity, computed on the basis of software metrics. However, this may not be enough since it has been shown that the recommendations provided by current approaches do not take the developer's perception of design issues into account. In this paper, we perform a first step toward the concept of developer-driven code smell prioritization and propose an approach based on machine learning able to rank code smells according to the perceived criticality that developers assign to them. We evaluate our technique in an empirical study to investigate its accuracy and the features that are more relevant for classifying the developer's perception. Finally, we compare our approach with a state-of-the-art technique. Key findings show that the our solution has an F-Measure up to 85% and outperforms the baseline approach.","Machine Learning for Software Engineering, Code smells, Empirical Software Engineering",MSR '20,,,
Conference Paper,"Fernandes E,Oliveira J,Vale G,Paiva T,Figueiredo E",A Review-Based Comparative Study of Bad Smell Detection Tools,,2016,,,,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 20th International Conference on Evaluation and Assessment in Software Engineering,"Limerick, Ireland",2016,9781450336918.0,,https://doi.org/10.1145/2915970.2915984;http://dx.doi.org/10.1145/2915970.2915984,10.1145/2915970.2915984,"Bad smells are symptoms that something may be wrong in the system design or code. There are many bad smells defined in the literature and detecting them is far from trivial. Therefore, several tools have been proposed to automate bad smell detection aiming to improve software maintainability. However, we lack a detailed study for summarizing and comparing the wide range of available tools. In this paper, we first present the findings of a systematic literature review of bad smell detection tools. As results of this review, we found 84 tools; 29 of them available online for download. Altogether, these tools aim to detect 61 bad smells by relying on at least six different detection techniques. They also target different programming languages, such as Java, C, C++, and C#. Following up the systematic review, we present a comparative study of four detection tools with respect to two bad smells: Large Class and Long Method. This study relies on two software systems and three metrics for comparison: agreement, recall, and precision. Our findings support that tools provide redundant detection results for the same bad smell. Based on quantitative and qualitative data, we also discuss relevant usability issues and propose guidelines for developers of detection tools.","comparative study, detection tools, bad smells, systematic literature review",EASE '16,,,
Conference Paper,"Charalampidou S,Ampatzoglou A,Avgeriou P",Size and Cohesion Metrics as Indicators of the Long Method Bad Smell: An Empirical Study,,2015,,,,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 11th International Conference on Predictive Models and Data Analytics in Software Engineering,"Beijing, China",2015,9781450337151.0,,https://doi.org/10.1145/2810146.2810155;http://dx.doi.org/10.1145/2810146.2810155,10.1145/2810146.2810155,"Source code bad smells are usually resolved through the application of well-defined solutions, i.e., refactorings. In the literature, software metrics are used as indicators of the existence and prioritization of resolving bad smells. In this paper, we focus on the long method smell (i.e. one of the most frequent and persistent bad smells) that can be resolved by the extract method refactoring. Until now, the identification of long methods or extract method opportunities has been performed based on cohesion, size or complexity metrics. However, the empirical validation of these metrics has exhibited relatively low accuracy with regard to their capacity to indicate the existence of long methods or extract method opportunities. Thus, we empirically explore the ability of size and cohesion metrics to predict the existence and the refactoring urgency of long method occurrences, through a case study on java open-source methods. The results of the study suggest that one size and four cohesion metrics are capable of characterizing the need and urgency for resolving the long method bad smell, with a higher accuracy compared to the previous studies. The obtained results are discussed by providing possible interpretations and implications to practitioners and researchers.","case study, metrics, Long method, cohesion, size",PROMISE '15,,,
Conference Paper,"Tiwari O,Joshi RK",Functionality Based Code Smell Detection and Severity Classification,,2020,,,,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 13th Innovations in Software Engineering Conference on Formerly Known as India Software Engineering Conference,"Jabalpur, India",2020,9781450375948.0,,https://doi.org/10.1145/3385032.3385048;http://dx.doi.org/10.1145/3385032.3385048,10.1145/3385032.3385048,"The Long Method code smell is a symptom of design defects caused by implementing multiple tasks within a single method. It limits reusability, evolvability and maintainability of a method. In this paper, we present a functionality based approach for detecting long methods. Functionalities are identified through a novel block based dependency analysis technique called Segmentation. It clusters sets of statements into extract method opportunities (or tasks). The approach uses interdependencies among various extract method opportunities identified within the method as a means to measure severity of the long method smell. The approach is validated over a Java based open source code. A comparison with expert's assessment shows that the approach is promising in detecting severe methods irrespective of their sizes.","extract method opportunity, code smell, segmentation, refactoring, long method smell severity",ISEC 2020,,,
Conference Paper,"Haque MS,Carver J,Atkison T","Causes, Impacts, and Detection Approaches of Code Smell: A Survey",,2018,,,,Association for Computing Machinery,"New York, NY, USA",Proceedings of the ACMSE 2018 Conference,"Richmond, Kentucky",2018,9781450356961.0,,https://doi.org/10.1145/3190645.3190697;http://dx.doi.org/10.1145/3190645.3190697,10.1145/3190645.3190697,"Code smells are anomalies often generated in design, implementation or maintenance phase of software development life cycle. Researchers established several catalogues characterizing the smells. Fowler and Beck developed the most popular catalogue of 22 smells covering varieties of development issues. This literature presents an overview of the existing research conducted on these 22 smells. Our motivation is to represent these smells with an easier interpretation for the software developers, determine the causes that generate these issues in applications and their impact from different aspects of software maintenance. This paper also highlights previous and recent research on smell detection with an effort to categorize the approaches based on the underlying concept.","software engineering, code smell, survey",ACMSE '18,,,
Conference Paper,"Hozano M,Ferreira H,Silva I,Fonseca B,Costa E",Using Developers' Feedback to Improve Code Smell Detection,,2015,,,1661–1663,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 30th Annual ACM Symposium on Applied Computing,"Salamanca, Spain",2015,9781450331968.0,,https://doi.org/10.1145/2695664.2696059;http://dx.doi.org/10.1145/2695664.2696059,10.1145/2695664.2696059,"Several studies are focused on the study of code smells and many detection techniques have been proposed. In this scenario, the use of rules involving software-metrics has been widely used in refactoring tools as a mechanism to detect code smells automatically. However, actual approaches present two unsatisfactory aspects: they present a low agreement in its results and, they do not consider the developers' feedback. In this way, these approaches detect smells that are not relevant to the developers. In order to solve the above mentioned unsatisfactory aspects in the state-of the-art of code smells detection, we propose the Smell Platform able to recognize code smells more relevant to developers by using its feedback. In this paper we present how such platform is able to detect four well known code smells. Finally, we evaluate the Smell Platform comparing its results with traditional detection techniques.","refactoring, code smell detection, developer's feedback",SAC '15,,,
Conference Paper,"Schumacher J,Zazworka N,Shull F,Seaman C,Shaw M",Building Empirical Support for Automated Code Smell Detection,,2010,,,,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 2010 ACM-IEEE International Symposium on Empirical Software Engineering and Measurement,"Bolzano-Bozen, Italy",2010,9781450300391.0,,https://doi.org/10.1145/1852786.1852797;http://dx.doi.org/10.1145/1852786.1852797,10.1145/1852786.1852797,"Identifying refactoring opportunities in software systems is an important activity in today's agile development environments. The concept of code smells has been proposed to characterize different types of design shortcomings in code. Additionally, metric-based detection algorithms claim to identify the ""smelly"" components automatically. This paper presents results for an empirical study performed in a commercial environment. The study investigates the way professional software developers detect god class code smells, then compares these results to automatic classification. The results show that, even though the subjects perceive detecting god classes as an easy task, the agreement for the classification is low. Misplaced methods are a strong driver for letting subjects identify god classes as such. Earlier proposed metric-based detection approaches performed well compared to the human classification. These results lead to the conclusion that an automated metric-based pre-selection decreases the effort spent on manual code inspections. Automatic detection accompanied by a manual review increases the overall confidence in the results of metric-based classifiers.","code inspection, god class, empirical study, code smells, maintainability",ESEM '10,,,
Journal Article,"Sahin D,Kessentini M,Bechikh S,Deb K",Code-Smell Detection as a Bilevel Problem,ACM Trans. Softw. Eng. Methodol.,2014,24.0,1.0,,Association for Computing Machinery,"New York, NY, USA",,,2014-10,,1049-331X,https://doi.org/10.1145/2675067;http://dx.doi.org/10.1145/2675067,10.1145/2675067,"Code smells represent design situations that can affect the maintenance and evolution of software. They make the system difficult to evolve. Code smells are detected, in general, using quality metrics that represent some symptoms. However, the selection of suitable quality metrics is challenging due to the absence of consensus in identifying some code smells based on a set of symptoms and also the high calibration effort in determining manually the threshold value for each metric. In this article, we propose treating the generation of code-smell detection rules as a bilevel optimization problem. Bilevel optimization problems represent a class of challenging optimization problems, which contain two levels of optimization tasks. In these problems, only the optimal solutions to the lower-level problem become possible feasible candidates to the upper-level problem. In this sense, the code-smell detection problem can be treated as a bilevel optimization problem, but due to lack of suitable solution techniques, it has been attempted to be solved as a single-level optimization problem in the past. In our adaptation here, the upper-level problem generates a set of detection rules, a combination of quality metrics, which maximizes the coverage of the base of code-smell examples and artificial code smells generated by the lower level. The lower level maximizes the number of generated artificial code smells that cannot be detected by the rules produced by the upper level. The main advantage of our bilevel formulation is that the generation of detection rules is not limited to some code-smell examples identified manually by developers that are difficult to collect, but it allows the prediction of new code-smell behavior that is different from those of the base of examples. The statistical analysis of our experiments over 31 runs on nine open-source systems and one industrial project shows that seven types of code smells were detected with an average of more than 86% in terms of precision and recall. The results confirm the outperformance of our bilevel proposal compared to state-of-art code-smell detection techniques. The evaluation performed by software engineers also confirms the relevance of detected code smells to improve the quality of software systems.","code smells, Search-based software engineering, software quality",,,,
Conference Paper,"Lujan S,Pecorelli F,Palomba F,De Lucia A,Lenarduzzi V",A Preliminary Study on the Adequacy of Static Analysis Warnings with Respect to Code Smell Prediction,,2020,,,1–6,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 4th ACM SIGSOFT International Workshop on Machine-Learning Techniques for Software-Quality Evaluation,"Virtual, USA",2020,9781450381246.0,,https://doi.org/10.1145/3416505.3423559;http://dx.doi.org/10.1145/3416505.3423559,10.1145/3416505.3423559,"Code smells are poor implementation choices applied during software evolution that can affect source code maintainability. While several heuristic-based approaches have been proposed in the past, machine learning solutions have recently gained attention since they may potentially address some limitations of state-of-the-art approaches. Unfortunately, however, machine learning-based code smell detectors still suffer from low accuracy. In this paper, we aim at advancing the knowledge in the field by investigating the role of static analysis warnings as features of machine learning models for the detection of three code smell types. We first verify the potential contribution given by these features. Then, we build code smell prediction models exploiting the most relevant features coming from the first analysis. The main finding of the study reports that the warnings given by the considered tools lead the performance of code smell prediction models to drastically increase with respect to what reported by previous research in the field.","Machine Learning, Static Analysis Tools, Code Smells",MaLTeSQuE 2020,,,
Conference Paper,"Sharma T,Fragkoulis M,Spinellis D",Does Your Configuration Code Smell?,,2016,,,189–200,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 13th International Conference on Mining Software Repositories,"Austin, Texas",2016,9781450341868.0,,https://doi.org/10.1145/2901739.2901761;http://dx.doi.org/10.1145/2901739.2901761,10.1145/2901739.2901761,"Infrastructure as Code (IaC) is the practice of specifying computing system configurations through code, and managing them through traditional software engineering methods. The wide adoption of configuration management and increasing size and complexity of the associated code, prompt for assessing, maintaining, and improving the configuration code's quality. In this context, traditional software engineering knowledge and best practices associated with code quality management can be leveraged to assess and manage configuration code quality. We propose a catalog of 13 implementation and 11 design configuration smells, where each smell violates recommended best practices for configuration code. We analyzed 4,621 Puppet repositories containing 8.9 million lines of code and detected the cataloged implementation and design configuration smells. Our analysis reveals that the design configuration smells show 9% higher average co-occurrence among themselves than the implementation configuration smells. We also observed that configuration smells belonging to a smell category tend to co-occur with configuration smells belonging to another smell category when correlation is computed by volume of identified smells. Finally, design configuration smell density shows negative correlation whereas implementation configuration smell density exhibits no correlation with the size of a configuration management system.","technical debt, maintainability, infrastructure as code, code quality, configuration smells",MSR '16,,,
Conference Paper,"Fontana FA,Ferme V,Zanoni M,Yamashita A",Automatic Metric Thresholds Derivation for Code Smell Detection,,2015,,,44–53,IEEE Press,"Florence, Italy",Proceedings of the Sixth International Workshop on Emerging Trends in Software Metrics,,2015,,,,,"Code smells are archetypes of design shortcomings in the code that can potentially cause problems during maintenance. One known approach for detecting code smells is via detection rules: a combination of different object-oriented metrics with pre-defined threshold values. The usage of inadequate thresholds when using this approach could lead to either having too few observations (too many false negatives) or too many observations (too many false positives). Furthermore, without a clear methodology for deriving thresholds, one is left with those suggested in literature (or by the tool vendors), which may not necessarily be suitable to the context of analysis. In this paper, we propose a data-driven (i.e., benchmark-based) method to derive threshold values for code metrics, which can be used for implementing detection rules for code smells. Our method is transparent, repeatable and enables the extraction of thresholds that respect the statistical properties of the metric in question (such as scale and distribution). Thus, our approach enables the calibration of code smell detection rules by selecting relevant systems as benchmark data. To illustrate our approach, we generated a benchmark dataset based on 74 systems of the Qualitas Corpus, and extracted the thresholds for five smell detection rules.",,WETSoM '15,,,
Conference Paper,"Cruz D,Santana A,Figueiredo E",Detecting Bad Smells with Machine Learning Algorithms: An Empirical Study,,2020,,,31–40,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 3rd International Conference on Technical Debt,"Seoul, Republic of Korea",2020,9781450379601.0,,https://doi.org/10.1145/3387906.3388618;http://dx.doi.org/10.1145/3387906.3388618,10.1145/3387906.3388618,"Bad smells are symptoms of bad design choices implemented on the source code. They are one of the key indicators of technical debts, specifically, design debt. To manage this kind of debt, it is important to be aware of bad smells and refactor them whenever possible. Therefore, several bad smell detection tools and techniques have been proposed over the years. These tools and techniques present different strategies to perform detections. More recently, machine learning algorithms have also been proposed to support bad smell detection. However, we lack empirical evidence on the accuracy and efficiency of these machine learning based techniques. In this paper, we present an evaluation of seven different machine learning algorithms on the task of detecting four types of bad smells. We also provide an analysis of the impact of software metrics for bad smell detection using a unified approach for interpreting the models' decisions. We found that with the right optimization, machine learning algorithms can achieve good performance (F1 score) for two bad smells: God Class (0.86) and Refused Parent Bequest (0.67). We also uncovered which metrics play fundamental roles for detecting each bad smell.","empirical software engineering, software measurement, machine learning, software quality, bad smells detection",TechDebt '20,,,
Conference Paper,"Oliveto R,Gethers M,Bavota G,Poshyvanyk D,De Lucia A",Identifying Method Friendships to Remove the Feature Envy Bad Smell (NIER Track),,2011,,,820–823,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 33rd International Conference on Software Engineering,"Waikiki, Honolulu, HI, USA",2011,9781450304450.0,,https://doi.org/10.1145/1985793.1985913;http://dx.doi.org/10.1145/1985793.1985913,10.1145/1985793.1985913,"We propose a novel approach to identify Move Method refactoring opportunities and remove the Feature Envy bad smell from source code. The proposed approach analyzes both structural and conceptual relationships between methods and uses Relational Topic Models to identify sets of methods that share several responsabilities, i.e., 'friend methods'. The analysis of method friendships of a given method can be used to pinpoint the target class (envied class) where the method should be moved in. The results of a preliminary empirical evaluation indicate that the proposed approach provides meaningful refactoring opportunities.","refactoring, source code quality, relational topic model",ICSE '11,,,
Conference Paper,"Charalampidou S,Ampatzoglou A,Chatzigeorgiou A,Avgeriou P",Assessing Code Smell Interest Probability: A Case Study,,2017,,,,Association for Computing Machinery,"New York, NY, USA",Proceedings of the XP2017 Scientific Workshops,"Cologne, Germany",2017,9781450352642.0,,https://doi.org/10.1145/3120459.3120465;http://dx.doi.org/10.1145/3120459.3120465,10.1145/3120459.3120465,"An important parameter in deciding to eliminate technical debt (TD) is the probability of a module to generate interest along software evolution. In this study, we explore code smells, which according to practitioners are the most commonly occurring type of TD in industry, by assessing the associated interest probability. As a proxy of smell interest probability we use the frequency of smell occurrences and the change proneness of the modules in which they are identified. To achieve this goal we present a case study on 47,751 methods extracted from two well-known open source projects. The results of the case study suggest that: (a) modules in which ""code smells"" are concentrated are more change-prone than smell-free modules, (b) there are specific types of ""code smells"" that are concentrated in the most change-prone modules, and (c) interest probability of code clones seems to be higher than the other two examined code smells. These results can be useful for both researchers and practitioners, in the sense that the former can focus their research on resolving ""code smells"" that produce more interest, and the latter can improve accordingly the prioritization of their repayment strategy and their training.","case study, technical debt, interest probability, change proneness",XP '17,,,
Conference Paper,Hamilton DE,Tracked Changes: Navigating the Document-Format Anti-Pattern,,2014,,,,Association for Computing Machinery,"New York, NY, USA","Proceedings of the 2nd International Workshop on (Document) Changes: Modeling, Detection, Storage and Visualization","Fort Collins, CO, USA",2014,9781450329644.0,,https://doi.org/10.1145/2723147.2723153;http://dx.doi.org/10.1145/2723147.2723153,10.1145/2723147.2723153,"Editing of word-processing documents at the presentation level, with visible tracking of changes, operates at a different level of abstraction and granularity than the recorded form in common document-file formats. The consequent mismatches along with other limitations of standards for document-file formats present an anti-pattern that impedes reliable inter-product exchange of change-tracked documents. Analysis of the situation for ODF change-tracking reveals simple extensions and definitions that supplement the current specification without introducing any conflicts. Patterns of systematic testing for conformant document files, compliant processing, and verifiable interoperability are identified as essential prerequisites to dependable improvement of change-tracking in collaborative settings.","document formats, WYSIWYG, OpenDocument, user conceptualization, change tracking, over-lapping markup, XML",DChanges '14,,,
Conference Paper,"Martins J,Bezerra C,Uchôa A,Garcia A",Are Code Smell Co-Occurrences Harmful to Internal Quality Attributes? A Mixed-Method Study,,2020,,,52–61,Association for Computing Machinery,"New York, NY, USA",Proceedings of the XXXIV Brazilian Symposium on Software Engineering,"Natal, Brazil",2020,9781450387538.0,,https://doi.org/10.1145/3422392.3422419;http://dx.doi.org/10.1145/3422392.3422419,10.1145/3422392.3422419,"Previous studies demonstrated how code smells (i.e., symptoms of the presence of system degradation) impact the software maintainability. However, few studies have investigated which code smell types tend to co-occur in the source code. Moreover, it is not clear to what extent the removal of code smell co-occurrences -through refactoring operations - has a positive impact on quality attributes such as cohesion, coupling, inheritance, complexity, and size. We aim at addressing these gaps through an empirical study. By investigating the impact of the smells co-occurrences in 11 releases of 3 closed-source systems, we observe (i) which code smells tend to co-occur together, (ii) the impact of the removal of code smell co-occurrences on quality internal attributes before and after refactoring, and (iii) which are the most difficult co-occurrences to refactoring from the developers' perspective. Our results show that 2 types of code smell co-occurrences generally tend to co-occur. Moreover, we observed that the removal of code smells co-occurrences lead to a significant reduction in the complexity of the systems studied was obtained. Conversely, cohesion and coupling tend to get worse. We also found that two code smells cooccurrences (God Class-Long Method and Disperse Coupling-Long Method) as the most difficult to refactor indicating that attention is needed not to insert these anomalies in the source code. Based on our findings, we argue that further research is needed on the impact of code smells co-occurrences on internal quality attributes.","Code Smells Co-occurrences, Refactoring, Quality Attributes",SBES '20,,,
Conference Paper,"de Mello R,Uchôa A,Oliveira R,Oliveira D,Fonseca B,Garcia A,de Mello F",Investigating the Social Representations of Code Smell Identification: A Preliminary Study,,2019,,,53–60,IEEE Press,"Montreal, Quebec, Canada",Proceedings of the 12th International Workshop on Cooperative and Human Aspects of Software Engineering,,2019,,,https://doi.org/10.1109/CHASE.2019.00022;http://dx.doi.org/10.1109/CHASE.2019.00022,10.1109/CHASE.2019.00022,"Context: The identification of code smells is one of the most subjective tasks in software engineering. A key reason is the influence of collective aspects of communities working on this task, such as their beliefs regarding the relevance of certain smells. However, collective aspects are often neglected in the context of smell identification. For this purpose, we can use the social representations theory. Social representations comprise the set of values, behaviors and practices of communities associated with a social object, such as the task of identifying smells. Aim: To characterize the social representations behind smell identification. Method: We conducted a preliminary study on the social representations of smell identification by two communities. One community is composed of postgraduate students involved in various investigations related to code smells. The other community is composed of practitioners from industry, with experience in code reviews. We analyzed the associations made by the study participants about smell identification, i.e., what immediately comes to their minds when they think about this task. Results: One of the key findings is that only the community of practitioners strongly associates this task with semantic smells. This finding suggests research directions on code smells may be revisited, as they focus mostly on measurable or structural smells. Considering the novelty of using the social representations theory in software engineering, we also compiled a set of lessons learned. For instance, we observed some key challenges we faced in using the theory. These challenges include: (i) the predominance of associations with technical rather than non-technical concepts, and (ii) the fuzzy definitions of key concepts in our field. Conclusion: We found initial evidence that social representations analysis is a useful instrument to reveal discrepancies and commonalities on how different communities deal with a subjective task. Thus, we expect the experience reported in this paper may encourage and contribute to future studies of social representations in the field.","qualitative research, social representation, code smells",CHASE '19,,,
Conference Paper,"da S. Carvalho LP,Novais R,Mendonça M","Investigating the Relationship between Code Smell Agglomerations and Architectural Concerns: Similarities and Dissimilarities from Distributed, Service-Oriented, and Mobile Systems",,2018,,,3–12,Association for Computing Machinery,"New York, NY, USA","Proceedings of the VII Brazilian Symposium on Software Components, Architectures, and Reuse","Sao Carlos, Brazil",2018,9781450365543.0,,https://doi.org/10.1145/3267183.3267184;http://dx.doi.org/10.1145/3267183.3267184,10.1145/3267183.3267184,"Context: software architects often decide on strategies before incorporating an asset (e.g., components) in software systems. At the same time, they are responsible for preventing code and architectural degradation caused by design problems. Problem: groups of code smells (a.k.a. agglomeration of code smells) have been recognized as a source of design problems, but no previous study has analyzed the relationship between such agglomerations and different types of software. Different types of software have different needs in terms of implementation of architectural concerns, which can lead to consequential variations in the way how code smells agglomerate. Goal: this study aims to understand how a varied set of projects and their respective architectural concerns relates to code smells agglomerations. Method: our study analyses the history of 15 Open Source Software (OSS) projects split as three groups of distributed, service-oriented, and mobile project types. It mines the projects for code smells and architectural concerns (identified from injected components). It agglomerates instances of code smells around these concerns, and analyzes them according to the grouped projects. Results/Discussion: the agglomerations of smells tend to follow a stratified pattern in which they group themselves through ramifications of similarities and dissimilarities of concerns and project types.","Component, Architecture, Code Smell, Agglomeration, Concern",SBCARS '18,,,
Conference Paper,"Fontana FA,Ferme V,Zanoni M",Towards Assessing Software Architecture Quality by Exploiting Code Smell Relations,,2015,,,1–7,IEEE Press,"Florence, Italy",Proceedings of the Second International Workshop on Software Architecture and Metrics,,2015,,,,,"We can evaluate software architecture quality using a plethora of metrics proposed in the literature, but interpreting and exploiting in the right way these metrics is not always a simple task. This is true for both fixing the right metric threshold values and determining the actions to be taken to improve the quality of the system. Instead of metrics, we can detect code or architectural anomalies that give us useful hints on the possible architecture degradation. In this paper, we focus our attention on the detection of code smells and in particular on their relations and co-occurrences, with the aim to evaluate tecnical debt in an architectural context. We start from the assumption that certain patterns of code anomalies tend to be better indicators of architectural degradation than simple metrics evaluation.",,SAM '15,,,
Conference Paper,"Parnin C,Görg C,Nnadi O",A Catalogue of Lightweight Visualizations to Support Code Smell Inspection,,2008,,,77–86,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 4th ACM Symposium on Software Visualization,"Ammersee, Germany",2008,9781605581125.0,,https://doi.org/10.1145/1409720.1409733;http://dx.doi.org/10.1145/1409720.1409733,10.1145/1409720.1409733,"Preserving the integrity of software systems is essential in ensuring future product success. Commonly, companies allocate only a limited budget toward perfective maintenance and instead pressure developers to focus on implementing new features. Traditional techniques, such as code inspection, consume many staff resources and attention from developers. Metrics automate the process of checking for problems but produce voluminous, imprecise, and incongruent results. An opportunity exists for visualization to assist where automated measures have failed; however, current software visualization techniques only handle the voluminous aspect of data but fail to address imprecise and incongruent aspects. In this paper, we describe several techniques for visualizing possible defects reported by automated inspection tools. We propose a catalogue of lightweight visualizations that assist reviewers in weeding out false positives. We implemented the visualizations in a tool called NOSEPRINTS and present a case study on several commercial systems and open source applications in which we examined the impact of our tool on the inspection process.","refactoring, lightweight visualization, code smells, code inspection",SoftVis '08,,,
Conference Paper,"Tummalapalli S,Kumar L,Murthy NL",Prediction of Web Service Anti-Patterns Using Aggregate Software Metrics and Machine Learning Techniques,,2020,,,,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 13th Innovations in Software Engineering Conference on Formerly Known as India Software Engineering Conference,"Jabalpur, India",2020,9781450375948.0,,https://doi.org/10.1145/3385032.3385042;http://dx.doi.org/10.1145/3385032.3385042,10.1145/3385032.3385042,"Service-Oriented Architecture(SOA) can be characterized as an approximately coupled engineering intended to meet the business needs of an association/organization. Service-Based Systems (SBSs) are inclined to continually change to enjoy new client necessities and adjust the execution settings, similar to some other huge and complex frameworks. These changes may lead to the evolution of designs/products with poor Quality of Service (QoS), resulting in the bad practiced solutions, commonly known as Anti-patterns. Anti-patterns makes the evolution and maintenance of the software systems hard and complex. Early identification of modules, classes, or source code regions where anti-patterns are more likely to occur can help in amending and maneuvering testing efforts leading to the improvement of software quality. In this work, we investigate the application of three sampling techniques, three feature selection techniques, and sixteen different classification techniques to develop the models for web service anti-pattern detection. We report the results of an empirical study by evaluating the approach proposed, on a data set of 226 Web Service Description Language(i.e., WSDL)files, a variety of five types of web-service anti-patterns. Experimental results demonstrated that SMOTE is the best performing data sampling techniques. The experimental results also reveal that the model developed by considering Uncorrelated Significant Predictors(SUCP) as the input obtained better performance compared to the model developed by other metrics. Experimental results also show that the Least Square Support Vector Machine with Linear(LSLIN) function has outperformed all other classifier techniques.","Classifiers, Web-Services, Anti-pattern, Machine Learning, WSDL, Aggregation measures, Feature Selection, Service-Based Systems(SBS), Class imbalance distribution, Source Code Metrics",ISEC 2020,,,
Conference Paper,"Verhaeghe B,Fuhrman C,Guerrouj L,Anquetil N,Ducasse S",Empirical Study of Programming to an Interface,,2020,,,847–850,IEEE Press,"San Diego, California",Proceedings of the 34th IEEE/ACM International Conference on Automated Software Engineering,,2020,9781728125084.0,,https://doi.org/10.1109/ASE.2019.00083;http://dx.doi.org/10.1109/ASE.2019.00083,10.1109/ASE.2019.00083,"A popular recommendation to programmers in object-oriented software is to ""program to an interface, not an implementation"" (PTI). Expected benefits include increased simplicity from abstraction, decreased dependency on implementations, and higher flexibility. Yet, interfaces must be immutable, excessive class hierarchies can be a form of complexity, and ""speculative generality"" is a known code smell. To advance the empirical knowledge of PTI, we conducted an empirical investigation that involves 126 Java projects on GitHub, aiming to measuring the decreased dependency benefits (in terms of cochange).","Java interfaces, empirical study, GitHub, software repositories, cochange, coupling",ASE '19,,,
Conference Paper,"Oliveira D,Assunção WK,Souza L,Oizumi W,Garcia A,Fonseca B",Applying Machine Learning to Customized Smell Detection: A Multi-Project Study,,2020,,,233–242,Association for Computing Machinery,"New York, NY, USA",Proceedings of the XXXIV Brazilian Symposium on Software Engineering,"Natal, Brazil",2020,9781450387538.0,,https://doi.org/10.1145/3422392.3422427;http://dx.doi.org/10.1145/3422392.3422427,10.1145/3422392.3422427,"Code smells are considered symptoms of poor implementation choices, which may hamper the software maintainability. Hence, code smells should be detected as early as possible to avoid software quality degradation. Unfortunately, detecting code smells is not a trivial task. Some preliminary studies investigated and concluded that machine learning (ML) techniques are a promising way to better support smell detection. However, these techniques are hard to be customized to promote an early and accurate detection of specific smell types. Yet, ML techniques usually require numerous code examples to be trained (composing a relevant dataset) in order to achieve satisfactory accuracy. Unfortunately, such a dependency on a large validated dataset is impractical and leads to late detection of code smells. Thus, a prevailing challenge is the early customized detection of code smells taking into account the typical limited training data. In this direction, this paper reports a study in which we collected code smells, from ten active projects, that were actually refactored by developers, differently from studies that rely on code smells inferred by researchers. These smells were used for evaluating the accuracy regarding early detection of code smells by using seven ML techniques. Once we take into account such smells that were considered as important by developers, the ML techniques are able to customize the detection in order to focus on smells observed as relevant in the investigated systems. The results showed that all the analyzed techniques are sensitive to the type of smell and obtained good results for the majority of them, especially JRip and Random Forest. We also observe that the ML techniques did not need a high number of examples to reach their best accuracy results. This finding implies that ML techniques can be successfully used for early detection of smells without depending on the curation of a large dataset.","software quality, code smell, code smell detection",SBES '20,,,
Conference Paper,"Rahkema K,Pfahl D",Empirical Study on Code Smells in IOS Applications,,2020,,,61–65,Association for Computing Machinery,"New York, NY, USA",Proceedings of the IEEE/ACM 7th International Conference on Mobile Software Engineering and Systems,"Seoul, Republic of Korea",2020,9781450379595.0,,https://doi.org/10.1145/3387905.3388597;http://dx.doi.org/10.1145/3387905.3388597,10.1145/3387905.3388597,"Code smells are recurring patterns in code that have been identified as bad practices. They have been analysed extensively in Java desktop applications. For mobile applications most of the research has been done for Android with very little research done for iOS. Although Android has the largest market share, iOS is a very popular platform. Our goal is to understand the distribution of code smells in iOS applications. For this analysis we used a collaborative list of open source iOS applications from GitHub. We combined code smells defined by Fowler and object oriented code smells studied on Android. We developed a tool that can detect these code smells in Swift applications. We discovered that iOS applications are most often affected by Lazy Class, Long Method and Message Chain code smells. Most often occurring code smells are Internal Duplication, Lazy Class and Long Method.","empirical study, iOS, mobile applications, code smells",MOBILESoft '20,,,
Conference Paper,"Sousa BL,Souza PP,Fernandes E,Ferreira KA,Bigonha MA",FindSmells: Flexible Composition of Bad Smell Detection Strategies,,2017,,,360–363,IEEE Press,"Buenos Aires, Argentina",Proceedings of the 25th International Conference on Program Comprehension,,2017,9781538605356.0,,https://doi.org/10.1109/ICPC.2017.8;http://dx.doi.org/10.1109/ICPC.2017.8,10.1109/ICPC.2017.8,"Bad smells are symptoms of problems in the source code of software systems. They may harm the maintenance and evolution of systems on different levels. Thus, detecting smells is essential in order to support the software quality improvement. Since even small systems may contain several bad smell instances, and considering that developers have to prioritize their elimination, its automated detection is a necessary support for developers. Regarding that, detection strategies have been proposed to formalize rules to detect specific bad smells, such as Large Class and Feature Envy. Several tools like JDeodorant and JSpIRIT implement these strategies but, in general, they do not provide full customization of the formal rules that define a detection strategy. In this paper, we propose FindSmells, a tool for detecting bad smells in software systems through software metrics and their thresholds. With FindSmells, the user can compose and manage different strategies, which run without source code analysis. We also provide a running example of the tool.A Video: https://youtu.be/LtomN93y6gg",,ICPC '17,,,
Conference Paper,"Tsantalis N,Guana V,Stroulia E,Hindle A",A Multidimensional Empirical Study on Refactoring Activity,,2013,,,132–146,IBM Corp.,USA,Proceedings of the 2013 Conference of the Center for Advanced Studies on Collaborative Research,"Ontario, Canada",2013,,,,,"In this paper we present an empirical study on the refactoring activity in three well-known projects. We have studied five research questions that explore the different types of refactorings applied to different types of sources, the individual contribution of team members on refactoring activities, the alignment of refactoring activity with release dates and testing periods, and the motivation behind the applied refactorings. The studied projects have a history of 12, 7, and 6 years, respectively. We have found that there is very little variation in the types of refactorings applied on test code, since the majority of the refactorings focus on the reorganization and renaming of classes. Additionally, we have identified that the refactoring decision making and application is often performed by individual refactoring ""managers"". We have found a strong alignment between refactoring activity and release dates. Moreover, we found that the development teams apply a considerable amount of refactorings during testing periods. Finally, we have also found that in addition to code smell resolution the main drivers for applying refactorings are the introduction of extension points, and the resolution of backward compatibility issues.",,CASCON '13,,,
Conference Paper,"Wang P,Brown C,Jennings JA,Stolee KT",An Empirical Study on Regular Expression Bugs,,2020,,,103–113,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 17th International Conference on Mining Software Repositories,"Seoul, Republic of Korea",2020,9781450375177.0,,https://doi.org/10.1145/3379597.3387464;http://dx.doi.org/10.1145/3379597.3387464,10.1145/3379597.3387464,"Understanding the nature of regular expression (regex) issues is important to tackle practical issues developers face in regular expression usage. Knowledge about the nature and frequency of various types of regular expression issues, such as those related to performance, API misuse, and code smells, can guide testing, inform documentation writers, and motivate refactoring efforts. However, beyond ReDoS (Regular expression Denial of Service), little is known about to what extent regular expression issues affect software development and how these issues are addressed in practice.This paper presents a comprehensive empirical study of 350 merged regex-related pull requests from Apache, Mozilla, Facebook, and Google GitHub repositories. Through classifying the root causes and manifestations of those bugs, we show that incorrect regular expression behavior is the dominant root cause of regular expression bugs (165/356, 46.3%). The remaining root causes are incorrect API usage (9.3%) and other code issues that require regular expression changes in the fix (29.5%). By studying the code changes of regex-related pull requests, we observe that fixing regular expression bugs is nontrivial as it takes more time and more lines of code to fix them compared to the general pull requests. The results of this study contribute to a broader understanding of the practical problems faced by developers when using regular expressions.","pull requests, Regular expression bug characteristics, bug fixes",MSR '20,,,
Conference Paper,"de Almeida Filho FG,Martins AD,Vinuto TS,Monteiro JM,de Sousa ÍP,de Castro Machado J,Rocha LS",Prevalence of Bad Smells in PL/SQL Projects,,2019,,,116–121,IEEE Press,"Montreal, Quebec, Canada",Proceedings of the 27th International Conference on Program Comprehension,,2019,,,https://doi.org/10.1109/ICPC.2019.00025;http://dx.doi.org/10.1109/ICPC.2019.00025,10.1109/ICPC.2019.00025,"Code Smell can be defined as any feature in the source code of a software that may indicate possible problems. In database languages, the term Bad Smell has been used as a generalization of Code Smell, once some features that are not directly related to code also can indicate problems, such as, for instance, the inappropriate type of an index structure or a SQL query written inefficiently. Bearing in mind the recurrence of different Bad Smell, they were catalogued. Along with these catalogs, tools were developed to automatically identify Bad Smell occurrences in a given code. With the help of these tools, it has become possible to perform quick and effective analysis. In this context, this paper proposes an exploratory study about Bad Smell in PL/SQL codes, from free software projects, published on GitHub. We analyzed 20 open-source PL/SQL projects and empirically study the prevalence of bad smells. Our results showed that some smells occur together. Besides, some smells are more frequent than others. Based on this principle, this paper has the potential to aid professionals from the databases area to avoid future problems during the development of a PL/SQL project.","bad smell, PL/SQL, prevalence, code smell",ICPC '19,,,
Conference Paper,"Foidl H,Felderer M",Risk-Based Data Validation in Machine Learning-Based Software Systems,,2019,,,13–18,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 3rd ACM SIGSOFT International Workshop on Machine Learning Techniques for Software Quality Evaluation,"Tallinn, Estonia",2019,9781450368551.0,,https://doi.org/10.1145/3340482.3342743;http://dx.doi.org/10.1145/3340482.3342743,10.1145/3340482.3342743,"Data validation is an essential requirement to ensure the reliability and quality of Machine Learning-based Software Systems. However, an exhaustive validation of all data fed to these systems (i.e. up to several thousand features) is practically unfeasible. In addition, there has been little discussion about methods that support software engineers of such systems in determining how thorough to validate each feature (i.e. data validation rigor). Therefore, this paper presents a conceptual data validation approach that prioritizes features based on their estimated risk of poor data quality. The risk of poor data quality is determined by the probability that a feature is of low data quality and the impact of this low (data) quality feature on the result of the machine learning model. Three criteria are presented to estimate the probability of low data quality (Data Source Quality, Data Smells, Data Pipeline Quality). To determine the impact of low (data) quality features, the importance of features according to the performance of the machine learning model (i.e. Feature Importance) is utilized. The presented approach provides decision support (i.e. data validation prioritization and rigor) for software engineers during the implementation of data validation techniques in the course of deploying a trained machine learning model and its software stack.","Risk-based Testing, Data Validation, Machine Learning",MaLTeSQuE 2019,,,
Journal Article,"Lucia,Lo D,Scanniello G,Marchetto A,Ali N,McMillan C","Leveraging Machine Learning and Information Retrieval Techniques in Software Evolution Tasks: Summary of the First MALIR-SE Workshop, at ASE 2013",SIGSOFT Softw. Eng. Notes,2014,39.0,1.0,1–2,Association for Computing Machinery,"New York, NY, USA",,,2014-02,,0163-5948,https://doi.org/10.1145/2557833.2560584;http://dx.doi.org/10.1145/2557833.2560584,10.1145/2557833.2560584,"The first International Workshop on MAchine Learning and Information Retrieval for Software Evolution (MALIR-SE) was held on the 11th of November 2013. The workshop was held in conjunction with the 28th IEEE/ACM International Conference on Automated Software Engineering (ASE) in Silicon Valley, California, USA. The workshop brought researchers and practitioners that were interested in leveraging machine learning and information retrieval techniques to automate various software evolution tasks. During the workshop, papers on the application of machine learning and information retrieval techniques to bug fix time prediction and anti-pattern detection were presented. There were also discussions on the presented papers and on future direction of research in the area.","machine learning, workshop report, information retrieval",,,,
Conference Paper,"Baudart G,Hirzel M,Kate K,Mandel L,Shinnar A",Machine Learning in Python with No Strings Attached,,2019,,,1–9,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 3rd ACM SIGPLAN International Workshop on Machine Learning and Programming Languages,"Phoenix, AZ, USA",2019,9781450367196.0,,https://doi.org/10.1145/3315508.3329972;http://dx.doi.org/10.1145/3315508.3329972,10.1145/3315508.3329972,"Machine-learning frameworks in Python, such as scikit-learn, Keras, Spark, or Pyro, use embedded domain specific languages (EDSLs) to assemble a computational graph. Unfortunately, these EDSLs make heavy use of strings as names for computational graph nodes and other entities, leading to repetitive and hard-to-maintain code that does not benefit from standard Python tooling. This paper proposes eliminating strings where possible, reusing Python variable names instead. We demonstrate this on two examples from opposite ends of the design space: Keras.na, a light-weight wrapper around the Keras library, and , a new embedding of Stan into Python. Our techniques do not require modifications to the underlying library. Avoiding strings removes redundancy, simplifies maintenance, and enables Python tooling to better reason about the code and assist users.","Programming Languages, Machine Learning",MAPL 2019,,,
Conference Paper,"De Ruvo G,Tempero E,Luxton-Reilly A,Giacaman N",Unencapsulated Collection: A Teachable Design Smell,,2018,,,332–337,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 49th ACM Technical Symposium on Computer Science Education,"Baltimore, Maryland, USA",2018,9781450351034.0,,https://doi.org/10.1145/3159450.3159469;http://dx.doi.org/10.1145/3159450.3159469,10.1145/3159450.3159469,"Design smells are design structures that indicate poor design quality. Many identified smells are difficult to teach as they require a degree of experience and judgement that novices, by definition, do not have. We have identified a design smell, which we call ""unencapsulated collection"", that is common in novice designs. It is simple to describe, allowing it to be objectively detected, and the refactoring steps needed to remove the smell are usually simple to illustrate. We give a description of the smell and present the results of an empirical study showing its prevalence. We outline the general steps for refactoring the smell, and illustrate it with a case study. The simplicity of this smell makes it a good candidate for teaching good design principles to novices.","software engineering, collections, design smells, abstraction",SIGCSE '18,,,
Conference Paper,"Trindade RP,da Silva Bigonha MA,Ferreira KA",Oracles of Bad Smells: A Systematic Literature Review,,2020,,,62–71,Association for Computing Machinery,"New York, NY, USA",Proceedings of the XXXIV Brazilian Symposium on Software Engineering,"Natal, Brazil",2020,9781450387538.0,,https://doi.org/10.1145/3422392.3422415;http://dx.doi.org/10.1145/3422392.3422415,10.1145/3422392.3422415,"A bad smell is an evidence of a design problem that may be harmful to the software maintenance. Several studies have been carried out to aid the identification of bad smells, by defining approaches or tools. Usually, the evaluation of these studies' results relies on data of oracles bad smells. An oracle is a set of data of bad smells found in a given software system. Such data serves as a referential template or a benchmark to evaluate the proposals on detecting bad smells. The availability and the quality of bad smell oracles are crucial to assert the quality of detection strategies of bad smells. This study aims to compile the bad smell oracles proposed in the literature. To achieve this, we conducted a Systematic Literature Review (SLR) to identify bad smell oracles and their characteristics. The main result of this study is a catalog of bad smell oracles that may be useful for research on bad smells, especially the studies that propose tools or detection strategies for bad smells.","systematic literature review, benchmark, code smell, bad smell, oracle, design anomaly",SBES '20,,,
Conference Paper,"Silva P,Bezerra CI,Lima R,Machado I",Classifying Feature Models Maintainability Based on Machine Learning Algorithms,,2020,,,1–10,Association for Computing Machinery,"New York, NY, USA","Proceedings of the 14th Brazilian Symposium on Software Components, Architectures, and Reuse","Natal, Brazil",2020,9781450387545.0,,https://doi.org/10.1145/3425269.3425276;http://dx.doi.org/10.1145/3425269.3425276,10.1145/3425269.3425276,"Maintenance in the context of SPLs is a topic of interest, and that still needs further investigation. There are several ways to evaluate the maintainability of a feature model (FM), one of which is a manual or automated analysis of quality measures. However, the use of measures does not allow to evaluate the FM quality as a whole, as each measure considers a specific characteristic of FM. In general, the measures have wide ranges of values and do not have a clear definition of what is appropriate and inappropriate. In this context, the goal of this work is to investigate the use of machine learning techniques to classify the feature model maintainability. The research questions investigated in the study were: (i) how could machine learning techniques aid to classify FMs maintainability; and, (ii) which FM classification model has the best accuracy and precision. In this work, we proposed an approach for FM maintainability classification using machine learning technics. For that, we used a dataset of 15 FM maintainability measures calculated for 326 FMs, and we used machine learning algorithms to clustering. After this, we used thresholds to evaluate the general maintainability of each cluster. With this, we built 5 maintainability classification models that have been evaluated with the accuracy and precision metrics.","machine learning, feature model, software product line, quality evaluation",SBCARS '20,,,
Conference Paper,"Yamashita A,Moonen L",Exploring the Impact of Inter-Smell Relations on Software Maintainability: An Empirical Study,,2013,,,682–691,IEEE Press,"San Francisco, CA, USA",Proceedings of the 2013 International Conference on Software Engineering,,2013,9781467330763.0,,,,"Code smells are indicators of issues with source code quality that may hinder evolution. While previous studies mainly focused on the effects of individual code smells on maintainability, we conjecture that not only the individual code smells but also the interactions between code smells affect maintenance. We empirically investigate the interactions amongst 12 code smells and analyze how those interactions relate to maintenance problems. Professional developers were hired for a period of four weeks to implement change requests on four medium-sized Java systems with known smells. On a daily basis, we recorded what specific problems they faced and which artifacts were associated with them. Code smells were automatically detected in the pre-maintenance versions of the systems and analyzed using Principal Component Analysis (PCA) to identify patterns of co-located code smells. Analysis of these factors with the observed maintenance problems revealed how smells that were co-located in the same artifact interacted with each other, and affected maintainability. Moreover, we found that code smell interactions occurred across coupled artifacts, with comparable negative effects as same-artifact co-location. We argue that future studies into the effects of code smells on maintainability should integrate dependency analysis in their process so that they can obtain a more complete understanding by including such coupled interactions.",,ICSE '13,,,
Conference Paper,"Steinhauer M,Palomba F",Speeding up the Data Extraction of Machine Learning Approaches: A Distributed Framework,,2020,,,13–18,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 4th ACM SIGSOFT International Workshop on Machine-Learning Techniques for Software-Quality Evaluation,"Virtual, USA",2020,9781450381246.0,,https://doi.org/10.1145/3416505.3423562;http://dx.doi.org/10.1145/3416505.3423562,10.1145/3416505.3423562,"In the last decade, mining software repositories (MSR) has become one of the most important sources to feed machine learning models. Especially open-source projects on platforms like GitHub are providing a tremendous amount of data and make them easily accessible. Nevertheless, there is still a lack of standardized pipelines to extract data in an automated and fast way. Even though several frameworks and tools exist which can fulfill specific tasks or parts of the data extraction process, none of them allow neither building an automated mining pipeline nor the possibility for full parallelization. As a consequence, researchers interested in using mining software repositories to feed machine learning models are often forced to re-implement commonly used tasks leading to additional development time and libraries may not be integrated optimally. This preliminary study aims to demonstrate current limitations of existing tools and Git itself which are threatening the prospects of standardization and parallelization. We also introduce the multi-dimensionality aspects of a Git repository and how they affect the computation time. Finally, as a proof of concept, we define an exemplary pipeline for predicting refactoring operations, assessing its performance. Finally, we discuss the limitations of the pipeline and further optimizations to be done.","Distributed Mining, Machine Learning Pipelines, Mining Software Repositories",MaLTeSQuE 2020,,,
Conference Paper,"Hecht G,Moha N,Rouvoy R",An Empirical Study of the Performance Impacts of Android Code Smells,,2016,,,59–69,Association for Computing Machinery,"New York, NY, USA",Proceedings of the International Conference on Mobile Software Engineering and Systems,"Austin, Texas",2016,9781450341783.0,,https://doi.org/10.1145/2897073.2897100;http://dx.doi.org/10.1145/2897073.2897100,10.1145/2897073.2897100,"Android code smells are bad implementation practices within Android applications (or apps) that may lead to poor software quality, in particular in terms of performance. Yet, performance is a main software quality concern in the development of mobile apps. Correcting Android code smells is thus an important activity to increase the performance of mobile apps and to provide the best experience to mobile end-users while considering the limited constraints of mobile devices (e.g., CPU, memory, battery). However, no empirical study has assessed the positive performance impacts of correcting mobile code smells.In this paper, we therefore conduct an empirical study focusing on the individual and combined performance impacts of three Android performance code smells (namely, Internal Getter/Setter, Member Ignoring Method, and HashMap Usage) on two open source Android apps. To perform this study, we use the Paprika toolkit to detect these three code smells in the analyzed apps, and we derive four versions of the apps by correcting each detected smell independently, and all of them. Then, we evaluate the performance of each version on a common user scenario test. In particular, we evaluate the UI and memory performance using the following metrics: frame time, number of delayed frames, memory usage, and number of garbage collection calls. Our results show that correcting these Android code smells effectively improve the UI and memory performance. In particular, we observe an improvement up to 12.4% on UI metrics when correcting Member Ignoring Method and up to 3.6% on memory-related metrics when correcting the three Android code smells. We believe that developers can benefit from these results to guide their refactoring, and thus improve the quality of their mobile apps.","Android, code smells, metrics, mobile computing, performance",MOBILESoft '16,,,
Conference Paper,"Guo Y,Seaman C,Zazworka N,Shull F",Domain-Specific Tailoring of Code Smells: An Empirical Study,,2010,,,167–170,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 32nd ACM/IEEE International Conference on Software Engineering - Volume 2,"Cape Town, South Africa",2010,9781605587196.0,,https://doi.org/10.1145/1810295.1810321;http://dx.doi.org/10.1145/1810295.1810321,10.1145/1810295.1810321,"Code smells refer to commonly occurring patterns in source code that indicate poor programming practices or code decay. Detecting code smells helps developers find design problems that can cause trouble in future maintenance. Detection rules for code smells, based on software metrics, have been proposed, but they do not take domain-specific characteristics into consideration. In this study we investigate whether such generic heuristics can be tailored to include domain-specific factors. Input into these domain-specific heuristics comes from an iterative empirical field study in a software maintenance project. The results yield valuable insight into code smell detection.","domain-specific, code smells, empirical study",ICSE '10,,,
Conference Paper,Khanve V,Are Existing Code Smells Relevant in Web Games? An Empirical Study,,2019,,,1241–1243,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering,"Tallinn, Estonia",2019,9781450355728.0,,https://doi.org/10.1145/3338906.3342504;http://dx.doi.org/10.1145/3338906.3342504,10.1145/3338906.3342504,"In software applications, code smells are considered as bad coding practices acquired at the time of development. The presence of such code smells in games may affect the process of game development adversely. Our preliminary study aims at investigating the existence of code smells in the games. To achieve this, we used JavaScript code smells detection tool JSNose against 361 JavaScript web games to find occurrences of JavaScript smells in games. Further, we conducted a manual study to find violations of known game programming patterns in 8 web games to verify the necessity of game-specific code smells detection tool. Our results shows that existing JavaScript code smells detection tool is not sufficient to find game-specific code smells in web games.","Code Smells, Game-specific Code Smells, Web Games",ESEC/FSE 2019,,,
Conference Paper,"Nurwidyantoro A,Ho-Quang T,Chaudron MR",Automated Classification of Class Role-Stereotypes via Machine Learning,,2019,,,79–88,Association for Computing Machinery,"New York, NY, USA",Proceedings of the Evaluation and Assessment on Software Engineering,"Copenhagen, Denmark",2019,9781450371452.0,,https://doi.org/10.1145/3319008.3319016;http://dx.doi.org/10.1145/3319008.3319016,10.1145/3319008.3319016,"Role stereotypes indicate generic roles that classes play in the design of software systems (e.g. controller, information holder, or interfacer). Knowledge about the role-stereotypes can help in various tasks in software development and maintenance, such as program understanding, program summarization, and quality assurance. This paper presents an automated machine learning-based approach for classifying the role-stereotype of classes in Java. We analyse the performance of this approach against a manually labelled ground truth for a sizable open source project (of 770+ Java classes) for the Android platform. Moreover, we compare our approach to an existing rule-based classification approach. The contributions of this paper include an analysis of which machine learning algorithms and which features provide the best classification performance. This analysis shows that the Random Forest algorithm yields the best classification performance. We find however, that the performance of the ML-classifier varies a lot for classifying different role-stereotypes. In particular its performs degrades for rare role-types. Our ML-classifier improves over the existing rule-based classification method in that the ML-approach classifies all classes, while rule-based approaches leave a significant number of classes unclassified.","Software Design, Reverse Engineering, Program Understanding",EASE '19,,,
Journal Article,"Peiris M,Hill JH",Towards Detecting Software Performance Anti-Patterns Using Classification Techniques,SIGSOFT Softw. Eng. Notes,2014,39.0,1.0,1–4,Association for Computing Machinery,"New York, NY, USA",,,2014-02,,0163-5948,https://doi.org/10.1145/2557833.2560586;http://dx.doi.org/10.1145/2557833.2560586,10.1145/2557833.2560586,This paper presents a non-intrusive machine learning approach called Non-intrusive Performance Anti-pattern Detecter (NiPAD) for identifying and classifying software performance anti-patterns. NiPAD uses only system performance metrics-as opposed to analyzing application level performance metrics or source code and the design of a software application to identify and classify software performance anti-patterns within an application. The results of applying NiPAD to an example application show that NiPAD is able to predict the One Lane Bridge software performance anti-pattern within a software application with 0.94 accuracy.,"dynamic software analysis, classification, machine learning, software performance anti-patterns",,,,
Conference Paper,"Luiz FC,de Oliveira Rodrigues BR,Parreiras FS",Machine Learning Techniques for Code Smells Detection: An Empirical Experiment on a Highly Imbalanced Setup,,2019,,,,Association for Computing Machinery,"New York, NY, USA",Proceedings of the XV Brazilian Symposium on Information Systems,"Aracaju, Brazil",2019,9781450372374.0,,https://doi.org/10.1145/3330204.3330275;http://dx.doi.org/10.1145/3330204.3330275,10.1145/3330204.3330275,,,SBSI'19,,,
Conference Paper,"Davis JC,Coghlan CA,Servant F,Lee D",The Impact of Regular Expression Denial of Service (ReDoS) in Practice: An Empirical Study at the Ecosystem Scale,,2018,,,246–256,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 2018 26th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering,"Lake Buena Vista, FL, USA",2018,9781450355735.0,,https://doi.org/10.1145/3236024.3236027;http://dx.doi.org/10.1145/3236024.3236027,10.1145/3236024.3236027,"Regular expressions (regexes) are a popular and powerful means of automatically manipulating text. Regexes are also an understudied denial of service vector (ReDoS). If a regex has super-linear worst-case complexity, an attacker may be able to trigger this complexity, exhausting the victim’s CPU resources and causing denial of service. Existing research has shown how to detect these superlinear regexes, and practitioners have identified super-linear regex anti-pattern heuristics that may lead to such complexity. In this paper, we empirically study three major aspects of ReDoS that have hitherto been unexplored: the incidence of super-linear regexes in practice, how they can be prevented, and how they can be repaired. In the ecosystems of two of the most popular programming languages — JavaScript and Python – we detected thousands of super-linear regexes affecting over 10,000 modules across diverse application domains. We also found that the conventional wisdom for super-linear regex anti-patterns has few false negatives but many false positives; these anti-patterns appear to be necessary, but not sufficient, signals of super-linear behavior. Finally, we found that when faced with a super-linear regex, developers favor revising it over truncating input or developing a custom parser, regardless of whether they had been shown examples of all three fix strategies. These findings motivate further research into ReDoS, since many modules are vulnerable to it and existing mechanisms to avoid it are insufficient. We believe that ReDoS vulnerabilities are a larger threat in practice than might have been guessed.","mining software repositories, catastrophic backtracking, empirical software engineering, Regular expressions, ReDoS",ESEC/FSE 2018,,,
Conference Paper,Rantala L,Towards Better Technical Debt Detection with NLP and Machine Learning Methods,,2020,,,242–245,Association for Computing Machinery,"New York, NY, USA",Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering: Companion Proceedings,"Seoul, South Korea",2020,9781450371223.0,,https://doi.org/10.1145/3377812.3381404;http://dx.doi.org/10.1145/3377812.3381404,10.1145/3377812.3381404,"Technical debt (TD) is an economical term used to depict non-optimal choices made in the software development process. It occurs usually when developers take shortcuts instead of following agreed upon development practices, and unchecked growth of technical debt can start to incur negative effects for software development processes.Technical debt detection and management is mainly done manually, and this is both slow and costly way of detecting technical debt. Automatic detection would solve this issue, but even state-of-the-art tools of today do not accurately detect the appearance of technical debt. Therefore, increasing the accuracy of automatic classification is of high importance, so that we could eliminate significant portion from the costs relating to technical debt detection.This research aims to solve the problem in detection accuracy by bringing in together static code analysis and natural language processing. This combination of techniques will allow more accurate detection of technical debt, when compared to them being used separately from each other. Research also aims to discover themes and topics from written developer messages that can be linked to technical debt. These can help us to understand technical debt from developers' viewpoint. Finally, we will build an open-source tool/plugin that can be used to accurately detect technical debt using both static analysis and natural language processing methods.","machine learning, technical debt, natural language processing",ICSE '20,,,
Conference Paper,"De Stefano M,Gambardella MS,Pecorelli F,Palomba F,De Lucia A",CASpER: A Plug-in for Automated Code Smell Detection and Refactoring,,2020,,,,Association for Computing Machinery,"New York, NY, USA",Proceedings of the International Conference on Advanced Visual Interfaces,"Salerno, Italy",2020,9781450375351.0,,https://doi.org/10.1145/3399715.3399955;http://dx.doi.org/10.1145/3399715.3399955,10.1145/3399715.3399955,"During software evolution, code is inevitably subject to continuous changes that are often performed by developers within short and strict deadlines. As a consequence, good design practices are often sacrificed, possibly leading to the introduction of sub-optimal design or implementation solutions, the so-called code smells. Several studies have shown that the presence of code smells makes the source code more change- and fault-prone, reduces productivity, and causes greater rework and more significant design efforts for developers. Refactoring is the practice that developers may use to remove code smells without changing the external behavior of the source code. However, it requires much time and effort and is poorly automated, often leading developers to prefer keeping low-quality code instead of spending time in designing and performing refactoring operations. To mitigate this problem and support developers throughout the process of code smell identification and refactoring, in this paper we present cASpER, a IntelliJ IDEA plugin that provides visual and semi-automatic support for detection and refactoring four different types of code smells.Tool. Jetbrains: https://plugins.jetbrains.com/plugin/13659-casperVideo. https://youtu.be/HBWF8fFJM8s","Code smells, Automated Software Engineering, Refactoring",AVI '20,,,
Conference Paper,"Zhao Y,Xiao L,Wang X,Sun L,Chen B,Liu Y,Bondi AB",How Are Performance Issues Caused and Resolved?-An Empirical Study from a Design Perspective,,2020,,,181–192,Association for Computing Machinery,"New York, NY, USA",Proceedings of the ACM/SPEC International Conference on Performance Engineering,"Edmonton AB, Canada",2020,9781450369916.0,,https://doi.org/10.1145/3358960.3379130;http://dx.doi.org/10.1145/3358960.3379130,10.1145/3358960.3379130,"Empirical experience regarding how real-life performance issues are caused and resolved can provide valuable insights for practitioners to effectively and efficiently prevent, detect, and fix performance issues. Prior work shows that most performance issues have their roots in poor architectural decisions. This paper contributes a large scale empirical study of 192 real-life performance issues, with an emphasis on software design. First, this paper contributes a holistic view of eight common root causes and typical resolutions that recur in different projects, and surveyed existing literature, in particular, tools, that can detect and fix each type of performance issue. Second, this study is first-of-its-kind to investigate performance issues from a design perspective. In the 192 issues, 33% required design-level optimization, i.e. simultaneously revising a group of related source files for resolving the issues. We reveal four design-level optimization patterns, which have shown different prevalence in resolving different root causes. Finally, this study investigated the Return on Investment for addressing performance issues, to help practitioners choose between localized or design-level optimization resolutions, and to prioritize issues due to different root causes.","software design structure, software performance, design patterns",ICPE '20,,,
Conference Paper,"de Mello RM,Oliveira R,Garcia A",On the Influence of Human Factors for Identifying Code Smells: A Multi-Trial Empirical Study,,2017,,,68–77,IEEE Press,"Markham, Ontario, Canada",Proceedings of the 11th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement,,2017,9781509040391.0,,https://doi.org/10.1109/ESEM.2017.13;http://dx.doi.org/10.1109/ESEM.2017.13,10.1109/ESEM.2017.13,"Context: Code smells are symptoms in the source code that represent poor design choices. Professional developers often perceive several types of code smells as indicators of actual design problems. However, the identification of code smells involves multiple steps that are subjective in nature, requiring the engagement of humans. Human factors are likely to play a key role in the precise identification of code smells in industrial settings. Unfortunately, there is limited knowledge about the influence of human factors on smell identification. Goal: We aim at investigating whether the precision of smell identification is influenced by three key human factors, namely reviewer's professional background, reviewer's module knowledge and collaboration of reviewers during the task. We also aim at deriving recommendations for allocating human resources to smell identification tasks. Method: We performed 19 comparisons among different subsamples from two trials of a controlled experiment conducted in the context of an empirical study on code smell identification. One trial was conducted in industrial settings while the other had involved graduate students. The diversity of the samples allowed us to analyze the influence of the three factors in isolation and in conjunction. Results: We found that (i) reviewers' collaboration significantly increases the precision of smell identification, but (ii) some professional background is required from the reviewers to reach high precision. Surprisingly, we also found that: (iii) having previous knowledge of the reviewed module does not affect the precision of reviewers with higher professional background. However, this factor was influential on successful identification of more complex smells. Conclusion: We expect that our findings are helpful to support researchers in conducting proper experimental procedures in the future. Besides, they may also be useful for supporting project managers in allocating resources for smell identification tasks.","collaboration, human factors, code review, replication, code smell identification, context",ESEM '17,,,
Journal Article,"Mastrangelo L,Hauswirth M,Nystrom N",Casting about in the Dark: An Empirical Study of Cast Operations in Java Programs,Proc. ACM Program. Lang.,2019,3.0,OOPSLA,,Association for Computing Machinery,"New York, NY, USA",,,2019-10,,,https://doi.org/10.1145/3360584;http://dx.doi.org/10.1145/3360584,10.1145/3360584,"The main goal of a static type system is to prevent certain kinds of errors from happening at run time. A type system is formulated as a set of constraints that gives any expression or term in a program a well-defined type. Yet mainstream programming languages are endowed with type systems that provide the means to circumvent their constraints through casting. We want to understand how and when developers escape the static type system to use dynamic typing. We empirically study how casting is used by developers in more than seven thousand Java projects. We find that casts are widely used (8.7% of methods contain at least one cast) and that 50% of casts we inspected are not guarded locally to ensure against potential run-time errors. To help us better categorize use cases and thus understand how casts are used in practice, we identify 25 cast-usage patterns---recurrent programming idioms using casts to solve a specific issue. This knowledge can be: (a) a recommendation for current and future language designers to make informed decisions (b) a reference for tool builders, e.g., by providing more precise or new refactoring analyses, (c) a guide for researchers to test new language features, or to carry out controlled programming experiments, and (d) a guide for developers for better practices.","type safety, Java, cast",,,,
Conference Paper,"Scalabrino S,Bavota G,Linares-Vásquez M,Lanza M,Oliveto R",Data-Driven Solutions to Detect API Compatibility Issues in Android: An Empirical Study,,2019,,,288–298,IEEE Press,"Montreal, Quebec, Canada",Proceedings of the 16th International Conference on Mining Software Repositories,,2019,,,https://doi.org/10.1109/MSR.2019.00055;http://dx.doi.org/10.1109/MSR.2019.00055,10.1109/MSR.2019.00055,"Android apps are inextricably linked to the official Android APIs. Such a strong form of dependency implies that changes introduced in new versions of the Android APIs can severely impact the apps' code, for example because of deprecated or removed APIs. In reaction to those changes, mobile app developers are expected to adapt their code and avoid compatibility issues. To support developers, approaches have been proposed to automatically identify API compatibility issues in Android apps. The state-of-the-art approach, named CiD, is a data-driven solution learning how to detect those issues by analyzing the changes in the history of Android APIs (""API side"" learning). While it can successfully identify compatibility issues, it cannot recommend coding solutions.We devised an alternative data-driven approach, named ACRyL. ACRyL learns from changes implemented in other apps in response to API changes (""client side"" learning). This allows not only to detect compatibility issues, but also to suggest a fix. When empirically comparing the two tools, we found that there is no clear winner, since the two approaches are highly complementary, in that they identify almost disjointed sets of API compatibility issues. Our results point to the future possibility of combining the two approaches, trying to learn detection/fixing rules on both the API and the client side.","API compatibility issues, empirical study, Android",MSR '19,,,
Conference Paper,"Flauzino M,Veríssimo J,Terra R,Cirilo E,Durelli VH,Durelli RS",Are You Still Smelling It? A Comparative Study between Java and Kotlin Language,,2018,,,23–32,Association for Computing Machinery,"New York, NY, USA","Proceedings of the VII Brazilian Symposium on Software Components, Architectures, and Reuse","Sao Carlos, Brazil",2018,9781450365543.0,,https://doi.org/10.1145/3267183.3267186;http://dx.doi.org/10.1145/3267183.3267186,10.1145/3267183.3267186,"Java is one of the most widely used programming languages. However, Java is a verbose language, thus one of the main drawbacks of the language is that even simple tasks often entail writing a significant amount of code. In some cases, writing too much code might lead to certain code smells, which are violations of fundamental design that can negatively impact the overall quality of programs. To allow programmers to write concise code, JetBrains created a new language named Kotlin. Nevertheless, few studies have evaluated whether Kotlin leads to concise and clearer code in comparison to Java. We conjecture that due to Java's verbosity, programs written in Java are likely to have more code smells than Kotlin programs. Therefore, we set out to evaluate whether some types of code smells are more common in Java programs. To this end, we carried out a large-scale empirical study involving more than 6 million lines of code from programs available in 100 repositories. We found that on average Kotlin programs have less code smells than Java programs.","Bad Smell, Kotlin Language, Code Smell, refactoring",SBCARS '18,,,
Conference Paper,"BenIdris M,Ammar H,Dzielski D,Benamer WH",Prioritizing Software Components Risk: Towards a Machine Learning-Based Approach,,2020,,,,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 6th International Conference on Engineering & MIS 2020,"Almaty, Kazakhstan",2020,9781450377362.0,,https://doi.org/10.1145/3410352.3410730;http://dx.doi.org/10.1145/3410352.3410730,10.1145/3410352.3410730,"Technical Debt (TD) can be detected using different methods. TD is a metaphor that refers to short-term solutions in software development, which may affect the cost of the software development life-cycle. Several tools have been developed to detect, estimate, or manage TD. TD can be indicated through smells, code comments, and software metrics. Machine learning Techniques (MLTs) are used in many software engineering topics such as fault-proneness, bug severity, and code smell. In this paper we use four internal structure metrics to identify and classify Architecture Technical Debt (ATD) risk by using MLTs. We show that MLTs can identify and classify the risk of ATD on software components to help the decision-makers to prioritizing the refactoring decisions based on the level of the risk.","Architecture Technical Debt, Architecture Smells, Software Risk, Machine Learning",ICEMIS'20,,,
Conference Paper,"Sena D,Coelho R,Kulesza U,Bonifácio R",Understanding the Exception Handling Strategies of Java Libraries: An Empirical Study,,2016,,,212–222,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 13th International Conference on Mining Software Repositories,"Austin, Texas",2016,9781450341868.0,,https://doi.org/10.1145/2901739.2901757;http://dx.doi.org/10.1145/2901739.2901757,10.1145/2901739.2901757,"This paper presents an empirical study whose goal was to investigate the exception handling strategies adopted by Java libraries and their potential impact on the client applications. In this study, exception flow analysis was used in combination with manual inspections in order: (i) to characterize the exception handling strategies of existing Java libraries from the perspective of their users; and (ii) to identify exception handling anti-patterns. We extended an existing static analysis tool to reason about exception flows and handler actions of 656 Java libraries selected from 145 categories in the Maven Central Repository. The study findings suggest a current trend of a high number of undocumented API runtime exceptions (i.e., @throws in Javadoc) and Unintended Handler problem. Moreover, we could also identify a considerable number of occurrences of exception handling anti-patterns (e.g. Catch and Ignore). Finally, we have also analyzed 647 bug issues of the 7 most popular libraries and identified that 20.71% of the reports are defects related to the problems of the exception strategies and anti-patterns identified in our study. The results of this study point to the need of tools to better understand and document the exception handling behavior of libraries.","software libraries, empirical study, exception handling, exception flows analysis, static analysis tool, exception handling anti-patterns",MSR '16,,,
Conference Paper,"Mori A,Vale G,Viggiato M,Oliveira J,Figueiredo E,Cirilo E,Jamshidi P,Kastner C",Evaluating Domain-Specific Metric Thresholds: An Empirical Study,,2018,,,41–50,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 2018 International Conference on Technical Debt,"Gothenburg, Sweden",2018,9781450357135.0,,https://doi.org/10.1145/3194164.3194173;http://dx.doi.org/10.1145/3194164.3194173,10.1145/3194164.3194173,"Software metrics and thresholds provide means to quantify several quality attributes of software systems. Indeed, they have been used in a wide variety of methods and tools for detecting different sorts of technical debts, such as code smells. Unfortunately, these methods and tools do not take into account characteristics of software domains, as the intrinsic complexity of geo-localization and scientific software systems or the simple protocols employed by messaging applications. Instead, they rely on generic thresholds that are derived from heterogeneous systems. Although derivation of reliable thresholds has long been a concern, we still lack empirical evidence about threshold variation across distinct software domains. To tackle this limitation, this paper investigates whether and how thresholds vary across domains by presenting a large-scale study on 3,107 software systems from 15 domains. We analyzed the derivation and distribution of thresholds based on 8 well-known source code metrics. As a result, we observed that software domain and size are relevant factors to be considered when building benchmarks for threshold derivation. Moreover, we also observed that domain-specific metric thresholds are more appropriated than generic ones for code smell detection.","software domains, thresholds, software metrics",TechDebt '18,,,
Conference Paper,"Jebnoun H,Ben Braiek H,Rahman MM,Khomh F",The Scent of Deep Learning Code: An Empirical Study,,2020,,,420–430,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 17th International Conference on Mining Software Repositories,"Seoul, Republic of Korea",2020,9781450375177.0,,https://doi.org/10.1145/3379597.3387479;http://dx.doi.org/10.1145/3379597.3387479,10.1145/3379597.3387479,"Deep learning practitioners are often interested in improving their model accuracy rather than the interpretability of their models. As a result, deep learning applications are inherently complex in their structures. They also need to continuously evolve in terms of code changes and model updates. Given these confounding factors, there is a great chance of violating the recommended programming practices by the developers in their deep learning applications. In particular, the code quality might be negatively affected due to their drive for the higher model performance. Unfortunately, the code quality of deep learning applications has rarely been studied to date. In this paper, we conduct an empirical study to investigate the distribution of code smells in deep learning applications. To this end, we perform a comparative analysis between deep learning and traditional open-source applications collected from GitHub. We have several major findings. First, long lambda expression, long ternary conditional expression, and complex container comprehension smells are frequently found in deep learning projects. That is, deep learning code involves more complex or longer expressions than the traditional code does. Second, the number of code smells increases across the releases of deep learning applications. Third, we found that there is a co-existence between code smells and software bugs in the studied deep learning code, which confirms our conjecture on the degraded code quality of deep learning applications.","code smells, Deep learning, code quality",MSR '20,,,
Conference Paper,"Santos JA,de Mendonça MG",Exploring Decision Drivers on God Class Detection in Three Controlled Experiments,,2015,,,1472–1479,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 30th Annual ACM Symposium on Applied Computing,"Salamanca, Spain",2015,9781450331968.0,,https://doi.org/10.1145/2695664.2695682;http://dx.doi.org/10.1145/2695664.2695682,10.1145/2695664.2695682,"Context: Code smells define potential problems in design of software. However, some empirical studies on the topic have shown findings in opposite direction. The misunderstanding is mainly caused by lack of works focusing on human role on code smell detection. Objective: Our aim is to build empirical support to exploration of the human role on code smell detection. specifically, we investigated what issues in code make a human identify a class as a code smell. We called these issues decision drivers. Method: We performed a controlled experiment and replicated it twice. We asked participants to detect god class (one of the most known smell) on different software, indicating what decision drivers they adopted. Results: The stronger drivers were ""class is high complex"" and ""method is misplaced"". We also found the agreement on drivers' choice is low. Another finding is: some important drivers are dependent of alternative support. In our case, ""dependency"" was an important driver only when visual resources were permitted. Conclusion: This study contributes with the comprehension of the human role on smell detection through the exploration of decision drivers. This perception contributes to characterize what we called the ""code smell conceptualization problem"".","god class, code smell, decision drivers, empirical study, controlled experiment",SAC '15,,,
Conference Paper,"Sae-Lim N,Hayashi S,Saeki M",Toward Proactive Refactoring: An Exploratory Study on Decaying Modules,,2019,,,39–46,IEEE Press,"Montreal, Quebec, Canada",Proceedings of the 3rd International Workshop on Refactoring,,2019,,,https://doi.org/10.1109/IWoR.2019.00015;http://dx.doi.org/10.1109/IWoR.2019.00015,10.1109/IWoR.2019.00015,"Source code quality is often measured using code smell, which is an indicator of design flaw or problem in the source code. Code smells can be detected using tools such as static analyzer that detects code smells based on source code metrics. Further, developers perform refactoring activities based on the result of such detection tools to improve source code quality. However, such approach can be considered as reactive refactoring, i.e., developers react to code smells after they occur. This means that developers first suffer the effects of low quality source code (e.g., low readability and understandability) before they start solving code smells. In this study, we focus on proactive refactoring, i.e., refactoring source code before it becomes smelly. This approach would allow developers to maintain source code quality without having to suffer the impact of code smells.To support the proactive refactoring process, we propose a technique to detect decaying modules, which are non-smelly modules that are about to become smelly. We present empirical studies on open source projects with the aim of studying the characteristics of decaying modules. Additionally, to facilitate developers in the refactoring planning process, we perform a study on using a machine learning technique to predict decaying modules and report a factor that contributes most to the performance of the model under consideration.","code quality, refactoring, code smell",IWOR '19,,,
Conference Paper,"Kaur H,Kaur PJ",Optimized Unit Testing for Antipattern Detection,,2014,,,,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 2014 International Conference on Information and Communication Technology for Competitive Strategies,"Udaipur, Rajasthan, India",2014,9781450332163.0,,https://doi.org/10.1145/2677855.2677909;http://dx.doi.org/10.1145/2677855.2677909,10.1145/2677855.2677909,"Antipatterns are poor design choices that are conjectured to make object oriented systems harder to maintain. We investigate the impact of antipatterns on classes in object-oriented systems by studying the relation between the presence of antipatterns and the change- and fault-proneness of the classes. Due to increased complexities in the software development, there is huge need of testing process to be carried on in better way. Also as the computer systems are significant to our society in everyday life and are performing an increasing number of critical tasks, so more work in software testing and analysis has become of great importance. Anti pattern testing is type of testing which is used to cut off the directly price associated with testing of different modules. This paper discusses various anti pattern detection techniques and proposes a new testing technique based on GUI for detection of anti patterns during software development.","Software Development Techniques, Loose Connectors, Antipattern, Maintenance Development, Fault Loop",ICTCS '14,,,
Conference Paper,"Nafees T,Coull N,Ferguson I,Sampson A",Vulnerability Anti-Patterns: A Timeless Way to Capture Poor Software Practices (Vulnerabilities),,2017,,,,The Hillside Group,USA,Proceedings of the 24th Conference on Pattern Languages of Programs,"Vancouver, British Columbia, Canada",2017,9781941652060.0,,,,"There is a distinct communication gap between the software engineering and cybersecurity communities when it comes to addressing reoccurring security problems, known as vulnerabilities. Many vulnerabilities are caused by software errors that are created by software developers. Insecure software development practices are common due to a variety of factors, which include inefficiencies within existing knowledge transfer mechanisms based on vulnerability databases (VDBs), software developers perceiving security as an afterthought, and lack of consideration of security as part of the software development lifecycle (SDLC). The resulting communication gap also prevents developers and security experts from successfully sharing essential security knowledge. The cybersecurity community makes their expert knowledge available in forms including vulnerability databases such as CAPEC and CWE, and pattern catalogues such as Security Patterns, Attack Patterns, and Software Fault Patterns. However, these sources are not effective at providing software developers with an understanding of how malicious hackers can exploit vulnerabilities in the software systems they create. As developers are familiar with pattern-based approaches, this paper proposes the use of Vulnerability Anti-Patterns (VAP) to transfer usable vulnerability knowledge to developers, bridging the communication gap between security experts and software developers. The primary contribution of this paper is twofold: (1) it proposes a new pattern template - Vulnerability Anti-Pattern - that uses anti-patterns rather than patterns to capture and communicate knowledge of existing vulnerabilities, and (2) it proposes a catalogue of Vulnerability Anti-Patterns (VAP) based on the most commonly occurring vulnerabilities that software developers can use to learn how malicious hackers can exploit errors in software.","software development lifecycle (SDLC), security pattern, attack pattern, vulnerability database (VDB), anti-pattern, vulnerability anti-pattern (VAP), vulnerability",PLoP '17,,,
Conference Paper,"Li Z,Chen TH,Yang J,Shang W",Dlfinder: Characterizing and Detecting Duplicate Logging Code Smells,,2019,,,152–163,IEEE Press,"Montreal, Quebec, Canada",Proceedings of the 41st International Conference on Software Engineering,,2019,,,https://doi.org/10.1109/ICSE.2019.00032;http://dx.doi.org/10.1109/ICSE.2019.00032,10.1109/ICSE.2019.00032,"Developers rely on software logs for a wide variety of tasks, such as debugging, testing, program comprehension, verification, and performance analysis. Despite the importance of logs, prior studies show that there is no industrial standard on how to write logging statements. Recent research on logs often only considers the appropriateness of a log as an individual item (e.g., one single logging statement); while logs are typically analyzed in tandem. In this paper, we focus on studying duplicate logging statements, which are logging statements that have the same static text message. Such duplications in the text message are potential indications of logging code smells, which may affect developers' understanding of the dynamic view of the system. We manually studied over 3K duplicate logging statements and their surrounding code in four large-scale open source systems: Hadoop, CloudStack, ElasticSearch, and Cassandra. We uncovered five patterns of duplicate logging code smells. For each instance of the code smell, we further manually identify the problematic (i.e., require fixes) and justifiable (i.e., do not require fixes) cases. Then, we contact developers in order to verify our manual study result. We integrated our manual study result and developers' feedback into our automated static analysis tool, DLFinder, which automatically detects problematic duplicate logging code smells. We evaluated DLFinder on the four manually studied systems and two additional systems: Camel and Wicket. In total, combining the results of DLFinder and our manual analysis, we reported 82 problematic code smell instances to developers and all of them have been fixed.","duplicate log, log, code smell, empirical study, static analysis",ICSE '19,,,
Conference Paper,"Brada P,Picha P",Software Process Anti-Patterns Catalogue,,2019,,,,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 24th European Conference on Pattern Languages of Programs,"Irsee, Germany",2019,9781450362061.0,,https://doi.org/10.1145/3361149.3361178;http://dx.doi.org/10.1145/3361149.3361178,10.1145/3361149.3361178,"For software project managers and other practitioners, an important activity is to detect, and consequently find solutions to, insufficiencies and mistakes in process and other project management (PM) activities. Particularly interesting among these are anti-patterns: commonly occurring solutions with known negative effects. Their detection in running, as well as finished, projects is often hard as it needs to be performed by specialists, demands expertise and skill, and is prone to biases. Obtaining the expertise is a long-time effort and the sources of relevant knowledge are scattered and vary in the depth of treatment.These issues could be alleviated by detecting PM and software process anti-patterns in data available in project Application Life-cycle Management tools. To facilitate the work towards such an approach, we have performed a review of academic, professional and grey literature sources to collect currently known and defined software PM anti-patterns. The collected set shows that they vary in terminology and description format which can lead to misunderstandings, different interpretations and other difficulties, especially when trying to devise a universally acceptable method of detection.In this paper we describe the findings of the review and the design of a catalogue of PM and process anti-patterns, based on the knowledge obtained. It uses a description template designed to support data-driven detection of anti-pattern occurrence. An initial version of the catalogue has been made publicly accessible, with the aim to reconcile the various sources and foster community discussion on understanding and descriptions of the individual anti-patterns.","project management anti-patterns, pattern detection, literature review, anti-pattern templates, software process anti-patterns, anti-pattern catalogue",EuroPLop '19,,,
Conference Paper,"Blasi A,Gorla A",Replicomment: Identifying Clones in Code Comments,,2018,,,320–323,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 26th Conference on Program Comprehension,"Gothenburg, Sweden",2018,9781450357142.0,,https://doi.org/10.1145/3196321.3196360;http://dx.doi.org/10.1145/3196321.3196360,10.1145/3196321.3196360,"Code comments are the primary means to document implementation and ease program comprehension. Thus, their quality should be a primary concern to improve program maintenance. While a lot of effort has been dedicated to detect bad smell in code, little work focuses on comments. In this paper we start working in this direction by detecting clones in comments. Our initial investigation shows that even well known projects have several comment clones, and just as clones are bad smell in code, they may be for comments. A manual analysis of the clones we identified revealed several issues in real Java projects.","code comments, clones, software quality, bad smell",ICPC '18,,,
Conference Paper,"Tahir A,Yamashita A,Licorish S,Dietrich J,Counsell S",Can You Tell Me If It Smells? A Study on How Developers Discuss Code Smells and Anti-Patterns in Stack Overflow,,2018,,,68–78,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 22nd International Conference on Evaluation and Assessment in Software Engineering 2018,"Christchurch, New Zealand",2018,9781450364034.0,,https://doi.org/10.1145/3210459.3210466;http://dx.doi.org/10.1145/3210459.3210466,10.1145/3210459.3210466,"This paper investigates how developers discuss code smells and anti-patterns over Stack Overflow to understand better their perceptions and understanding of these two concepts. Understanding developers' perceptions of these issues are important in order to inform and align future research efforts and direct tools vendors in the area of code smells and anti-patterns. In addition, such insights could lead the creation of solutions to code smells and anti-patterns that are better fit to the realities developers face in practice. We applied both quantitative and qualitative techniques to analyse discussions containing terms associated with code smells and anti-patterns. Our findings show that developers widely use Stack Overflow to ask for general assessments of code smells or anti-patterns, instead of asking for particular refactoring solutions. An interesting finding is that developers very often ask their peers 'to smell their code' (i.e., ask whether their own code 'smells' or not), and thus, utilize Stack Overflow as an informal, crowd-based code smell/anti-pattern detector. We conjecture that the crowd-based detection approach considers contextual factors, and thus, tends to be more trusted by developers over automated detection tools. We also found that developers often discuss the downsides of implementing specific design patterns, and 'flag' them as potential anti-patterns to be avoided. Conversely, we found discussions on why some anti-patterns previously considered harmful should not be flagged as anti-patterns. Our results suggest that there is a need for: 1) more context-based evaluations of code smells and anti-patterns, and 2) better guidelines for making trade-offs when applying design patterns or eliminating smells/anti-patterns in industry.","Stack Overflow, anti-patterns, empirical study, Code smells, mining software repositories",EASE'18,,,
Conference Paper,"Lenarduzzi V,Martini A,Taibi D,Tamburri DA",Towards Surgically-Precise Technical Debt Estimation: Early Results and Research Roadmap,,2019,,,37–42,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 3rd ACM SIGSOFT International Workshop on Machine Learning Techniques for Software Quality Evaluation,"Tallinn, Estonia",2019,9781450368551.0,,https://doi.org/10.1145/3340482.3342747;http://dx.doi.org/10.1145/3340482.3342747,10.1145/3340482.3342747,"The concept of technical debt has been explored from many perspectives but its precise estimation is still under heavy empirical and experimental inquiry. We aim to understand whether, by harnessing approximate, data-driven, machine-learning approaches it is possible to improve the current techniques for technical debt estimation, as represented by a top industry quality analysis tool such as SonarQube. For the sake of simplicity, we focus on relatively simple regression modelling techniques and apply them to modelling the additional project cost connected to the sub-optimal conditions existing in the projects under study. Our results shows that current techniques can be improved towards a more precise estimation of technical debt and the case study shows promising results towards the identification of more accurate estimation of technical debt.","Empirical Study, Machine Learning, Technical Debt",MaLTeSQuE 2019,,,
Conference Paper,"Eposhi A,Oizumi W,Garcia A,Sousa L,Oliveira R,Oliveira A",Removal of Design Problems through Refactorings: Are We Looking at the Right Symptoms?,,2019,,,148–153,IEEE Press,"Montreal, Quebec, Canada",Proceedings of the 27th International Conference on Program Comprehension,,2019,,,https://doi.org/10.1109/ICPC.2019.00032;http://dx.doi.org/10.1109/ICPC.2019.00032,10.1109/ICPC.2019.00032,"A design problem is the result of design decisions that negatively impact quality attributes. For example, a stakeholder introduces a design problem when he decides to addresses multiple unrelated responsibilities in a single class, impacting the modifiability and reusability of the system. Given their negative consequences, design problems should be identified and refactored. The literature still lacks evidence on which symptoms' characteristics can be used as strong indicators of design problems. For example, it is unknown if the density and diversity of certain symptoms (e.g., violations of object-oriented principles) are correlated with the occurrence of design problems. Thus, in this paper, we report a case study involving two C# systems. We evaluated the impact of refactoring, focused on removing design problems, on the density and diversity of symptoms. Results indicate that refactored classes usually present higher density and diversity of symptoms. However, the density and diversity of some symptoms, such as the violation of object-oriented principles, was not predominantly higher in refactored classes. Moreover, contrary to our expectations, refactorings caused almost no positive impact on the density and diversity of symptoms.","code smell, refactoring, design problem, technical debt, design smell",ICPC '19,,,
Conference Paper,"Vassallo C,Proksch S,Gall HC,Di Penta M",Automated Reporting of Anti-Patterns and Decay in Continuous Integration,,2019,,,105–115,IEEE Press,"Montreal, Quebec, Canada",Proceedings of the 41st International Conference on Software Engineering,,2019,,,https://doi.org/10.1109/ICSE.2019.00028;http://dx.doi.org/10.1109/ICSE.2019.00028,10.1109/ICSE.2019.00028,"Continuous Integration (CI) is a widely-used software engineering practice. The software is continuously built so that changes can be easily integrated and issues such as unmet quality goals or style inconsistencies get detected early. Unfortunately, it is not only hard to introduce CI into an existing project, but it is also challenging to live up to the CI principles when facing tough deadlines or business decisions. Previous work has identified common anti-patterns that reduce the promised benefits of CI. Typically, these anti-patterns slowly creep into a project over time before they are identified. We argue that automated detection can help with early identification and prevent such a process decay. In this work, we further analyze this assumption and survey 124 developers about CI anti-patterns. From the results, we build CI-Odor, a reporting tool for CI processes that detects the existence of four relevant anti-patterns by analyzing regular build logs and repository information. In a study on the 18,474 build logs of 36 popular Java projects, we reveal the presence of 3,823 high-severity warnings spread across projects. We validate our reports in a survey among 13 original developers of these projects and through general feedback from 42 developers that confirm the relevance of our reports.","detection, CI-Decay, CI-Smell, anti-pattern, continuous integration",ICSE '19,,,
Conference Paper,"Mannan UA,Ahmed I,Sarma A",Towards Understanding Code Readability and Its Impact on Design Quality,,2018,,,18–21,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 4th ACM SIGSOFT International Workshop on NLP for Software Engineering,"Lake Buena Vista, FL, USA",2018,9781450360555.0,,https://doi.org/10.1145/3283812.3283820;http://dx.doi.org/10.1145/3283812.3283820,10.1145/3283812.3283820,"Readability of code is commonly believed to impact the overall quality of software. Poor readability not only hinders developers from understanding what the code is doing but also can cause developers to make sub-optimal changes and introduce bugs. Developers also recognize this risk and state readability among their top information needs. Researchers have modeled readability scores. However, thus far, no one has investigated how readability evolves over time and how that impacts design quality of software. We perform a large scale study of 49 open source Java projects, spanning 8296 commits and 1766 files. We find that readability is high in open source projects and does not fluctuate over project’s lifetime unlike design quality of a project. Also readability has a non-significant correlation of 0.151 (Kendall’s τ ) with code smell count (indicator of design quality). Since current readability measure is unable to capture the increased difficulty in reading code due to the degraded design quality, our results hint towards the need of a better measurement and modeling of code readability.","Code smell, Design quality, Readability",NL4SE 2018,,,
Conference Paper,"Peldszus S,Kulcsár G,Lochau M,Schulze S",Continuous Detection of Design Flaws in Evolving Object-Oriented Programs Using Incremental Multi-Pattern Matching,,2016,,,578–589,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 31st IEEE/ACM International Conference on Automated Software Engineering,"Singapore, Singapore",2016,9781450338455.0,,https://doi.org/10.1145/2970276.2970338;http://dx.doi.org/10.1145/2970276.2970338,10.1145/2970276.2970338,"Design flaws in object-oriented programs may seriously corrupt code quality thus increasing the risk for introducing subtle errors during software maintenance and evolution. Most recent approaches identify design flaws in an ad-hoc manner, either focusing on software metrics, locally restricted code smells, or on coarse-grained architectural anti-patterns. In this paper, we utilize an abstract program model capturing high-level object-oriented code entities, further augmented with qualitative and quantitative design-related information such as coupling/cohesion. Based on this model, we propose a comprehensive methodology for specifying object-oriented design flaws by means of compound rules integrating code metrics, code smells and anti-patterns in a modular way. This approach allows for efficient, automated design-flaw detection through incremental multi-pattern matching, by facilitating systematic information reuse among multiple detection rules as well as between subsequent detection runs on continuously evolving programs. Our tool implementation comprises well-known anti-patterns for Java programs. The results of our experimental evaluation show high detection precision, scalability to real-size programs, as well as a remarkable gain in efficiency due to information reuse.","continuous software evolution, object-oriented software architecture, design-flaw detection",ASE 2016,,,
Conference Paper,"Guo X,Shi C,Jiang H",Deep Semantic-Based Feature Envy Identification,,2019,,,,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 11th Asia-Pacific Symposium on Internetware,"Fukuoka, Japan",2019,9781450377010.0,,https://doi.org/10.1145/3361242.3361257;http://dx.doi.org/10.1145/3361242.3361257,10.1145/3361242.3361257,"Code smells regularly cause potential software quality problems in software development. Thus, code smell detection has attracted the attention of many researchers. A number of approaches have been suggested in order to improve the accuracy of code smell detection. Most of these approaches rely solely on structural information (code metrics) extracted from source code and heuristic rules designed by people. In this paper, We propose a method-representation based model to represent the methods in textual code, which can effectively reflect the semantic relationships embedded in textual code. We also propose a deep learning based approach that combines method-representation and a CNN model to detect feature envy. The proposed approach can automatically extract semantic and features from textual code and code metrics, and can also automatically build complex mapping between these features and predictions. Evaluation results on open-source projects demonstrate that our proposed approach achieves better performance than the state-of-the-art in detecting feature envy.","Feature Envy, Software Refactoring, Deep Learning, Deep Semantic, Code Smell",Internetware '19,,,
Conference Paper,"Griffith I,Wahl S,Izurieta C",Evolution of Legacy System Comprehensibility through Automated Refactoring,,2011,,,35–42,Association for Computing Machinery,"New York, NY, USA",Proceedings of the International Workshop on Machine Learning Technologies in Software Engineering,"Lawrence, Kansas, USA",2011,9781450310222.0,,https://doi.org/10.1145/2070821.2070826;http://dx.doi.org/10.1145/2070821.2070826,10.1145/2070821.2070826,"Software engineering is a continually evolving discipline, wherein researchers and members of industry are working towards defining and refining what are known as ""best practices."" Best practices are the set of known correct engineering techniques that lead to quality software.When a software artifact is produced, it becomes temporally locked into a single instantiation of these best practices at a given point in time. If such software is not maintained in such a way as to keep it current with the evolution of practice, then there is a good chance that subsequent engineers may not understand the design choices made. There are known techniques, called refactorings, which allow for structural changes to software without altering the outward appearance and behavior, thus maintaining the intent of the original design. Unfortunately, refactoring requires an engineer to both understand the techniques to be applied and the code to which they are applied to. This is not always feasible.We have developed an automated system utilizing Evolutionary Algorithms to manipulate refactorings correctly without requiring an underlying understanding of the software. This then allows for sustained levels of quality of evolving software systems. The understandability, maintainability, and reusability of the software regenerate as best practices evolve.","software engineering, code smell, measurement, software evolution, refactoring, automation",MALETS '11,,,
Conference Paper,"Lelli V,Blouin A,Baudry B,Coulon F,Beaudoux O",Automatic Detection of GUI Design Smells: The Case of Blob Listener,,2016,,,263–274,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 8th ACM SIGCHI Symposium on Engineering Interactive Computing Systems,"Brussels, Belgium",2016,9781450343220.0,,https://doi.org/10.1145/2933242.2933260;http://dx.doi.org/10.1145/2933242.2933260,10.1145/2933242.2933260,"Graphical User Interfaces (GUIs) intensively rely on event-driven programming: widgets send GUI events, which capture users' interactions, to dedicated objects called controllers. Controllers implement several GUI listeners that handle these events to produce GUI commands. In this work, we conducted an empirical study on 13 large Java Swing open-source software systems. We study to what extent the number of GUI commands that a GUI listener can produce has an impact on the change- and fault-proneness of the GUI listener code. We identify a new type of design smell, called Blob listener that characterizes GUI listeners that can produce more than two GUI commands. We show that 21% of the analyzed GUI controllers are Blob listeners. We propose a systematic static code analysis procedure that searches for Blob listener that we implement in InspectorGuidget. We conducted experiments on six software systems for which we manually identified 37 instances of Blob listener. InspectorGuidget successfully detected 36 Blob listeners out of 37. The results exhibit a precision of 97.37% and a recall of 97.59%. Finally, we propose coding practices to avoid the use of Blob listeners.","design smell, software validation, user interface, code quality",EICS '16,,,
Conference Paper,"Sousa BL,Bigonha MA,Ferreira KA",A Systematic Literature Mapping on the Relationship between Design Patterns and Bad Smells,,2018,,,1528–1535,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 33rd Annual ACM Symposium on Applied Computing,"Pau, France",2018,9781450351911.0,,https://doi.org/10.1145/3167132.3167295;http://dx.doi.org/10.1145/3167132.3167295,10.1145/3167132.3167295,"Bad Smells are symptoms that appear in the source code of a software system and may indicate a structural problem that requires code refactoring. Design patterns are solutions known as good practices that help building software systems with high quality and flexibility. Intuitively, it is possible to assume that the use of design patterns might avoid bad smells. Intriguingly, some recent studies have pointed out that this assumption is not true. This paper presents a systematic literature mapping of studies that investigate the relationship between design patterns and bad smells. We identified 16 papers which were categorized into three different approaches: impact on software quality, refactoring and co-occurrence. Amongst these three approaches, the co-occurrence relationship is the less explored in the literature. In addition, we identified that studies focusing on co-occurrence between design patterns and bad smells have generally analyzed the relationship between the GOF design patterns and bad smells described by Fowler and Beck. In this context, the Command design pattern was identified as the one with greater relationship with bad smells.","bad smell, systematic literature mapping, design pattern",SAC '18,,,
Conference Paper,"Sae-Lim N,Hayashi S,Saeki M",Revisiting Context-Based Code Smells Prioritization: On Supporting Referred Context,,2017,,,,Association for Computing Machinery,"New York, NY, USA",Proceedings of the XP2017 Scientific Workshops,"Cologne, Germany",2017,9781450352642.0,,https://doi.org/10.1145/3120459.3120463;http://dx.doi.org/10.1145/3120459.3120463,10.1145/3120459.3120463,"Because numerous code smells are revealed by code smell detectors, many attempts have been undertaken to mitigate related problems by prioritizing and filtering code smells. We earlier proposed a technique to prioritize code smells by leveraging the context of the developers, i.e., the modules that the developers plan to implement. Our empirical studies revealed that the results of code smells prioritized using our technique are useful to support developers' implementation on the modules they intend to change. Nonetheless, in software change processes, developers often navigate through many modules and refer to them before making actual changes. Such modules are important when considering the developers' context. Therefore, it is essential to ascertain whether our technique can also support developers on modules to which they are going to refer to make changes. We conducted an empirical study of an open source project adopting tools for recording developers' interaction history. Our results demonstrate that the code smells prioritized using our approach can also be used to support developers for modules to which developers are going to refer, irrespective of the need for modification.","issue tracking system, code smell, impact analysis, interaction history",XP '17,,,
Conference Paper,"Santos JA,de Mendonça MG,Silva CV",An Exploratory Study to Investigate the Impact of Conceptualization in God Class Detection,,2013,,,48–59,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 17th International Conference on Evaluation and Assessment in Software Engineering,"Porto de Galinhas, Brazil",2013,9781450318488.0,,https://doi.org/10.1145/2460999.2461007;http://dx.doi.org/10.1145/2460999.2461007,10.1145/2460999.2461007,"Context: The concept of code smells is widespread in Software Engineering. However, in spite of the many discussions and claims about them, there are few empirical studies to support or contest these ideas. In particular, the study of the human perception of what is a code smell and how to deal with it has been mostly neglected. Objective: To build empirical support to understand the effect of god classes, one of the most known code smells. In particular, this paper focuses on how conceptualization affects identification of god classes, i.e., how different people perceive the god class concept. Method: A controlled experiment that extends and builds upon another empirical study about how humans detect god classes [19]. Our study: i) deepens and details some of the research questions of the previous study, ii) introduces a new research question and, iii) when possible, compares the results of both studies. Result: Our findings show that participants have different personal criteria and preferences in choosing drivers to identify god classes. The agreement between participants is not high, which is in accordance with previous studies. Conclusion: This study contributes to expand the empirical data about the human perception of code smells. It also presents a new way to evaluate effort and distraction in experiments through the use of automatic logging of participant actions.","controlled experiment, god class, code smell",EASE '13,,,
Conference Paper,"Islam MR,Zibran MF,Nagpal A",Security Vulnerabilities in Categories of Clones and Non-Cloned Code: An Empirical Study,,2017,,,20–29,IEEE Press,"Markham, Ontario, Canada",Proceedings of the 11th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement,,2017,9781509040391.0,,https://doi.org/10.1109/ESEM.2017.9;http://dx.doi.org/10.1109/ESEM.2017.9,10.1109/ESEM.2017.9,"Background: Software security has drawn immense importance in the recent years. While efforts are expected in minimizing security vulnerabilities in source code, the developers' practice of code cloning often causes multiplication of such vulnerabilities and program faults. Although previous studies examined the bug-proneness, stability, and changeability of clones against non-cloned code, the security aspects remained ignored. Aims: The objective of this work is to explore and understand the security vulnerabilities and their severity in different types of clones compared to non-clone code. Method: Using a state-of-the-art clone detector and two reputed security vulnerability detection tools, we detect clones and vulnerabilities in 8.7 million lines of code over 34 software systems. We perform a comparative study of the vulnerabilities identified in different types of clones and non-cloned code. The results are derived based on quantitative analyses with statistical significance. Results: Our study reveals that the security vulnerabilities found in code clones have higher severity of security risks compared to those in non-cloned code. However, the proportion (i.e., density) of vulnerabilities in clones and non-cloned code does not have any significant difference. Conclusion: The findings from this work add to our understanding of the characteristics and impacts of clones, which will be useful in clone-aware software development with improved software security.",,ESEM '17,,,
Conference Paper,"Oizumi W,Sousa L,Garcia A,Oliveira R,Oliveira A,Agbachi OI,Lucena C",Revealing Design Problems in Stinky Code: A Mixed-Method Study,,2017,,,,Association for Computing Machinery,"New York, NY, USA","Proceedings of the 11th Brazilian Symposium on Software Components, Architectures, and Reuse","Fortaleza, Ceará, Brazil",2017,9781450353250.0,,https://doi.org/10.1145/3132498.3132514;http://dx.doi.org/10.1145/3132498.3132514,10.1145/3132498.3132514,"Developers often have to locate design problems in the source code. Several types of design problem may manifest as code smells in the program. A code smell is a source code structure that may reveal a partial hint about the manifestation of a design problem. Recent studies suggest that developers should ignore smells occurring in isolation in a program location. Instead, they should focus on analyzing stinkier code, i.e. program locations - e.g., a class or a hierarchy - affected by multiple smells. The stinkier a program location is, more likely it contains a design problem. However, there is limited understanding if developers can effectively identify a design problem in stinkier code. Developers may struggle to make a meaning out of inter-related smells affecting the same program location. To address this matter, we applied a mixed-method approach to analyze if and how developers can effectively find design problems when reflecting upon stinky code - i.e., a program location affected by multiple smells. We performed an experiment and an interview with 11 professionals. Surprisingly, our analysis revealed that only 36.36% of the developers found more design problems when explicitly reasoning about multiple smells as compared to single smells. On the other hand, 63.63% of the developers reported much lesser false positives. Developers reported that analyses of stinky code scattered in class hierarchies or packages is often difficult, time consuming, and requires proper visualization support. Moreover, it remains time-consuming to discard stinky program locations that do not represent design problems.","agglomeration, code smell, design problem, software design",SBCARS '17,,,
Journal Article,"Singh S,Kahlon KS",Effectiveness of Encapsulation and Object-Oriented Metrics to Refactor Code and Identify Error Prone Classes Using Bad Smells,SIGSOFT Softw. Eng. Notes,2011,36.0,5,1–10,Association for Computing Machinery,"New York, NY, USA",,,2011-09,,0163-5948,https://doi.org/10.1145/2020976.2020994;http://dx.doi.org/10.1145/2020976.2020994,10.1145/2020976.2020994,"To assist maintenance and evolution teams, work needs to be done at the onset of software development. One such facilitation is refactoring the code, making it easier to read, understand and maintain. Refactoring is done by identifying bad smell areas in the code. In this paper, based on empirical analysis, we develop a metrics model to identify smelly classes. The role of two new metrics (encapsulation and information hiding) is also investigated for identifying smelly and faulty classes in software code. This paper first presents a binary statistical analysis of thev relationship between metrics and bad smells, the results of which show a significant relationship. Then, the metrics model (with significant metrics shortlisted from the binary analysis) for bad smell categorization (divided into five categories) is developed. To verify our model, we examine the open source Firefox system, which has a strong industrial usage. The results show that proposed metrics model for bad smell can predict faulty classes with high accuracy, but in the case of the categorized model not all categories of bad smells can adequately identified the faulty and smelly classes. Due to certain limitations of our study more experiments are required to generalize the results of bad smell and faulty class identification in software code.","information hiding, encapsulation, evolution, bad smells, refactoring, empirical analysis",,,,
Conference Paper,"dos Santos HM,Durelli VH,Souza M,Figueiredo E,da Silva LT,Durelli RS",CleanGame: Gamifying the Identification of Code Smells,,2019,,,437–446,Association for Computing Machinery,"New York, NY, USA",Proceedings of the XXXIII Brazilian Symposium on Software Engineering,"Salvador, Brazil",2019,9781450376518.0,,https://doi.org/10.1145/3350768.3352490;http://dx.doi.org/10.1145/3350768.3352490,10.1145/3350768.3352490,"Refactoring is the process of transforming the internal structure of existing code without changing its observable behavior. Many studies have shown that refactoring increases program maintainability and understandability. Due to these benefits, refactoring is recognized as a best practice in the software development community. However, prior to refactoring activities, developers need to look for refactoring opportunities, i.e., developers need to be able to identify code smells, which essentially are instances of poor design and ill-considered implementation choices that may hinder code maintainability and understandability. However, code smell identification is overlooked in the Computer Science curriculum. Recently, Software Engineering educators have started exploring gamification, which entails using game elements in non-game contexts, to improve instructional outcomes in educational settings. The potential of gamification lies in supporting and motivating students, enhancing the learning process and its outcomes. We set out to evaluate the extent to which such claim is valid in the context of post-training reinforcement. To this end, we devised and implemented CleanGame, which is a gamified tool that covers one important aspect of the refactoring curriculum: code smell identification. We also carried out an experiment involving eighteen participants to probe into the effectiveness of gamification in the context of post-training reinforcement. We found that, on average, participants managed to identify twice as much code smells during learning reinforcement with a gamified approach in comparison to a non-gamified approach. Moreover, we administered a post-experiment attitudinal survey to the participants. According to the results of such survey, most participants showed a positive attitude towards CleanGame.","post-training reinforcement, gamification, Software Engineering education, code smell, Refactoring",SBES '19,,,
Conference Paper,"Ahmed I,Brindescu C,Mannan UA,Jensen C,Sarma A",An Empirical Examination of the Relationship between Code Smells and Merge Conflicts,,2017,,,58–67,IEEE Press,"Markham, Ontario, Canada",Proceedings of the 11th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement,,2017,9781509040391.0,,https://doi.org/10.1109/ESEM.2017.12;http://dx.doi.org/10.1109/ESEM.2017.12,10.1109/ESEM.2017.12,"Background: Merge conflicts are a common occurrence in software development. Researchers have shown the negative impact of conflicts on the resulting code quality and the development workflow. Thus far, no one has investigated the effect of bad design (code smells) on merge conflicts. Aims: We posit that entities that exhibit certain types of code smells are more likely to be involved in a merge conflict. We also postulate that code elements that are both ""smelly"" and involved in a merge conflict are associated with other undesirable effects (more likely to be buggy). Method: We mined 143 repositories from GitHub and recreated 6,979 merge conflicts to obtain metrics about code changes and conflicts. We categorized conflicts into semantic or non-semantic, based on whether changes affected the Abstract Syntax Tree. For each conflicting change, we calculate the number of code smells and the number of future bug-fixes associated with the affected lines of code. Results: We found that entities that are smelly are three times more likely to be involved in merge conflicts. Method-level code smells (Blob Operation and Internal Duplication) are highly correlated with semantic conflicts. We also found that code that is smelly and experiences merge conflicts is more likely to be buggy. Conclusion: Bad code design not only impacts maintainability, it also impacts the day to day operations of a project, such as merging contributions, and negatively impacts the quality of the resulting code. Our findings indicate that research is needed to identify better ways to support merge conflict resolution to minimize its effect on code quality.","code smell, empirical analysis, machine learning, merge conflict",ESEM '17,,,
Conference Paper,"Takahashi A,Sae-Lim N,Hayashi S,Saeki M",A Preliminary Study on Using Code Smells to Improve Bug Localization,,2018,,,324–327,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 26th Conference on Program Comprehension,"Gothenburg, Sweden",2018,9781450357142.0,,https://doi.org/10.1145/3196321.3196361;http://dx.doi.org/10.1145/3196321.3196361,10.1145/3196321.3196361,"Bug localization is a technique that has been proposed to support the process of identifying the locations of bugs specified in a bug report. A traditional approach such as information retrieval (IR)-based bug localization calculates the similarity between the bug description and the source code and suggests locations that are likely to contain the bug. However, while many approaches have been proposed to improve the accuracy, the likelihood of each module having a bug is often overlooked or they are treated equally, whereas this may not be the case. For example, modules having code smells have been found to be more prone to changes and faults. Therefore, in this paper, we explore a first step toward leveraging code smells to improve bug localization. By combining the code smell severity with the textual similarity from IR-based bug localization, we can identify the modules that are not only similar to the bug description but also have a higher likelihood of containing bugs. Our preliminary evaluation on four open source projects shows that our technique can improve the baseline approach by 142.25% and 30.50% on average for method and class levels, respectively.","information retrieval, code smell, bug localization",ICPC '18,,,
Conference Paper,"Lima R,Souza J,Fonseca B,Teixeira L,Gheyi R,Ribeiro M,Garcia A,de Mello R",Understanding and Detecting Harmful Code,,2020,,,223–232,Association for Computing Machinery,"New York, NY, USA",Proceedings of the XXXIV Brazilian Symposium on Software Engineering,"Natal, Brazil",2020,9781450387538.0,,https://doi.org/10.1145/3422392.3422420;http://dx.doi.org/10.1145/3422392.3422420,10.1145/3422392.3422420,"Code smells typically indicate poor design implementation and choices that may degrade software quality. Hence, they need to be carefully detected to avoid such poor design. In this context, some studies try to understand the impact of code smells on the software quality, while others propose rules or machine learning-based techniques to detect code smells. However, none of those studies or techniques focus on analyzing code snippets that are really harmful to software quality. This paper presents a study to understand and classify code harmfulness. We analyze harmfulness in terms of CLEAN, SMELLY, BUGGY, and HARMFUL code. By HARMFUL CODE, we define a SMELLY code element having one or more bugs reported. These bugs may have been fixed or not. Thus, the incidence of HARMFUL CODE may represent a increased risk of introducing new defects and/or design problems during its fixing. We perform our study with 22 smell types, 803 versions of 13 open-source projects, 40,340 bugs and 132,219 code smells. The results show that even though we have a high number of code smells, only 0.07% of those smells are harmful. The Abstract Function Call From Constructor is the smell type more related to HARMFUL CODE. To cross-validate our results, we also perform a survey with 60 developers. Most of them (98%) consider code smells harmful to the software, and 85% of those developers believe that code smells detection tools are important. But, those developers are not concerned about selecting tools that are able to detect HARMFUL CODE. We also evaluate machine learning techniques to classify code harmfulness: they reach the effectiveness of at least 97% to classify HARMFUL CODE. While the Random Forest is effective in classifying both SMELLY and HARMFUL CODE, the Gaussian Naive Bayes is the less effective technique. Our results also suggest that both software and developers' metrics are important to classify HARMFUL CODE.","Software Quality, Machine Learning, Code Smells",SBES '20,,,
Conference Paper,"Dietrich J,Yakovlev V,McCartin C,Jenson G,Duchrow M",Cluster Analysis of Java Dependency Graphs,,2008,,,91–94,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 4th ACM Symposium on Software Visualization,"Ammersee, Germany",2008,9781605581125.0,,https://doi.org/10.1145/1409720.1409735;http://dx.doi.org/10.1145/1409720.1409735,10.1145/1409720.1409735,"We present a novel approach to the analysis of dependency graphs of object-oriented programs. We propose to use the Girvan-Newman clustering algorithm to compute the modular structure of programs. This is useful in assisting software engineers to redraw component boundaries in software, in order to improve the level of reuse and maintainability. The results of this analysis can be used as a starting point for refactoring the software. We present BARRIO, an Eclipse plugin that can detect and visualise clusters in dependency graphs extracted from Java programs by means of source code and byte code analysis. These clusters are then compared with the modular structure of the analysed programs defined by package and container specifications. Two metrics are introduced to measure the degree of overlap between the defined and the computed modular structure. Some empirical results obtained from analysing non-trivial software packages are presented.","dependency analysis, refactoring, cluster analysis, anti-pattern detection",SoftVis '08,,,
Conference Paper,"Laigner R,Kalinowski M,Carvalho L,Mendonça D,Garcia A",Towards a Catalog of Java Dependency Injection Anti-Patterns,,2019,,,104–113,Association for Computing Machinery,"New York, NY, USA",Proceedings of the XXXIII Brazilian Symposium on Software Engineering,"Salvador, Brazil",2019,9781450376518.0,,https://doi.org/10.1145/3350768.3350771;http://dx.doi.org/10.1145/3350768.3350771,10.1145/3350768.3350771,"[Context] Dependency Injection (DI) is a commonly applied mechanism to decouple classes from their dependencies in order to provide better modularization of software. In the context of Java, the availability of a DI specification and popular frameworks, such as Spring, facilitate DI usage in software projects. However, bad DI implementation practices can have negative consequences, such as increasing coupling. Even though the literature suggests the existence of DI anti-patterns, there is no detailed catalog of such bad practices. Moreover, there is no evidence on their occurrence and perceived usefulness from the developer's point of view. [Goal] Our goal is to review the reported DI anti-patterns in order to analyze their completeness and to propose and evaluate a novel catalog of DI anti-patterns in the context of Java. [Method] We propose an initial catalog containing twelve Java DI anti-patterns. We selected four open source software projects that adopt a DI framework and developed a tool to statically analyze the occurrence of the DI anti-patterns within their source code. Also, we conducted a survey through face to face interviews with three experienced developers that regularly apply DI. [Results] At least nine different DI anti-patterns appeared in each analyzed project. In addition, the feedback received from the developers confirmed their relevance and the importance of investing further effort towards a catalog. [Conclusion] The results indicate that the initial catalog contains Java DI anti-patterns that occur in practice and are useful. Sharing it with practitioners may help them to avoid such anti-patterns. Sharing it with the research community will enable further improving the catalog.","inversion of control, dependency inversion, java, catalog, anti-pattern, dependency injection, modularization, coupling",SBES '19,,,
Conference Paper,"Falessi D,Russo B,Mullen K",What If i Had No Smells?,,2017,,,78–84,IEEE Press,"Markham, Ontario, Canada",Proceedings of the 11th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement,,2017,9781509040391.0,,https://doi.org/10.1109/ESEM.2017.14;http://dx.doi.org/10.1109/ESEM.2017.14,10.1109/ESEM.2017.14,"What would have happened if I did not have any code smell? This is an interesting question that no previous study, to the best of our knowledge, has tried to answer. In this paper, we present a method for implementing a what-if scenario analysis estimating the number of defective files in the absence of smells. Our industrial case study shows that 20% of the total defective files were likely avoidable by avoiding smells. Such estimation needs to be used with the due care though as it is based on a hypothetical history (i.e., zero number of smells and same process and product change characteristics). Specifically, the number of defective files could even increase for some types of smells. In addition, we note that in some circumstances, accepting code with smells might still be a good option for a company.","machine learning, software estimation, technical debt, code smells",ESEM '17,,,
Conference Paper,"Yamashita A,Abtahizadeh SA,Khomh F,Guéhéneuc YG","Software Evolution and Quality Data from Controlled, Multiple, Industrial Case Studies",,2017,,,507–510,IEEE Press,"Buenos Aires, Argentina",Proceedings of the 14th International Conference on Mining Software Repositories,,2017,9781538615447.0,,https://doi.org/10.1109/MSR.2017.44;http://dx.doi.org/10.1109/MSR.2017.44,10.1109/MSR.2017.44,"A main difficulty to study the evolution and quality of real-life software systems is the effect of moderator factors, such as: programming skill, type of maintenance task, and learning effect. Experimenters must account for moderator factors to identify the relationships between the variables of interest. In practice, controlling for moderator factors in realistic (industrial) settings is expensive and rather difficult. The data presented in this paper has two particularities: First, it involves six professional developers and four real-life, industrial systems. Second, it was obtained from controlled, multiple case studies where the moderator variables: programming skill, maintenance task, and learning effect were controlled for. This data set is relevant to experimenters studying evolution and quality of reallife systems, in particular those interested in studying industrial systems and replicating empirical studies.","empirical study, case study, software defects, industrial data, moderator factors, software evolution, software replicability, software quality, replication, code smells",MSR '17,,,
Conference Paper,"Cai Y,Kazman R",DV8: Automated Architecture Analysis Tool Suites,,2019,,,53–54,IEEE Press,"Montreal, Quebec, Canada",Proceedings of the Second International Conference on Technical Debt,,2019,,,https://doi.org/10.1109/TechDebt.2019.00015;http://dx.doi.org/10.1109/TechDebt.2019.00015,10.1109/TechDebt.2019.00015,"I. Purpose of the ToolAlthough software measurement and source code analysis techniques have been researched for decades, making project decisions that have significant economic impact---especially decisions about technical debt and refactoring---is still a challenge for management and development teams. Development teams feel the increasing challenges of maintenance as the architecture degrades, and often have intuitions about where the problems are, but have difficulty pinpointing which files are problematic and why. It is still a challenge for the development teams to quantify their projects' maintenance problems---their debts---as a way of justifying the investment in refactoring.Here we present our tool suite called DV81. The objective of DV8 is to measure software modularity, detect architecture anti-patterns as technical debts, quantify the maintenance cost of each instance of an anti-pattern, and enable return on investment analyses of architectural debts. Different from other tools, DV8 integrates data from both source code and revision history. We now elaborate on each of DV8's capabilities.","software quality, software architecture, software maintenance",TechDebt '19,,,
Journal Article,"Singh S,Kahlon KS",Effectiveness of Refactoring Metrics Model to Identify Smelly and Error Prone Classes in Open Source Software,SIGSOFT Softw. Eng. Notes,2012,37.0,2,1–11,Association for Computing Machinery,"New York, NY, USA",,,2012-04,,0163-5948,https://doi.org/10.1145/2108144.2108157;http://dx.doi.org/10.1145/2108144.2108157,10.1145/2108144.2108157,"In order to improve software maintainability, possible improvement efforts must be made measurable. One such effort is refactoring the code which makes the code easier to read, understand and maintain. It is done by identifying the bad smell area in the code. This paper presents the results of an empirical study to develop a metrics model to identify the smelly classes. In addition, this metrics model is validated by identifying the smelly and error prone classes. The role of two new metrics (encapsulation and information hiding) is also investigated for identifying smelly and faulty classes in software code. This paper first presents a binary statistical analysis of the relationship between metrics and bad smells, the results of which show a significant relationship. Then, the metrics model (with significant metrics shortlisted from the binary analysis) for bad smell categorization (divided into five categories) is developed. To develop the model, three releases of the open source Mozila Firefox system are examined and the model is validated on one version of Mozila Sea Monkey, which has a strong industrial usage. The results show that metrics can predict smelly and faulty classes with high accuracy, but in the case of the categorized model, not all categories of bad smells can adequately be identified. Further, few categorised models can predict the faulty classes. Based on these results, we recommend more training for our model.","empirical analysis, bad smells, evolution, refactoring, encapsulation, information hiding",,,,
Conference Paper,"Chen TH,Shang W,Jiang ZM,Hassan AE,Nasser M,Flora P",Detecting Performance Anti-Patterns for Applications Developed Using Object-Relational Mapping,,2014,,,1001–1012,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 36th International Conference on Software Engineering,"Hyderabad, India",2014,9781450327565.0,,https://doi.org/10.1145/2568225.2568259;http://dx.doi.org/10.1145/2568225.2568259,10.1145/2568225.2568259,"Object-Relational Mapping (ORM) provides developers a conceptual abstraction for mapping the application code to the underlying databases. ORM is widely used in industry due to its convenience; permitting developers to focus on developing the business logic without worrying too much about the database access details. However, developers often write ORM code without considering the impact of such code on database performance, leading to cause transactions with timeouts or hangs in large-scale systems. Unfortunately, there is little support to help developers automatically detect suboptimal database accesses. In this paper, we propose an automated framework to detect ORM performance anti-patterns. Our framework automatically flags performance anti-patterns in the source code. Furthermore, as there could be hundreds or even thousands of instances of anti-patterns, our framework provides sup- port to prioritize performance bug fixes based on a statistically rigorous performance assessment. We have successfully evaluated our framework on two open source and one large-scale industrial systems. Our case studies show that our framework can detect new and known real-world performance bugs and that fixing the detected performance anti- patterns can improve the system response time by up to 98%.","Performance Evaluation, Performance Anti-pattern, Dynamic Analysis, Static Analysis, Performance",ICSE 2014,,,
Conference Paper,"Tuma K,Sion L,Scandariato R,Yskout K",Automating the Early Detection of Security Design Flaws,,2020,,,332–342,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 23rd ACM/IEEE International Conference on Model Driven Engineering Languages and Systems,"Virtual Event, Canada",2020,9781450370196.0,,https://doi.org/10.1145/3365438.3410954;http://dx.doi.org/10.1145/3365438.3410954,10.1145/3365438.3410954,"Security by design is a key principle for realizing secure software systems and it is advised to hunt for security flaws from the very early stages of development. At design-time, security analysis is often performed manually by means of either threat modeling or expert-based design inspections. However, when leveraging the wide range of established knowledge bases on security design flaws (e.g., CWE, CAWE), these manual assessments become too time consuming, error-prone, and infeasible in the context of contemporary development practices with frequent iterations. This paper focuses on design inspection and explores the potential for automating the application of inspection rules to speed up the security analysis.The contributions of this paper are: (i) the creation of a publicly available data set consisting of 26 design models annotated with security flaws, (ii) an automated approach for following inspection guidelines using model query patterns, and (iii) an empirical comparison of the results from this automated approach with those from manual inspection. Even though our results show that a complete automation of the security design flaw detection is hard to achieve, we find that some flaws (e.g., insecure data exposure) are more amenable to automation. Compared to manual analysis techniques, our results are encouraging and suggest that the automated technique could guide security analysts towards a more complete inspection of the software design, especially for large models.","empirical software engineering, automation, security-by-design, secure design, design flaw detection, security flaw",MODELS '20,,,
Conference Paper,"Vassallo C,Proksch S,Jancso A,Gall HC,Di Penta M",Configuration Smells in Continuous Delivery Pipelines: A Linter and a Six-Month Study on GitLab,,2020,,,327–337,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering,"Virtual Event, USA",2020,9781450370431.0,,https://doi.org/10.1145/3368089.3409709;http://dx.doi.org/10.1145/3368089.3409709,10.1145/3368089.3409709,"An effective and efficient application of Continuous Integration (CI) and Delivery (CD) requires software projects to follow certain principles and good practices. Configuring such a CI/CD pipeline is challenging and error-prone. Therefore, automated linters have been proposed to detect errors in the pipeline. While existing linters identify syntactic errors, detect security vulnerabilities or misuse of the features provided by build servers, they do not support developers that want to prevent common misconfigurations of a CD pipeline that potentially violate CD principles (“CD smells”). To this end, we propose CD-Linter, a semantic linter that can automatically identify four different smells in pipeline configuration files. We have evaluated our approach through a large-scale and long-term study that consists of (i) monitoring 145 issues (opened in as many open-source projects) over a period of 6 months, (ii) manually validating the detection precision and recall on a representative sample of issues, and (iii) assessing the magnitude of the observed smells on 5,312 open-source projects on GitLab. Our results show that CD smells are accepted and fixed by most of the developers and our linter achieves a precision of 87% and a recall of 94%. Those smells can be frequently observed in the wild, as 31% of projects with long configurations are affected by at least one smell.","Anti-pattern, Linter, Configuration, Continuous Delivery, Continuous Integration, DevOps",ESEC/FSE 2020,,,
Conference Paper,"Krüger J,Fenske W,Thüm T,Aporius D,Saake G,Leich T",Apo-Games: A Case Study for Reverse Engineering Variability from Cloned Java Variants,,2018,,,251–256,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 22nd International Systems and Software Product Line Conference - Volume 1,"Gothenburg, Sweden",2018,9781450364645.0,,https://doi.org/10.1145/3233027.3236403;http://dx.doi.org/10.1145/3233027.3236403,10.1145/3233027.3236403,"Software-product-line engineering is an approach to systematically manage reusable software features and has been widely adopted in practice. Still, in most cases, organizations start with a single product that they clone and modify when new customer requirements arise (a.k.a. clone-and-own). With an increasing number of variants, maintenance can become challenging and organizations may consider migrating towards a software product line, which is referred to as extractive approach. While this is the most common approach in practice, techniques to extract variability from cloned variants still fall short in several regards. In particular, this accounts for the low accuracy of automated analyses and refactoring, our limited understanding of the costs involved, and the high manual effort. A main reason for these limitations is the lack of realistic case studies. To tackle this problem, we provide a set of cloned variants. In this paper, we characterize these variants and challenge the research community to apply techniques for reverse engineering feature models, feature location, code smell analysis, architecture recovery, and the migration towards a software product line. By evaluating solutions with the developer of these variants, we aim to contribute to a larger body of knowledge on this real-world case study.","data set, reverse engineering, extractive approach, case study, software-product-line engineering, feature location",SPLC '18,,,
Conference Paper,"Dósea M,Sant'Anna C,da Silva BC",How Do Design Decisions Affect the Distribution of Software Metrics?,,2018,,,74–85,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 26th Conference on Program Comprehension,"Gothenburg, Sweden",2018,9781450357142.0,,https://doi.org/10.1145/3196321.3196337;http://dx.doi.org/10.1145/3196321.3196337,10.1145/3196321.3196337,"Background. Source code analysis techniques usually rely on metric-based assessment. However, most of these techniques have low accuracy. One possible reason is because metric thresholds are extracted from classes driven by distinct design decisions. Previous studies have already shown that classes implemented according to some coarse-grained design decisions, such as programming languages, have different distribution of metric values. Therefore, these design decisions should be taken into account when using benchmarks for metric-based source code analysis. Goal. Our goal is to investigate whether other fine-grained design decisions also influence over distribution of software metrics. Method. We conduct an empirical study to evaluate the distributions of four metrics applied over fifteen real-world systems based on three different domains. Initially, we evaluated the influence of the class design role on the distributions of measures. For this purpose, we have defined an automatic approach to identify the design role played by each class. Then, we looked for other fine-grained design decisions that could have influenced the measures. Results. Our findings show that distribution of metrics are sensitive to the following design decisions: (i) design role of the class (ii) used libraries, (iii) coding style, (iv) exception handling, and (v) logging and debugging code mechanisms. Conclusion. The distribution of software metrics are sensitive to fine-grained design decisions and we should consider taking them into account when building benchmarks for metric-based source code analysis.","design decisions, design role, empirical study, metrics",ICPC '18,,,
Conference Paper,"Kontokostas D,Westphal P,Auer S,Hellmann S,Lehmann J,Cornelissen R,Zaveri A",Test-Driven Evaluation of Linked Data Quality,,2014,,,747–758,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 23rd International Conference on World Wide Web,"Seoul, Korea",2014,9781450327442.0,,https://doi.org/10.1145/2566486.2568002;http://dx.doi.org/10.1145/2566486.2568002,10.1145/2566486.2568002,"Linked Open Data (LOD) comprises an unprecedented volume of structured data on the Web. However, these datasets are of varying quality ranging from extensively curated datasets to crowdsourced or extracted data of often relatively low quality. We present a methodology for test-driven quality assessment of Linked Data, which is inspired by test-driven software development. We argue that vocabularies, ontologies and knowledge bases should be accompanied by a number of test cases, which help to ensure a basic level of quality. We present a methodology for assessing the quality of linked data resources, based on a formalization of bad smells and data quality problems. Our formalization employs SPARQL query templates, which are instantiated into concrete quality test case queries. Based on an extensive survey, we compile a comprehensive library of data quality test case patterns. We perform automatic test case instantiation based on schema constraints or semi-automatically enriched schemata and allow the user to generate specific test case instantiations that are applicable to a schema or dataset. We provide an extensive evaluation of five LOD datasets, manual test case instantiation for five schemas and automatic test case instantiations for all available schemata registered with Linked Open Vocabularies (LOV). One of the main advantages of our approach is that domain specific semantics can be encoded in the data quality test cases, thus being able to discover data quality problems beyond conventional quality heuristics.","linked data, data quality, dbpedia",WWW '14,,,
Conference Paper,"Hermans F,Pinzger M,van Deursen A",Detecting and Visualizing Inter-Worksheet Smells in Spreadsheets,,2012,,,441–451,IEEE Press,"Zurich, Switzerland",Proceedings of the 34th International Conference on Software Engineering,,2012,9781467310673.0,,,,"Spreadsheets are often used in business, for simple tasks, as well as for mission critical tasks such as finance or forecasting. Similar to software, some spreadsheets are of better quality than others, for instance with respect to usability, maintainability or reliability. In contrast with software however, spreadsheets are rarely checked, tested or certified. In this paper, we aim at developing an approach for detecting smells that indicate weak points in a spreadsheet's design. To that end we first study code smells and transform these code smells to their spreadsheet counterparts. We then present an approach to detect the smells, and communicate located smells to spreadsheet users with data flow diagrams. We analyzed occurrences of these smells in the Euses corpus. Furthermore we conducted ten case studies in an industrial setting. The results of the evaluation indicate that smells can indeed reveal weaknesses in a spreadsheet's design, and that data flow diagrams are an appropriate way to show those weaknesses.",,ICSE '12,,,
Conference Paper,Dugan RF,Performance Lies My Professor Told Me: The Case for Teaching Software Performance Engineering to Undergraduates,,2004,,,37–48,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 4th International Workshop on Software and Performance,"Redwood Shores, California",2004,9781581136739.0,,https://doi.org/10.1145/974044.974050;http://dx.doi.org/10.1145/974044.974050,10.1145/974044.974050,"In this paper we report a survey examining the approach to performance and software engineering in courses at highly ranked computer science schools in the United States. An analysis of the survey shows serious shortcomings including inadequate or missing definitions of performance, reactive ""fix it later"" mentality, vague performance requirements, and a general lack of awareness of the practices developed by the Software Performance Engineering (SPE) community. The survey is followed by guidelines for teaching SPE to undergraduates based on a semester long course we have developed. It is our plan to incorporate these guidelines into the curriculum of our senior capstone software engineering course.","education, software engineering, performance",WOSP '04,,,
Journal Article,Dugan RF,Performance Lies My Professor Told Me: The Case for Teaching Software Performance Engineering to Undergraduates,SIGSOFT Softw. Eng. Notes,2004,29.0,1,37–48,Association for Computing Machinery,"New York, NY, USA",,,2004-01,,0163-5948,https://doi.org/10.1145/974043.974050;http://dx.doi.org/10.1145/974043.974050,10.1145/974043.974050,"In this paper we report a survey examining the approach to performance and software engineering in courses at highly ranked computer science schools in the United States. An analysis of the survey shows serious shortcomings including inadequate or missing definitions of performance, reactive ""fix it later"" mentality, vague performance requirements, and a general lack of awareness of the practices developed by the Software Performance Engineering (SPE) community. The survey is followed by guidelines for teaching SPE to undergraduates based on a semester long course we have developed. It is our plan to incorporate these guidelines into the curriculum of our senior capstone software engineering course.","education, performance, software engineering",,,,
Conference Paper,Mileva YM,Learning from Deletions,,2008,,,17–20,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 2008 Foundations of Software Engineering Doctoral Symposium,"Atlanta, Georgia",2008,9781605583785.0,,https://doi.org/10.1145/1496653.1496658;http://dx.doi.org/10.1145/1496653.1496658,10.1145/1496653.1496658,"When somethings gets deleted from source code it has been deleted because it is wrong, no longer used or inappropriate. What does this mean for other places that still use the same feature? By mining software archives and the stored deletion information, I hope to detect project specific evolutionary patterns. This knowledge can later be used for recommendation of a substitution for the deleted element, detection and correction of unknown code defects1 and prediction of future deletions.",,FSEDS '08,,,
Conference Paper,"Lam W,Godefroid P,Nath S,Santhiar A,Thummalapenta S",Root Causing Flaky Tests in a Large-Scale Industrial Setting,,2019,,,101–111,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 28th ACM SIGSOFT International Symposium on Software Testing and Analysis,"Beijing, China",2019,9781450362245.0,,https://doi.org/10.1145/3293882.3330570;http://dx.doi.org/10.1145/3293882.3330570,10.1145/3293882.3330570,"In today’s agile world, developers often rely on continuous integration pipelines to help build and validate their changes by executing tests in an efficient manner. One of the significant factors that hinder developers’ productivity is flaky tests—tests that may pass and fail with the same version of code. Since flaky test failures are not deterministically reproducible, developers often have to spend hours only to discover that the occasional failures have nothing to do with their changes. However, ignoring failures of flaky tests can be dangerous, since those failures may represent real faults in the production code. Furthermore, identifying the root cause of flakiness is tedious and cumbersome, since they are often a consequence of unexpected and non-deterministic behavior due to various factors, such as concurrency and external dependencies. As developers in a large-scale industrial setting, we first describe our experience with flaky tests by conducting a study on them. Our results show that although the number of distinct flaky tests may be low, the percentage of failing builds due to flaky tests can be substantial. To reduce the burden of flaky tests on developers, we describe our end-to-end framework that helps identify flaky tests and understand their root causes. Our framework instruments flaky tests and all relevant code to log various runtime properties, and then uses a preliminary tool, called RootFinder, to find differences in the logs of passing and failing runs. Using our framework, we collect and publicize a dataset of real-world, anonymized execution logs of flaky tests. By sharing the findings from our study, our framework and tool, and a dataset of logs, we hope to encourage more research on this important problem.","regression testing, debugging, flaky tests",ISSTA 2019,,,
Conference Paper,Pohjalainen P,Self-Configuring User Interface Components,,2010,,,33–37,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 1st International Workshop on Semantic Models for Adaptive Interactive Systems,"Hong Kong, China",2010,9781450300001.0,,https://doi.org/10.1145/2002375.2002383;http://dx.doi.org/10.1145/2002375.2002383,10.1145/2002375.2002383,"In development phases of a software, its user interface is crucial to acceptance. In early phases, rapid prototyping helps in gaining sponsors for the development project. During development, the user interface is updated to meet changing requirements and, finally, maintenance-related tasks consume a major portion of effort. Some of this exertion is inherent and unavoidable, but very often it is just unnecessary overhead which is hindered by tedious internal dependencies being out of synchrony. In this paper, we show how a self-configuration via software introspection combined with semantic mapping of backend methods can be used to maintain quality of a user-interface even under pressure of changing requirements.","software engineering, user interface, self-configurating components",SEMAIS '10,,,
Conference Paper,"Okur S,Hartveld DL,Dig D,van Deursen A",A Study and Toolkit for Asynchronous Programming in C#,,2014,,,1117–1127,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 36th International Conference on Software Engineering,"Hyderabad, India",2014,9781450327565.0,,https://doi.org/10.1145/2568225.2568309;http://dx.doi.org/10.1145/2568225.2568309,10.1145/2568225.2568309,"Asynchronous programming is in demand today, because responsiveness is increasingly important on all modern devices. Yet, we know little about how developers use asynchronous programming in practice. Without such knowledge, developers, researchers, language and library designers, and tool providers can make wrong assumptions. We present the first study that analyzes the usage of asynchronous programming in a large experiment. We analyzed 1378 open source Windows Phone (WP) apps, comprising 12M SLOC, produced by 3376 developers. Using this data, we answer 2 research questions about use and misuse of asynchronous constructs. Inspired by these findings, we developed (i) Asyncifier, an automated refactoring tool that converts callback-based asynchronous code to use async/await; (ii) Corrector, a tool that finds and corrects common misuses of async/await. Our empirical evaluation shows that these tools are (i) applicable and (ii) efficient. Developers accepted 314 patches generated by our tools.","Program transformation, asynchronous, C#",ICSE 2014,,,
Conference Paper,"Arrieta A,Agirre JA,Sagardui G",Seeding Strategies for Multi-Objective Test Case Selection: An Application on Simulation-Based Testing,,2020,,,1222–1231,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 2020 Genetic and Evolutionary Computation Conference,"Cancún, Mexico",2020,9781450371285.0,,https://doi.org/10.1145/3377930.3389810;http://dx.doi.org/10.1145/3377930.3389810,10.1145/3377930.3389810,"The time it takes software systems to be tested is usually long. This is often caused by the time it takes the entire test suite to be executed. To optimize this, regression test selection approaches have allowed for improvements to the cost-effectiveness of verification and validation activities in the software industry. In this area, multi-objective algorithms have played a key role in selecting the appropriate subset of test cases from the entire test suite. In this paper, we propose a set of seeding strategies for the test case selection problem that generate the initial population of multi-objective algorithms. We integrated these seeding strategies with an NSGA-II algorithm for solving the test case selection problem in the context of simulation-based testing. We evaluated the strategies with six case studies and a total of 21 fitness combinations for each case study (i.e., a total of 126 problems). Our evaluation suggests that these strategies are indeed helpful for solving the multi-objective test case selection problem. In fact, two of the proposed seeding strategies outperformed the NSGA-II algorithm without seeding population with statistical significance for 92.8 and 96% of the problems.","regression testing, search-based software testing, test case selection",GECCO '20,,,
Journal Article,"Welborn CR,de Voogt D,Eatough M",An Analysis of Database Caching Policies,J. Comput. Sci. Coll.,2016,32.0,2,4–10,Consortium for Computing Sciences in Colleges,"Evansville, IN, USA",,,2016-12,,1937-4771,,,"This research paper will examine how two different caching policies can affect the performance of inserting, reading, and deleting elements from a relational database management system (RDBMS) built using a B+ Tree. Each of the student co-authors independently developed their own RDBMS and testing protocol. This paper will at times present a unified description of their results. The basic observation that implementing a cache with various caching policies will cause it to outperform an RDBMS with no cache seems obvious. The less obvious observation and actual focus of this paper is that optimizations must be carefully monitored with accurate metrics, as it is possible to implement what clearly appear to be optimizations but when measured provide no actual performance improvement.",,,,,
Conference Paper,"Kruschitz C,Hitz M",Analyzing the HCI Design Pattern Variety,,2010,,,,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 1st Asian Conference on Pattern Languages of Programs,"Tokyo, Japan",2010,9781450301268.0,,https://doi.org/10.1145/2371736.2371745;http://dx.doi.org/10.1145/2371736.2371745,10.1145/2371736.2371745,"Human-Computer Interaction (HCI) design patterns are an often used tool for developing user interfaces. They render the communication among stakeholders more efficient and allow for a faster design of user interfaces. However, today there exists a vast amount of patterns written by many different authors, published on Web repositories, in scientific papers, and books. This causes the form or structure of the patterns to vary according to the authors' preferences. This paper presents the results of a survey that analyses the structure and relationships of HCI design patterns from 21 different design pattern resources.","survey, pattern structure, design patterns, formalization, usability, standardization, human-computer interaction, design pattern",AsianPLoP '10,,,
Journal Article,"Lagouvardos S,Grech N,Tsatiris I,Smaragdakis Y",Precise Static Modeling of Ethereum “Memory”,Proc. ACM Program. Lang.,2020,4.0,OOPSLA,,Association for Computing Machinery,"New York, NY, USA",,,2020-11,,,https://doi.org/10.1145/3428258;http://dx.doi.org/10.1145/3428258,10.1145/3428258,"Static analysis of smart contracts as-deployed on the Ethereum blockchain has received much recent attention. However, high-precision analyses currently face significant challenges when dealing with the Ethereum VM (EVM) execution model. A major such challenge is the modeling of low-level, transient “memory” (as opposed to persistent, on-blockchain “storage”) that smart contracts employ. Statically understanding the usage patterns of memory is non-trivial, due to the dynamic allocation nature of in-memory buffers. We offer an analysis that models EVM memory, recovering high-level concepts (e.g., arrays, buffers, call arguments) via deep modeling of the flow of values. Our analysis opens the door to Ethereum static analyses with drastically increased precision. One such analysis detects the extraction of ERC20 tokens by unauthorized users. For another practical vulnerability (redundant calls, possibly used as an attack vector), our memory modeling yields analysis precision of 89%, compared to 16% for a state-of-the-art tool without precise memory modeling. Additionally, precise memory modeling enables the static computation of a contract’s gas cost. This gas-cost analysis has recently been instrumental in the evaluation of the impact of the EIP-1884 repricing (in terms of gas costs) of EVM operations, leading to a reward and significant publicity from the Ethereum Foundation.","ethereum, static analysis, EVM",,,,
Conference Paper,"Al-Kofahi JM,Kothari S,Kästner C",Four Languages and Lots of Macros: Analyzing Autotools Build Systems,,2017,,,176–186,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 16th ACM SIGPLAN International Conference on Generative Programming: Concepts and Experiences,"Vancouver, BC, Canada",2017,9781450355247.0,,https://doi.org/10.1145/3136040.3136051;http://dx.doi.org/10.1145/3136040.3136051,10.1145/3136040.3136051,"Build systems are crucial for software system development, however there is a lack of tool support to help with their high maintenance overhead. GNU Autotools are widely used in the open source community, but users face various challenges from its hard to comprehend nature and staging of multiple code generation steps, often leading to low quality and error-prone build code. In this paper, we present a platform, AutoHaven, to provide a foundation for developers to create analysis tools to help them understand, maintain, and migrate their GNU Autotools build systems. Internally it uses approximate parsing and symbolic analysis of the build logic. We illustrate the use of the platform with two tools: ACSense helps developers to better understand their build systems and ACSniff detects build smells to improve build code quality. Our evaluation shows that AutoHaven can support most GNU Autotools build systems and can detect build smells in the wild.","Autoconf, build-system, GNU Autotool, build maintenance",GPCE 2017,,,
Journal Article,"Al-Kofahi JM,Kothari S,Kästner C",Four Languages and Lots of Macros: Analyzing Autotools Build Systems,SIGPLAN Not.,2017,52.0,12,176–186,Association for Computing Machinery,"New York, NY, USA",,,2017-10,,0362-1340,https://doi.org/10.1145/3170492.3136051;http://dx.doi.org/10.1145/3170492.3136051,10.1145/3170492.3136051,"Build systems are crucial for software system development, however there is a lack of tool support to help with their high maintenance overhead. GNU Autotools are widely used in the open source community, but users face various challenges from its hard to comprehend nature and staging of multiple code generation steps, often leading to low quality and error-prone build code. In this paper, we present a platform, AutoHaven, to provide a foundation for developers to create analysis tools to help them understand, maintain, and migrate their GNU Autotools build systems. Internally it uses approximate parsing and symbolic analysis of the build logic. We illustrate the use of the platform with two tools: ACSense helps developers to better understand their build systems and ACSniff detects build smells to improve build code quality. Our evaluation shows that AutoHaven can support most GNU Autotools build systems and can detect build smells in the wild.","GNU Autotool, build-system, Autoconf, build maintenance",,,,
Conference Paper,"Beller M,Gousios G,Zaidman A","Oops, My Tests Broke the Build: An Explorative Analysis of Travis CI with GitHub",,2017,,,356–367,IEEE Press,"Buenos Aires, Argentina",Proceedings of the 14th International Conference on Mining Software Repositories,,2017,9781538615447.0,,https://doi.org/10.1109/MSR.2017.62;http://dx.doi.org/10.1109/MSR.2017.62,10.1109/MSR.2017.62,"Continuous Integration (CI) has become a best practice of modern software development. Yet, at present, we have a shortfall of insight into the testing practices that are common in CI-based software development. In particular, we seek quantifiable evidence on how central testing is to the CI process, how strongly the project language influences testing, whether different integration environments are valuable and if testing on the CI can serve as a surrogate to local testing in the IDE. In an analysis of 2,640,825 Java and Ruby builds on Travis CI, we find that testing is the single most important reason why builds fail. Moreover, the programming language has a strong influence on both the number of executed tests, their run time, and proneness to fail. The use of multiple integration environments leads to 10% more failures being caught at build time. However, testing on Travis CI does not seem an adequate surrogate for running tests locally in the IDE. To further research on Travis CI with GitHub, we introduce TravisTorrent.",,MSR '17,,,
Conference Paper,"Jabbarvand R,Malek S",ΜDroid: An Energy-Aware Mutation Testing Framework for Android,,2017,,,208–219,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 2017 11th Joint Meeting on Foundations of Software Engineering,"Paderborn, Germany",2017,9781450351058.0,,https://doi.org/10.1145/3106237.3106244;http://dx.doi.org/10.1145/3106237.3106244,10.1145/3106237.3106244,"The rising popularity of mobile apps deployed on battery-constrained devices underlines the need for effectively evaluating their energy properties. However, currently there is a lack of testing tools for evaluating the energy properties of apps. As a result, for energy testing, developers are relying on tests intended for evaluating the functional correctness of apps. Such tests may not be adequate for revealing energy defects and inefficiencies in apps. This paper presents an energy-aware mutation testing framework, called μDROID, that can be used by developers to assess the adequacy of their test suite for revealing energy-related defects. μDROID implements fifty energy-aware mutation operators and relies on a novel, automatic oracle to determine if a mutant can be killed by a test. Our evaluation on real-world Android apps shows the ability of proposed mutation operators for evaluating the utility of tests in revealing energy defects. Moreover, our automated oracle can detect whether tests kill the energy mutants with an overall accuracy of 94%, thereby making it possible to apply μDROID automatically.","Energy Testing, Software Testing, Mutation Testing, Android",ESEC/FSE 2017,,,
Journal Article,"Coelho W,Murphy G",ClassCompass: A Software Design Mentoring System,J. Educ. Resour. Comput.,2007,7.0,1,2–es,Association for Computing Machinery,"New York, NY, USA",,,2007-03,,1531-4278,https://doi.org/10.1145/1227846.1227848;http://dx.doi.org/10.1145/1227846.1227848,10.1145/1227846.1227848,"Becoming a quality software developer requires practice under the guidance of an expert mentor. Unfortunately, in most academic environments, there are not enough experts to provide any significant design mentoring for software engineering students. To address this problem, we present a collaborative software design tool intended to maximize an instructor's ability to mentor a group of students. Students use the system to create software designs for a given set of requirements. While they work, students receive automated feedback regarding common design mistakes. The system then provides support and guidance for students to manually critique each other's work. Students can view and learn from the design approaches taken by other students, as well as the critiques associated with them. We have tried this approach in software engineering classes with some positive results. We believe that this collaborative and partially automated approach can significantly improve the quality of software design education when few mentors are available.","Collaborative education, design critiquing",,,,
Conference Paper,Kongsli V,Towards Agile Security in Web Applications,,2006,,,805–808,Association for Computing Machinery,"New York, NY, USA","Companion to the 21st ACM SIGPLAN Symposium on Object-Oriented Programming Systems, Languages, and Applications","Portland, Oregon, USA",2006,9781595934918.0,,https://doi.org/10.1145/1176617.1176727;http://dx.doi.org/10.1145/1176617.1176727,10.1145/1176617.1176727,"In this paper, we present an approach that we have used to address security when running projects according to agile principles. Misuse stories have been added to user stories to capture malicious use of the application. Furthermore, misuse stories have been implemented as automated tests (unit tests, acceptance tests) in order to perform security regression testing. Penetration testing, system hardening and securing deployment have been started in early iterations of the project.","agile software development, security",OOPSLA '06,,,
Conference Paper,"Nadi S,Holt R,Davis I,Mankovskii S",DRACA: Decision Support for Root Cause Analysis and Change Impact Analysis for CMDBs,,2009,,,1–11,IBM Corp.,USA,Proceedings of the 2009 Conference of the Center for Advanced Studies on Collaborative Research,"Ontario, Canada",2009,,,https://doi.org/10.1145/1723028.1723030;http://dx.doi.org/10.1145/1723028.1723030,10.1145/1723028.1723030,"As business services become increasingly dependent on information technology (IT), it also becomes increasingly important to maximize the decision support for managing IT. Configuration Management Data Bases (CMDBs) store fundamental information about IT systems, such as the system's hardware, software and services. This information can help provide decision support for root cause analysis and change impact analysis. We have worked with our industrial research partner, CA, and with CA customers to identify challenges to the use of CMDBs to semi-automatically solve these problems. In this paper we propose a framework called DRACA (Decision Support for Root Cause Analysis and Change Impact Analysis). This framework mines key facts from the CMDB and in a sequence of three steps combines these facts with incident reports, change reports and expert knowledge, along with temporal information, to construct a probabilistic causality graph. Root causes are predicted and ranked by probabilistically tracing causality edges backwards from incidents to likely causes. Conversely, change impacts can be predicted and ranked by tracing from a proposed change forward along causality edges to locate likely undesirable impacts.",,CASCON '09,,,
Book,,"SPLASH Companion 2016: Companion Proceedings of the 2016 ACM SIGPLAN International Conference on Systems, Programming, Languages and Applications: Software for Humanity",,2016,,,,Association for Computing Machinery,"New York, NY, USA",,"Amsterdam, Netherlands",2016,9781450344371.0,,,,,,,Proceedings,,
Conference Paper,Yamashita A,"Integration of SE Research and Industry: Reflections, Theories and Illustrative Example",,2015,,,11–17,IEEE Press,"Florence, Italy",Proceedings of the Second International Workshop on Software Engineering Research and Industrial Practice,,2015,9781467370851.0,,,,"Currently, there is limited literature in Software Engineering (SE) that sheds light on the success factors and challenges for knowledge transfer between SE scientists and practitioners. Upon reflections on personal experiences from both academia and industry, this paper attempts to underpin some of the challenges for a successful collaboration, and relate them back to existing theories in the fields of Management, Medicine and Social Sciences. Furthermore, strategies for overcoming some of the challenges are provided and illustrated via a simplified example within the topic of Software Evolution. The intention of this paper is to establish a dialogue for an overall strategy within our field, by providing an illustrative example; and to promote a deeper reflection on the term 'knowledge transfer', which has predominantly focused on an unidirectional knowledge flow from academia to industry.","knowledge transfer, innovation, collaboration",SER&IP '15,,,
Conference Paper,Trubiani C,Introducing Software Performance Antipatterns in Cloud Computing Environments: Does It Help or Hurt?,,2015,,,207–210,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 6th ACM/SPEC International Conference on Performance Engineering,"Austin, Texas, USA",2015,9781450332484.0,,https://doi.org/10.1145/2668930.2695528;http://dx.doi.org/10.1145/2668930.2695528,10.1145/2668930.2695528,"Performance assessment of cloud-based big data applications require new methodologies and tools to take into consideration on one hand the volume, the variability and the complexity of big data, and on the other hand the intrinsic dynamism of cloud environments. To this end, we introduce software performance antipatterns as reference knowledge to capture the well-known bad design practices that lead to software products suffering by poor performance. This paper discusses some of the challenges and opportunities of research while introducing software performance antipatterns in cloud computing environments. We present a model-based framework that makes use of software performance antipatterns to improve the Quality-of-Service (QoS) objectives of cloud-based big data applications.","big data applications, software performance antipatterns, cloud computing environments",ICPE '15,,,
Conference Paper,"Foster SR,Griswold WG,Lerner S",WitchDoctor: IDE Support for Real-Time Auto-Completion of Refactorings,,2012,,,222–232,IEEE Press,"Zurich, Switzerland",Proceedings of the 34th International Conference on Software Engineering,,2012,9781467310673.0,,,,"Integrated Development Environments (IDEs) have come to perform a wide variety of tasks on behalf of the programmer, refactoring being a classic example. These operations have undeniable benefits, yet their large (and growing) number poses a cognitive scalability problem. Our main contribution is WitchDoctor -- a system that can detect, on the fly, when a programmer is hand-coding a refactoring. The system can then complete the refactoring in the background and propose it to the user long before the user can complete it. This implies a number of technical challenges. The algorithm must be 1) highly efficient, 2) handle unparseable programs, 3) tolerate the variety of ways programmers may perform a given refactoring, 4) use the IDE's proven and familiar refactoring engine to perform the refactoring, even though the the refactoring has already begun, and 5) support the wide range of refactorings present in modern IDEs. Our techniques for overcoming these challenges are the technical contributions of this paper. We evaluate WitchDoctor's design and implementation by simulating over 5,000 refactoring operations across three open-source projects. The simulated user is faster and more efficient than an average human user, yet WitchDoctor can detect more than 90% of refactoring operations as they are being performed -- and can complete over a third of refactorings before the simulated user does. All the while, WitchDoctor remains robust in the face of non-parseable programs and unpredictable refactoring scenarios. We also show that WitchDoctor is efficient enough to perform computation on a keystroke-by-keystroke basis, adding an average overhead of only 15 milliseconds per keystroke.",,ICSE '12,,,
Conference Paper,"Filho JB,Barais O,Acher M,Baudry B,Le Noir J",Generating Counterexamples of Model-Based Software Product Lines: An Exploratory Study,,2013,,,72–81,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 17th International Software Product Line Conference,"Tokyo, Japan",2013,9781450319683.0,,https://doi.org/10.1145/2491627.2491639;http://dx.doi.org/10.1145/2491627.2491639,10.1145/2491627.2491639,"Model-based Software Product Line (MSPL) engineering aims at deriving customized models corresponding to individual products of a family. MSPL approaches usually promote the joint use of a variability model, a base model expressed in a specific formalism, and a realization layer that maps variation points to model elements. The design space of an MSPL is extremely complex to manage for the engineer, since the number of variants may be exponential and the derived product models have to be conformant to numerous well-formedness and business rules. In this paper, the objective is to provide a way to generate MSPLs, called counterexamples, that can produce invalid product models despite a valid configuration in the variability model. We provide a systematic and automated process, based on the Common Variability Language (CVL), to randomly search the space of MSPLs for a specific formalism. We validate the effectiveness of this process for three formalisms at different scales (up to 247 metaclasses and 684 rules). We also explore and discuss how counterexamples could guide practitioners when customizing derivation engines, when implementing checking rules that prevent early incorrect CVL models, or simply when specifying an MSPL.",,SPLC '13,,,
Conference Paper,"Dantas CE,de A. Maia M",On the Actual Use of Inheritance and Interface in Java Projects: Evolution and Implications,,2017,,,151–160,IBM Corp.,USA,Proceedings of the 27th Annual International Conference on Computer Science and Software Engineering,"Markham, Ontario, Canada",2017,,,,,"Background: Inheritance is one of the main features in the object-oriented paradigm (OOP). Nonetheless, previous work recommend carefully using it, suggesting alternatives such as the adoption of composition with implementation of interfaces. Despite of being a well-studied theme, there is still little knowledge if such recommendations have been widely adopted by developers in general. Aims: This work aims at evaluating how the inheritance and composition with interfaces have been used in Java, comparing new projects with older ones (transversal), and also the different releases of the same projects (longitudinal). Method: A total of 1, 656 open-source projects built between 1997 and 2013, hosted in the repositories GitHub and SourceForge, were analyzed. The likelihood of more recent projects using inheritance and interfaces differently from older ones was analyzed considering indicators, such as, the prevalence of corrective changes, instanceof operations, and code smells. Regression analysis, chi-squared test of proportions and descriptive statistics were used to analyze the data. In addition, a thematic analysis based method was used to verify how often and why inheritance and interface are added or removed from classes. Results: We observed that developers still use inheritance primarily for code reuse, motivated by the need to avoid duplicity of source code. In newer projects, classes in inheritance had fewer corrective changes and subclasses had fewer use of the instance of operator. However, as they evolve, classes in inheritance tend to become complex as changes occur. Classes implementing interfaces have shown little relation to the interfaces, and there is indication that interfaces are still underutilized. Conclusion: These results show there is still some lack of knowledge about the use of recommended object-oriented practices, suggesting the need of training developers on how to design better classes.","inheritance, cohesion, GitHub, interfaces, sourceforge, code smells",CASCON '17,,,
Conference Paper,"Mendonça WD,Assunção WK,Linsbauer L",Multi-Objective Optimization for Reverse Engineering of Apo-Games Feature Models,,2018,,,279–283,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 22nd International Systems and Software Product Line Conference - Volume 1,"Gothenburg, Sweden",2018,9781450364645.0,,https://doi.org/10.1145/3233027.3236397;http://dx.doi.org/10.1145/3233027.3236397,10.1145/3233027.3236397,"Software Product Lines Engineering (SPLE) is a software development approach intended for the development and maintenance of variable systems, i.e. systems that exist in many different variants. In the long run SPLE has many advantages. However, it requires a large upfront investment of time and money, which is why in practice Software Product Lines (SPLs) are rarely developed from scratch. Instead, they are often built using an extractive approach by which a set of existing system variants is consolidated (i.e. reverse engineered) into an SPL. A crucial part of this process is the construction of a variability model like a Feature Model (FM) that describes the common and variable parts of the system variants. In this paper we apply an approach for reverse engineering feature models based on a multi-objective optimization algorithm to the given challenge of constructing a feature model for a set of game variants and we present the results.","reverse engineering, software product line, feature model",SPLC '18,,,
Conference Paper,"Kannabiran G,Petersen MG",Politics at the Interface: A Foucauldian Power Analysis,,2010,,,695–698,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 6th Nordic Conference on Human-Computer Interaction: Extending Boundaries,"Reykjavik, Iceland",2010,9781605589343.0,,https://doi.org/10.1145/1868914.1869007;http://dx.doi.org/10.1145/1868914.1869007,10.1145/1868914.1869007,"At the birth of participatory design, there was a strong political consciousness surrounding the design of new technology, the design process in particular, establishing a rich set of methods and tools for user-centered design. Today, the term design has extended its scope of concern beyond the process of design and into how users interact with the designed product on a day-to-day basis.This paper is an attempt to call to attention the need for a new set of methods, attitudes and approaches, along with the existing, to discuss, analyze and reflect upon the politics at the interface. By presenting a critical analysis of two design cases, we elicit the importance of such an agenda and the implications for design in doing so. We use the Foucauldian notion of power to analyze the power relationships in these two cases and to articulate the politics at the interface. We conclude by emphasizing the need for furthering this agenda and outlining future work.","interface design, transgender, foucault, politics, gender, power, critical analysis",NordiCHI '10,,,
Conference Paper,"Pradel M,Jaspan C,Aldrich J,Gross TR",Statically Checking API Protocol Conformance with Mined Multi-Object Specifications,,2012,,,925–935,IEEE Press,"Zurich, Switzerland",Proceedings of the 34th International Conference on Software Engineering,,2012,9781467310673.0,,,,"Programmers using an API often must follow protocols that specify when it is legal to call particular methods. Several techniques have been proposed to find violations of such protocols based on mined specifications. However, existing techniques either focus on single-object protocols or on particular kinds of bugs, such as missing method calls. There is no practical technique to find multi-object protocol bugs without a priori known specifications. In this paper, we combine a dynamic analysis that infers multi-object protocols and a static checker of API usage constraints into a fully automatic protocol conformance checker. The combined system statically detects illegal uses of an API without human-written specifications. Our approach finds 41 bugs and code smells in mature, real-world Java programs with a true positive rate of 51%. Furthermore, we show that the analysis reveals bugs not found by state of the art approaches.",,ICSE '12,,,
Conference Paper,"Reis L,Bispo J,Cardoso JM",SSA-Based MATLAB-to-C Compilation and Optimization,,2016,,,55–62,Association for Computing Machinery,"New York, NY, USA","Proceedings of the 3rd ACM SIGPLAN International Workshop on Libraries, Languages, and Compilers for Array Programming","Santa Barbara, CA, USA",2016,9781450343848.0,,https://doi.org/10.1145/2935323.2935330;http://dx.doi.org/10.1145/2935323.2935330,10.1145/2935323.2935330,"Many fields of engineering, science and finance use models that are developed and validated in high-level languages such as MATLAB. However, when moving to environments with resource constraints or portability challenges, these models often have to be rewritten in lower-level languages such as C. Doing so manually is costly and error-prone, but automated approaches tend to generate code that can be substantially less efficient than the handwritten equivalents. Additionally, it is usually difficult to read and improve code generated by these tools. In this paper, we describe how we improved our MATLAB-to-C compiler, based on the MATISSE framework, to be able to compete with handwritten C code. We describe our new IR and the most important optimizations that we use in order to obtain acceptable performance. We also analyze multiple C code versions to identify where the generated code is slower than the handwritten code and identify a few key improvements to generate code capable of outperforming handwritten C. We evaluate the new version of our compiler using a set of benchmarks, including the Disparity benchmark, from the San Diego Vision Benchmark Suite, on a desktop computer and on an embedded device. The achieved results clearly show the efficiency of the current version of the compiler.","SSA, optimizing compiler, source-to-source compiler, MATLAB",ARRAY 2016,,,
Conference Paper,"Livshits B,Zimmermann T",DynaMine: Finding Common Error Patterns by Mining Software Revision Histories,,2005,,,296–305,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 10th European Software Engineering Conference Held Jointly with 13th ACM SIGSOFT International Symposium on Foundations of Software Engineering,"Lisbon, Portugal",2005,9781595930149.0,,https://doi.org/10.1145/1081706.1081754;http://dx.doi.org/10.1145/1081706.1081754,10.1145/1081706.1081754,"A great deal of attention has lately been given to addressing software bugs such as errors in operating system drivers or security bugs. However, there are many other lesser known errors specific to individual applications or APIs and these violations of application-specific coding rules are responsible for a multitude of errors. In this paper we propose DynaMine, a tool that analyzes source code check-ins to find highly correlated method calls as well as common bug fixes in order to automatically discover application-specific coding patterns. Potential patterns discovered through mining are passed to a dynamic analysis tool for validation; finally, the results of dynamic analysis are presented to the user.The combination of revision history mining and dynamic analysis techniques leveraged in DynaMine proves effective for both discovering new application-specific patterns and for finding errors when applied to very large applications with many man-years of development and debugging effort behind them. We have analyzed Eclipse and jEdit, two widely-used, mature, highly extensible applications consisting of more than 3,600,000 lines of code combined. By mining revision histories, we have discovered 56 previously unknown, highly application-specific patterns. Out of these, 21 were dynamically confirmed as very likely valid patterns and a total of 263 pattern violations were found.","revision histories, data mining, software bugs, error patterns, one-line check-ins, coding patterns, dynamic analysis",ESEC/FSE-13,,,
Journal Article,"Livshits B,Zimmermann T",DynaMine: Finding Common Error Patterns by Mining Software Revision Histories,SIGSOFT Softw. Eng. Notes,2005,30.0,5,296–305,Association for Computing Machinery,"New York, NY, USA",,,2005-09,,0163-5948,https://doi.org/10.1145/1095430.1081754;http://dx.doi.org/10.1145/1095430.1081754,10.1145/1095430.1081754,"A great deal of attention has lately been given to addressing software bugs such as errors in operating system drivers or security bugs. However, there are many other lesser known errors specific to individual applications or APIs and these violations of application-specific coding rules are responsible for a multitude of errors. In this paper we propose DynaMine, a tool that analyzes source code check-ins to find highly correlated method calls as well as common bug fixes in order to automatically discover application-specific coding patterns. Potential patterns discovered through mining are passed to a dynamic analysis tool for validation; finally, the results of dynamic analysis are presented to the user.The combination of revision history mining and dynamic analysis techniques leveraged in DynaMine proves effective for both discovering new application-specific patterns and for finding errors when applied to very large applications with many man-years of development and debugging effort behind them. We have analyzed Eclipse and jEdit, two widely-used, mature, highly extensible applications consisting of more than 3,600,000 lines of code combined. By mining revision histories, we have discovered 56 previously unknown, highly application-specific patterns. Out of these, 21 were dynamically confirmed as very likely valid patterns and a total of 263 pattern violations were found.","coding patterns, software bugs, data mining, one-line check-ins, revision histories, dynamic analysis, error patterns",,,,
Conference Paper,"Schuler D,Dallmeier V,Zeller A",Efficient Mutation Testing by Checking Invariant Violations,,2009,,,69–80,Association for Computing Machinery,"New York, NY, USA",Proceedings of the Eighteenth International Symposium on Software Testing and Analysis,"Chicago, IL, USA",2009,9781605583389.0,,https://doi.org/10.1145/1572272.1572282;http://dx.doi.org/10.1145/1572272.1572282,10.1145/1572272.1572282,"Mutation testing measures the adequacy of a test suite by seeding artificial defects (mutations) into a program. If a mutation is not detected by the test suite, this usually means that the test suite is not adequate. However, it may also be that the mutant keeps the program's semantics unchanged-and thus cannot be detected by any test. Such equivalent mutants have to be eliminated manually, which is tedious.We assess the impact of mutations by checking dynamic invariants. In an evaluation of our JAVALANCHE framework on seven industrial-size programs, we found that mutations that violate invariants are significantly more likely to be detectable by a test suite. As a consequence, mutations with impact on invariants should be focused upon when improving test suites. With less than 3% of equivalent mutants, our approach provides an efficient, precise, and fully automatic measure of the adequacy of a test suite.","mutation testing, dynamic invariants",ISSTA '09,,,
Journal Article,"Castro P,Ishakian V,Muthusamy V,Slominski A",The Rise of Serverless Computing,Commun. ACM,2019,62.0,12,44–54,Association for Computing Machinery,"New York, NY, USA",,,2019-11,,0001-0782,https://doi.org/10.1145/3368454;http://dx.doi.org/10.1145/3368454,10.1145/3368454,"The server is dead, long live the server.",,,,,
Conference Paper,"Ammar M,Crispo B",Verify&Revive: Secure Detection and Recovery of Compromised Low-End Embedded Devices,,2020,,,717–732,Association for Computing Machinery,"New York, NY, USA",Annual Computer Security Applications Conference,"Austin, USA",2020,9781450388580.0,,https://doi.org/10.1145/3427228.3427253;http://dx.doi.org/10.1145/3427228.3427253,10.1145/3427228.3427253,"Tiny and specialized computing platforms, so-called embedded or Internet of Things (IoT) devices, are increasingly used in safety- and privacy-critical application scenarios. A significant number of such devices offer limited or no security features, making them attractive targets for a wide variety of cyber attacks, exemplified by malware infestations. One key component in securing these devices is establishing a root of trust, which is typically attained via remote attestation (RA), a security service that aims to ascertain the current state of a remote device and detect any malicious tampering. Although several (software-based, hardware-based, and hybrid) RA approaches have been proposed to address this problem, two main issues remain, regardless of the type of RA. First, all but one of the existing RA approaches are vulnerable to Time-Of-Check Time-Of-Use (TOCTOU) attack, where a transient malware may infect the corresponding embedded device between two consecutive RA routines without being detected. Second, little attention has been devoted to efficiently and securely rescuing devices that are determined to be compromised, increasing the maintenance cost of IoT deployments, especially in industrial control systems, where (re-)deploying a new device is often a cost-sensitive operation. Motivated by the fact that many low-end devices neither support hardware-based RA nor can afford hardware modifications required by hybrid approaches, we tackle the aforementioned issues by proposing Verify&Revive, the first reliable pure-software approach to remote attestation with recovery techniques, targeting the low-end range of IoT devices. It consists of two components: Verify and Revive. Verify is a TOCTOU-secure RA scheme with a built-in secure erasure module that is automatically executed as a countermeasure in case of detection of a malware infection on the IoT device. Revive is a secure code update scheme that is executed upon request to install regular updates or as a recovery technique to restore the last benign settings of the cleaned, yet non-functioning, IoT device. A proof of attestation, erasure, and update/recovery is obtained relying on trustworthy software, leveraging and extending a formally-verified software-based memory isolation technique, called the Security MicroVisor (SμV). We implement and evaluate Verify&Revive on industrial resource-constrained IoT devices, showing very low overhead in terms of a memory footprint, performance, and battery lifetime.","TOCTOU attack, secure erasure, remote attestation, secure update",ACSAC '20,,,
Conference Paper,"Wu J,Liu S,Ji S,Yang M,Luo T,Wu Y,Wang Y","Exception beyond Exception: Crashing Android System by Trapping in ""UncaughtException""",,2017,,,283–292,IEEE Press,"Buenos Aires, Argentina",Proceedings of the 39th International Conference on Software Engineering: Software Engineering in Practice Track,,2017,9781538627174.0,,https://doi.org/10.1109/ICSE-SEIP.2017.12;http://dx.doi.org/10.1109/ICSE-SEIP.2017.12,10.1109/ICSE-SEIP.2017.12,"Android is characterized as a complicated open source software stack created for a wide array of phones with different form of factors, whose latest release has over one hundred million lines of code. Such code is mainly developed with the Java language, which builds complicated logic and brings implicit information flows among components and the inner framework. By studying the source code of system service interfaces, we discovered an unknown type of code flaw, which is named uncaughtException flaw, caused by unwell implemented exceptions that could crash the system and be further vulnerable to system level Denial-of-Service (DoS) attacks. We found that exceptions are used to handle the errors and other exceptional events but sometimes they would kill some critical system services exceptionally. We designed and implemented ExHunter, a new tool for automatic detection of this uncaughtException flaw by dynamically reflecting service interfaces, continuously fuzzing parameters and verifying the running logs. On 11 new popular Android phones, ExHunter extracted 1045 system services, reflected 758 suspicious functions, discovered 132 uncaughtException flaws which have never been known before and generated 275 system DoS attack exploitations. The results showed that: (1) almost every type of Android phone suffers from this flaw; (2) the flaws are different from phone by phone; and (3) all the vulnerabilities can be exploited by direct/indirect trapping. To mitigate uncaughtException flaws, we further developed ExCatcher to re-catch the exceptions. Finally, we informed four leading Android phones manufactures and provided secure improvements in their commercial phones.","vulnerability, DoS attack, exception, Android system service",ICSE-SEIP '17,,,
Conference Paper,"Eshkevari L,Mazinanian D,Rostami S,Tsantalis N",JSDeodorant: Class-Awareness for JavaScript Programs,,2017,,,71–74,IEEE Press,"Buenos Aires, Argentina",Proceedings of the 39th International Conference on Software Engineering Companion,,2017,9781538615898.0,,https://doi.org/10.1109/ICSE-C.2017.6;http://dx.doi.org/10.1109/ICSE-C.2017.6,10.1109/ICSE-C.2017.6,"Until the recent updates to JavaScript specifications, adding syntactical support for class and namespace declaration, developers used custom solutions to emulate modular decomposition (e.g., classes and namespaces) and other object-oriented constructs, such as interfaces, and inheritance relationships. However, the lack of standards for several years led to a large variation and diversity of custom solutions for emulating object-oriented constructs, making maintenance and comprehension activities rather difficult in JavaScript projects developed based on the previous language specifications. In this paper, we present JSDeodorant, an Eclipse plug-in that enables class-aware maintenance and comprehension for JavaScript programs. (https://youtu.be/k4U2LwkL6JU)",,ICSE-C '17,,,
Conference Paper,"Marginean A,Bader J,Chandra S,Harman M,Jia Y,Mao K,Mols A,Scott A",SapFix: Automated End-to-End Repair at Scale,,2019,,,269–278,IEEE Press,"Montreal, Quebec, Canada",Proceedings of the 41st International Conference on Software Engineering: Software Engineering in Practice,,2019,,,https://doi.org/10.1109/ICSE-SEIP.2019.00039;http://dx.doi.org/10.1109/ICSE-SEIP.2019.00039,10.1109/ICSE-SEIP.2019.00039,"We report our experience with SAPFIX: the first deployment of automated end-to-end fault fixing, from test case design through to deployed repairs in production code1. We have used SAPFIX at Facebook to repair 6 production systems, each consisting of tens of millions of lines of code, and which are collectively used by hundreds of millions of people worldwide.",,ICSE-SEIP '19,,,
Conference Paper,"Pinto G,Castor F",On the Implications of Language Constructs for Concurrent Execution in the Energy Efficiency of Multicore Applications,,2013,,,95–96,Association for Computing Machinery,"New York, NY, USA","Proceedings of the 2013 Companion Publication for Conference on Systems, Programming, & Applications: Software for Humanity","Indianapolis, Indiana, USA",2013,9781450319959.0,,https://doi.org/10.1145/2508075.2508097;http://dx.doi.org/10.1145/2508075.2508097,10.1145/2508075.2508097,"Our study analyzed the performance and energy consumption of multicore applications, using a number of techniques to manage concurrent execution. We concluded that language constructs for concurrent execution can impact energy consumption. Nonetheless, the tradeoff between performance and energy consumption in multicore applications is not as obvious as it seems.","concurrent execution, language constructs, energy-efficiency",SPLASH '13,,,
Conference Paper,"Dietrich J,McCartin C,Tempero E,Shah SM",On the Existence of High-Impact Refactoring Opportunities in Programs,,2012,,,37–48,"Australian Computer Society, Inc.",AUS,Proceedings of the Thirty-Fifth Australasian Computer Science Conference - Volume 122,"Melbourne, Australia",2012,9781921770036.0,,,,"The refactoring of large systems is difficult, with the possibility of many refactorings having to be done before any useful benefit is attained. We present a novel approach to detect starting points for the architectural refactoring of large and complex systems based on the analysis and manipulation of the type dependency graph extracted from programs. The proposed algorithm is based on the simultaneous analysis of multiple architectural antipatterns, and outputs dependencies between artefacts that participate in large numbers of instances of these antipatterns. If these dependencies can be removed, they represent high-impact refactoring opportunities: a small number of changes that have a major impact on the overall quality of the system, measured by counting architectural antipattern instances. The proposed algorithm is validated using an experiment where we analyse a set of 95 open-source Java programs for instances of four architectural patterns representing modularisation problems. We discuss some examples demonstrating how the computed dependencies can be removed from programs. This research is motivated by the emergence of technologies such as dependency injection frameworks and dynamic component models. These technologies try to improve the maintainability of systems by removing dependencies between system parts from program source code and managing them explicitly in configuration files.",,ACSC '12,,,
Conference Paper,"Tansey W,Tilevich E",Annotation Refactoring: Inferring Upgrade Transformations for Legacy Applications,,2008,,,295–312,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 23rd ACM SIGPLAN Conference on Object-Oriented Programming Systems Languages and Applications,"Nashville, TN, USA",2008,9781605582153.0,,https://doi.org/10.1145/1449764.1449788;http://dx.doi.org/10.1145/1449764.1449788,10.1145/1449764.1449788,"Since annotations were added to the Java language, many frameworks have moved to using annotated Plain Old Java Objects (POJOs) in their newest releases. Legacy applications are thus forced to undergo extensive restructuring in order to migrate from old framework versions to new versions based on annotations (Version Lock-in). Additionally, because annotations are embedded in the application code, changing between framework vendors may also entail largescale manual changes (Vendor Lock-in).This paper presents a novel refactoring approach that effectively solves these two problems. Our approach infers a concise set of semantics-preserving transformation rules from two versions of a single class. Unlike prior approaches that detect only simple structural refactorings, our algorithm can infer general composite refactorings and is more than 97% accurate on average. We demonstrate the effectiveness of our approach by automatically upgrading more than 80K lines of the unit testing code of four open-source Java applications to use the latest version of the popular JUnit testing framework.","eclipse, frameworks, java, JUnit, metadata, refactoring, upgrading, annotations",OOPSLA '08,,,
Journal Article,"Tansey W,Tilevich E",Annotation Refactoring: Inferring Upgrade Transformations for Legacy Applications,SIGPLAN Not.,2008,43.0,10,295–312,Association for Computing Machinery,"New York, NY, USA",,,2008-10,,0362-1340,https://doi.org/10.1145/1449955.1449788;http://dx.doi.org/10.1145/1449955.1449788,10.1145/1449955.1449788,"Since annotations were added to the Java language, many frameworks have moved to using annotated Plain Old Java Objects (POJOs) in their newest releases. Legacy applications are thus forced to undergo extensive restructuring in order to migrate from old framework versions to new versions based on annotations (Version Lock-in). Additionally, because annotations are embedded in the application code, changing between framework vendors may also entail largescale manual changes (Vendor Lock-in).This paper presents a novel refactoring approach that effectively solves these two problems. Our approach infers a concise set of semantics-preserving transformation rules from two versions of a single class. Unlike prior approaches that detect only simple structural refactorings, our algorithm can infer general composite refactorings and is more than 97% accurate on average. We demonstrate the effectiveness of our approach by automatically upgrading more than 80K lines of the unit testing code of four open-source Java applications to use the latest version of the popular JUnit testing framework.","upgrading, refactoring, eclipse, JUnit, annotations, metadata, frameworks, java",,,,
Conference Paper,"Dash SK,Allamanis M,Barr ET",RefiNym: Using Names to Refine Types,,2018,,,107–117,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 2018 26th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering,"Lake Buena Vista, FL, USA",2018,9781450355735.0,,https://doi.org/10.1145/3236024.3236042;http://dx.doi.org/10.1145/3236024.3236042,10.1145/3236024.3236042,"Source code is bimodal: it combines a formal, algorithmic channel and a natural language channel of identifiers and comments. In this work, we model the bimodality of code with name flows, an assignment flow graph augmented to track identifier names. Conceptual types are logically distinct types that do not always coincide with program types. Passwords and URLs are example conceptual types that can share the program type string. Our tool, RefiNym, is an unsupervised method that mines a lattice of conceptual types from name flows and reifies them into distinct nominal types. For string, RefiNym finds and splits conceptual types originally merged into a single type, reducing the number of same-type variables per scope from 8.7 to 2.2 while eliminating 21.9% of scopes that have more than one same-type variable in scope. This makes the code more self-documenting and frees the type system to prevent a developer from inadvertently assigning data across conceptual types.","Type Reinement, Information-theoretic Clustering",ESEC/FSE 2018,,,
Conference Paper,"Kim SH,Yi JS,Elmqvist N",Oopsy-Daisy: Failure Stories in Quantitative Evaluation Studies for Visualizations,,2014,,,142–146,Association for Computing Machinery,"New York, NY, USA",Proceedings of the Fifth Workshop on Beyond Time and Errors: Novel Evaluation Methods for Visualization,"Paris, France",2014,9781450332095.0,,https://doi.org/10.1145/2669557.2669576;http://dx.doi.org/10.1145/2669557.2669576,10.1145/2669557.2669576,"Designing, conducting, and interpreting evaluation studies with human participants is challenging. While researchers in cognitive psychology, social science, and human-computer interaction view competence in evaluation study methodology a key job skill, it is only recently that visualization researchers have begun to feel the need to learn this skill as well. Acquiring such competence is a lengthy and difficult process fraught with much trial and error. Recent work on patterns for visualization evaluation is now providing much-needed best practices for how to evaluate a visualization technique with human participants. However, negative examples of evaluation methods that fail, yield no usable results, or simply do not work are still missing, mainly because of the difficulty and lack of incentive for publishing negative results or failed research. In this paper, we take the position that there are many good ideas with the best intentions for how to evaluate a visualization tool that simply do not work. We call upon the community to help collect these negative examples in order to show the other side of the coin: what not to do when trying to evaluate visualization.","evaluation studies, mistakes, lessons learned, failures",BELIV '14,,,
Conference Paper,"Sudhakaran V,Chue Hong NP",Evaluating the Suitability of Mapreduce for Surface Temperature Analysis Codes,,2011,,,3–12,Association for Computing Machinery,"New York, NY, USA",Proceedings of the Second International Workshop on Data Intensive Computing in the Clouds,"Seattle, Washington, USA",2011,9781450311441.0,,https://doi.org/10.1145/2087522.2087526;http://dx.doi.org/10.1145/2087522.2087526,10.1145/2087522.2087526,"Processing large volumes of scientific data requires an efficient and scalable parallel computing framework to obtain meaningful information quickly. In this paper, we evaluate a scientific application from the environmental sciences for its suitability to use the MapReduce framework. We consider cccgistemp -- a Python reimplementation of the original NASA GISS model for estimating global temperature change -- which takes land and ocean temperature records from different sites, removes duplicate records, and adjusts for urbanisation effects before calculating the 12 month running mean global temperature. The application consists of several stages, each displaying differing characteristics, and three stages have been ported to use Hadoop with the mrjob library. We note performance bottlenecks encountered while porting and suggest possible solutions, including modification of data access patterns to overcome uneven distribution of input data.","hadoop, mapreduce, environmental sciences, data-intensive",DataCloud-SC '11,,,
Conference Paper,"Jasser S,Riebisch M",Reusing Security Solutions: A Repository for Architectural Decision Support,,2016,,,,Association for Computing Machinery,"New York, NY, USA",Proccedings of the 10th European Conference on Software Architecture Workshops,"Copenhagen, Denmark",2016,9781450347815.0,,https://doi.org/10.1145/2993412.3007556;http://dx.doi.org/10.1145/2993412.3007556,10.1145/2993412.3007556,"Today, the interplay of security design and architecting is still poorly understood and architects lack knowledge about security and architectural security design. Yet, architectural knowledge on security design and its impact on other architectural properties is essential for making right decisions in architecture design. Knowledge is covered within solutions such as architectural patterns, tactics, and tools. Sharing it including the experience other architects gained using these solutions would enable better reuse of security solutions.In this paper, we present a repository for security solutions that supports architectural decisions including quality goal trade-offs. Its metamodel was adapted to special demands of security as a quality goal. The repository supports architecture decisions not only through populating approved solutions but through a recommender system that documents knowledge and experiences of architecture and security experts. We provide a case study to illustrate the repository's features and its application during architecture design.","secure software development, security by design, security engineering, reusing security solutions, secure architecture, software architecture",ECSAW '16,,,
Conference Paper,"Pellegrini L,Lenarduzzi V",Are Code Smells the Root Cause of Faults? A Continuous Experimentation Approach,,2018,,,,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 19th International Conference on Agile Software Development: Companion,"Porto, Portugal",2018,9781450364225.0,,https://doi.org/10.1145/3234152.3234153;http://dx.doi.org/10.1145/3234152.3234153,10.1145/3234152.3234153,"Code Smells are quite a good instrument to evaluate code's quality, even if they provide a general analysis: no one can assure that determined code smells are really responsible for faults. More and more software companies pay attention to produce qualitative software, in order to reduce the number of bugs. But how can they know, which code-refactoring really can decrease the faults' number? In this work, we aim to find out which code smells are really the cause of bugs and, through a continuous monitoring system, continuously propose companies code refactoring, with the aim of reduce drastically the number of bugs.","code smells, continuous experimentation, fault",XP '18,,,
Conference Paper,"Kessentini M,Dea TJ,Ouni A",A Context-Based Refactoring Recommendation Approach Using Simulated Annealing: Two Industrial Case Studies,,2017,,,1303–1310,Association for Computing Machinery,"New York, NY, USA",Proceedings of the Genetic and Evolutionary Computation Conference,"Berlin, Germany",2017,9781450349208.0,,https://doi.org/10.1145/3071178.3071334;http://dx.doi.org/10.1145/3071178.3071334,10.1145/3071178.3071334,"Refactoring is a highly valuable solution to reduce and manage the growing complexity of software systems. However, programmers are ""opportunistic"" when they apply refactorings since most of them are interested in improving the quality of the code fragments that they frequently update or those related to the planned activities for the next release (fixing bugs, adding new functionalities, etc.). In this paper, we describe a search based approach to recommend refactorings based on the analysis of the history of changes to maximize the recommended refactorings for recently modified classes, classes containing incomplete refactorings detected in previous releases, and buggy classes identified in the history of previous bug reports. The obtained results on two industrial projects show significant improvements of the relevance of recommended refactorings, as evaluated by the original developers of the systems.","software quality, refactoring, search based software engineering",GECCO '17,,,
Conference Paper,"Stolee KT,Saylor J,Lund T",Exploring the Benefits of Using Redundant Responses in Crowdsourced Evaluations,,2015,,,38–44,IEEE Press,"Florence, Italy",Proceedings of the Second International Workshop on CrowdSourcing in Software Engineering,,2015,,,,,"Crowdsourcing can be an efficient and cost-effective way to evaluate software engineering research, particularly when the evaluation can be broken down into small, independent tasks. In prior work, we crowdsourced evaluations for a refactoring technique for web mashups and for a source code search engine, both using Amazon's Mechanical Turk. In the refactoring study, preference information was gathered when comparing a refactored with an unrefactored pipe, in addition to a free-text justification. In the code search study, information was gathered about whether a code snippet was relevant to a programming task and why. In both studies, we used redundant metrics and gathered quantitative and qualitative data in an effort to control response quality. Our prior work only analyzed the quantitative results.In this work, we explore the value of using such redundant metrics in crowdsourced evaluations. We code the free-text responses to unveil common themes among the responses and then compare those themes with the quantitative results. Our findings indicate high similarity between the quantitative and free-text responses, that the quantitative results are sometimes more positive than the free-text response, and that some of the qualitative responses point to potential inadequacies with the quantitative questions from the studies.",,CSI-SE '15,,,
Journal Article,"Cogumbreiro T,Hu R,Martins F,Yoshida N",Dynamic Deadlock Verification for General Barrier Synchronisation,ACM Trans. Program. Lang. Syst.,2018,41.0,1,,Association for Computing Machinery,"New York, NY, USA",,,2018-12,,0164-0925,https://doi.org/10.1145/3229060;http://dx.doi.org/10.1145/3229060,10.1145/3229060,"We present Armus, a verification tool for dynamically detecting or avoiding barrier deadlocks. The core design of Armus is based on phasers, a generalisation of barriers that supports split-phase synchronisation, dynamic membership, and optional-waits. This allows Armus to handle the key barrier synchronisation patterns found in modern languages and libraries. We implement Armus for X10 and Java, giving the first sound and complete barrier deadlock verification tools in these settings.Armus introduces a novel event-based graph model of barrier concurrency constraints that distinguishes task-event and event-task dependencies. Decoupling these two kinds of dependencies facilitates the verification of distributed barriers with dynamic membership, a challenging feature of X10. Further, our base graph representation can be dynamically switched between a task-to-task model, Wait-for Graph (WFG), and an event-to-event model, State Graph (SG), to improve the scalability of the analysis.Formally, we show that the verification is sound and complete with respect to the occurrence of deadlock in our core phaser language, and that switching graph representations preserves the soundness and completeness properties. These results are machine checked with the Coq proof assistant. Practically, we evaluate the runtime overhead of our implementations using three benchmark suites in local and distributed scenarios. Regarding deadlock detection, distributed scenarios show negligible overheads and local scenarios show overheads below 1.15×. Deadlock avoidance is more demanding, and highlights the potential gains from dynamic graph selection. In one benchmark scenario, the runtime overheads vary from 1.8× for dynamic selection, 2.6× for SG-static selection, and 5.9× for WFG-static selection.","Java, deadlock avoidance, phasers, Barrier synchronisation, deadlock detection, X10",,,,
Conference Paper,"Cortellessa V,Di Marco A,Eramo R,Pierantonio A,Trubiani C",Digging into UML Models to Remove Performance Antipatterns,,2010,,,9–16,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 2010 ICSE Workshop on Quantitative Stochastic Models in the Verification and Design of Software Systems,"Cape Town, South Africa",2010,9781605589725.0,,https://doi.org/10.1145/1808877.1808880;http://dx.doi.org/10.1145/1808877.1808880,10.1145/1808877.1808880,"Performance antipatterns have been informally defined and characterized as bad practices in software design that can originate performance problems. Such special type of patterns can involve static and dynamic aspects of software as well as deployment features. It has been shown that quite often the inability to meet performance requirements is due to the presence of antipatterns in the software design. However the problem of formally defining antipatterns and automatically detect them within a design model has not been investigated yet. In this paper we examine this problem within the UML context and show how performance antipatterns can be defined and detected in UML models by mean of OCL. A case study in UML annotated with the MARTE profile is presented where, after a performance analysis that shows unsatisfactory results, performance antipatterns are detected through an OCL engine. The identification of an antipattern suggests the architectural alternatives that can remove that specific problem. We show in our example that the removal of a certain antipattern actually allows to overcome a specific performance problem.","software performance engineering, object constraint language, antipatterns, unified modeling language",QUOVADIS '10,,,
Conference Paper,"Femmer H,Fernández DM,Juergens E,Klose M,Zimmer I,Zimmer J",Rapid Requirements Checks with Requirements Smells: Two Case Studies,,2014,,,10–19,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 1st International Workshop on Rapid Continuous Software Engineering,"Hyderabad, India",2014,9781450328562.0,,https://doi.org/10.1145/2593812.2593817;http://dx.doi.org/10.1145/2593812.2593817,10.1145/2593812.2593817,"Bad requirements quality can have expensive consequences during the software development lifecycle. Especially, if iterations are long and feedback comes late - the faster a problem is found, the cheaper it is to fix. We propose to detect issues in requirements based on requirements (bad) smells by applying a light-weight static requirements analysis. This light-weight technique allows for instant checks as soon as a requirement is written down. In this paper, we derive a set of smells, including automatic smell detection, from the natural language criteria of the ISO/IEC/IEEE 29148 standard. We evaluated the approach with 336 requirements and 53 use cases from 9 specifications that were written by the car manufacturer Daimler AG and the chemical business company Wacker Chemie AG, and discussed the results with their requirements and domain experts. While not all problems can be detected, the case study shows that lightweight smell analysis can uncover many practically relevant requirements defects. Based on these results and the discussion with our industry partners, we conclude that requirements smells can serve as an efficient supplement to traditional reviews or team discussions, in order to create fast feedback on requirements quality.","Requirements Smells, Requirements Engineering, Analytical Quality Assurance",RCoSE 2014,,,
Conference Paper,"Athanasiadis IN,Villa F",A Roadmap to Domain Specific Programming Languages for Environmental Modeling: Key Requirements and Concepts,,2013,,,27–32,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 2013 ACM Workshop on Domain-Specific Modeling,"Indianapolis, Indiana, USA",2013,9781450326001.0,,https://doi.org/10.1145/2541928.2541934;http://dx.doi.org/10.1145/2541928.2541934,10.1145/2541928.2541934,"The limited reuse of current environmental software can be blamed in part on the tools used to develop it; the use of generic-purpose programming languages makes it particularly hard. As environmental scientists strive to prioritize the clear statement and communication of the semantics of natural systems in favor of understanding software implementations of their models, Domain-Specific Languages may come to help, offering the option of truly declarative environment for environmental modeling. This paper discusses some key requirements and concepts for developing Domain-Specific Languages that can inform and streamline environmental modeling, and previews some use scenarios using examples from a DSL in development.","semantic modeling, software interoperability, ecoinformatics, domain specific languages, environmental modeling, software reuse",DSM '13,,,
Conference Paper,"Porat T,Schclar A,Shapira B",MATE: A Mobile Analysis Tool for Usability Experts,,2013,,,265–270,Association for Computing Machinery,"New York, NY, USA",CHI '13 Extended Abstracts on Human Factors in Computing Systems,"Paris, France",2013,9781450319522.0,,https://doi.org/10.1145/2468356.2468404;http://dx.doi.org/10.1145/2468356.2468404,10.1145/2468356.2468404,"The 'SmartMobile' research project is directed to design and develop tools to help mobile companies view and analyze data related to the usage and performance of their applications and services.In this paper we focus on one of the main tools that were developed - MATE (Mobile Analysis Tool for usability Experts). MATE is designed to highlight potential usability problems in specific mobile applications, tasks and screens. This is done by extracting and aggregating relevant usage and performance metrics from real customers using their mobile devices. Subjective metrics received from usability tests may be inserted to MATE in order to compare objective and subjective metrics per scenario and per task. Usability evaluations performed by experienced usability experts strengthened the necessity of this tool as an important complement to usability testing.","human-computer interaction, mobile devices, usability testing, usability",CHI EA '13,,,
Conference Paper,"Santos AR,do Carmo Machado I,de Almeida ES",RiPLE-HC: Javascript Systems Meets Spl Composition,,2016,,,154–163,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 20th International Systems and Software Product Line Conference,"Beijing, China",2016,9781450340502.0,,https://doi.org/10.1145/2934466.2934486;http://dx.doi.org/10.1145/2934466.2934486,10.1145/2934466.2934486,"Context. Software Product Lines (SPL) engineering is increasingly being applied to handle variability in industrial software systems. Problem. The research community has pointed out a series of benefits which modularity brings to software composition, a key aspect in SPL engineering. However, in practice, the reuse in Javascript-based systems relies on the use of package managers (e.g., npm, jam, bower, requireJS), but these approaches do not allow the management of project features. Method. This paper presents the RiPLE-HC, a strategy aimed at blending compositional and annotative approaches to implement variability in Javascript-based systems. Results. We applied the approach in an industrial environment and conducted an academic case study with six open-source systems to evaluate its robustness and scalability. Additionally, we carried a controlled experiment to analyze the impact of the RiPLE-HC code organization on the feature location maintenance tasks. Conclusion. The empirical evaluations yielded evidence of reduced effort in feature location, and positive benefits when introducing systematic reuse aspects in Javascript-based systems.","web systems domain, featureIDE, software product line engineering, eclipse plugin, feature composition",SPLC '16,,,
Conference Paper,Cox C,Generalization Refactorings for Reusable Aspects,,2013,,,,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 51st ACM Southeast Conference,"Savannah, Georgia",2013,9781450319010.0,,https://doi.org/10.1145/2498328.2500050;http://dx.doi.org/10.1145/2498328.2500050,10.1145/2498328.2500050,"Aspect-oriented programming techniques have shown potential for improving the modularity of software by factoring out the code related to implementing cross-cutting concerns into separate aspect modules. The necessary cross-cutting functionality is then injected into the application as needed at runtime or compile-time by the aspects. Refactoring catalogs exist to provide guidance for this refactoring but could be expanded to improve the reusability of the resulting aspect code. This work presents three new aspect refactorings that provide generally-applicable guidance for improving the reusability of refactored aspects. The proposed refactorings further modularize the aspects themselves by separating the general concern-related code from the application-specific concern-related code, thus improving the potential reusability of the aspects.","aspect-oriented refactoring, design patterns, software modularity",ACMSE '13,,,
Conference Paper,"Strauch J,Schreier S",RESTify: From RPCs to RESTful HTTP Design,,2012,,,11–18,Association for Computing Machinery,"New York, NY, USA",Proceedings of the Third International Workshop on RESTful Design,"Lyon, France",2012,9781450311908.0,,https://doi.org/10.1145/2307819.2307824;http://dx.doi.org/10.1145/2307819.2307824,10.1145/2307819.2307824,"Starting with RESTful design is a difficult task -- even more if the designer has a RPC or object-oriented background. To support the adaption from RPC- to REST-oriented thinking, we propose RESTify, a straightforward procedure model to redesign a RPC-oriented interface into a hypermedia-enabled REST interface. RESTfiy uses a WSDL document of an existing SOAP service and consists of three iterations. The result of each iteration is an enhanced version of the preceding one concerning the REST constraints and is meant to be implemented as a HTTP service. Beside the technical result of the process and the design of a RESTful interface, the developer becomes acquainted to the main elements of a RESTful design, the constraints and their application. The results of the evaluation, using a prototypical web application and public SOAP services, are promising.","SOAP, RPC, iterative, hypermedia, HTTP, resource design, REST, procedure model",WS-REST '12,,,
Conference Paper,"Di Penta M,Bavota G,Zampetti F",On the Relationship between Refactoring Actions and Bugs: A Differentiated Replication,,2020,,,556–567,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering,"Virtual Event, USA",2020,9781450370431.0,,https://doi.org/10.1145/3368089.3409695;http://dx.doi.org/10.1145/3368089.3409695,10.1145/3368089.3409695,"Software refactoring aims at improving code quality while preserving the system's external behavior. Although in principle refactoring is a behavior-preserving activity, a study presented by Bavota etal in 2012 reported the proneness of some refactoring actions (eg pull up method) to induce faults. The study was performed by mining refactoring activities and bugs from three systems. Taking profit of the advances made in the mining software repositories field (eg better tools to detect refactoring actions at commit-level granularity), we present a differentiated replication of the work by Bavota etal in which we (i) overcome some of the weaknesses that affect their experimental design, (ii) answer the same research questions of the original study on a much larger dataset (3 vs 103 systems), and (iii) complement the quantitative analysis of the relationship between refactoring and bugs with a qualitative, manual inspection of commits aimed at verifying the extent to which refactoring actions trigger bug-fixing activities. The results of our quantitative analysis confirm the findings of the replicated study, while the qualitative analysis partially demystifies the role played by refactoring actions in the bug introduction.","bug introduction, refactoring, mining software repositories",ESEC/FSE 2020,,,
Conference Paper,"Williams LG,Smith CU",PASASM: A Method for the Performance Assessment of Software Architectures,,2002,,,179–189,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 3rd International Workshop on Software and Performance,"Rome, Italy",2002,9781581135633.0,,https://doi.org/10.1145/584369.584397;http://dx.doi.org/10.1145/584369.584397,10.1145/584369.584397,"Architectural decisions are among the earliest made in a software development project. They are also the most costly to fix if, when the software is completed, the architecture is found to be inappropriate for meeting quality objectives. Thus, it is important to be able to assess the impact of architectural decisions on quality objectives such as performance and reliability at the time that they are made.This paper describes PASA, a method for performance assessment of software architectures. It was developed from our experience in conducting performance assessments of software architectures in a variety of application domains including web-based systems, financial applications, and real-time systems. PASA uses the principles and techniques of software performance engineering (SPE) to determine whether an architecture is capable of supporting its performance objectives. The method may be applied to new development to uncover potential problems when they are easier and less expensive to fix. It may also be used when upgrading legacy systems to decide whether to continue to commit resources to the current architecture or migrate to a new one. The method is illustrated with an example drawn from an actual assessment.",,WOSP '02,,,
Conference Paper,"Veado L,Vale G,Fernandes E,Figueiredo E",TDTool: Threshold Derivation Tool,,2016,,,,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 20th International Conference on Evaluation and Assessment in Software Engineering,"Limerick, Ireland",2016,9781450336918.0,,https://doi.org/10.1145/2915970.2916014;http://dx.doi.org/10.1145/2915970.2916014,10.1145/2915970.2916014,"Software metrics provide basic means to quantify quality of software systems. However, the effectiveness of the measurement process is directly dependent on the definition of reliable thresholds. If thresholds are not properly defined, it is difficult to know, for instance, whether a given metric value indicates a potential problem in a class implementation. There are several methods proposed in literature to derive thresholds for software metrics. However, most of these methods (i) do not respect the skewed distribution of software metrics and (ii) do not provide a supporting tool. Aiming to fill the second gap, we propose a tool, called TDTool, to derive metric thresholds. TDTool is open source and supports four different methods for threshold derivation. This paper presents TDTool architecture and illustrates how to use it. It also presents the thresholds derived using each method based on a benchmark of 33 software product lines.","metrics, thresholds, software systems",EASE '16,,,
Conference Paper,"Oliva GA,Steinmacher I,Wiese I,Gerosa MA",What Can Commit Metadata Tell Us about Design Degradation?,,2013,,,18–27,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 2013 International Workshop on Principles of Software Evolution,"Saint Petersburg, Russia",2013,9781450323116.0,,https://doi.org/10.1145/2501543.2501547;http://dx.doi.org/10.1145/2501543.2501547,10.1145/2501543.2501547,"Design degradation has long been assessed by means of structural analyses applied on successive versions of a software system. More recently, repository mining techniques have been developed in order to uncover rich historical information of software projects. In this paper, we leverage such information and propose an approach to assess design degradation that is programming language agnostic and relies almost exclusively on commit metadata. Our approach currently focuses on the assessment of two particular design smells: rigidity and fragility. Rigidity refer to designs that are difficult to change due to ripple effects and fragility refer to designs that tend to break in different areas every time a change is performed. We conducted an evaluation of our approach in the project Apache Maven 1 and the results indicated that our approach is feasible and that the project suffered from increasing fragility.","commit metadata, mining software repositories, software metrics, version control systems, Design degradation",IWPSE 2013,,,
Conference Paper,"Seng O,Stammel J,Burkhart D",Search-Based Determination of Refactorings for Improving the Class Structure of Object-Oriented Systems,,2006,,,1909–1916,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 8th Annual Conference on Genetic and Evolutionary Computation,"Seattle, Washington, USA",2006,9781595931863.0,,https://doi.org/10.1145/1143997.1144315;http://dx.doi.org/10.1145/1143997.1144315,10.1145/1143997.1144315,"A software system's structure degrades over time, a phenomenon that is known as software decay or design drift. Since the quality of the structure has major impact on the maintainability of a system, the structure has to be reconditioned from time to time. Even if recent advances in the fields of automated detection of bad smells and refactorings have made life easier for software engineers, this is still a very complex and resource consuming task.Search-based approaches have turned out to be helpful in aiding a software engineer to improve the subsystem structure of a software system. In this paper we show that such techniques are also applicable when reconditioning the class structure of a system. We describe a novel search-based approach that assists a software engineer who has to perform this task by suggesting a list of refactorings. Our approach uses an evolutionary algorithm and simulated refactorings that do not change the system's externally visible behavior. The approach is evaluated using the open-source case study JHotDraw.","refactoring, software metrics, design heuristics, evolutionary algorithms",GECCO '06,,,
Conference Paper,"Tasharofi S,Johnson R",Patterns of Optimized Loops,,2010,,,,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 2010 Workshop on Parallel Programming Patterns,"Carefree, Arizona, USA",2010,9781450301275.0,,https://doi.org/10.1145/1953611.1953617;http://dx.doi.org/10.1145/1953611.1953617,10.1145/1953611.1953617,"Loop constructs play an important role in obtaining speed-up in parallel programming. In this paper, we explain the solutions applied for optimizing loops in parallel programs in terms of three patterns for parallel programming. These patterns can be used for parallel programming by users who have medium background on parallel programming and compiler courses. They can also be implemented in the compilers as a part of program optimization process.","parallel programming patterns, loop optimizations, program transformations",ParaPLoP '10,,,
Conference Paper,"Guo X,Peng X,Wang H,Li W,Jiang H,Ding D,Xie T,Su L",Graph-Based Trace Analysis for Microservice Architecture Understanding and Problem Diagnosis,,2020,,,1387–1397,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering,"Virtual Event, USA",2020,9781450370431.0,,https://doi.org/10.1145/3368089.3417066;http://dx.doi.org/10.1145/3368089.3417066,10.1145/3368089.3417066,"Microservice systems are highly dynamic and complex. For such systems, operation engineers and developers highly rely on trace analysis to understand architectures and diagnose various problems such as service failures and quality degradation. However, the huge number of traces produced at runtime makes it challenging to capture the required information in real-time. To address the faced challenges, in this paper, we propose a graph-based microservice trace analysis approach GMTA for understanding architecture and diagnosing various problems. Built on a graph-based representation, GMTA includes efficient processing of traces produced on the fly. It abstracts traces into different paths and further groups them into business flows. To support various analytical applications, GMTA includes an efficient storage and access mechanism by combining a graph database and a real-time analytics database and using a carefully designed storage structure. Based on GMTA, we construct analytical applications for architecture understanding and problem diagnosis, these applications support various needs such as visualizing service dependencies, making architectural decisions, analyzing the changes of services behaviors, detecting performance issues, and locating root causes. GMTA has been implemented and deployed in eBay. An experimental study based on trace data produced by eBay demonstrates GMTA's effectiveness and efficiency for architecture understanding and problem diagnosis. Case studies conducted in eBay's monitoring team and Site Reliability Engineering (SRE) team further confirm GMTA's substantial benefits in industrial-scale microservice systems.","graph, fault localization, Microservice, visualization, architecture, tracing",ESEC/FSE 2020,,,
Conference Paper,"Latib MA,Ismail SA,Yusop OM,Magalingam P,Azmi A",Analysing Log Files For Web Intrusion Investigation Using Hadoop,,2018,,,12–21,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 7th International Conference on Software and Information Engineering,"Cairo, Egypt",2018,9781450364690.0,,https://doi.org/10.1145/3220267.3220269;http://dx.doi.org/10.1145/3220267.3220269,10.1145/3220267.3220269,"The process of analyzing large amount of data from the log file helps organization to identify the web intruders' activities as well as the vulnerabilities of the website. However, analyzing them is totally a great challenge as the process is time consuming and sometimes can be inefficient. Existing or traditional log analyzers may not able to analyze such big chunk of data. Therefore, the aim of this research is to produce an analysis result for web intrusion investigation in Big Data environment. In this study, web log was analyzed based on attacks that are captured through web server log files. The web log was cleaned and refined through a log-preprocessing program before it was analyzed. An experimental simulation was conducted using Hadoop framework to produce the required analysis results. The results of this experimental simulation indicate that Hadoop application is able to produce analysis results from large size web log files in order to assist the web intrusion investigation. Besides that, the execution time performance analysis shows that the total execution time will not increase linearly with the size of the data. This study also provides solution on visualizing the analysis result using Power View and Hive.","web log file, Hadoop, Big Data, log pre-processing, web intrusion",ICSIE '18,,,
Conference Paper,"Zhong H,Su Z",Detecting API Documentation Errors,,2013,,,803–816,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 2013 ACM SIGPLAN International Conference on Object Oriented Programming Systems Languages & Applications,"Indianapolis, Indiana, USA",2013,9781450323741.0,,https://doi.org/10.1145/2509136.2509523;http://dx.doi.org/10.1145/2509136.2509523,10.1145/2509136.2509523,"When programmers encounter an unfamiliar API library, they often need to refer to its documentations, tutorials, or discussions on development forums to learn its proper usage. These API documents contain valuable information, but may also mislead programmers as they may contain errors (e.g., broken code names and obsolete code samples). Although most API documents are actively maintained and updated, studies show that many new and latent errors do exist. It is tedious and error-prone to find such errors manually as API documents can be enormous with thousands of pages. Existing tools are ineffective in locating documentation errors because traditional natural language (NL) tools do not understand code names and code samples, and traditional code analysis tools do not understand NL sentences. In this paper, we propose the first approach, DOCREF, specifically designed and developed to detect API documentation errors. We formulate a class of inconsistencies to indicate potential documentation errors, and combine NL and code analysis techniques to detect and report such inconsistencies. We have implemented DOCREF and evaluated its effectiveness on the latest documentations of five widely-used API libraries. DOCREF has detected more than 1,000 new documentation errors, which we have reported to the authors. Many of the errors have already been confirmed and fixed, after we reported them.","api documentation error, outdated documentation",OOPSLA '13,,,
Journal Article,"Zhong H,Su Z",Detecting API Documentation Errors,SIGPLAN Not.,2013,48.0,10,803–816,Association for Computing Machinery,"New York, NY, USA",,,2013-10,,0362-1340,https://doi.org/10.1145/2544173.2509523;http://dx.doi.org/10.1145/2544173.2509523,10.1145/2544173.2509523,"When programmers encounter an unfamiliar API library, they often need to refer to its documentations, tutorials, or discussions on development forums to learn its proper usage. These API documents contain valuable information, but may also mislead programmers as they may contain errors (e.g., broken code names and obsolete code samples). Although most API documents are actively maintained and updated, studies show that many new and latent errors do exist. It is tedious and error-prone to find such errors manually as API documents can be enormous with thousands of pages. Existing tools are ineffective in locating documentation errors because traditional natural language (NL) tools do not understand code names and code samples, and traditional code analysis tools do not understand NL sentences. In this paper, we propose the first approach, DOCREF, specifically designed and developed to detect API documentation errors. We formulate a class of inconsistencies to indicate potential documentation errors, and combine NL and code analysis techniques to detect and report such inconsistencies. We have implemented DOCREF and evaluated its effectiveness on the latest documentations of five widely-used API libraries. DOCREF has detected more than 1,000 new documentation errors, which we have reported to the authors. Many of the errors have already been confirmed and fixed, after we reported them.","api documentation error, outdated documentation",,,,
Conference Paper,"Sedano T,Ralph P,Péraire C",The Product Backlog,,2019,,,200–211,IEEE Press,"Montreal, Quebec, Canada",Proceedings of the 41st International Conference on Software Engineering,,2019,,,https://doi.org/10.1109/ICSE.2019.00036;http://dx.doi.org/10.1109/ICSE.2019.00036,10.1109/ICSE.2019.00036,"Context: One of the most common artifacts in contemporary software projects is a product backlog comprising user stories, bugs, chores or other work items. However, little research has investigated how the backlog is generated or the precise role it plays in a project.Objective: The purpose of this paper is to determine what is a product backlog, what is its role, and how does it emerge?Method: Following Constructivist Grounded Theory, we conducted a two-year, five-month participant-observation study of eight software development projects at Pivotal, a large, international software company. We interviewed 56 software engineers, product designers, and product managers. We conducted a survey of 27 product designers. We alternated between analysis and theoretical sampling until achieving theoretical saturation.Results: We observed 13 practices and 6 obstacles related to product backlog generation.Limitations: Grounded Theory does not support statistical generalization. While the proposed theory of product backlogs appears widely applicable, organizations with different software development cultures may use different practices.Conclusion: The product backlog is simultaneously a model of work to be done and a boundary object that helps bridge the gap between the processes of generating user stories and realizing them in working code. It emerges from sensemaking (the team making sense of the project context) and coevolution (a cognitive process where the team simultaneously refines its understanding of the problematic context and fledgling solution concepts).","user stories, product backlog, extreme programming, dual-track agile, lean, user-centered design, design thinking, feature engineering, scrum",ICSE '19,,,
Conference Paper,"Reza SM,Badreddin O,Rahad K",ModelMine: A Tool to Facilitate Mining Models from Open Source Repositories,,2020,,,,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 23rd ACM/IEEE International Conference on Model Driven Engineering Languages and Systems: Companion Proceedings,"Virtual Event, Canada",2020,9781450381352.0,,https://doi.org/10.1145/3417990.3422006;http://dx.doi.org/10.1145/3417990.3422006,10.1145/3417990.3422006,"Mining Software Repositories (MSR) has opened up new pathways and rich sources of data for research and practical purposes. This research discipline facilitates mining data from open source repositories and analyzing software defects, development activities, processes, patterns, and more. Contemporary mining tools are geared towards data extraction, analysis primarily from textual artifacts and have limitations in representation, ranking and availability. This paper presents ModelMine, a novel mining tool focuses on mining model-based artifacts and designs from open source repositories. ModelMine is designed particularly to mine software repositories, artifacts and commit history to uncover information about software designs and practices in open-source communities. ModelMine supports features that include identification and ranking of open source repositories based on the extent of presence of model-based artifacts and querying repositories to extract models and design artifacts based on customizable criteria. It supports phase-by-phase caching of intermediate results to speed up the processing to enable efficient mining of data. We compare ModelMine against a state-of-the-art tool named PyDriller in terms of performance and usability. The results show that ModelMine has the potential to become instrumental for cross-disciplinary research that combines modeling and design with repository mining and artifacts extraction. URL: https://www.smreza.com/projects/modelmine/","mining software repositories, model mining, software engineering",MODELS '20,,,
Conference Paper,"Maggioni E,Cobden R,Dmitrenko D,Obrist M",Smell-O-Message: Integration of Olfactory Notifications into a Messaging Application to Improve Users' Performance,,2018,,,45–54,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 20th ACM International Conference on Multimodal Interaction,"Boulder, CO, USA",2018,9781450356923.0,,https://doi.org/10.1145/3242969.3242975;http://dx.doi.org/10.1145/3242969.3242975,10.1145/3242969.3242975,"Smell is a powerful tool for conveying and recalling information without requiring visual attention. Previous work identified, however, some challenges caused by user's unfamiliarity with this modality and complexity in the scent delivery. We are now able to overcome these challenges, introducing a training approach to familiarise scent-meaning associations (urgency of a message, and sender identity) and using a controllable device for the scent-delivery. Here we re-validate the effectiveness of smell as notification modality and present findings on the performance of smell in conveying information. In a user study composed of two sessions, we compared the effectiveness of visual, olfactory, and combined visual-olfactory notifications in a messaging application. We demonstrated that olfactory notifications improve users' confidence and performance in identifying the urgency level of a message, with the same reaction time and disruption levels as for visual notifications. We discuss the design implications and opportunities for future work in the domain of multimodal interactions.","multimodal notifications, performance, olfactory notifications, olfactory familiarisation/training, slack, disruptiveness",ICMI '18,,,
Conference Paper,"Groner R,Tichy M,Becker S",Towards Performance Engineering of Model Transformation,,2018,,,33–36,Association for Computing Machinery,"New York, NY, USA",Companion of the 2018 ACM/SPEC International Conference on Performance Engineering,"Berlin, Germany",2018,9781450356299.0,,https://doi.org/10.1145/3185768.3186305;http://dx.doi.org/10.1145/3185768.3186305,10.1145/3185768.3186305,"Model transformations are an essential operation on models which is applied at design time and even at run time. For this, the performance of transformations is an important aspect, which needs to be considered. The current research takes only the improvement of transformation engines into account but there is no method or tool support to help engineers to identify performance bottlenecks in their transformation definition. In this paper we present our proposed approach to develop a method for performance engineering of model transformations. This method should support engineers to improve the performance of their defined transformations by providing visualizations of reasons for performance problems and offering possible refactorings for a transformation which can improve its performance.","model transformation, Henshin, model driven software engineering, performance engineering",ICPE '18,,,
Journal Article,"Bouwers E,Visser J,van Deursen A",Getting What You Measure,Commun. ACM,2012,55.0,7,54–59,Association for Computing Machinery,"New York, NY, USA",,,2012-07,,0001-0782,https://doi.org/10.1145/2209249.2209266;http://dx.doi.org/10.1145/2209249.2209266,10.1145/2209249.2209266,Four common pitfalls in using software metrics for project management.,,,,,
Conference Paper,"Košir A,Odić A,Tkalčič M",How to Improve the Statistical Power of the 10-Fold Cross Validation Scheme in Recommender Systems,,2013,,,3–6,Association for Computing Machinery,"New York, NY, USA",Proceedings of the International Workshop on Reproducibility and Replication in Recommender Systems Evaluation,"Hong Kong, China",2013,9781450324656.0,,https://doi.org/10.1145/2532508.2532510;http://dx.doi.org/10.1145/2532508.2532510,10.1145/2532508.2532510,"At this stage development of recommender systems (RS), an evaluation of competing approaches (methods) yielding similar performances in terms of experiment reproduction is of crucial importance in order to direct the further development toward the most promising direction. These comparisons are usually based on the 10-fold cross validation scheme. Since the compared performances are often similar to each other, the application of statistical significance testing is inevitable in order to not to get misled by randomly caused differences of achieved performances. For the same reason, to reproduce experiments on a different set of experimental data, the most powerful significance testing should be applied. In this work we provide guidelines on how to achieve the highest power in the comparison of RS and we demonstrate them on a comparison of RS performances when different variables are contextualized.","experimental design, paired testing, evaluation, folding, recommender systems",RepSys '13,,,
Conference Paper,"Dai T,He J,Gu X,Lu S,Wang P",DScope: Detecting Real-World Data Corruption Hang Bugs in Cloud Server Systems,,2018,,,313–325,Association for Computing Machinery,"New York, NY, USA",Proceedings of the ACM Symposium on Cloud Computing,"Carlsbad, CA, USA",2018,9781450360111.0,,https://doi.org/10.1145/3267809.3267844;http://dx.doi.org/10.1145/3267809.3267844,10.1145/3267809.3267844,"Cloud server systems such as Hadoop and Cassandra have enabled many real-world data-intensive applications running inside computing clouds. However, those systems present many data-corruption and performance problems which are notoriously difficult to debug due to the lack of diagnosis information. In this paper, we present DScope, a tool that statically detects data-corruption related software hang bugs in cloud server systems. DScope statically analyzes I/O operations and loops in a software package, and identifies loops whose exit conditions can be affected by I/O operations through returned data, returned error code, or I/O exception handling. After identifying those loops which are prone to hang problems under data corruption, DScope conducts loop bound and loop stride analysis to prune out false positives. We have implemented DScope and evaluated it using 9 common cloud server systems. Our results show that DScope can detect 42 real software hang bugs including 29 newly discovered software hang bugs. In contrast, existing bug detection tools miss detecting most of those bugs.","performance bug detection, static analysis, data corruption",SoCC '18,,,
Conference Paper,"Hosseini A,Mavroidis D,Konas P",Code Generation and Analysis for the Functional Verification of Micro Processors,,1996,,,305–310,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 33rd Annual Design Automation Conference,"Las Vegas, Nevada, USA",1996,9780897917797.0,,https://doi.org/10.1145/240518.240574;http://dx.doi.org/10.1145/240518.240574,10.1145/240518.240574,,,DAC '96,,,
Conference Paper,"Carver JC,Dieste O,Kraft NA,Lo D,Zimmermann T",How Practitioners Perceive the Relevance of ESEM Research,,2016,,,,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 10th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement,"Ciudad Real, Spain",2016,9781450344272.0,,https://doi.org/10.1145/2961111.2962597;http://dx.doi.org/10.1145/2961111.2962597,10.1145/2961111.2962597,"Background: The relevance of ESEM research to industry practitioners is key to the long-term health of the conference. Aims: The goal of this work is to understand how ESEM research is perceived within the practitioner community and provide feedback to the ESEM community ensure our research remains relevant. Method: To understand how practitioners perceive ESEM research, we replicated previous work by sending a survey to several hundred industry practitioners at a number of companies around the world. We asked the survey participants to rate the relevance of the research described in 156 ESEM papers published between 2011 and 2015. Results: We received 9,941 ratings by 437 practitioners who labeled ideas as Essential, Worth-while, Unimportant, or Unwise. The results showed that overall, industrial practitioners find the work published in ESEM to be valuable: 67% of all ratings were essential or worthwhile. We found no correlation between citation count and perceived relevance of the papers. Through a qualitative analysis, we also identified a number of research themes on which practitioners would like to see an increased research focus. Conclusions: The work published in ESEM is generally relevant to industrial practitioners. There are a number of topics for which those practitioners would like to see additional research undertaken.","ESEM Conference, Industrial Relevance, Survey",ESEM '16,,,
Conference Paper,"Ramos M,Valente MT,Terra R,Santos G",AngularJS in the Wild: A Survey with 460 Developers,,2016,,,9–16,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 7th International Workshop on Evaluation and Usability of Programming Languages and Tools,"Amsterdam, Netherlands",2016,9781450346382.0,,https://doi.org/10.1145/3001878.3001881;http://dx.doi.org/10.1145/3001878.3001881,10.1145/3001878.3001881,"To implement modern web applications, a new family of JavaScript frameworks has emerged, using the MVC pattern. Among these frameworks, the most popular one is ANGULARJS, which is supported by Google. In spite of its popularity, there is not a clear knowledge on how ANGULARJS design and features affect the development experience of Web applications. Therefore, this paper reports the results of a survey about ANGULARJS, including answers from 460 developers. Our contributions include the identification of the most appreciated features of ANGULARJS (e.g., custom interface components, dependency injection, and two-way data binding) and the most problematic aspects of the framework (e.g., performance and implementation of directives).","MVC frameworks, AngularJS, JavaScript",PLATEAU 2016,,,
Conference Paper,"Marforio C,Ritzdorf H,Francillon A,Capkun S",Analysis of the Communication between Colluding Applications on Modern Smartphones,,2012,,,51–60,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 28th Annual Computer Security Applications Conference,"Orlando, Florida, USA",2012,9781450313124.0,,https://doi.org/10.1145/2420950.2420958;http://dx.doi.org/10.1145/2420950.2420958,10.1145/2420950.2420958,"Modern smartphones that implement permission-based security mechanisms suffer from attacks by colluding applications. Users are not made aware of possible implications of application collusion attacks---quite the contrary---on existing platforms, users are implicitly led to believe that by approving the installation of each application independently, they can limit the damage that an application can cause.We implement and analyze a number of covert and overt communication channels that enable applications to collude and therefore indirectly escalate their permissions. Furthermore, we present and implement a covert channel between an installed application and a web page loaded in the system browser. We measure the throughput of all these channels as well as their bit-error rate and required synchronization for successful data transmission. The measured throughput of covert channels ranges from 3.7 bps to 3.27 kbps on a Nexus One phone and from 0.47 bps to 4.22 kbps on a Samsung Galaxy S phone; such throughputs are sufficient to efficiently exchange users' sensitive information (e.g., GPS coordinates or contacts). We test two popular research tools that track information flow or detect communication channels on mobile platforms, and confirm that even if they detect some channels, they still do not detect all the channels and therefore fail to fully prevent application collusion. Attacks using covert communication channels remain, therefore, a real threat to smartphone security and an open problem for the research community.",,ACSAC '12,,,
Conference Paper,"Bräuer J,Saft M,Plösch R,Körner C",Improving Object-Oriented Design Quality: A Portfolio- and Measurement-Based Approach,,2017,,,244–254,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 27th International Workshop on Software Measurement and 12th International Conference on Software Process and Product Measurement,"Gothenburg, Sweden",2017,9781450348539.0,,https://doi.org/10.1145/3143434.3143454;http://dx.doi.org/10.1145/3143434.3143454,10.1145/3143434.3143454,"Current software development trends have shortened release cycles and forced developers to implement short-term solutions that cannot cope with increasing product complexity. This phenomenon of introducing hasty design choices or applying bad design practices becomes something known as technical debt, in particular design debt. To pay off this debt, the literature offers approaches for identifying these design flaws; however, few methods for properly prioritizing investment efforts are available. In this paper, we propose an approach that supports the decision-making process regarding design improvements. It identifies violations of design best practices that are then arranged within a two-dimensional portfolio matrix. This matrix combines the importance of practices of design quality with actual achievement relative to a benchmark suite. To show the application of the approach in a quality-improvement process, we performed a feasibility study on three open-source projects and a benchmark suite containing 50 projects. This study clearly shows that the importance of the design best practices greatly impacts the improvement decisions and must be aligned with the strategic quality goals of the product.","design quality, technical debt, design debt, software quality",IWSM Mensura '17,,,
Conference Paper,"Bouhours C,Leblanc H,Percebois C",Sharing Bad Practices in Design to Improve the Use of Patterns,,2010,,,,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 17th Conference on Pattern Languages of Programs,"Reno, Nevada, USA",2010,9781450301077.0,,https://doi.org/10.1145/2493288.2493310;http://dx.doi.org/10.1145/2493288.2493310,10.1145/2493288.2493310,"To ensure the use of good analysis and design practices and an easier maintenance of software, analysts and designers may use patterns. To help them, we propose models inspection in order to detect instantiations of ""spoiled pattern"" and models reworking through the use of the design patterns. As a design pattern allows the instantiation of the best known solution for a given problem, a ""spoiled pattern"" allows the instantiation of alternative solutions for the same problem: requirements are respected, but architecture is improvable. We have collected a set of alternative solutions and deduced the corresponding spoiled patterns. We have defined a first catalog of these improvable practices from several experiments with students. To overcome the limits imposed by this method (restricted public, limited problems and tiresome validation process), we would like to open this problematic to the expert community. Therefore, we propose a collaborative website sharing bad practices in object oriented design to improve the use of patterns.","spoiled pattern, design patterns",PLOP '10,,,
Conference Paper,"Mazinanian D,Tsantalis N",Migrating Cascading Style Sheets to Preprocessors by Introducing Mixins,,2016,,,672–683,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 31st IEEE/ACM International Conference on Automated Software Engineering,"Singapore, Singapore",2016,9781450338455.0,,https://doi.org/10.1145/2970276.2970348;http://dx.doi.org/10.1145/2970276.2970348,10.1145/2970276.2970348,"Cascading Style Sheets (CSS) is the standard language for styling web documents and is extensively used in the industry. However, CSS lacks constructs that would allow code reuse (e.g., functions). Consequently, maintaining CSS code is often a cumbersome and error-prone task. Preprocessors (e.g., Less and Sass) have been introduced to fill this gap, by extending CSS with the missing constructs. Despite the clear maintainability benefits coming from the use of preprocessors, there is currently no support for migrating legacy CSS code to preprocessors. In this paper, we propose a technique for automatically detecting duplicated style declarations in CSS code that can be migrated to preprocessor functions (i.e., mixins). Our technique can parameterize differences in the style values of duplicated declarations, and ensure that the migration will not change the presentation semantics of the web documents. The evaluation has shown that our technique is able to detect 98% of the mixins that professional developers introduced in websites and Style Sheet libraries, and can safely migrate real CSS code.","migration, duplication, refactoring, Cascading style sheets",ASE 2016,,,
Conference Paper,"Alexandru CV,Merchante JJ,Panichella S,Proksch S,Gall HC,Robles G",On the Usage of Pythonic Idioms,,2018,,,1–11,Association for Computing Machinery,"New York, NY, USA","Proceedings of the 2018 ACM SIGPLAN International Symposium on New Ideas, New Paradigms, and Reflections on Programming and Software","Boston, MA, USA",2018,9781450360319.0,,https://doi.org/10.1145/3276954.3276960;http://dx.doi.org/10.1145/3276954.3276960,10.1145/3276954.3276960,"Developers discuss software architecture and concrete source code implementations on a regular basis, be it on question-answering sites, online chats, mailing lists or face to face. In many cases, there is more than one way of solving a programming task. Which way is best may be decided based on case-specific circumstances and constraints, but also based on convention. Having strong conventions, and a common vocabulary to express them, simplifies communication and strengthens common understanding of software development problems and their solutions. While many programming ecosystems have a common vocabulary, Python's relationship to conventions and common language is a particularly pronounced. The ""Zen of Python"", a famous set of high-level coding conventions authored by Tim Peters, states ""There should be one, and preferably only one, obvious way to do it"". This 'one way to do it' is often referred to as the 'Pythonic' way: the ideal solution to a particular problem. Few other programming languages have coined a unique term to label the quality of craftsmanship gone into a software artifact. In this paper, we explore how Python developers understand the term 'Pythonic' by means of structured interviews, build a catalogue of 'pythonic idioms' gathered from literature, and conjecture on the effects of having a language-specific term for quality code, considering the potential it could hold for other programming languages and ecosystems. We find that while the term means different things to novice versus experienced Python developers, it encompasses not only concrete implementation, but a way of thinking — a culture — in general.","Programming, Python, Community, Conventions, Culture, Idioms, Pythonic",Onward! 2018,,,
Conference Paper,Griscom M,MetaAutomation: A Pattern Language to Apply Automation to Software Quality,,2017,,,,The Hillside Group,USA,Proceedings of the 24th Conference on Pattern Languages of Programs,"Vancouver, British Columbia, Canada",2017,9781941652060.0,,,,"MetaAutomation is a pattern language for automated measurements and communication of functional software quality and performance within a team or company that is developing software. The ""whole"" of MetaAutomation addresses the quality automation problem space: automation to measure and communicate quality, bound on the technology side by operations on and measurements of software under development or maintenance for quality purposes, and bound on the business side by human customers of the quality information, and other automated processes that depend on quality, for example, operations.The focus of MetaAutomation is on answering the question for the business ""Does the system do what we need it to do?"" quickly and reliably, with highly trustworthy and structured detail that supports unprecedented visibility into and communication around the larger team - including dev, QA, and every team member concerned with quality - of what the automation drives the product to do, and how the product responds.Each of the patterns is based at least in part on existing patterns of human behavior and/or software development.MetaAutomation clarifies the value of what an intentional, designed approach to measuring and reporting software quality with automation can achieve, as opposed to common patterns of doing this which make poor use of automation's capabilities.The target audience is more than just the quality assurance role; it is anybody doing, managing, or leading quality work with automated verifications and communication of functional and performance requirements on a software development project, including software developers who would like to create software faster and with higher confidence.MetaAutomation has 9 patterns currently: Hierarchical Steps, Atomic Check, Event-Driven Check, Extension Check, Precondition Pool, Parallel Run, Smart Retry, Automated Triage, and Queryable Quality. It is open to extension with more patterns that address the quality automation problem space, and a community to make it the living pattern language that it deserves to be.","automation, test, quality automation, MetaAutomation, software quality",PLoP '17,,,
Journal Article,"Křikava F,Miller H,Vitek J",Scala Implicits Are Everywhere: A Large-Scale Study of the Use of Scala Implicits in the Wild,Proc. ACM Program. Lang.,2019,3.0,OOPSLA,,Association for Computing Machinery,"New York, NY, USA",,,2019-10,,,https://doi.org/10.1145/3360589;http://dx.doi.org/10.1145/3360589,10.1145/3360589,"The Scala programming language offers two distinctive language features implicit parameters and implicit conversions, often referred together as implicits. Announced without fanfare in 2004, implicits have quickly grown to become a widely and pervasively used feature of the language. They provide a way to reduce the boilerplate code in Scala programs. They are also used to implement certain language features without having to modify the compiler. We report on a large-scale study of the use of implicits in the wild. For this, we analyzed 7,280 Scala projects hosted on GitHub, spanning over 8.1M call sites involving implicits and 370.7K implicit declarations across 18.7M lines of Scala code.","Scala, corpora analysis, implicit conversions, Implicit parameters",,,,
Conference Paper,"Mihindukulasooriya N,Esteban-Gutiérrez M,García-Castro R",Seven Challenges for RESTful Transaction Models,,2014,,,949–952,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 23rd International Conference on World Wide Web,"Seoul, Korea",2014,9781450327459.0,,https://doi.org/10.1145/2567948.2579218;http://dx.doi.org/10.1145/2567948.2579218,10.1145/2567948.2579218,"The REpresentational State Transfer (REST) architectural style describes the design principles that made the World Wide Web scalable and the same principles can be applied in enterprise context to do loosely coupled and scalable application integration. In recent years, RESTful services are gaining traction in the industry and are commonly used as a simpler alternative to SOAP Web Services. However, one of the main drawbacks of RESTful services is the lack of standard mechanisms to support advanced quality-of-service requirements that are common to enterprises. Transaction processing is one of the essential features of enterprise information systems and several transaction models have been proposed in the past years to fulfill the gap of transaction processing in RESTful services. The goal of this paper is to analyze the state-of-the-art RESTful transaction models and identify the current challenges.","REST, challenges, transactions",WWW '14 Companion,,,
Conference Paper,"Zschaler S,Rashid A",Aspect Assumptions: A Retrospective Study of AspectJ Developers' Assumptions about Aspect Usage,,2011,,,93–104,Association for Computing Machinery,"New York, NY, USA",Proceedings of the Tenth International Conference on Aspect-Oriented Software Development,"Porto de Galinhas, Brazil",2011,9781450306058.0,,https://doi.org/10.1145/1960275.1960288;http://dx.doi.org/10.1145/1960275.1960288,10.1145/1960275.1960288,"Aspect developers constantly make a range of assumptions about the context in which their aspects will be deployed; ranging from assumptions about other aspects deployed to assumptions about semantic properties of the base and the joinpoints at which an aspect is woven. Although it has been acknowledged that such assumptions need to be made explicit to validate aspects in the face of evolution (both of aspects and the base) and reuse as well as to mitigate the fragile-pointcut problem, so far no study exists that identifies the types of assumptions aspect developers make. In this paper, we present a retrospective study of three medium-sized open-source AspectJ projects and assumptions identified in these. This leads to an initial classification of assumptions that can form the basis for further research into how best to support each type of assumption.","developer assumptions, aspectj",AOSD '11,,,
Conference Paper,Sauvage S,Agent Oriented Design Patterns: A Case Study,,2004,,,1496–1497,IEEE Computer Society,USA,Proceedings of the Third International Joint Conference on Autonomous Agents and Multiagent Systems - Volume 3,"New York, New York",2004,9781581138641.0,,,,,,AAMAS '04,,,
Conference Paper,"Borrelli A,Nardone V,Di Lucca GA,Canfora G,Di Penta M",Detecting Video Game-Specific Bad Smells in Unity Projects,,2020,,,198–208,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 17th International Conference on Mining Software Repositories,"Seoul, Republic of Korea",2020,9781450375177.0,,https://doi.org/10.1145/3379597.3387454;http://dx.doi.org/10.1145/3379597.3387454,10.1145/3379597.3387454,"The growth of the video game market, the large proportion of games targeting mobile devices or streaming services, and the increasing complexity of video games trigger the availability of video game-specific tools to assess performance and maintainability problems. This paper proposes UnityLinter, a static analysis tool that supports Unity video game developers to detect seven types of bad smells we have identified as relevant in video game development. Such smell types pertain to performance, maintainability and incorrect behavior problems. After having defined the smells by analyzing the existing literature and discussion forums, we have assessed their relevance with a survey involving 68 participants. Then, we have analyzed the occurrence of the studied smells in 100 open-source Unity projects, and also assessed UnityLinter's accuracy. Results of our empirical investigation indicate that developers well-received performance- and behavior-related issues, while some maintainability issues are more controversial. UnityLinter is, in general, accurate enough in detecting smells (86%-100% precision and 50%-100% recall), and our study shows that the studied smell types occur in 39%-97% of the analyzed projects.","Bad Smells, Video Game Development, Static Analysis, Linters",MSR '20,,,
Conference Paper,"Dennis C,Krutz DE,Mkaouer MW",P-Lint: A Permission Smell Detector for Android Applications,,2017,,,219–220,IEEE Press,"Buenos Aires, Argentina",Proceedings of the 4th International Conference on Mobile Software Engineering and Systems,,2017,9781538626696.0,,https://doi.org/10.1109/MOBILESoft.2017.24;http://dx.doi.org/10.1109/MOBILESoft.2017.24,10.1109/MOBILESoft.2017.24,"Android is built upon a permission-based structure, where apps require access to specific permissions in order to carry out specific functionalities. While Android has provided a set of best practices intended to aid the developer in properly defining and manipulating these permissions on their source code, developers do not always adhere to these guidelines. Although some of the resulting issues may be minor and lead to slight user confusion, other mistakes may create more serious privacy and security related issues. We've defined improper usage of these permission best practices to be permission smells to indicate possible permissions related syntactic issues and have created a tool P-Lint to assist in the identification of these smells on the source code. P-Lint's goal is to not only help developers create better, more secure apps by providing guidance on properly using permissions, but also in allowing researchers to better understand the common permission smells through empirical analysis on existing apps. P-Lint is publicly available on the project website: https://p-lint.github.io",,MOBILESoft '17,,,
Conference Paper,"Barišić A,Monteiro P,Amaral V,Goulão M,Monteiro M",Patterns for Evaluating Usability of Domain-Specific Languages,,2012,,,,The Hillside Group,USA,Proceedings of the 19th Conference on Pattern Languages of Programs,"Tucson, Arizona",2012,9781450327862.0,,,,"For years the development of software artifacts was the sole domain of developers and project managers. However, experience has taught us that the users play a very important role in software development and construction. The inclusion of the Domain Experts directly in the development cycle is a very important characteristic of Domain-Specific Languages, as they have often an important role in making and constraining the domain of the language.DSLs are credited with increased productivity and ease of use, but this fact is hardly ever proven. Moreover, Usability tests are frequently only performed at the final stages of the project when changes have a significant impact on the budget. To help prevent this, in this paper we present a pattern language for evaluating the usability of DSLs. These patterns can help show how to use an iterative usability validation development strategy to produce DSLs that can achieve a high degree of Usability.","usability evaluation, pattern language, domain-specific language, usability evaluation of domain-specific language",PLoP '12,,,
Conference Paper,"Izurieta C,Rice D,Kimball K,Valentien T",A Position Study to Investigate Technical Debt Associated with Security Weaknesses,,2018,,,138–142,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 2018 International Conference on Technical Debt,"Gothenburg, Sweden",2018,9781450357135.0,,https://doi.org/10.1145/3194164.3194167;http://dx.doi.org/10.1145/3194164.3194167,10.1145/3194164.3194167,"Context: Managing technical debt (TD) associated with potential security breaches found during design can lead to catching vulnerabilities (i.e., exploitable weaknesses) earlier in the software lifecycle; thus, anticipating TD principal and interest that can have decidedly negative impacts on businesses. Goal: To establish an approach to help assess TD associated with security weaknesses by leveraging the Common Weakness Enumeration (CWE) and its scoring mechanism, the Common Weakness Scoring System (CWSS). Method: We present a position study with a five-step approach employing the Quamoco quality model to operationalize the scoring of architectural CWEs. Results: We use static analysis to detect design level CWEs, calculate their CWSS scores, and provide a relative ranking of weaknesses that help practitioners identify the highest risks in an organization with a potential to impact TD. Conclusion: CWSS is a community agreed upon method that should be leveraged to help inform the ranking of security related TD items.","software quality, quality assurance, technical debt",TechDebt '18,,,
Conference Paper,"Zaparanuks D,Hauswirth M",The Beauty and the Beast: Separating Design from Algorithm,,2011,,,27–51,Springer-Verlag,"Berlin, Heidelberg",Proceedings of the 25th European Conference on Object-Oriented Programming,"Lancaster, UK",2011,9783642226540.0,,,,"We present an approach that partitions a software system into its algorithmically essential parts and the parts that manifest its design. Our approach is inspired by the notion of an algorithm and its asymptotic complexity. However, we do not propose a metric for measuring asymptotic complexity (efficiency). Instead, we use the one aspect of algorithms that drives up their asymptotic complexity - repetition, in the form of loops and recursions - to determine the algorithmically essential parts of a software system. Those parts of a system that are not algorithmically essential represent aspects of the design. A large fraction of inessential parts is indicative of ""overdesign"", where a small fraction indicates a lack of modularization. We present a metric, relative essence, to quantify the fraction of the program that is algorithmically essential. We evaluate our approach by studying the algorithmic essence of a large corpus of software system, and by comparing the measured essence to an intuitive view of design ""overhead"".",,ECOOP'11,,,
Conference Paper,"Mussbacher G,Combemale B,Abrahão S,Bencomo N,Burgueño L,Engels G,Kienzle J,Kühn T,Mosser S,Sahraoui H,Weyssow M",Towards an Assessment Grid for Intelligent Modeling Assistance,,2020,,,,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 23rd ACM/IEEE International Conference on Model Driven Engineering Languages and Systems: Companion Proceedings,"Virtual Event, Canada",2020,9781450381352.0,,https://doi.org/10.1145/3417990.3421396;http://dx.doi.org/10.1145/3417990.3421396,10.1145/3417990.3421396,"The ever-growing complexity of systems, the growing number of stakeholders, and the corresponding continuous emergence of new domain-specific modeling abstractions has led to significantly higher cognitive load on modelers. There is an urgent need to provide modelers with better, more Intelligent Modeling Assistants (IMAs). An important factor to consider is the ability to assess and compare, to learn from existing and inform future IMAs, while potentially combining them. Recently, a conceptual Reference Framework for Intelligent Modeling Assistance (RF-IMA) was proposed. RF-IMA defines the main required components and high-level properties of IMAs. In this paper, we present a detailed, level-wise definition for the properties of RF-IMA to enable a better understanding, comparison, and selection of existing and future IMAs. The proposed levels are a first step towards a comprehensive assessment grid for intelligent modeling assistance. For an initial validation of the proposed levels, we assess the existing landscape of intelligent modeling assistance and three future scenarios of intelligent modeling assistance against these levels.","model-based software engineering, assessment levels, artificial intelligence, feedback, intelligent modeling assistance, integrated development environment",MODELS '20,,,
Conference Paper,"Deka B,Huang Z,Franzen C,Nichols J,Li Y,Kumar R",ZIPT: Zero-Integration Performance Testing of Mobile App Designs,,2017,,,727–736,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 30th Annual ACM Symposium on User Interface Software and Technology,"Québec City, QC, Canada",2017,9781450349819.0,,https://doi.org/10.1145/3126594.3126647;http://dx.doi.org/10.1145/3126594.3126647,10.1145/3126594.3126647,"To evaluate the performance of mobile app designs, designers and researchers employ techniques such as A/B, usability, and analytics-driven testing. While these are all useful strategies for evaluating known designs, comparing many divergent solutions to identify the most performant remains a costly and difficult problem. This paper introduces a design performance testing approach that leverages existing app implementations and crowd workers to enable comparative testing at scale. This approach is manifest in ZIPT, a zero-integration performance testing platform that allows designers to collect detailed design and interaction data over any Android app -- including apps they do not own and did not build. Designers can deploy scripted tests via ZIPT to collect aggregate user performance metrics (e.g., completion rate, time on task) and qualitative feedback over third-party apps. Through case studies, we demonstrate that designers can use ZIPT's aggregate data and visualizations to understand the relative performance of interaction patterns found in the wild, and identify usability issues in existing Android apps.","design support tools, zero-integration performance testing, app design",UIST '17,,,
Conference Paper,"Kemmann S,Kuhn T,Trapp M",Extensible and Automated Model-Evaluations with INProVE,,2010,,,193–208,Springer-Verlag,"Berlin, Heidelberg",Proceedings of the 6th International Conference on System Analysis and Modeling: About Models,"Oslo, Norway",2010,9783642216510.0,,,,"Model-based development is gaining more and more importance for the creation of software-intensive embedded systems. One important aspect of software models is model quality. This does not imply functional correctness, but non-functional properties, such as maintainability, scalability, extensibility. Lots of effort was put into development of metrics for control flow models. In the embedded systems domain however, domain specific- and data flow languages are commonly applied for model creation. For these languages, existing metrics are not applicable. Domain and project specific quality metrics therefore are informally defined; tracking conformance to these metrics is a manual and effort consuming task. To resolve this situation, we developed INProVE. INProVE is a model-based framework that supports definition of quality metrics in an intuitive, yet formal notion. It provides automated evaluation of design models through its indicators. Applied in different industry projects to complex models, INProVE has proven its applicability for quality assessment of data flow-oriented design models not only in research, but also in practice.","model quality, automated quality evaluation, quality evolution, quality assurance, Quality modeling, simulink",SAM'10,,,
Conference Paper,"Hannebauer C,Link C,Gruhn V",Patterns for the Distribution of Power in FLOSS Projects,,2014,,,,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 19th European Conference on Pattern Languages of Programs,"Irsee, Germany",2014,9781450334167.0,,https://doi.org/10.1145/2721956.2721973;http://dx.doi.org/10.1145/2721956.2721973,10.1145/2721956.2721973,"This paper presents two patterns about the government of Free/Libre and Open Source Software (FLOSS) projects. The first pattern, Single Maintainer, describes the situation where all power in the FLOSS project stems from one individual. The other pattern, Meritocracy, shows how to distribute power based on the project participants' merit for the project.","management, benevolent dictator, governing FLOSS projects, patterns, single maintainer, meritocracy, FLOSS, open source",EuroPLoP '14,,,
Conference Paper,"Gómez VU,Kellens A,Gybels K,D'Hondt T",Experiments with Pro-Active Declarative Meta-Programming,,2009,,,68–76,Association for Computing Machinery,"New York, NY, USA",Proceedings of the International Workshop on Smalltalk Technologies,"Brest, France",2009,9781605588995.0,,https://doi.org/10.1145/1735935.1735947;http://dx.doi.org/10.1145/1735935.1735947,10.1145/1735935.1735947,"Program querying has become a valuable asset in the programmer's toolbox. Using dedicated querying languages, developers can reason about their source code in order to find errors, refactoring opportunities and so on. Within Smalltalk, the SOUL language has been proposed as one such language that offers a declarative and expressive means to query the source code of object-oriented programs.Ever since its inception, SOUL has been used as the underlying technique for a number of academic software engineering tools. Despite its success, one of the problems of SOUL is that, due to its backward chained implementation, it is less suited as a basis for such pro-active software tools. Using SOUL, a developer has to launch the queries over the system manually, rather than automatically receiving feedback whenever the underlying source code is changed. In this paper we present PARACHUT, an alternative logic query language that is based on forward chaining and temporal logic and that allows developers to express queries over the change history of the system. Furthermore, PARACHUT's data-driven nature makes it possible to provide instant feedback to developers when the source code is changed, thus providing better support for pro-active software tools.","source-code history, program querying, temporal logic",IWST '09,,,
Conference Paper,"Lin Z,Whitehead J",Why Power Laws? An Explanation from Fine-Grained Code Changes,,2015,,,68–75,IEEE Press,"Florence, Italy",Proceedings of the 12th Working Conference on Mining Software Repositories,,2015,9780769555942.0,,,,"Throughout the years, empirical studies have found power law distributions in various measures across many software systems. However, surprisingly little is known about how they are produced. What causes these power law distributions? We offer an explanation from the perspective of fine-grained code changes. A model based on preferential attachment and self-organized criticality is proposed to simulate software evolution. The experiment shows that the simulation is able to render power law distributions out of fine-grained code changes, suggesting preferential attachment and self-organized criticality are the underlying mechanism causing the power law distributions in software systems.",,MSR '15,,,
Conference Paper,"Chan-Jong-Chu K,Islam T,Exposito MM,Sheombar S,Valladares C,Philippot O,Grua EM,Malavolta I",Investigating the Correlation between Performance Scores and Energy Consumption of Mobile Web Apps,,2020,,,190–199,Association for Computing Machinery,"New York, NY, USA",Proceedings of the Evaluation and Assessment in Software Engineering,"Trondheim, Norway",2020,9781450377317.0,,https://doi.org/10.1145/3383219.3383239;http://dx.doi.org/10.1145/3383219.3383239,10.1145/3383219.3383239,"Context. Developers have access to tools like Google Lighthouse to assess the performance of web apps and to guide the adoption of development best practices. However, when it comes to energy consumption of mobile web apps, these tools seem to be lacking. Goal. This study investigates on the correlation between the performance scores produced by Lighthouse and the energy consumption of mobile web apps.Method. We design and conduct an empirical experiment where 21 real mobile web apps are (i) analyzed via the Lighthouse performance analysis tool and (ii) measured on an Android device running a software-based energy profiler. Then, we statistically assess how energy consumption correlates with the obtained performance scores and carry out an effect size estimation.Results. We discover a statistically significant negative correlation between performance scores and the energy consumption of mobile web apps (with medium to large effect sizes), implying that an increase of the performance score tend to lead to a decrease of energy consumption.Conclusions. We recommend developers to strive to improve the performance level of their mobile web apps, as this can also have a positive impact on their energy consumption on Android devices.","requirements engineering, requirements elicitation, Mobile app development",EASE '20,,,
Conference Paper,Rost W,Mining of DSLs and Generator Templates from Reference Applications,,2020,,,,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 23rd ACM/IEEE International Conference on Model Driven Engineering Languages and Systems: Companion Proceedings,"Virtual Event, Canada",2020,9781450381352.0,,https://doi.org/10.1145/3417990.3419492;http://dx.doi.org/10.1145/3417990.3419492,10.1145/3417990.3419492,"Domain-Specific Languages (DSLs) found application in different domains. The development of Model-Driven Development (MDD) components is facilitated by a wealth of frameworks like EMF, Xtext, and Xtend. However, the development of the necessary IDE components still can take up to several weeks or even months until it can be used in a production environment. The first step during the development of such an MDD infrastructure is to analyse a set of reference applications to deduce the DSL used by the domain experts and the templates used in the generator. The analysis requires technical expertise and is usually performed by MDD infrastructure developers, who have to adhere to a close communication with domain experts and are exposed to high cognitive load and time-consuming tasks.The objective of this PhD project is to reduce the initial effort during the creation of new MDD infrastructure facilities for either a new domain or newly discovered platforms within a known domain. This should be made possible by the (semi-)automatic analysis of multiple codebases using Code Clone Detection (CCD) tools in a defined process flow. Code clones represent schematically redundant and generic code fragments which were found in the provided codebase. In the process, the key steps include (i) choosing appropriate reference applications (ii) distinguishing the codebase by clustering the files, (iii) reviewing the quality of the clusters, (iv) analysing the cluster by tailored CCD, and (v) transforming of the code clones, depending on the code clone type, to extract a DSL and the corresponding generator templates.","model-driven software engineering, code clone detection, MDD component creation, information extraction, clustering and classification",MODELS '20,,,
Conference Paper,"Zampetti F,Serebrenik A,Di Penta M",Was Self-Admitted Technical Debt Removal a Real Removal? An in-Depth Perspective,,2018,,,526–536,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 15th International Conference on Mining Software Repositories,"Gothenburg, Sweden",2018,9781450357166.0,,https://doi.org/10.1145/3196398.3196423;http://dx.doi.org/10.1145/3196398.3196423,10.1145/3196398.3196423,"Technical Debt (TD) has been defined as ""code being not quite right yet"", and its presence is often self-admitted by developers through comments. The purpose of such comments is to keep track of TD and appropriately address it when possible. Building on a previous quantitative investigation by Maldonado et al. on the removal of self-admitted technical debt (SATD), in this paper we perform an in-depth quantitative and qualitative study of how SATD is addressed in five Java open source projects. On the one hand, we look at whether SATD is ""accidentally"" removed, and the extent to which the SATD removal is being documented. We found that that (i) between 20% and 50% of SATD comments are accidentally removed while entire classes or methods are dropped, (ii) 8% of the SATD removal is acknowledged in commit messages, and (iii) while most of the changes addressing SATD require complex source code changes, very often SATD is addressed by specific changes to method calls or conditionals. Our results can be used to better plan TD management or learn patterns for addressing certain kinds of TD and provide recommendations to developers.",,MSR '18,,,
Conference Paper,"Nelson MJ,Mateas M",A Requirements Analysis for Videogame Design Support Tools,,2009,,,137–144,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 4th International Conference on Foundations of Digital Games,"Orlando, Florida",2009,9781605584379.0,,https://doi.org/10.1145/1536513.1536543;http://dx.doi.org/10.1145/1536513.1536543,10.1145/1536513.1536543,"Designing videogames involves weaving together systems of rules, called game mechanics, which support and structure compelling player experiences. Thus a significant portion of game design involves reasoning about the effects of different potential game mechanics on player experience. Unlike some design fields, such as architecture and mechanical design, that have CAD tools to support designers in reasoning about and visualizing designs, game designers have no tools for reasoning about and visualizing systems of game mechanics. In this paper we perform a requirements analysis for design-support tool for game design. We develop a proposal in two phases. First, we review the design-support-system and game-design literatures to arrive at a plausible system that helps designers reason about game mechanics and gameplay. We then refine these requirements in a study of three teams of game designers, investigating their current design problems and gauging interest in our tool proposals and reactions to prototype tools. Our study finds that a game design assistant that is able to formally reason about abstract game mechanics would provide significant leverage to designers during multiple stages of the design process.","videogames, game mechanics, authoring tools",FDG '09,,,
Conference Paper,"Orrú M,Porru S,Marchesi M,Tonelli R",The Evolution of Knowledge in the Refactoring Research Field,,2015,,,,Association for Computing Machinery,"New York, NY, USA",Scientific Workshop Proceedings of the XP2015,"Helsinki, Finland",2015,9781450334099.0,,https://doi.org/10.1145/2764979.2764989;http://dx.doi.org/10.1145/2764979.2764989,10.1145/2764979.2764989,"Refactoring is certainly one of the most widespread practices used by developers to improve software quality. During the last two decades, it has been the subject of an increasing number of research studies. But how much do we know about the trends, and the appearance of new topics, in the research field of refactoring? What have researchers found about its application since the time of its first introduction? In this work we provide a preliminary analysis of the state of the art and the evolution of the research on refactoring. We attempt to represent the actual body of knowledge in this field through the analysis of its cognitive structure, leveraging science mapping methodology to focus on the most relevant concepts in this research area. We model the body of knowledge by mining bibliographic databases and by retrieving the co-occurrence of keywords. We have found that some different general themes can be recognized, but not all of them have the same role and equally catalyzed researchers' interest. In addition, we provide a preliminary analysis on the trends and directions as well as the hot topics we identified in the refactoring research field.","bibliographic network, science mapping, refactoring",XP '15 workshops,,,
Conference Paper,"Mileva YM,Wasylkowski A,Zeller A",Mining Evolution of Object Usage,,2011,,,105–129,Springer-Verlag,"Berlin, Heidelberg",Proceedings of the 25th European Conference on Object-Oriented Programming,"Lancaster, UK",2011,9783642226540.0,,,,"As software evolves, so does the interaction between its components. But how can we check if components are updated consistently? By abstracting object usage into temporal properties, we can learn evolution patterns that express how object usage evolves over time. Software can then be checked against these patterns, revealing code that is in need of update: ""Your check for isValidWidget() is now superseded by checkWidget()."" In an evaluation of seven different versions of three open source projects, our LAMARCK tool was able to detect existing code issues with a precision of 33%-64% and to prevent such issues with a precision of 90%-100%.",,ECOOP'11,,,
Conference Paper,"Arendt T,Taentzer G",Integration of Smells and Refactorings within the Eclipse Modeling Framework,,2012,,,8–15,Association for Computing Machinery,"New York, NY, USA",Proceedings of the Fifth Workshop on Refactoring Tools,"Rapperswil, Switzerland",2012,9781450315005.0,,https://doi.org/10.1145/2328876.2328878;http://dx.doi.org/10.1145/2328876.2328878,10.1145/2328876.2328878,"Models are primary artifacts in model-based, and especially, in model-driven software development processes. Therefore, software quality and quality assurance frequently leads back to the quality and quality assurance of the involved models. In our approach, we propose a model quality assurance process that is based on static model analysis and uses techniques like model metrics and model smells. Based on the outcome of the model analysis, appropriate model refactoring steps are performed. Appropriate tools support the included techniques, i.e. metrics, smells, and refactorings, for models that are based on the Eclipse Modeling Framework (EMF). In this paper, we present the integration of the two model quality tools EMF Smell and EMF Refactor. This integration provides modelers with a quick and easy way to erase model smells by automatically suggesting appropriate model refactorings, and to get warnings in cases where new model smells come in by applying a certain refactoring.","Eclipse modeling framework, model refactoring, model smell, model quality",WRT '12,,,
Conference Paper,"Balaji B,Koh J,Weibel N,Agarwal Y",Genie: A Longitudinal Study Comparing Physical and Software Thermostats in Office Buildings,,2016,,,1200–1211,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 2016 ACM International Joint Conference on Pervasive and Ubiquitous Computing,"Heidelberg, Germany",2016,9781450344616.0,,https://doi.org/10.1145/2971648.2971719;http://dx.doi.org/10.1145/2971648.2971719,10.1145/2971648.2971719,"Thermostats are the primary interface for occupants of office buildings to express their thermal comfort preferences. However, traditional thermostats are often ineffective due to physical inaccessibility, lack of information or limited responsiveness, which lead to occupant discomfort. Modern thermostat designs do overcome some of these limitations, but retrofitting them to existing buildings is prohibitively expensive. Software thermostats based on web or smartphone apps provide an alternate interaction mechanism with minimal deployment cost. However, their usage and effectiveness have not been studied extensively in real settings. We present Genie, a novel software thermostat that we designed and deployed in our university for over 21 months. We compare the use of Genie to traditional thermostats. Our data and user study show that due to the clarity of information and wider thermal control provided by Genie, users feel more comfortable in their offices. Furthermore, the improved comfort did not affect the overall energy consumption or lead to misuse of HVAC controls.","thermal comfort, software thermostat, smart buildings, thermostat design, hvac energy efficiency",UbiComp '16,,,
Journal Article,Smaalders B,Performance Anti-Patterns: Want Your Apps to Run Faster? Here’s What Not to Do,Queue,2006,4.0,1,44–50,Association for Computing Machinery,"New York, NY, USA",,,2006-02,,1542-7730,https://doi.org/10.1145/1117389.1117403;http://dx.doi.org/10.1145/1117389.1117403,10.1145/1117389.1117403,"Performance pathologies can be found in almost any software, from user to kernel, applications, drivers, etc. At Sun we’ve spent the last several years applying state-of-the-art tools to a Unix kernel, system libraries, and user applications, and have found that many apparently disparate performance problems in fact have the same underlying causes. Since software patterns are considered abstractions of positive experience, we can talk about the various approaches that led to these performance problems as anti-patterns: something to be avoided rather than emulated.",,,,,
Journal Article,"Bagherzadeh M,Fireman N,Shawesh A,Khatchadourian R","Actor Concurrency Bugs: A Comprehensive Study on Symptoms, Root Causes, API Usages, and Differences",Proc. ACM Program. Lang.,2020,4.0,OOPSLA,,Association for Computing Machinery,"New York, NY, USA",,,2020-11,,,https://doi.org/10.1145/3428282;http://dx.doi.org/10.1145/3428282,10.1145/3428282,"Actor concurrency is becoming increasingly important in the development of real-world software systems. Although actor concurrency may be less susceptible to some multithreaded concurrency bugs, such as low-level data races and deadlocks, it comes with its own bugs that may be different. However, the fundamental characteristics of actor concurrency bugs, including their symptoms, root causes, API usages, examples, and differences when they come from different sources are still largely unknown. Actor software development can significantly benefit from a comprehensive qualitative and quantitative understanding of these characteristics, which is the focus of this work, to foster better API documentation, development practices, testing, debugging, repairing, and verification frameworks. To conduct this study, we take the following major steps. First, we construct a set of 186 real-world Akka actor bugs from Stack Overflow and GitHub via manual analysis of 3,924 Stack Overflow questions, answers, and comments and 3,315 GitHub commits, messages, original and modified code snippets, issues, and pull requests. Second, we manually study these actor bugs and their fixes to understand and classify their symptoms, root causes, and API usages. Third, we study the differences between the commonalities and distributions of symptoms, root causes, and API usages of our Stack Overflow and GitHub actor bugs. Fourth, we discuss real-world examples of our actor bugs with these symptoms and root causes. Finally, we investigate the relation of our findings with those of previous work and discuss their implications. A few findings of our study are: (1) symptoms of our actor bugs can be classified into five categories, with Error as the most common symptom and Incorrect Exceptions as the least common, (2) root causes of our actor bugs can be classified into ten categories, with Logic as the most common root cause and Untyped Communication as the least common, (3) a small number of Akka API packages are responsible for most of API usages by our actor bugs, and (4) our Stack Overflow and GitHub actor bugs can differ significantly in commonalities and distributions of their symptoms, root causes, and API usages. While some of our findings agree with those of previous work, others sharply contrast.","GitHub, Actor bug symptoms, Akka actor bugs, Actor bug differences, Actor bug root causes, Stack Overflow, Actor bug API usages",,,,
Conference Paper,"Ouyang J,Liu Y",A Novel Type-Based API Search Engine for Open Source Elm Packages,,2020,,,294–298,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 2019 3rd International Conference on Computer Science and Artificial Intelligence,"Normal, IL, USA",2020,9781450376273.0,,https://doi.org/10.1145/3374587.3374633;http://dx.doi.org/10.1145/3374587.3374633,10.1145/3374587.3374633,"Searching for API is a hard problem, as text is not commonly representative of what a program does. In this paper, we present Moogle, a type-based API search engine for open-source Elm packages, with an online demonstration1. In Moogle, queries are based on names or type signatures, whose adequate information is leveraged for matching APIs from open-source libraries. Moogle applies unification algorithm, where generic type signatures can be matched to concrete ones and vice versa. In order to optimize the performance of applied matches, Moogle stores information of type signatures in an AST-based graph model, by applying graph DBMS, Neo4j. Moogle also has its implementation on a parser to convert its DSL, MoogleQL, into AST data models or string queries. According to our test, Moogle outperforms its congeneric work, Elm-search2, on many aspects including search range and allowed patterns with acceptable trade-offs.","API search, graph database, code reuse",CSAI2019,,,
Conference Paper,"Hicks M,Sturton C,King ST,Smith JM",SPECS: A Lightweight Runtime Mechanism for Protecting Software from Security-Critical Processor Bugs,,2015,,,517–529,Association for Computing Machinery,"New York, NY, USA",Proceedings of the Twentieth International Conference on Architectural Support for Programming Languages and Operating Systems,"Istanbul, Turkey",2015,9781450328357.0,,https://doi.org/10.1145/2694344.2694366;http://dx.doi.org/10.1145/2694344.2694366,10.1145/2694344.2694366,"Processor implementation errata remain a problem, and worse, a subset of these bugs are security-critical. We classified 7 years of errata from recent commercial processors to understand the magnitude and severity of this problem, and found that of 301 errata analyzed, 28 are security-critical. We propose the SECURITY-CRITICAL PROCESSOR ER- RATA CATCHING SYSTEM (SPECS) as a low-overhead solution to this problem. SPECS employs a dynamic verification strategy that is made lightweight by limiting protection to only security-critical processor state. As a proof-of- concept, we implement a hardware prototype of SPECS in an open source processor. Using this prototype, we evaluate SPECS against a set of 14 bugs inspired by the types of security-critical errata we discovered in the classification phase. The evaluation shows that SPECS is 86% effective as a defense when deployed using only ISA-level state; incurs less than 5% area and power overhead; and has no software run-time overhead.","hardware security exploits, security-critical processor errata, processor errata",ASPLOS '15,,,
Journal Article,"Hicks M,Sturton C,King ST,Smith JM",SPECS: A Lightweight Runtime Mechanism for Protecting Software from Security-Critical Processor Bugs,SIGPLAN Not.,2015,50.0,4,517–529,Association for Computing Machinery,"New York, NY, USA",,,2015-03,,0362-1340,https://doi.org/10.1145/2775054.2694366;http://dx.doi.org/10.1145/2775054.2694366,10.1145/2775054.2694366,"Processor implementation errata remain a problem, and worse, a subset of these bugs are security-critical. We classified 7 years of errata from recent commercial processors to understand the magnitude and severity of this problem, and found that of 301 errata analyzed, 28 are security-critical. We propose the SECURITY-CRITICAL PROCESSOR ER- RATA CATCHING SYSTEM (SPECS) as a low-overhead solution to this problem. SPECS employs a dynamic verification strategy that is made lightweight by limiting protection to only security-critical processor state. As a proof-of- concept, we implement a hardware prototype of SPECS in an open source processor. Using this prototype, we evaluate SPECS against a set of 14 bugs inspired by the types of security-critical errata we discovered in the classification phase. The evaluation shows that SPECS is 86% effective as a defense when deployed using only ISA-level state; incurs less than 5% area and power overhead; and has no software run-time overhead.","processor errata, security-critical processor errata, hardware security exploits",,,,
Journal Article,"Hicks M,Sturton C,King ST,Smith JM",SPECS: A Lightweight Runtime Mechanism for Protecting Software from Security-Critical Processor Bugs,SIGARCH Comput. Archit. News,2015,43.0,1,517–529,Association for Computing Machinery,"New York, NY, USA",,,2015-03,,0163-5964,https://doi.org/10.1145/2786763.2694366;http://dx.doi.org/10.1145/2786763.2694366,10.1145/2786763.2694366,"Processor implementation errata remain a problem, and worse, a subset of these bugs are security-critical. We classified 7 years of errata from recent commercial processors to understand the magnitude and severity of this problem, and found that of 301 errata analyzed, 28 are security-critical. We propose the SECURITY-CRITICAL PROCESSOR ER- RATA CATCHING SYSTEM (SPECS) as a low-overhead solution to this problem. SPECS employs a dynamic verification strategy that is made lightweight by limiting protection to only security-critical processor state. As a proof-of- concept, we implement a hardware prototype of SPECS in an open source processor. Using this prototype, we evaluate SPECS against a set of 14 bugs inspired by the types of security-critical errata we discovered in the classification phase. The evaluation shows that SPECS is 86% effective as a defense when deployed using only ISA-level state; incurs less than 5% area and power overhead; and has no software run-time overhead.","hardware security exploits, security-critical processor errata, processor errata",,,,
Conference Paper,"Aritajati C,Narayanan NH",Facilitating Students' Collaboration and Learning in a Question and Answer System,,2013,,,101–106,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 2013 Conference on Computer Supported Cooperative Work Companion,"San Antonio, Texas, USA",2013,9781450313322.0,,https://doi.org/10.1145/2441955.2441983;http://dx.doi.org/10.1145/2441955.2441983,10.1145/2441955.2441983,"Green Dolphin (GD) is a question and answer system for students learning programming, with a social web interface. It crowd-sources the task of answering technical questions to the peers of students who ask questions. GD has several original features that make it different from existing systems. It automatically identifies students who are knowledgeable based on their activity, and tags them as experts to whom other students can ask questions. GD provides students with automatic feedback of the quality of code they submit. Thus, students get fast and high quality answers from their peers and the system, freeing up time for teachers. After a student posts a question in GD, it delays making visible answers from instructors and teaching assistants so that other students are encouraged to participate, and have time to answer the question. We believe that this can significantly increase student participation, collaboration and sense of ownership. Students gain new knowledge from the flow of questions and answers in the system. They develop communication skills by asking and answering questions as well as programming and debugging skills.","crowd-sourcing, question and answer board, expert identification, education",CSCW '13,,,
Journal Article,"Garbervetsky D,Kim S",Report from 2nd International Workshop on Developing Tools as Plug-Ins (TOPI 2012),SIGSOFT Softw. Eng. Notes,2012,37.0,6,24–27,Association for Computing Machinery,"New York, NY, USA",,,2012-11,,0163-5948,https://doi.org/10.1145/2382756.2382775;http://dx.doi.org/10.1145/2382756.2382775,10.1145/2382756.2382775,"The International Workshop on Developing Tools as Plug-Ins (TOPI) is a venue for researchers and practitioners interested in plug-in development. The main interest is understanding the opportunities and challenges of developing tools as plug-ins, and thus, we seek for discussions regarding the characteristics of good plug-ins, interoperability requirements to making tools available across platforms, recent successful tools as plug-ins as well as foreseen medium and long term challenges of tools as plug-ins. The second edition of this workshop, TOPI 2012 was co-located with the International Conference on Software Engineering (ICSE 2012). TOPI 2012 received a total of 32 submissions. Among them, 14 were accepted as full papers and 4 as short papers. The audience during the whole workshop ranged from 25 to 30 participants. The final program comprised position papers including new proposals for plug-in architectures as well as their interaction with development environments and run-times, and papers discussing the implementation of di??erent kind of tools as plug-ins. This report describes the main results of TOPI 2012.",,,,,
Conference Paper,"Henriques H,Lourenço H,Amaral V,Goulão M",Improving the Developer Experience with a Low-Code Process Modelling Language,,2018,,,200–210,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 21th ACM/IEEE International Conference on Model Driven Engineering Languages and Systems,"Copenhagen, Denmark",2018,9781450349499.0,,https://doi.org/10.1145/3239372.3239387;http://dx.doi.org/10.1145/3239372.3239387,10.1145/3239372.3239387,"Context: The OutSystems Platform is a development environment composed of several DSLs, used to specify, quickly build and validate web and mobile applications. The DSLs allow users to model different perspectives such as interfaces and data models, define custom business logic and construct process models. Problem: The DSL for process modelling (Business Process Technology (BPT)), has a low adoption rate and is perceived as having usability problems hampering its adoption. This is problematic given the language maintenance costs. Method: We used a combination of interviews, a critical review of BPT using the ""Physics of Notation"" and empirical evaluations of BPT using the System Usability Scale (SUS) and the NASA Task Load indeX (TLX), to develop a new version of BPT, taking these inputs and Outsystems' engineers culture into account. Results: Evaluations conducted with 25 professional software engineers showed an increase of the semantic transparency on the new version, from 31% to 69%, an increase in the correctness of responses, from 51% to 89%, an increase in the SUS score, from 42.25 to 64.78, and a decrease of the TLX score, from 36.50 to 20.78. These differences were statistically significant. Conclusions: These results suggest the new version of BPT significantly improved the developer experience of the previous version. The end users background with OutSystems had a relevant impact on the final concrete syntax choices and achieved usability indicators.","Low-Code Languages, Developer Experience",MODELS '18,,,
Conference Paper,"Niklas K,Greenyer J,Schneider K",Towards Application and Evolution of Model-Based Heuristics for Improving SOA Service Design,,2015,,,60–65,IEEE Press,"Florence, Italy",Proceedings of the Seventh International Workshop on Modeling in Software Engineering,,2015,,,,,"Good service design is key to acceptance and success for a service-oriented architecture (SOA) in an enterprise. Enterprises try to achieve good service design by using guidelines which combine experts' experience, company policies and best practices. Applying, evolving and maintaining guidelines overburdens service designers and reviewers due to the amount and volume. This results in inefficient, costly and frustrating processes. Without an automated support, guidelines provide only limited value to the design process. We describe how our design environment prototype addresses these problems and introduce automatic guideline checks using heuristics on service models. Our evaluation confirms applicability and advantages of our tool. We present a selection of heuristics which are used in our tool. As the second contribution we describe our plan of how to support evolution and maintenance of guidelines and heuristics.","experience, modeling, heuristics, static analysis, service oriented architecture (SOA)",MiSE '15,,,
Conference Paper,"Kesper A,Wenz V,Taentzer G",Detecting Quality Problems in Research Data: A Model-Driven Approach,,2020,,,354–364,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 23rd ACM/IEEE International Conference on Model Driven Engineering Languages and Systems,"Virtual Event, Canada",2020,9781450370196.0,,https://doi.org/10.1145/3365438.3410987;http://dx.doi.org/10.1145/3365438.3410987,10.1145/3365438.3410987,"As scientific progress highly depends on the quality of research data, there are strict requirements for data quality coming from the scientific community. A major challenge in data quality assurance is to localise quality problems that are inherent to data. Due to the dynamic digitalisation in specific scientific fields, especially the humanities, different database technologies and data formats may be used in rather short terms to gain experiences. We present a model-driven approach to analyse the quality of research data. It allows abstracting from the underlying database technology. Based on the observation that many quality problems show anti-patterns, a data engineer formulates analysis patterns that are generic concerning the database format and technology. A domain expert chooses a pattern that has been adapted to a specific database technology and concretises it for a domain-specific database format. The resulting concrete patterns are used by data analysts to locate quality problems in their databases. As proof of concept, we implemented tool support that realises this approach for XML databases. We evaluated our approach concerning expressiveness and performance in the domain of cultural heritage based on a qualitative study on quality problems occurring in cultural heritage data.","data quality, model-driven development, pattern matching",MODELS '20,,,
Conference Paper,"Kula E,Rastogi A,Huijgens H,van Deursen A,Gousios G",Releasing Fast and Slow: An Exploratory Case Study at ING,,2019,,,785–795,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering,"Tallinn, Estonia",2019,9781450355728.0,,https://doi.org/10.1145/3338906.3338978;http://dx.doi.org/10.1145/3338906.3338978,10.1145/3338906.3338978,"The appeal of delivering new features faster has led many software projects to adopt rapid releases. However, it is not well understood what the effects of this practice are. This paper presents an exploratory case study of rapid releases at ING, a large banking company that develops software solutions in-house, to characterize rapid releases. Since 2011, ING has shifted to a rapid release model. This switch has resulted in a mixed environment of 611 teams releasing relatively fast and slow. We followed a mixed-methods approach in which we conducted a survey with 461 participants and corroborated their perceptions with 2 years of code quality data and 1 year of release delay data. Our research shows that: rapid releases are more commonly delayed than their non-rapid counterparts, however, rapid releases have shorter delays; rapid releases can be beneficial in terms of reviewing and user-perceived quality; rapidly released software tends to have a higher code churn, a higher test coverage and a lower average complexity; challenges in rapid releases are related to managing dependencies and certain code aspects, e.g., design debt.","rapid release, software quality, technical debt, release delay",ESEC/FSE 2019,,,
Conference Paper,Christensen HB,Implications of Perspective in Teaching Objects First and Object Design,,2005,,,94–98,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 10th Annual SIGCSE Conference on Innovation and Technology in Computer Science Education,"Caparica, Portugal",2005,9781595930248.0,,https://doi.org/10.1145/1067445.1067474;http://dx.doi.org/10.1145/1067445.1067474,10.1145/1067445.1067474,"There are an increasing number of books published on the important topics of ""object-oriented programming"" and ""object-oriented design"" for use in education. However, object-orientation can be viewed from a number of different perspectives---each perspective having its benefits and liabilities. A perspective has a strong influence on the kind of designs students can and will produce, the kind of domains that are easy or difficult to analyze, and the kind of frame of reference in which design techniques are understood and applied. In this paper we argue that most books make an implicit choice of perspective with the unfortunate effect that our students leave our courses with limited design abilities. We present a coarse-grained classification, discuss implications of perspective in a teaching context, and illustrate consequences using a small case study. Our main point is that teachers should be aware of the different perspectives, and that all perspectives are important for students to achieve high quality designs.","responsibility-driven design, objects, object-oriented design, design patterns, role",ITiCSE '05,,,
Journal Article,Christensen HB,Implications of Perspective in Teaching Objects First and Object Design,SIGCSE Bull.,2005,37.0,3,94–98,Association for Computing Machinery,"New York, NY, USA",,,2005-06,,0097-8418,https://doi.org/10.1145/1151954.1067474;http://dx.doi.org/10.1145/1151954.1067474,10.1145/1151954.1067474,"There are an increasing number of books published on the important topics of ""object-oriented programming"" and ""object-oriented design"" for use in education. However, object-orientation can be viewed from a number of different perspectives---each perspective having its benefits and liabilities. A perspective has a strong influence on the kind of designs students can and will produce, the kind of domains that are easy or difficult to analyze, and the kind of frame of reference in which design techniques are understood and applied. In this paper we argue that most books make an implicit choice of perspective with the unfortunate effect that our students leave our courses with limited design abilities. We present a coarse-grained classification, discuss implications of perspective in a teaching context, and illustrate consequences using a small case study. Our main point is that teachers should be aware of the different perspectives, and that all perspectives are important for students to achieve high quality designs.","objects, object-oriented design, role, responsibility-driven design, design patterns",,,,
Conference Paper,"Wood D,Apthorpe N,Feamster N",Cleartext Data Transmissions in Consumer IoT Medical Devices,,2017,,,7–12,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 2017 Workshop on Internet of Things Security and Privacy,"Dallas, Texas, USA",2017,9781450353960.0,,https://doi.org/10.1145/3139937.3139939;http://dx.doi.org/10.1145/3139937.3139939,10.1145/3139937.3139939,"This paper introduces a method to capture network traffic from medical IoT devices and automatically detect cleartext information that may reveal sensitive medical conditions and behaviors. The research follows a three-step approach involving traffic collection, cleartext detection, and metadata analysis. We analyze four popular consumer medical IoT devices, including one smart medical device that leaks sensitive health information in cleartext. We also present a traffic capture and analysis system that seamlessly integrates with a home network and offers a user-friendly interface for consumers to monitor and visualize data transmissions of IoT devices in their homes.","medical devices, personal health information, internet of things, privacy",IoTS&P '17,,,
Journal Article,"Strüber D,Rubin J,Arendt T,Chechik M,Taentzer G,Plöger J",Variability-Based Model Transformation: Formal Foundation and Application,Form. Asp. Comput.,2018,30.0,1,133–162,Springer-Verlag,"Berlin, Heidelberg",,,2018-01,,0934-5043,https://doi.org/10.1007/s00165-017-0441-3;http://dx.doi.org/10.1007/s00165-017-0441-3,10.1007/s00165-017-0441-3,"Model transformation systems often contain transformation rules that are substantially similar to each other, causing maintenance issues and performance bottlenecks. To address these issues, we introduce variability-based model transformation. The key idea is to encode a set of similar rules into a compact representation, called variability-based rule. We provide an algorithm for applying such rules in an efficient manner. In addition, we introduce rule merging, a three-component mechanism for enabling the automatic creation of variability-based rules. Our rule application and merging mechanisms are supported by a novel formal framework, using category theory to provide precise definitions and to prove correctness. In two realistic application scenarios, the created variability-based rules enabled considerable speedups, while also allowing the overall specifications to become more compact.","Graph transformation, Category theory, Model transformation, Variability",,,,
Journal Article,Bland M,Finding More than One Worm in the Apple,Commun. ACM,2014,57.0,7,58–64,Association for Computing Machinery,"New York, NY, USA",,,2014-07,,0001-0782,https://doi.org/10.1145/2622630;http://dx.doi.org/10.1145/2622630,10.1145/2622630,"If you see something, say something.",,,,,
Conference Paper,"Couto CM,Rocha H,Terra R",A Quality-Oriented Approach to Recommend Move Method Refactorings,,2018,,,11–20,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 17th Brazilian Symposium on Software Quality,"Curitiba, Brazil",2018,9781450365659.0,,https://doi.org/10.1145/3275245.3275247;http://dx.doi.org/10.1145/3275245.3275247,10.1145/3275245.3275247,"Refactoring is an important activity to improve software internal structure. Even though there are many refactoring approaches, very few consider their impact on the software quality. In this paper, we propose a software refactoring approach based on quality attributes. We rely on the measurements of the Quality Model for Object Oriented Design (QMOOD) to recommend Move Method refactorings that improve software quality. In a nutshell, given a software system S, our approach recommends a sequence of refactorings R1,R2,...,Rn that result in system versions S1, S2,..., Sn, where quality (Si+1) > quality (Si). We empirically calibrated our approach, using four systems, to find the best criteria to measure the quality improvement. We performed three types of evaluation to verify the usefulness of our implemented tool, named QMove. First, we applied our approach on 13 open-source systems achieving an average recall of 84.2%. Second, we compared QMove with two state-of-art refactoring tools (JMove and JDeodorant) on the 13 previously evaluated systems, and QMove showed better recall, precision, and f-score values than the others. Third, we evaluated QMove, JMove, and JDeodorant in a real scenario with two proprietary systems on the eyes of their software architects. As result, the experts positively evaluated a greater number of QMove recommendations.","Refactoring, Move Method, Quality Metrics, Software Architecture",SBQS,,,
Conference Paper,Raab M,Improving System Integration Using a Modular Configuration Specification Language,,2016,,,152–157,Association for Computing Machinery,"New York, NY, USA",Companion Proceedings of the 15th International Conference on Modularity,"Málaga, Spain",2016,9781450340335.0,,https://doi.org/10.1145/2892664.2892691;http://dx.doi.org/10.1145/2892664.2892691,10.1145/2892664.2892691,"In today's systems we often plug together configurable standard components in a modular way. Most software, however, does not specify its configuration in a way suitable for other software. The aim of our configuration specification language SpecElektra is to fill this gap. It allows us to externally specify the configuration items of non-standardized configuration files. In SpecElektra we assign properties that enable additional validations and transformations. As a result, we can safely and easily configure software at run-time. The approach integrates standard software while retaining its modularity. We demonstrate how high-level configuration items help us to cope with changes in system-oriented goals.","Modularity, System-oriented goals, System Integration",MODULARITY Companion 2016,,,
Conference Paper,"Sun C,Le V,Su Z",Finding and Analyzing Compiler Warning Defects,,2016,,,203–213,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 38th International Conference on Software Engineering,"Austin, Texas",2016,9781450339001.0,,https://doi.org/10.1145/2884781.2884879;http://dx.doi.org/10.1145/2884781.2884879,10.1145/2884781.2884879,"Good compiler diagnostic warnings facilitate software development as they indicate likely programming mistakes or code smells. However, due to compiler bugs, the warnings may be erroneous, superfluous or missing, even for mature production compilers like GCC and Clang. In this paper, we (1) propose the first randomized differential testing technique to detect compiler warning defects and (2) describe our extensive evaluation in finding warning defects in widely-used C compilers.At the high level, our technique starts with generating random programs to trigger compilers to emit a variety of compiler warnings, aligns the warnings from different compilers, and identifies inconsistencies as potential bugs. We develop effective techniques to overcome three specific challenges: (1) How to generate random programs, (2) how to align textual warnings, and (3) how to reduce test programs for bug reporting?Our technique is very effective --- we have found and reported 60 bugs for GCC (38 confirmed, assigned or fixed) and 39 for Clang (14 confirmed or fixed). This case study not only demonstrates our technique's effectiveness, but also highlights the need to continue improving compilers' warning support, an essential, but rather neglected aspect of compilers.",,ICSE '16,,,
Conference Paper,"Machulak MP,Maler EL,Catalano D,van Moorsel A",User-Managed Access to Web Resources,,2010,,,35–44,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 6th ACM Workshop on Digital Identity Management,"Chicago, Illinois, USA",2010,9781450300902.0,,https://doi.org/10.1145/1866855.1866865;http://dx.doi.org/10.1145/1866855.1866865,10.1145/1866855.1866865,"Web 2.0 technologies have made it possible to migrate traditional desktop applications to the Web, resulting in a rich and dynamic user experience and in expanded functionality. Individuals can create and manage their content online, and they are not only consumers of Web services, but also active participants on the Web platform. As a result, potentially large amounts of personal, sensitive, and valuable data is put online, spread across various Web services. Users sometimes share this data with other users and services on the Web, but are also concerned about maintaining privacy and sharing their data securely.Currently, users must use diverse access control solutions available for each Web service to secure data and control its dissemination. When such mechanisms are used on a daily basis, they add considerable overhead, especially since these mechanisms often lack sophistication with respect to functionality as well as user interfaces. To alleviate this problem, we discuss a novel approach to access management for Web resources that includes a user as a core part of its model. The proposal puts the user in charge of assigning access rights to resources that may be hosted at various Web applications. It facilitates the ability of users to share data more selectively using a centralized authorization manager which makes access decisions based on user instructions.","security, web 2.0, authorization, access control",DIM '10,,,
Conference Paper,"Blue D,Raz O,Tzoref-Brill R,Wojciak P,Zalmanovici M",Proactive and Pervasive Combinatorial Testing,,2018,,,144–152,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 40th International Conference on Software Engineering: Software Engineering in Practice,"Gothenburg, Sweden",2018,9781450356596.0,,https://doi.org/10.1145/3183519.3183522;http://dx.doi.org/10.1145/3183519.3183522,10.1145/3183519.3183522,"Combinatorial testing (CT) is a well-known technique for improving the quality of test plans while reducing testing costs. Traditionally, CT is used by testers at testing phase to design a test plan based on a manual definition of the test space. In this work, we extend the traditional use of CT to other parts of the development life cycle. We use CT at early design phase to improve design quality. We also use CT after test cases have been created and executed, in order to find gaps between design and test. For the latter use case we deploy a novel technique for a semi-automated definition of the test space, which significantly reduces the effort associated with manual test space definition. We report on our practical experience in applying CT for these use cases to three large and heavily deployed industrial products. We demonstrate the value gained from extending the use of CT by (1) discovering latent design flaws with high potential impact, and (2) correlating CT-uncovered gaps between design and test with field reported problems.","design review, combinatorial testing, desk checking",ICSE-SEIP '18,,,
Conference Paper,"Zimmermann O,Pautasso C,Lübke D,Zdun U,Stocker M",Data-Oriented Interface Responsibility Patterns: Types of Information Holder Resources,,2020,,,,Association for Computing Machinery,"New York, NY, USA",Proceedings of the European Conference on Pattern Languages of Programs 2020,"Virtual Event, Germany",2020,9781450377690.0,,https://doi.org/10.1145/3424771.3424821;http://dx.doi.org/10.1145/3424771.3424821,10.1145/3424771.3424821,"Remote Application Programming Interfaces (APIs) are used in almost any distributed system today, for instance in microservices-based systems, and are thus enablers for many digitalization efforts. API design not only impacts whether software provided as a service is easy and efficient to develop applications with, but also affects the long term evolution of the software system. In general, APIs are responsible for providing remote and controlled access to the functionality provided as services; however, APIs often are also used to expose and share information. We focus on such data-related aspects of microservice APIs in this paper. Depending on the life cycle of the information published through the API, its mutability and the endpoint role, data-oriented APIs can be designed following patterns such as Operational Data Holder, Master Data Holder, Reference Data Holder, Data Transfer Holder, and Link Lookup Resource. Known uses and examples of the patterns are drawn from public Web APIs as well as application development and integration projects we have been involved in.",,EuroPLoP '20,,,
Conference Paper,"Wasylkowski A,Zeller A",Mining Temporal Specifications from Object Usage,,2009,,,295–306,IEEE Computer Society,USA,Proceedings of the 2009 IEEE/ACM International Conference on Automated Software Engineering,,2009,9780769538914.0,,https://doi.org/10.1109/ASE.2009.30;http://dx.doi.org/10.1109/ASE.2009.30,10.1109/ASE.2009.30,"A caller must satisfy the callee's precondition--that is, reach a state in which the callee may be called. Preconditions describe the state that needs to be reached, but not how to reach it. We combine static analysis with model checking to mine Computation Tree Logic (CTL) formulas that describe the operations a parameter goes through: ""In parseProperties(String xml), the parameter xml normally stems from getProperties()."" Such operational preconditions can be learned from program code, and the code can be checked for their violations. Applied to AspectJ, our Tikanga prototype found 189 violations of operational preconditions, uncovering 9 unique defects and 36 unique code smells---with 44% true positives in the 50 top-ranked violations.",,ASE '09,,,
Journal Article,"Klint P,Lämmel R,Verhoef C",Toward an Engineering Discipline for Grammarware,ACM Trans. Softw. Eng. Methodol.,2005,14.0,3,331–380,Association for Computing Machinery,"New York, NY, USA",,,2005-07,,1049-331X,https://doi.org/10.1145/1072997.1073000;http://dx.doi.org/10.1145/1072997.1073000,10.1145/1072997.1073000,"Grammarware comprises grammars and all grammar-dependent software. The term grammar is meant here in the sense of all established grammar formalisms and grammar notations including context-free grammars, class dictionaries, and XML schemas as well as some forms of tree and graph grammars. The term grammar-dependent software refers to all software that involves grammar knowledge in an essential manner. Archetypal examples of grammar-dependent software are parsers, program converters, and XML document processors. Despite the pervasive role of grammars in software systems, the engineering aspects of grammarware are insufficiently understood. We lay out an agenda that is meant to promote research on increasing the productivity of grammarware development and on improving the quality of grammarware. To this end, we identify the problems with the current grammarware practices, the barriers that currently hamper research, and the promises of an engineering discipline for grammarware, its principles, and the research challenges that have to be addressed.","grammar-dependent software, Grammarware, language processing, automated software engineering, model-driven development, software transformation, generic language technology, metamodeling, grammars, best practices, parsers, software evolution",,,,
Conference Paper,"Romano S,Fucci D,Scanniello G,Turhan B,Juristo N",Results from an Ethnographically-Informed Study in the Context of Test Driven Development,,2016,,,,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 20th International Conference on Evaluation and Assessment in Software Engineering,"Limerick, Ireland",2016,9781450336918.0,,https://doi.org/10.1145/2915970.2915996;http://dx.doi.org/10.1145/2915970.2915996,10.1145/2915970.2915996,"Background: Test-driven development (TDD) is an iterative software development technique where unit tests are defined before production code. Previous studies fail to analyze the values, beliefs, and assumptions that inform and shape TDD.Aim: We designed and conducted a qualitative study to understand the values, beliefs, and assumptions of TDD. In particular, we sought to understand how novice and professional software developers, arranged in pairs (a driver and a pointer), perceive and apply TDD.Method: 14 novice software developers, i.e., graduate students in Computer Science at the University of Basilicata, and six professional software developers (with one to 10 years work experience) participated in our ethnographically informed study. We asked the participants to implement a new feature for an existing software written in Java. We immersed ourselves in the context of the study, and collected data by means of contemporaneous field notes, audio recordings, and other artifacts.Results: A number of insights emerge from our analysis of the collected data, the main ones being: (i) refactoring (one of the phases of TDD) is not performed as often as the process requires and it is considered less important than other phases, (ii) the most important phase is implementation, (iii) unit tests are almost never up-to-date, (iv) participants first build a sort of mental model of the source code to be implemented and only then write test cases on the basis of this model; and (v) apart from minor differences, professional developers and students applied TDD in a similar fashion. Conclusions: Developers write quick-and-dirty production code to pass the tests and ignore refactoring.","Test Driven Development, Qualitative Study, Ethnographically-informed Study",EASE '16,,,
Conference Paper,"Zimmermann O,Lübke D,Zdun U,Pautasso C,Stocker M",Interface Responsibility Patterns: Processing Resources and Operation Responsibilities,,2020,,,,Association for Computing Machinery,"New York, NY, USA",Proceedings of the European Conference on Pattern Languages of Programs 2020,"Virtual Event, Germany",2020,9781450377690.0,,https://doi.org/10.1145/3424771.3424822;http://dx.doi.org/10.1145/3424771.3424822,10.1145/3424771.3424822,"Remote Application Programming Interfaces (APIs), as for instance offered in microservices architectures, are used in almost any distributed system today and are thus enablers for many digitalization efforts. It is hard to design such APIs so that they are easy and effective to use; maintaining their runtime qualities while preserving backward compatibility is equally challenging. Finding well suited granularities in terms of the architectural capabilities of endpoints and the read-write semantics of their operations are particularly important design concerns. Existing pattern languages have dealt with local APIs in object-oriented programming, with remote objects, with queue-based messaging and with service-oriented computing platforms. However, patterns or equivalent guidances for the architectural design of API endpoints, operations and their request and response message structures are still missing. In this paper, we extend our microservice API pattern language (MAP) and introduce endpoint role and operation responsibility patterns, namely Processing Resource, Computation Function, State Creation Operation, Retrieval Operation, and State Transition Operation. Known uses and examples of the patterns are drawn from public Web APIs, as well as application development and system integration projects the authors have been involved in.",,EuroPLoP '20,,,
Conference Paper,"Roy S,van Deursen A,Hermans F",Perceived Relevance of Automatic Code Inspection in End-User Development: A Study on VBA,,2019,,,167–176,Association for Computing Machinery,"New York, NY, USA",Proceedings of the Evaluation and Assessment on Software Engineering,"Copenhagen, Denmark",2019,9781450371452.0,,https://doi.org/10.1145/3319008.3319028;http://dx.doi.org/10.1145/3319008.3319028,10.1145/3319008.3319028,"Microsoft VBA (Visual Basic for Applications) is a programming language widely used by end-user programmers, often alongside the popular spreadsheet software Excel. Together they form the popular Excel-VBA application ecosystem. Despite being popular, spreadsheets are known to be fault-prone, and to minimize risk of faults in the overall Excel-VBA ecosystem, it is important to support end-user programmers in improving the code quality of their VBA programs also, in addition to improving spreadsheet technology and practices. In traditional software development, automatic code inspection using static analysis tools has been found effective in improving code quality, but the practical relevance of this technique in an end-user development context remains unexplored. With the aim of popularizing it in the end-user community, in this paper we examine the relevance of automatic code inspection in terms of how inspection rules are perceived by VBA programmers. We conduct a qualitative study consisting of interviews with 14 VBA programmers, who share their perceptions about 20 inspection rules that most frequently detected code quality issues in an industrial dataset of 25 VBA applications, obtained from a financial services company. Results show that the 20 studied inspection rules can be grouped into three categories of user perceptions based on the type of issues they warn about: i) 11 rules that warn about serious problems which need fixing, ii) 7 rules that warn about bad practices which do not mandate fixing, and iii) 2 rules that warn about purposeful code elements rather than issues. Based on these perceptions, we conclude that automatic code inspection is considerably relevant in an end-user development context such as VBA. The perceptions also indicate which inspection rules deserve the most attention from interested researchers and tool developers. Lastly, our results also reveal 3 additional issue types that are not covered by the existing inspection rules, and are therefore impetus for creating new rules.","VBA, code quality, static analysis, developer perceptions, end-user development",EASE '19,,,
Conference Paper,"Reichelt DG,Kühne S,Hasselbring W",PeASS: A Tool for Identifying Performance Changes at Code Level,,2020,,,1146–1149,IEEE Press,"San Diego, California",Proceedings of the 34th IEEE/ACM International Conference on Automated Software Engineering,,2020,9781728125084.0,,https://doi.org/10.1109/ASE.2019.00123;http://dx.doi.org/10.1109/ASE.2019.00123,10.1109/ASE.2019.00123,"We present PeASS (Performance Analysis of Software System versions), a tool for detecting performance changes at source code level that occur between different code versions. By using PeASS, it is possible to identify performance regressions that happened in the past to fix them.PeASS measures the performance of unit tests in different source code versions. To achieve statistic rigor, measurements are repeated and analyzed using an agnostic t-test. To execute a minimal amount of tests, PeASS uses a regression test selection.We evaluate PeASS on a selection of Apache Commons projects and show that 81% of all unit test covered performance changes can be found by PeASS. A video presentation is available at https://www.youtube.com/watch?v=RORFEGSCh6Y and PeASS can be downloaded from https://github.com/DaGeRe/peass.",,ASE '19,,,
Journal Article,"Feo-Arenis S,Westphal B,Dietsch D,Muñiz M,Andisha S,Podelski A",Ready for Testing: Ensuring Conformance to Industrial Standards through Formal Verification,Form. Asp. Comput.,2016,28.0,3,499–527,Springer-Verlag,"Berlin, Heidelberg",,,2016-05,,0934-5043,https://doi.org/10.1007/s00165-016-0365-3;http://dx.doi.org/10.1007/s00165-016-0365-3,10.1007/s00165-016-0365-3,"The design of distributed, safety-critical real-time systems is challenging due to their high complexity, the potentially large number of components, and complicated requirements and environment assumptions that stem from international standards. We present a case study that shows that despite those challenges, the automated formal verification of such systems is not only possible, but practicable even in the context of small to medium-sized enterprises. We considered a wireless fire alarm system, regulated by the EN 54 standard. We performed formal requirements engineering, modeling and verification and uncovered severe design flaws that would have prevented its certification. For an improved design, we provided dependable verification results which in particular ensure that certification tests for a relevant regulation standard will be passed. In general we observe that if system tests are specified by generalized test procedures, then verifying that a system will pass any test following those test procedures is a cost-efficient approach to improve the product quality based on formal methods. Based on our experience, we propose an approach useful to integrate the application of formal methods to product development in SME.","Safety-critical systems, Verification, Dependability, SME, Certification tests, Model Checking",,,,
Conference Paper,"Beck F,Diehl S",On the Congruence of Modularity and Code Coupling,,2011,,,354–364,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 19th ACM SIGSOFT Symposium and the 13th European Conference on Foundations of Software Engineering,"Szeged, Hungary",2011,9781450304436.0,,https://doi.org/10.1145/2025113.2025162;http://dx.doi.org/10.1145/2025113.2025162,10.1145/2025113.2025162,"Software systems are modularized to make their inherent complexity manageable. While there exists a set of well-known principles that may guide software engineers to design the modules of a software system, we do not know which principles are followed in practice. In a study based on 16 open source projects, we look at different kinds of coupling concepts between source code entities, including structural dependencies, fan-out similarity, evolutionary coupling, code ownership, code clones, and semantic similarity. The congruence between these coupling concepts and the modularization of the system hints at the modularity principles used in practice. Furthermore, the results provide insights on how to support developers to modularize software systems.","code coupling, package design, modularity",ESEC/FSE '11,,,
Conference Paper,"Zimmermann O,Stocker M,Lübke D,Zdun U",Interface Representation Patterns: Crafting and Consuming Message-Based Remote APIs,,2017,,,,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 22nd European Conference on Pattern Languages of Programs,"Irsee, Germany",2017,9781450348485.0,,https://doi.org/10.1145/3147704.3147734;http://dx.doi.org/10.1145/3147704.3147734,10.1145/3147704.3147734,"Remote Application Programming Interfaces (APIs) are technology enablers for major distributed system trends such as mobile and cloud computing and the Internet of Things. In such settings, message-based APIs dominate over procedural and object-oriented ones. It is hard to design such APIs so that they are easy and efficient to use for client developers. Maintaining their runtime qualities while preserving backward compatibility is equally challenging for API providers. For instance, finding a well suited granularity for services and their operations is a particularly important design concern in APIs that realize service-oriented software architectures. Due to the fallacies of distributed computing, the forces for message-based APIs and service interfaces differ from those for local APIs -- for instance, network latency and security concerns deserve special attention. Existing pattern languages have dealt with local APIs in object-oriented programming, with remote objects, with queue-based messaging and with service-oriented computing platforms. However, patterns or equivalent guidance for the structural design of request and response messages in message-based remote APIs is still missing. In this paper, we outline such a pattern language and introduce five basic interface representation patterns to promote platform-independent design advice for common remote API technologies such as RESTful HTTP and Web services (WSDL/SOAP). Known uses and examples of the patterns are drawn from public Web APIs, as well as application development and software integration projects the authors have been involved in.",,EuroPLoP '17,,,
Journal Article,Bland M,"Finding More Than One Worm in the Apple: If You See Something, Say Something",Queue,2014,12.0,5,10–21,Association for Computing Machinery,"New York, NY, USA",,,2014-05,,1542-7730,https://doi.org/10.1145/2620660.2620662;http://dx.doi.org/10.1145/2620660.2620662,10.1145/2620660.2620662,"In February Apple revealed and fixed an SSL (Secure Sockets Layer) vulnerability that had gone undiscovered since the release of iOS 6.0 in September 2012. It left users vulnerable to man-in-the-middle attacks thanks to a short circuit in the SSL/TLS (Transport Layer Security) handshake algorithm introduced by the duplication of a goto statement. Since the discovery of this very serious bug, many people have written about potential causes. A close inspection of the code, however, reveals not only how a unit test could have been written to catch the bug, but also how to refactor the existing code to make the algorithm testable - as well as more clues to the nature of the error and the environment that produced it.",,,,,
Conference Paper,"Huang H,Li C",A Unified Graph Model for Chinese Product Review Summarization Using Richer Information,,2012,,,,Association for Computing Machinery,"New York, NY, USA",Proceedings of the First International Workshop on Issues of Sentiment Discovery and Opinion Mining,"Beijing, China",2012,9781450315432.0,,https://doi.org/10.1145/2346676.2346678;http://dx.doi.org/10.1145/2346676.2346678,10.1145/2346676.2346678,"With e-commerce growing rapidly, online product reviews open amounts of studies of extracting useful information from numerous reviews. How to generate informative and concise summaries from reviews automatically has become a critical issue. In this paper, we present a novel unified graph model, composited information graph (CIG), to represent reviews with lexical, topic and together with sentiment information. Based on the model, we propose an automatic approach to address this issue. We use probabilistic methods to model the lexical, topic and sentiment information separately, associate with the discovered information in the CIG model, and generate summaries with a HITS-like algorithm called Mix-HITS considering both the Representativeness and Proportion Approximation. The experiments demonstrate that our method has improved performance over LexRank and ClusterHITS with Chinese and English datasets. Experimental results show that the proposed approach helps to build an effective way towards both the overall and contrastive summarization.","product facet detection, sentiment classification, graph ranking, review summarization",WISDOM '12,,,
Conference Paper,"Elyashar A,Uziel S,Paradise A,Puzis R",The Chameleon Attack: Manipulating Content Display in Online Social Media,,2020,,,848–859,Association for Computing Machinery,"New York, NY, USA",Proceedings of The Web Conference 2020,"Taipei, Taiwan",2020,9781450370233.0,,https://doi.org/10.1145/3366423.3380165;http://dx.doi.org/10.1145/3366423.3380165,10.1145/3366423.3380165,"Online social networks (OSNs) are ubiquitous attracting millions of users all over the world. Being a popular communication media OSNs are exploited in a variety of cyber-attacks. In this article, we discuss the chameleon attack technique, a new type of OSN-based trickery where malicious posts and profiles change the way they are displayed to OSN users to conceal themselves before the attack or avoid detection. Using this technique, adversaries can, for example, avoid censorship by concealing true content when it is about to be inspected; acquire social capital to promote new content while piggybacking a trending one; cause embarrassment and serious reputation damage by tricking a victim to like, retweet, or comment a message that he wouldn’t normally do without any indication for the trickery within the OSN. An experiment performed with closed Facebook groups of sports fans shows that (1) chameleon pages can pass by the moderation filters by changing the way their posts are displayed and (2) moderators do not distinguish between regular and chameleon pages. We list the OSN weaknesses that facilitate the chameleon attack and propose a set of mitigation guidelines.","Chameleon Attack, Link Previews, Online Social Networks",WWW '20,,,
Conference Paper,"Shambaugh R,Weiss A,Guha A",Rehearsal: A Configuration Verification Tool for Puppet,,2016,,,416–430,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 37th ACM SIGPLAN Conference on Programming Language Design and Implementation,"Santa Barbara, CA, USA",2016,9781450342612.0,,https://doi.org/10.1145/2908080.2908083;http://dx.doi.org/10.1145/2908080.2908083,10.1145/2908080.2908083,"Large-scale data centers and cloud computing have turned system configuration into a challenging problem. Several widely-publicized outages have been blamed not on software bugs, but on configuration bugs. To cope, thousands of organizations use system configuration languages to manage their computing infrastructure. Of these, Puppet is the most widely used with thousands of paying customers and many more open-source users. The heart of Puppet is a domain-specific language that describes the state of a system. Puppet already performs some basic static checks, but they only prevent a narrow range of errors. Furthermore, testing is ineffective because many errors are only triggered under specific machine states that are difficult to predict and reproduce. With several examples, we show that a key problem with Puppet is that configurations can be non-deterministic. This paper presents Rehearsal, a verification tool for Puppet configurations. Rehearsal implements a sound, complete, and scalable determinacy analysis for Puppet. To develop it, we (1) present a formal semantics for Puppet, (2) use several analyses to shrink our models to a tractable size, and (3) frame determinism-checking as decidable formulas for an SMT solver. Rehearsal then leverages the determinacy analysis to check other important properties, such as idempotency. Finally, we apply Rehearsal to several real-world Puppet configurations.","verification, domain-specific languages, system configuration, Puppet",PLDI '16,,,
Journal Article,"Shambaugh R,Weiss A,Guha A",Rehearsal: A Configuration Verification Tool for Puppet,SIGPLAN Not.,2016,51.0,6,416–430,Association for Computing Machinery,"New York, NY, USA",,,2016-06,,0362-1340,https://doi.org/10.1145/2980983.2908083;http://dx.doi.org/10.1145/2980983.2908083,10.1145/2980983.2908083,"Large-scale data centers and cloud computing have turned system configuration into a challenging problem. Several widely-publicized outages have been blamed not on software bugs, but on configuration bugs. To cope, thousands of organizations use system configuration languages to manage their computing infrastructure. Of these, Puppet is the most widely used with thousands of paying customers and many more open-source users. The heart of Puppet is a domain-specific language that describes the state of a system. Puppet already performs some basic static checks, but they only prevent a narrow range of errors. Furthermore, testing is ineffective because many errors are only triggered under specific machine states that are difficult to predict and reproduce. With several examples, we show that a key problem with Puppet is that configurations can be non-deterministic. This paper presents Rehearsal, a verification tool for Puppet configurations. Rehearsal implements a sound, complete, and scalable determinacy analysis for Puppet. To develop it, we (1) present a formal semantics for Puppet, (2) use several analyses to shrink our models to a tractable size, and (3) frame determinism-checking as decidable formulas for an SMT solver. Rehearsal then leverages the determinacy analysis to check other important properties, such as idempotency. Finally, we apply Rehearsal to several real-world Puppet configurations.","domain-specific languages, Puppet, verification, system configuration",,,,
Conference Paper,"Zhao B,Weng H,Ji S,Chen J,Wang T,He Q,Beyah R",Towards Evaluating the Security of Real-World Deployed Image CAPTCHAs,,2018,,,85–96,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 11th ACM Workshop on Artificial Intelligence and Security,"Toronto, Canada",2018,9781450360043.0,,https://doi.org/10.1145/3270101.3270104;http://dx.doi.org/10.1145/3270101.3270104,10.1145/3270101.3270104,"Nowadays, image captchas are being widely used across the Internet to defend against abusive programs. However, the ever-advancing capabilities of computer vision techniques are gradually diminishing the security of image captchas; yet, little is known thus far about the vulnerability of image captchas deployed in real-world settings. In this paper, we conduct the first systematic study on the security of image captchas in the wild. We classify the currently popular image captchas into three categories: selection-, slide- and click-based captchas. We propose three effective and generic attacks, each against one of these categories. We evaluate our attacks against 10 real-world popular image captchas, including those from tencent.com, google.com, and 12306.cn. Furthermore, we compare our attacks with 9 online image recognition services and human labors from 8 underground captcha-solving services. Our studies show that: (1) all of those popular image captchas are vulnerable to our attacks; (2) our attacks significantly outperform the state-of-the-arts in almost all the scenarios; and (3) our attacks achieve effectiveness comparable to human labors but with much higher efficiency. Based on our evaluation, we identify the design flaws of those popular schemes, the best practices, and the design principles towards more secure captchas.","image captchas, deep learning, captcha-solving services",AISec '18,,,
Conference Paper,"Marr S,Daloze B",Few Versatile vs. Many Specialized Collections: How to Design a Collection Library for Exploratory Programming?,,2018,,,135–143,Association for Computing Machinery,"New York, NY, USA","Conference Companion of the 2nd International Conference on Art, Science, and Engineering of Programming","Nice, France",2018,9781450355131.0,,https://doi.org/10.1145/3191697.3214334;http://dx.doi.org/10.1145/3191697.3214334,10.1145/3191697.3214334,"While an integral part of all programming languages, the design of collection libraries is rarely studied. This work briefly reviews the collection libraries of 14 languages to identify possible design dimensions. Some languages have surprisingly few but versatile collections, while others have large libraries with many specialized collections. Based on the identified design dimensions, we argue that a small collection library with only a sequence, a map, and a set type are a suitable choice to facilitate exploratory programming. Such a design minimizes the number of decisions programmers have to make when dealing with collections, and it improves discoverability of collection operations. We further discuss techniques that make their implementation practical from a performance perspective. Based on these arguments, we conclude that languages which aim to support exploratory programming should strive for small and versatile collection libraries.","Exploratory Programming, Collection Libraries, Implementation, Design",Programming'18 Companion,,,
Conference Paper,"Yamashita A,Moonen L","Assembling Multiple-Case Studies: Potential, Principles and Practical Considerations",,2014,,,,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 18th International Conference on Evaluation and Assessment in Software Engineering,"London, England, United Kingdom",2014,9781450324762.0,,https://doi.org/10.1145/2601248.2601286;http://dx.doi.org/10.1145/2601248.2601286,10.1145/2601248.2601286,"Case studies are a research method aimed at holistically analyzing a phenomenon in its context. Despite the fact that they cannot be used to answer the same precise research questions as, e.g., can be addressed by controlled experiments, case studies can cope much better with situations having several variables of interest, multiple sources of evidence, or rich contexts that cannot be controlled or isolated. As such, case studies are a promising instrument to study the complex phenomena at play in Software Engineering.However, the use of case studies as research methodology entails certain challenges. We argue that one of the biggest challenges is the case selection bias when conducting multiple-case studies. In practice, cases are frequently selected based on their availability, without appropriate control over moderator factors. This hinders the level of comparability across cases, leading to internal validity issues.In this paper, we discuss the notion of assembling cases as a plausible alternative to selecting cases to overcome the selection bias problem when conducting multiple-case studies. In addition, we present and discuss our experiences from applying this approach in a study designed to investigate the impact of software design on maintainability.","internal validity, empirical studies, methodology, case study",EASE '14,,,
Conference Paper,"Abi-Antoun M,Ammar N,Hailat Z",Extraction of Ownership Object Graphs from Object-Oriented Code: An Experience Report,,2012,,,133–142,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 8th International ACM SIGSOFT Conference on Quality of Software Architectures,"Bertinoro, Italy",2012,9781450313469.0,,https://doi.org/10.1145/2304696.2304719;http://dx.doi.org/10.1145/2304696.2304719,10.1145/2304696.2304719,"Despite receiving much research attention, the extraction of runtime architecture remains hard. One approach, SCHOLIA, relies on adding typecheckable annotations to the code, and uses static analysis to extract a global, hierarchical Ownership Object Graph (OOG). The OOG provides architectural abstraction by ownership hierarchy and by types, and can be abstracted into a run-time architecture represented in an architectural description language, for documentation or conformance analysis.We report on our experience in analyzing a medium-sized object-oriented system undergoing maintenance to: (1) extract an OOG; and (2) refine the OOG based on the maintainers' feedback.We evaluate the effectiveness of abstraction by ownership hierarchy and by types to extract an OOG that the system maintainers understand. We measure the extraction effort to be about 1 hour/KLOC. An evaluation with the lead maintainer confirms that he understands abstraction by ownership hierarchy and by types. Finally, we illustrate how to incrementally refine an extracted OOG (without starting all over) to better match the maintainer's mental model.","architecture recovery, ownership types, runtime architecture",QoSA '12,,,
Conference Paper,"Wert A,Happe J,Happe L",Supporting Swift Reaction: Automatically Uncovering Performance Problems by Systematic Experiments,,2013,,,552–561,IEEE Press,"San Francisco, CA, USA",Proceedings of the 2013 International Conference on Software Engineering,,2013,9781467330763.0,,,,"Performance problems pose a significant risk to software vendors. If left undetected, they can lead to lost customers, increased operational costs, and damaged reputation. Despite all efforts, software engineers cannot fully prevent performance problems being introduced into an application. Detecting and resolving such problems as early as possible with minimal effort is still an open challenge in software performance engineering. In this paper, we present a novel approach for Performance Problem Diagnostics (PPD) that systematically searches for well-known performance problems (also called performance antipatterns) within an application. PPD automatically isolates the problem's root cause, hence facilitating problem solving. We applied PPD to a well established transactional web e-Commerce benchmark (TPC-W) in two deployment scenarios. PPD automatically identified four performance problems in the benchmark implementation and its deployment environment. By fixing the problems, we increased the maximum throughput of the benchmark from 1800 requests per second to more than 3500.",,ICSE '13,,,
Conference Paper,"Morrisett G,Tan G,Tassarotti J,Tristan JB,Gan E","RockSalt: Better, Faster, Stronger SFI for the X86",,2012,,,395–404,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 33rd ACM SIGPLAN Conference on Programming Language Design and Implementation,"Beijing, China",2012,9781450312059.0,,https://doi.org/10.1145/2254064.2254111;http://dx.doi.org/10.1145/2254064.2254111,10.1145/2254064.2254111,"Software-based fault isolation (SFI), as used in Google's Native Client (NaCl), relies upon a conceptually simple machine-code analysis to enforce a security policy. But for complicated architectures such as the x86, it is all too easy to get the details of the analysis wrong. We have built a new checker that is smaller, faster, and has a much reduced trusted computing base when compared to Google's original analysis. The key to our approach is automatically generating the bulk of the analysis from a declarative description which we relate to a formal model of a subset of the x86 instruction set architecture. The x86 model, developed in Coq, is of independent interest and should be usable for a wide range of machine-level verification tasks.","software fault isolation, domain-specific languages",PLDI '12,,,
Journal Article,"Morrisett G,Tan G,Tassarotti J,Tristan JB,Gan E","RockSalt: Better, Faster, Stronger SFI for the X86",SIGPLAN Not.,2012,47.0,6,395–404,Association for Computing Machinery,"New York, NY, USA",,,2012-06,,0362-1340,https://doi.org/10.1145/2345156.2254111;http://dx.doi.org/10.1145/2345156.2254111,10.1145/2345156.2254111,"Software-based fault isolation (SFI), as used in Google's Native Client (NaCl), relies upon a conceptually simple machine-code analysis to enforce a security policy. But for complicated architectures such as the x86, it is all too easy to get the details of the analysis wrong. We have built a new checker that is smaller, faster, and has a much reduced trusted computing base when compared to Google's original analysis. The key to our approach is automatically generating the bulk of the analysis from a declarative description which we relate to a formal model of a subset of the x86 instruction set architecture. The x86 model, developed in Coq, is of independent interest and should be usable for a wide range of machine-level verification tasks.","software fault isolation, domain-specific languages",,,,
Conference Paper,"de Andrade Cardieri G,Zaina LM","Analyzing User Experience in Mobile Web, Native and Progressive Web Applications: A User and HCI Specialist Perspectives",,2018,,,,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 17th Brazilian Symposium on Human Factors in Computing Systems,"Belém, Brazil",2018,9781450366014.0,,https://doi.org/10.1145/3274192.3274201;http://dx.doi.org/10.1145/3274192.3274201,10.1145/3274192.3274201,"Progressive Web App (PWA) is a new approach to the development of mobile application proposed by Google in 2015. It combines technology resources of both web and native applications. The challenges of designing interfaces for different applications platforms, such as web and native Android, has been discussed in recent years. However, PWAs are a recent technology and their impact regarding user experience have been little exploited. In this paper, we present the findings of an experimental study with 8 participants that explored the aspects of user experience on three different platforms. We carried out a qualitative analysis that focused on the comparison of the user experience during the participants' interaction with PWA, web mobile and native Android applications. Two distinct perspectives were defined to support our data analysis. First, the user perspective was considered and the participants' feedback was explored. After, focusing on the human-computer interaction specialist perspective we examined the users' facial expressions with the aims of identifying which emotions they sensed during interactions with each application. We gathered evidence that an overall positive user experience can be achieved even if the user had some interaction issues. There is no bias indicating that either a specific platform or interface element offer more enjoyable interactions.","user experience, native applications, mobile devices, user interface, progressive web application, mobile web applications",IHC 2018,,,
Conference Paper,Motohashi M,"A Language of Harmony: A Pattern Language of Mikoshi, Yoriai, and Kuuki",,2010,,,,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 1st Asian Conference on Pattern Languages of Programs,"Tokyo, Japan",2010,9781450301268.0,,https://doi.org/10.1145/2371736.2371751;http://dx.doi.org/10.1145/2371736.2371751,10.1145/2371736.2371751,"This paper describes the pattern language of Mikoshi, which is a movable shrine, and Yoriai, which is a traditional Japanese meeting style, which are used in today's local festivals throughout Tokyo. This paper also describes the pattern language of Kuuki, which is the ""atmosphere"" created when people communicate. These pattern languages are useful in community and organization management and software development.","accommodation, sustainability, pattern language, requirement engineering, Japanese culture, discussion, dialogue, consensus, business analysis, argue, peer-review",AsianPLoP '10,,,
Conference Paper,"Spreitzer R,Griesmayr S,Korak T,Mangard S",Exploiting Data-Usage Statistics for Website Fingerprinting Attacks on Android,,2016,,,49–60,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 9th ACM Conference on Security & Privacy in Wireless and Mobile Networks,"Darmstadt, Germany",2016,9781450342704.0,,https://doi.org/10.1145/2939918.2939922;http://dx.doi.org/10.1145/2939918.2939922,10.1145/2939918.2939922,"The browsing behavior of a user allows to infer personal details, such as health status, political interests, sexual orientation, etc. In order to protect this sensitive information and to cope with possible privacy threats, defense mechanisms like SSH tunnels and anonymity networks (e.g., Tor) have been established. A known shortcoming of these defenses is that website fingerprinting attacks allow to infer a user's browsing behavior based on traffic analysis techniques. However, website fingerprinting typically assumes access to the client's network or to a router near the client, which restricts the applicability of these attacks.In this work, we show that this rather strong assumption is not required for website fingerprinting attacks. Our client-side attack overcomes several limitations and assumptions of network-based fingerprinting attacks, e.g., network conditions and traffic noise, disabled browser caches, expensive training phases, etc. Thereby, we eliminate assumptions used for academic purposes and present a practical attack that can be implemented easily and deployed on a large scale. Eventually, we show that an unprivileged application can infer the browsing behavior by exploiting the unprotected access to the Android data-usage statistics. More specifically, we are able to infer 97% of 2,500 page visits out of a set of 500 monitored pages correctly. Even if the traffic is routed through Tor by using the Orbot proxy in combination with the Orweb browser, we can infer 95% of 500 page visits out of a set of 100 monitored pages correctly. Thus, the READ_HISTORY_BOOKMARKS permission, which is supposed to protect the browsing behavior, does not provide protection.","data-usage statistics, side-channel attack, website fingerprinting, mobile malware, mobile security",WiSec '16,,,
Conference Paper,"Sasao T,Konomi S,Suzuki R",Supporting Community-Centric Use and Management of Vacant Houses: A Crowdsourcing-Based Approach,,2016,,,1454–1459,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 2016 ACM International Joint Conference on Pervasive and Ubiquitous Computing: Adjunct,"Heidelberg, Germany",2016,9781450344623.0,,https://doi.org/10.1145/2968219.2968587;http://dx.doi.org/10.1145/2968219.2968587,10.1145/2968219.2968587,"In recent years, the number of vacant houses is increasing in Japan, as the population decline in local communities. In this context, there is an urgent need to develop smart socio-technical systems that enable effective use and management of vacant houses, thereby preventing them from having negative impacts on local communities. In this paper, we propose an approach to support community-centric use and management of vacant houses based on an integrated crowdsourcing platform. We describe our ongoing project in Kashiwa City, and argue for an open, inclusive, and community-centric distributed platform to cater for some of the important needs in the community.","collaboration, mobile and situated crowdsourcing, open source, local communities, vacant houses",UbiComp '16,,,
Conference Paper,"Kappel G,Kapsammer E,Kargl H,Kramler G,Reiter T,Retschitzegger W,Schwinger W,Wimmer M",Lifting Metamodels to Ontologies: A Step to the Semantic Integration of Modeling Languages,,2006,,,528–542,Springer-Verlag,"Berlin, Heidelberg",Proceedings of the 9th International Conference on Model Driven Engineering Languages and Systems,"Genova, Italy",2006,9783540457725.0,,https://doi.org/10.1007/11880240_37;http://dx.doi.org/10.1007/11880240_37,10.1007/11880240_37,"The use of different modeling languages in software development makes their integration a must. Most existing integration approaches are metamodel-based with these metamodels representing both an abstract syntax of the corresponding modeling language and also a data structure for storing models. This implementation specific focus, however, does not make explicit certain language concepts, which can complicate integration tasks. Hence, we propose a process which semi-automatically lifts metamodels into ontologies by making implicit concepts in the metamodel explicit in the ontology. Thus, a shift of focus from the implementation of a certain modeling language towards the explicit reification of the concepts covered by this language is made. This allows matching on a solely conceptual level, which helps to achieve better results in terms of mappings that can in turn be a basis for deriving implementation specific transformation code.",,MoDELS'06,,,
Conference Paper,Wachsmuth G,Metamodel Adaptation and Model Co-Adaptation,,2007,,,600–624,Springer-Verlag,"Berlin, Heidelberg",Proceedings of the 21st European Conference on Object-Oriented Programming,"Berlin, Germany",2007,9783540735885.0,,,,"Like other software artefacts, metamodels evolve over time. We propose a transformational approach to assist metamodel evolution by stepwise adaptation. In the first part of the paper, we adopt ideas from grammar engineering to define several semantics- and instancepreservation properties in terms of metamodel relations. This part is not restricted to any metamodel formalism. In the second part, we present a library of QVT Relations for the stepwise adaptation of MOF compliant metamodels. Transformations from this library separate preservation properties. We distinguish three kinds of adaptation according to these properties; namely refactoring, construction, and destruction. Coadaptation of models is discussed with respect to instance-preservation. In most cases, co-adaptation is achieved automatically. Finally, we point out applications in the areas of metamodel design, implementation, refinement, maintenance, and recovery.",,ECOOP'07,,,
Conference Paper,"Brewster S,McGookin D,Miller C",Olfoto: Designing a Smell-Based Interaction,,2006,,,653–662,Association for Computing Machinery,"New York, NY, USA",Proceedings of the SIGCHI Conference on Human Factors in Computing Systems,"Montréal, Québec, Canada",2006,9781595933720.0,,https://doi.org/10.1145/1124772.1124869;http://dx.doi.org/10.1145/1124772.1124869,10.1145/1124772.1124869,"We present a study into the use of smell for searching digi-tal photo collections. Many people now have large photo libraries on their computers and effective search tools are needed. Smell has a strong link to memory and emotion so may be a good way to cue recall when searching. Our study compared text and smell based tagging. For the first stage we generated a set of smell and tag names from user de-scriptions of photos, participants then used these to tag pho-tos, returning two weeks later to answer questions on their photos. Results showed that participants could tag effec-tively with text labels, as this is a common and familiar task. Performance with smells was lower but participants performed significantly above chance, with some partici-pants using smells well. This suggests that smell has poten-tial. Results also showed that some smells were consistently identified and useful, but some were not and highlighted issues with smell delivery devices. We also discuss some practical issues of using smell for interaction.","olfaction, searching, smell, tagging, digital photographs",CHI '06,,,
Conference Paper,"Khan MM,Le HK,Ahmadi H,Abdelzaher TF,Han J",Dustminer: Troubleshooting Interactive Complexity Bugs in Sensor Networks,,2008,,,99–112,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 6th ACM Conference on Embedded Network Sensor Systems,"Raleigh, NC, USA",2008,9781595939906.0,,https://doi.org/10.1145/1460412.1460423;http://dx.doi.org/10.1145/1460412.1460423,10.1145/1460412.1460423,"This paper presents a tool for uncovering bugs due to interactive complexity in networked sensing applications. Such bugs are not localized to one component that is faulty, but rather result from complex and unexpected interactions between multiple often individually non-faulty components. Moreover, the manifestations of these bugs are often not repeatable, making them particularly hard to find, as the particular sequence of events that invokes the bug may not be easy to reconstruct. Because of the distributed nature of failure scenarios, our tool looks for sequences of events that may be responsible for faulty behavior, as opposed to localized bugs such as a bad pointer in a module. An extensible framework is developed where a front-end collects runtime data logs of the system being debugged and an offline back-end uses frequent discriminative pattern mining to uncover likely causes of failure. We provide a case study of debugging a recent multichannel MAC protocol that was found to exhibit corner cases of poor performance (worse than single channel MAC). The tool helped uncover event sequences that lead to a highly degraded mode of operation. Fixing the problem significantly improved the performance of the protocol.We also provide a detailed analysis of tool overhead in terms of memory requirements and impact on the running application.","distributed automated debugging, wireless sensor networks, protocol debugging",SenSys '08,,,
Conference Paper,Romano D,Analyzing the Change-Proneness of Service-Oriented Systems from an Industrial Perspective,,2013,,,1365–1368,IEEE Press,"San Francisco, CA, USA",Proceedings of the 2013 International Conference on Software Engineering,,2013,9781467330763.0,,,,"Antipatterns and code smells have been widely proved to affect the change-proneness of software components. However, there is a lack of studies that propose indicators of changes for service-oriented systems. Like any other software systems, such systems evolve to address functional and non func- tional requirements. In this research, we investigate the change- proneness of service-oriented systems from the perspective of software engineers. Based on the feedback from our industrial partners we investigate which indicators can be used to highlight change-prone application programming interfaces (APIs) and service interfaces in order to improve their reusability and response time. The output of this PhD research will assist software engineers in designing stable APIs and reusable services with adequate response time.",,ICSE '13,,,
Conference Paper,"Buchenscheit A,Könings B,Neubert A,Schaub F,Schneider M,Kargl F",Privacy Implications of Presence Sharing in Mobile Messaging Applications,,2014,,,20–29,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 13th International Conference on Mobile and Ubiquitous Multimedia,"Melbourne, Victoria, Australia",2014,9781450333047.0,,https://doi.org/10.1145/2677972.2677980;http://dx.doi.org/10.1145/2677972.2677980,10.1145/2677972.2677980,"Mobile messaging applications, such as WhatsApp, provide a free alternative for mobile texting on smartphones. Mobile messengers typically also share presence information about users to indicate when a user is online. We investigated the privacy implications of such presence updates, using WhatsApp as an example. We conducted a user study with two independent groups (19 participants in total), in which we collected and analyzed their presence information over four weeks of regular WhatsApp use and conducted follow-up interviews. Our results show that presence information alone is sufficient to accurately identify, for example, daily routines, deviations, times of inappropriate mobile messaging, or conversation partners. We discuss resulting privacy implications of presence information and potential solutions to mitigate these issues.",,MUM '14,,,
Conference Paper,"Doubleday A,Ryan M,Springett M,Sutcliffe A",A Comparison of Usability Techniques for Evaluating Design,,1997,,,101–110,Association for Computing Machinery,"New York, NY, USA","Proceedings of the 2nd Conference on Designing Interactive Systems: Processes, Practices, Methods, and Techniques","Amsterdam, The Netherlands",1997,9780897918633.0,,https://doi.org/10.1145/263552.263583;http://dx.doi.org/10.1145/263552.263583,10.1145/263552.263583,,"evaluation, user interface design, usability, information retrieval, heuristic evaluation",DIS '97,,,
Conference Paper,"Chen Y,Li T,Wang X,Chen K,Han X",Perplexed Messengers from the Cloud: Automated Security Analysis of Push-Messaging Integrations,,2015,,,1260–1272,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 22nd ACM SIGSAC Conference on Computer and Communications Security,"Denver, Colorado, USA",2015,9781450338325.0,,https://doi.org/10.1145/2810103.2813652;http://dx.doi.org/10.1145/2810103.2813652,10.1145/2810103.2813652,"In this paper, we report the first large-scale, systematic study on the security qualities of emerging push-messaging services, focusing on their app-side service integrations. We identified a set of security properties different push-messaging services (e.g., Google Cloud Messaging) need to have, and automatically verified them in different integrations using a new technique, called Seminal. Seminal is designed to extract semantic information from a service's sample code, and leverage the information to evaluate the security qualities of the service's SDKs and its integrations within different apps. Using this tool, we studied 30 leading services around the world, and scanned 35,173 apps. Our findings are astonishing: over 20% apps in Google Play and 50% apps in mainstream Chinese app markets are riddled with security-critical loopholes, putting a huge amount of sensitive user data at risk. Also, our research brought to light new types of security flaws never known before, which can be exploited to cause serious confusions among popular apps and services (e.g., Facebook, Skype, Yelp, Baidu Push). Taking advantage of such confusions, the adversary can post his content to the victim's apps in the name of trusted parties and intercept her private messages. The study highlights the serious challenges in securing push-messaging services and an urgent need for improving their security qualities.","security analysis, android security, mobile push-messaging services, mobile cloud security",CCS '15,,,
Conference Paper,"Bourimi M,Tesoriero R",Non-Functional Requirements for Distributable User Interfaces in Agile Processes,,2014,,,54–66,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 2014 Workshop on Distributed User Interfaces and Multimodal Interaction,"Toulouse, France",2014,9781605587240.0,,https://doi.org/10.1145/2677356.2677668;http://dx.doi.org/10.1145/2677356.2677668,10.1145/2677356.2677668,"This paper presents a two-folded approach to deal with non-functional requirements for distributable user interfaces (DeUIs) in agile processes. This proposal employs a conceptual agile framework that ensures earlier consideration of nonfunctional requirements and stakeholders' involvement to solve tensions among agility, requirements engineering practices and continuous system architecture adaptation. Besides, it improves the step of continuous architecture adaptation as established in the DeUI field by employing model-driven architectures. Thus, while this approach profits from the conceptual framework by means of continuous feedback on how to technically better support the classical tension between agility and requirement engineering; it also takes advantage of model-driven architecture to cope with the tension between agility and distributable user interface architecture changes.","Agile methodologies, AFFINE, Scrum, Model-driven development, Distributable User Interfaces",DUI '14,,,
Journal Article,"Shrestha P,Saxena N",An Offensive and Defensive Exposition of Wearable Computing,ACM Comput. Surv.,2017,50.0,6,,Association for Computing Machinery,"New York, NY, USA",,,2017-11,,0360-0300,https://doi.org/10.1145/3133837;http://dx.doi.org/10.1145/3133837,10.1145/3133837,"Wearable computing is rapidly getting deployed in many—commercial, medical, and personal—domains of day-to-day life. Wearable devices appear in various forms, shapes, and sizes and facilitate a wide variety of applications in many domains of life. However, wearables raise unique security and privacy concerns. Wearables also hold the promise to help enhance the existing security, privacy, and safety paradigms in unique ways while preserving the system’s usability.The contribution of this research literature survey is threefold. First, as a background, we identify a wide range of existing as well as upcoming wearable devices and investigate their broad applications. Second, we provide an exposition of the security and privacy of wearable computing, studying dual aspects, that is, both attacks and defenses. Third, we provide a comprehensive study of the potential security, privacy, and safety enhancements to existing systems based on the emergence of wearable technology. Although several research works have emerged exploring different offensive and defensive uses of wearables, there is a lack of a broad and precise literature review systematizing all those security and privacy aspects and the underlying threat models. This research survey also analyzes current and emerging research trends and provides directions for future research.","side-channel analysis and countermeasures, Wearable computing, security requirements",,,,
Journal Article,"Bayle E,Bellamy R,Casaday G,Erickson T,Fincher S,Grinter B,Gross B,Lehder D,Marmolin H,Moore B,Potts C,Skousen G,Thomas J",Putting It All Together: Towards a Pattern Language for Interaction Design: A CHI 97 Workshop,SIGCHI Bull.,1998,30.0,1,17–23,Association for Computing Machinery,"New York, NY, USA",,,1998-01,,0736-6906,https://doi.org/10.1145/280571.280580;http://dx.doi.org/10.1145/280571.280580,10.1145/280571.280580,,,,,,
Conference Paper,Bernstein M,Can We Talk about Spatial Hypertext,,2011,,,103–112,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 22nd ACM Conference on Hypertext and Hypermedia,"Eindhoven, The Netherlands",2011,9781450302562.0,,https://doi.org/10.1145/1995966.1995983;http://dx.doi.org/10.1145/1995966.1995983,10.1145/1995966.1995983,"Spatial hypertexts are difficult to explain and to share because we have so little vocabulary with which to discuss them. From examination of actual spatial hypertexts drawn from a variety of domains and created in a variety of systems, we may identify and name several common patterns.","visualization, graphs, patterns, knowledge representation, diagrams, hypertext, spatial hypertext",HT '11,,,
Conference Paper,"Ribic H,Liu YD",Energy-Efficient Work-Stealing Language Runtimes,,2014,,,513–528,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 19th International Conference on Architectural Support for Programming Languages and Operating Systems,"Salt Lake City, Utah, USA",2014,9781450323055.0,,https://doi.org/10.1145/2541940.2541971;http://dx.doi.org/10.1145/2541940.2541971,10.1145/2541940.2541971,"Work stealing is a promising approach to constructing multithreaded program runtimes of parallel programming languages. This paper presents HERMES, an energy-efficient work-stealing language runtime. The key insight is that threads in a work-stealing environment -- thieves and victims - have varying impacts on the overall program running time, and a coordination of their execution ""tempo"" can lead to energy efficiency with minimal performance loss. The centerpiece of HERMES is two complementary algorithms to coordinate thread tempo: the workpath-sensitive algorithm determines tempo for each thread based on thief-victim relationships on the execution path, whereas the workload-sensitive algorithm selects appropriate tempo based on the size of work-stealing deques. We construct HERMES on top of Intel Cilk Plus's runtime, and implement tempo adjustment through standard Dynamic Voltage and Frequency Scaling (DVFS). Benchmarks running on HERMES demonstrate an average of 11-12% energy savings with an average of 3-4% performance loss through meter-based measurements over commercial CPUs.","dvfs, thread management, language runtimes, work stealing, energy efficiency",ASPLOS '14,,,
Journal Article,"Ribic H,Liu YD",Energy-Efficient Work-Stealing Language Runtimes,SIGPLAN Not.,2014,49.0,4,513–528,Association for Computing Machinery,"New York, NY, USA",,,2014-02,,0362-1340,https://doi.org/10.1145/2644865.2541971;http://dx.doi.org/10.1145/2644865.2541971,10.1145/2644865.2541971,"Work stealing is a promising approach to constructing multithreaded program runtimes of parallel programming languages. This paper presents HERMES, an energy-efficient work-stealing language runtime. The key insight is that threads in a work-stealing environment -- thieves and victims - have varying impacts on the overall program running time, and a coordination of their execution ""tempo"" can lead to energy efficiency with minimal performance loss. The centerpiece of HERMES is two complementary algorithms to coordinate thread tempo: the workpath-sensitive algorithm determines tempo for each thread based on thief-victim relationships on the execution path, whereas the workload-sensitive algorithm selects appropriate tempo based on the size of work-stealing deques. We construct HERMES on top of Intel Cilk Plus's runtime, and implement tempo adjustment through standard Dynamic Voltage and Frequency Scaling (DVFS). Benchmarks running on HERMES demonstrate an average of 11-12% energy savings with an average of 3-4% performance loss through meter-based measurements over commercial CPUs.","language runtimes, work stealing, thread management, dvfs, energy efficiency",,,,
Journal Article,"Ribic H,Liu YD",Energy-Efficient Work-Stealing Language Runtimes,SIGARCH Comput. Archit. News,2014,42.0,1,513–528,Association for Computing Machinery,"New York, NY, USA",,,2014-02,,0163-5964,https://doi.org/10.1145/2654822.2541971;http://dx.doi.org/10.1145/2654822.2541971,10.1145/2654822.2541971,"Work stealing is a promising approach to constructing multithreaded program runtimes of parallel programming languages. This paper presents HERMES, an energy-efficient work-stealing language runtime. The key insight is that threads in a work-stealing environment -- thieves and victims - have varying impacts on the overall program running time, and a coordination of their execution ""tempo"" can lead to energy efficiency with minimal performance loss. The centerpiece of HERMES is two complementary algorithms to coordinate thread tempo: the workpath-sensitive algorithm determines tempo for each thread based on thief-victim relationships on the execution path, whereas the workload-sensitive algorithm selects appropriate tempo based on the size of work-stealing deques. We construct HERMES on top of Intel Cilk Plus's runtime, and implement tempo adjustment through standard Dynamic Voltage and Frequency Scaling (DVFS). Benchmarks running on HERMES demonstrate an average of 11-12% energy savings with an average of 3-4% performance loss through meter-based measurements over commercial CPUs.","language runtimes, thread management, dvfs, energy efficiency, work stealing",,,,
Conference Paper,"Gudka K,Watson RN,Anderson J,Chisnall D,Davis B,Laurie B,Marinos I,Neumann PG,Richardson A",Clean Application Compartmentalization with SOAAP,,2015,,,1016–1031,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 22nd ACM SIGSAC Conference on Computer and Communications Security,"Denver, Colorado, USA",2015,9781450338325.0,,https://doi.org/10.1145/2810103.2813611;http://dx.doi.org/10.1145/2810103.2813611,10.1145/2810103.2813611,"Application compartmentalization, a vulnerability mitigation technique employed in programs such as OpenSSH and the Chromium web browser, decomposes software into isolated components to limit privileges leaked or otherwise available to attackers. However, compartmentalizing applications -- and maintaining that compartmentalization -- is hindered by ad hoc methodologies and significantly increased programming effort. In practice, programmers stumble through (rather than overtly reason about) compartmentalization spaces of possible decompositions, unknowingly trading off correctness, security, complexity, and performance. We present a new conceptual framework embodied in an LLVM-based tool: the Security-Oriented Analysis of Application Programs (SOAAP) that allows programmers to reason about compartmentalization using source-code annotations (compartmentalization hypotheses). We demonstrate considerable benefit when creating new compartmentalizations for complex applications, and analyze existing compartmentalized applications to discover design faults and maintenance issues arising from application evolution.","security, vulnerability mitigation, compartmentalization",CCS '15,,,
Conference Paper,"Yoder JW,Wirfs-Brock R,Washizaki H","QA to AQ Part Six: Being Agile at Quality ""Enabling and Infusing Quality""",,2016,,,,The Hillside Group,USA,Proceedings of the 23rd Conference on Pattern Languages of Programs,"Monticello, Illinois",2016,,,,,"To achieve quality systems and products, it is vital to enable and infuse quality work throughout the entire process, rather than piling it on at the end. Thus paying attention to when to this, how to do this, and who is involved can increase quality. This paper presents three patterns from the collection of patterns on being agile at quality: System Quality Specialist, Spread the Quality Workload, and Automate As You Go. System Quality Specialists can define, test, and implement system-quality characteristics that are complex or require specialized skills and expertise to get right. Spreading the Quality Workload throughout the development process keeps the team from being overly burdened with quality-related work at any point in time. Automating First enables teams to streamline their build and testing processes, eliminate tedious or mundane tasks, and allow more time for team members to focus on implementing and testing important system qualities.","testing, agile quality, agile software development, automate as you go, software quality, patterns, spread the quality workload, system qualities, Quality Assurance, system quality specialist, agile",PLoP '16,,,
Conference Paper,"Ricken M,Cartwright R",Test-First Java Concurrency for the Classroom,,2010,,,219–223,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 41st ACM Technical Symposium on Computer Science Education,"Milwaukee, Wisconsin, USA",2010,9781450300063.0,,https://doi.org/10.1145/1734263.1734340;http://dx.doi.org/10.1145/1734263.1734340,10.1145/1734263.1734340,"Concurrent programming is becoming more important due to the growing dominance of multi-core processors and the prevalence of graphical user interfaces (GUIs). To prepare students for the concurrent future, instructors have begun to address concurrency earlier in their curricula. Unfortunately, test-driven development, which enables students and practitioners to quickly develop reliable single-threaded programs, is not as effective in the domain of concurrent programming. This paper describes how ConcJUnit can simplify the task of writing unit tests for multi-threaded programs, and provides examples that can be used to introduce students to concurrent programming.","junit, cs education, software engineering, java, unit testing, concurrent programming, tools",SIGCSE '10,,,
Conference Paper,"Dong Y,Marwan S,Catete V,Price T,Barnes T",Defining Tinkering Behavior in Open-Ended Block-Based Programming Assignments,,2019,,,1204–1210,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 50th ACM Technical Symposium on Computer Science Education,"Minneapolis, MN, USA",2019,9781450358903.0,,https://doi.org/10.1145/3287324.3287437;http://dx.doi.org/10.1145/3287324.3287437,10.1145/3287324.3287437,"Tinkering has been shown to have a positive influence on students in open-ended making activities. Open-ended programming assignments in block-based programming resemble making activities in that both of them encourage students to tinker with tools to create their own solutions to achieve a goal. However, previous studies of tinkering in programming discussed tinkering as a broad, ambiguous term, and investigated only self-reported data. To our knowledge, no research has studied student tinkering behaviors while solving problems in block-based programming environments. In this position paper, we propose a definition for tinkering in block-based programming environments as a kind of behavior that students exhibit when testing, exploring, and struggling during problem-solving. We introduce three general categories of tinkering behaviors (test-based, prototype-based, and construction-based tinkering) derived from student data, and use case studies to demonstrate how students exhibited these behaviors in problem-solving. We created the definitions using a mixed-methods research design combining a literature review with data-driven insights from submissions of two open-ended programming assignments in iSnap, a block-based programming environment. We discuss the implication of each type of tinkering behavior for learning. Our study and results are the first in this domain to define tinkering based on student behaviors in a block-based programming environment.","tinkering, programming, novice, block-based",SIGCSE '19,,,
Conference Paper,"Dufour B,Goard C,Hendren L,de Moor O,Sittampalam G,Verbrugge C",Measuring the Dynamic Behaviour of AspectJ Programs,,2004,,,150–169,Association for Computing Machinery,"New York, NY, USA","Proceedings of the 19th Annual ACM SIGPLAN Conference on Object-Oriented Programming, Systems, Languages, and Applications","Vancouver, BC, Canada",2004,9781581138313.0,,https://doi.org/10.1145/1028976.1028990;http://dx.doi.org/10.1145/1028976.1028990,10.1145/1028976.1028990,"This paper proposes and implements a rigorous method for studying the dynamic behaviour of AspectJ programs. As part of this methodology several new metrics specific to AspectJ programs are proposed and tools for collecting the relevant metrics are presented. The major tools consist of: (1) a modified version of the AspectJ compiler that tags bytecode instructions with an indication of the cause of their generation, such as a particular feature of AspectJ; and (2) a modified version of the *J dynamic metrics collection tool which is composed of a JVMPI-based trace generator and an analyzer which propagates tags and computes the proposed metrics. This dynamic propagation is essential, and thus this paper contributes not only new metrics, but also non-trivial ways of computing them.We furthermore present a set of benchmarks that exercise a wide range of AspectJ's features, and the metrics that we measured on these benchmarks. The results provide guidance to AspectJ users on how to avoid efficiency pitfalls, to AspectJ implementors on promising areas for future optimization, and to tool builders on ways to understand the runtime behaviour of AspectJ.","aspect-oriented programming, program analysis, optimization, dynamic metrics, performance, AspectJ, java",OOPSLA '04,,,
Journal Article,"Dufour B,Goard C,Hendren L,de Moor O,Sittampalam G,Verbrugge C",Measuring the Dynamic Behaviour of AspectJ Programs,SIGPLAN Not.,2004,39.0,10,150–169,Association for Computing Machinery,"New York, NY, USA",,,2004-10,,0362-1340,https://doi.org/10.1145/1035292.1028990;http://dx.doi.org/10.1145/1035292.1028990,10.1145/1035292.1028990,"This paper proposes and implements a rigorous method for studying the dynamic behaviour of AspectJ programs. As part of this methodology several new metrics specific to AspectJ programs are proposed and tools for collecting the relevant metrics are presented. The major tools consist of: (1) a modified version of the AspectJ compiler that tags bytecode instructions with an indication of the cause of their generation, such as a particular feature of AspectJ; and (2) a modified version of the *J dynamic metrics collection tool which is composed of a JVMPI-based trace generator and an analyzer which propagates tags and computes the proposed metrics. This dynamic propagation is essential, and thus this paper contributes not only new metrics, but also non-trivial ways of computing them.We furthermore present a set of benchmarks that exercise a wide range of AspectJ's features, and the metrics that we measured on these benchmarks. The results provide guidance to AspectJ users on how to avoid efficiency pitfalls, to AspectJ implementors on promising areas for future optimization, and to tool builders on ways to understand the runtime behaviour of AspectJ.","java, optimization, program analysis, dynamic metrics, AspectJ, performance, aspect-oriented programming",,,,
Conference Paper,"McGee DR,Cohen PR,Wesson RM,Horman S","Comparing Paper and Tangible, Multimodal Tools",,2002,,,407–414,Association for Computing Machinery,"New York, NY, USA",Proceedings of the SIGCHI Conference on Human Factors in Computing Systems,"Minneapolis, Minnesota, USA",2002,9781581134537.0,,https://doi.org/10.1145/503376.503449;http://dx.doi.org/10.1145/503376.503449,10.1145/503376.503449,"In command posts, officers maintain situational awareness using paper maps, Post-it notes, and hand-written annotations. They do so because paper is robust to failure, it is portable, it offers a flexible means of capturing information, it has ultra-high resolution, and it readily supports face-to-face collaboration. We report herein on an evaluation comparing maps and Post-its with a tangible multimodal system called Rasa. Rasa augments these paper tools with sensors, enabling it to recognize the multimodal language (both written and spoken) that naturally occurs on them. In this study, we found that not only do users prefer Rasa to paper alone, they find it as easy or easier to use than paper tools. Moreover, Rasa introduces no discernible overhead in its operation other than error repair, yet grants the benefits inherent in digital systems. Finally, subjects confirmed that by combining physical and computational tools, Rasa is resistant to computational failure","mixed reality, invisible interfaces, tangible interfaces, multimodal interfaces, augmented reality",CHI '02,,,
Book,,WWW '19: The World Wide Web Conference,,2019,,,,Association for Computing Machinery,"New York, NY, USA",,"San Francisco, CA, USA",2019,9781450366748.0,,,,"It is our great pleasure to welcome you to The Web Conference 2019. The Web Conference is the premier venue focused on understanding the current state and the evolution of the Web through the lens of computer science, computational social science, economics, policy, and many other disciplines. The 2019 edition of the conference is a reflection point as we celebrate the 30th anniversary of the Web.",,,Proceedings,,
Journal Article,"Silva C,Masci P,Zhang Y,Jones P,Campos JC",A Use Error Taxonomy for Improving Human-Machine Interface Design in Medical Devices,SIGBED Rev.,2019,16.0,2,24–30,Association for Computing Machinery,"New York, NY, USA",,,2019-08,,,https://doi.org/10.1145/3357495.3357498;http://dx.doi.org/10.1145/3357495.3357498,10.1145/3357495.3357498,"Use error is one of the leading causes of medical device incidents. It is crucial for all stakeholders to have a unified means to better understand, classify, communicate, and prevent/avoid medical device use errors. In this paper, we present our ongoing work on developing a new use error taxonomy for medical devices that has the potential to enable fine-grained analysis of use errors and their root causes in system design. Our ultimate goal is to create a generic framework that can be used by medical device designers to better identify effective design solutions to mitigating use errors.","use error, medical devices, human-machine interface (HMI)",,,,
Conference Paper,"Carter J,Bouvier D,Cardell-Oliver R,Hamilton M,Kurkovsky S,Markham S,McClung OW,McDermott R,Riedesel C,Shi J,White S",Motivating All Our Students?,,2011,,,1–18,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 16th Annual Conference Reports on Innovation and Technology in Computer Science Education - Working Group Reports,"Darmstadt, Germany",2011,9781450311229.0,,https://doi.org/10.1145/2078856.2078858;http://dx.doi.org/10.1145/2078856.2078858,10.1145/2078856.2078858,"Academics expend a large amount of time and effort to sustain and enhance the motivation of undergraduate students. Typically based on a desire to ensure that all students achieve their full potential, approaches are based on an understanding that students who are highly motivated will learn more. Furthermore, institutional rewards accrue from effective use of academics' time, along with financial benefits associated with high levels of retention and progression. This working group report, based on practice in Europe, Australasia and North America, builds on previous work. It provides an updated and revised literature review, analyses a larger collection of survey data and has sought to triangulate earlier findings with qualitative data from practitioner interviews. The report covers established approaches in teaching, support and extra-curricular activities. It tracks emerging practice such as streamed and differentiated teaching, and research based and authentic learning. It also considers contemporary innovations in student activities. Finally it reports on a repository of tips and techniques which has been established to support faculty wishing to change or review current methods.","otivation, learning programming, differentiation in the classroom",ITiCSE-WGR '11,,,
Conference Paper,"Van Der Straeten R,D'Hondt M",Model Refactorings through Rule-Based Inconsistency Resolution,,2006,,,1210–1217,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 2006 ACM Symposium on Applied Computing,"Dijon, France",2006,9781595931085.0,,https://doi.org/10.1145/1141277.1141564;http://dx.doi.org/10.1145/1141277.1141564,10.1145/1141277.1141564,"The goal of model-driven engineering is to raise the level of abstraction by shifting the focus to models. As a result, complex software development activities move to the modelling level as well. One such activity is model refactoring, a technique for restructuring the models in order to improve some quality attributes of the models. As a first contribution of this paper, we argue and show that refactoring a model is enabled by inconsistency detection and resolution. Inconsistencies in or between models occur since models typically describe a software system from different viewpoints and on different levels of abstraction. A second contribution of this paper is rule-based inconsistency resolution, which enables reuse of different inconsistency resolutions across model refactorings and manages the flow of inconsistency resolution steps automatically.","rule-based systems, model refactoring, inconsistency management, description logics",SAC '06,,,
Conference Paper,"Laroque C,Klaas A,Fischer JH,Kuntze M","Fast Converging, Automated Experiment Runs for Material Flow Simulations Using Distributed Computing and Combined Metaheuristics",,2012,,,,Winter Simulation Conference,"Berlin, Germany",Proceedings of the Winter Simulation Conference,,2012,,,,,"The analysis of production systems using discrete, event-based simulation is wide spread and generally accepted as a decision support technology. It aims either at the comparison of competitive system designs or the identification of a ""best possible"" parameter configuration of a simulation model. Here, combinatorial techniques of simulation and optimization methods support the user in finding optimal solutions, but typically result in long computation times, which often prohibits a practical application in industry. This paper presents a fast converging procedure as a combination of heuristic approaches, namely Particle Swarm Optimization and Genetic Algorithm, within a material flow simulation to close this gap. Our integrated implementation allows automated, distributed simulation runs for practical, complex production systems. First results show the proof of concept with a reference model and demonstrate the benefits of combinatorial and parallel processing.",,WSC '12,,,
Journal Article,Neumann PG,Risks to the Public,SIGSOFT Softw. Eng. Notes,2019,44.0,4,5–10,Association for Computing Machinery,"New York, NY, USA",,,2019-12,,0163-5948,https://doi.org/10.1145/3364452.3364453;http://dx.doi.org/10.1145/3364452.3364453,10.1145/3364452.3364453,"RISKS items seem to be burgeoning, making it very difficult to devote detailed accounts in the six pages that our recent SEN sections are occupying. We try to emphasize those items here that have the most content relating to software engineer- ing and system engineering. Safety issues continue to recur, and security issues are always rampant. Fodder for this section continues to manifest itself.",,,,,
Conference Paper,"Loaiza CR,Moreno Rocha MA",Usability Study and Proposal to the RENAUT (Registro Nacional de Usuarios de Telefonía Móvil de México),,2010,,,62–67,Universidad Politécnica de San Luis Potosí,"San Luis Potosí, S.L.P, MEX",Proceedings of the 3rd Mexican Workshop on Human Computer Interaction,"San Luis Potosí, Mexico",2010,,,,,"The Mexican Registrar for Users of Mobile Phones, (Registro Nacional de Usuarios de Telefonia Móvil, or RENAUT) is a federal Mexican government programme created in order to built an official inventory with all the mobile phone lines in the country. As part of this project, a support web site has been created to facilitate the registration process to all citizens. However, the website has got a number of usability problems which could complicate enormously the registration process. This article presents a few proposals created using a User Centered Design approach in order to overcome this.","usability, mobile phones, national security, Mexico",MexIHC '10,,,
Conference Paper,"Chan W,Anderson RJ,Beame P,Jones DH,Notkin D,Warner WE",Decoupling Synchronization from Local Control for Efficient Symbolic Model Checking of Statecharts,,1999,,,142–151,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 21st International Conference on Software Engineering,"Los Angeles, California, USA",1999,9781581130744.0,,https://doi.org/10.1145/302405.302460;http://dx.doi.org/10.1145/302405.302460,10.1145/302405.302460,,"software specification, formal methods, statecharts, formal verification, fault tolerance, binary decision diagrams, symbolic model checking",ICSE '99,,,
Conference Paper,"Greenberg S,Boring S,Vermeulen J,Dostal J",Dark Patterns in Proxemic Interactions: A Critical Perspective,,2014,,,523–532,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 2014 Conference on Designing Interactive Systems,"Vancouver, BC, Canada",2014,9781450329026.0,,https://doi.org/10.1145/2598510.2598541;http://dx.doi.org/10.1145/2598510.2598541,10.1145/2598510.2598541,"Proxemics theory explains peoples' use of interpersonal distances to mediate their social interactions with others. Within Ubicomp, proxemic interaction researchers argue that people have a similar social understanding of their spatial relations with nearby digital devices, which can be exploited to better facilitate seamless and natural interactions. To do so, both people and devices are tracked to determine their spatial relationships. While interest in proxemic interactions has increased over the last few years, it also has a dark side: knowledge of proxemics may (and likely will) be easily exploited to the detriment of the user. In this paper, we offer a critical perspective on proxemic interactions in the form of dark patterns: ways proxemic interactions can be misused. We discuss a series of these patterns and describe how they apply to these types of interactions. In addition, we identify several root problems that underlie these patterns and discuss potential solutions that could lower their harmfulness.","dark patterns, anti-patterns, proxemic interactions",DIS '14,,,
Journal Article,"Tosch E,Bakshy E,Berger ED,Jensen DD,Moss JE",PlanAlyzer: Assessing Threats to the Validity of Online Experiments,Proc. ACM Program. Lang.,2019,3.0,OOPSLA,,Association for Computing Machinery,"New York, NY, USA",,,2019-10,,,https://doi.org/10.1145/3360608;http://dx.doi.org/10.1145/3360608,10.1145/3360608,"Online experiments have become a ubiquitous aspect of design and engineering processes within Internet firms. As the scale of experiments has grown, so has the complexity of their design and implementation. In response, firms have developed software frameworks for designing and deploying online experiments. Ensuring that experiments in these frameworks are correctly designed and that their results are trustworthy---referred to as internal validity---can be difficult. Currently, verifying internal validity requires manual inspection by someone with substantial expertise in experimental design. We present the first approach for statically checking the internal validity of online experiments. Our checks are based on well-known problems that arise in experimental design and causal inference. Our analyses target PlanOut, a widely deployed, open-source experimentation framework that uses a domain-specific language to specify and run complex experiments. We have built a tool called PlanAlyzer that checks PlanOut programs for a variety of threats to internal validity, including failures of randomization, treatment assignment, and causal sufficiency. PlanAlyzer uses its analyses to automatically generate contrasts, a key type of information required to perform valid statistical analyses over the results of these experiments. We demonstrate PlanAlyzer's utility on a corpus of PlanOut scripts deployed in production at Facebook, and we evaluate its ability to identify threats to validity on a mutated subset of this corpus. PlanAlyzer has both precision and recall of 92% on the mutated corpus, and 82% of the contrasts it generates match hand-specified data.","Threats to Validity, Online Experiments, Experimental Design",,,,
Journal Article,"Madsen M,Lhoták O,Tip F",A Model for Reasoning about JavaScript Promises,Proc. ACM Program. Lang.,2017,1.0,OOPSLA,,Association for Computing Machinery,"New York, NY, USA",,,2017-10,,,https://doi.org/10.1145/3133910;http://dx.doi.org/10.1145/3133910,10.1145/3133910,"In JavaScript programs, asynchrony arises in situations such as web-based user-interfaces, communicating with servers through HTTP requests, and non-blocking I/O. Event-based programming is the most popular approach for managing asynchrony, but suffers from problems such as lost events and event races, and results in code that is hard to understand and debug. Recently, ECMAScript 6 has added support for promises, an alternative mechanism for managing asynchrony that enables programmers to chain asynchronous computations while supporting proper error handling. However, promises are complex and error-prone in their own right, so programmers would benefit from techniques that can reason about the correctness of promise-based code. Since the ECMAScript 6 specification is informal and intended for implementers of JavaScript engines, it does not provide a suitable basis for formal reasoning. This paper presents λp, a core calculus that captures the essence of ECMAScript 6 promises. Based on λp, we introduce the promise graph, a program representation that can assist programmers with debugging of promise-based code. We then report on a case study in which we investigate how the promise graph can be helpful for debugging errors related to promises in code fragments posted to the StackOverflow website.","Promises, Promise Graph, JavaScript, Formal Semantics, EcmaScript 6",,,,
Journal Article,"Leite L,Rocha C,Kon F,Milojicic D,Meirelles P",A Survey of DevOps Concepts and Challenges,ACM Comput. Surv.,2019,52.0,6,,Association for Computing Machinery,"New York, NY, USA",,,2019-11,,0360-0300,https://doi.org/10.1145/3359981;http://dx.doi.org/10.1145/3359981,10.1145/3359981,"DevOpsis a collaborative and multidisciplinary organizational effort to automate continuous delivery of new software updates while guaranteeing their correctness and reliability. The present survey investigates and discusses DevOps challenges from the perspective of engineers, managers, and researchers. We review the literature and develop a DevOps conceptual map, correlating the DevOps automation tools with these concepts. We then discuss their practical implications for engineers, managers, and researchers. Finally, we critically explore some of the most relevant DevOps challenges reported by the literature.","release process, configuration management, and build process, continuous (delivery, deployment, integration), versioning, DevOps",,,,
Conference Paper,Bondi AB,Predicting the Time to Migrate Into Deadlock Using a Discrete Time Markov Chain,,2018,,,177–182,Association for Computing Machinery,"New York, NY, USA",Companion of the 2018 ACM/SPEC International Conference on Performance Engineering,"Berlin, Germany",2018,9781450356299.0,,https://doi.org/10.1145/3185768.3186403;http://dx.doi.org/10.1145/3185768.3186403,10.1145/3185768.3186403,"When processes join a common FCFS queue to acquire or release resources in an object pool of fixed size, deadlock occurs if the process at the head of the queue wishes to acquire a resource when the pool is empty, even if a process wishing to relinquish a resource is queued behind. We describe a state machine representation of this problem. We use the representation to develop a discrete time Markov chain analysis to identify the load conditions under which deadlock is most likely to occur and how soon it is likely to occur. We show that deadlock occurs almost surely regardless of the load, and that the time to the onset of deadlock depends on combinations of the request rate for resources in the pool, the average holding time of the resources, and the size of the pool. Calculations corroborate the intuition that deadlock will occur sooner at heavy loads or when the resource pool is small. A connection will be made between this problem and the problem of random walks with a single absorbing and a single reflecting barrier.","random walks, Markov chain analysis, deadlock prediction and prevention",ICPE '18,,,
Journal Article,Churchill EF,Sugared Puppy-Dog Tails: Gender and Design,Interactions,2010,17.0,2,52–56,Association for Computing Machinery,"New York, NY, USA",,,2010-03,,1072-5520,https://doi.org/10.1145/1699775.1699787;http://dx.doi.org/10.1145/1699775.1699787,10.1145/1699775.1699787,,,,,,
Journal Article,Feldman S,"Quality Assurance: Much More than Testing: Good QA is Not Only about Technology, but Also Methods and Approaches",Queue,2005,3.0,1,26–29,Association for Computing Machinery,"New York, NY, USA",,,2005-02,,1542-7730,https://doi.org/10.1145/1046931.1046943;http://dx.doi.org/10.1145/1046931.1046943,10.1145/1046931.1046943,"Quality assurance isn’t just testing, or analysis, or wishful thinking. Although it can be boring, difficult, and tedious, QA is nonetheless essential. Ensuring that a system will work when delivered requires much planning and discipline. Convincing others that the system will function properly requires even more careful and thoughtful effort. QA is performed through all stages of the project, not just slapped on at the end. It is a way of life.",,,,,
Conference Paper,"Le Blond S,Zhang C,Legout A,Ross K,Dabbous W",I Know Where You Are and What You Are Sharing: Exploiting P2P Communications to Invade Users' Privacy,,2011,,,45–60,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 2011 ACM SIGCOMM Conference on Internet Measurement Conference,"Berlin, Germany",2011,9781450310130.0,,https://doi.org/10.1145/2068816.2068822;http://dx.doi.org/10.1145/2068816.2068822,10.1145/2068816.2068822,"In this paper, we show how to exploit real-time communication applications to determine the IP address of a targeted user. We focus our study on Skype, although other real-time communication applications may have similar privacy issues. We first design a scheme that calls an identified-targeted user inconspicuously to find his IP address, which can be done even if he is behind a NAT. By calling the user periodically, we can then observe the mobility of the user. We show how to scale the scheme to observe the mobility patterns of tens of thousands of users. We also consider the linkability threat, in which the identified user is linked to his Internet usage. We illustrate this threat by combining Skype and BitTorrent to show that it is possible to determine the filesharing usage of identified users. We devise a scheme based on the identification field of the IP datagrams to verify with high accuracy whether the identified user is participating in specific torrents. We conclude that any Internet user can leverage Skype, and potentially other real-time communication systems, to observe the mobility and filesharing usage of tens of millions of identified users.","mobility, file sharing, skype, privacy",IMC '11,,,
Journal Article,"Davern MJ,Wilkin CL",Evolving Innovations through Design and Use,Commun. ACM,2008,51.0,12,133–137,Association for Computing Machinery,"New York, NY, USA",,,2008-12,,0001-0782,https://doi.org/10.1145/1409360.1409390;http://dx.doi.org/10.1145/1409360.1409390,10.1145/1409360.1409390,,,,,,
Conference Paper,Karlsson F,A Wiki-Based Approach to Method Tailoring,,2008,,,13–22,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 3rd International Conference on the Pragmatic Web: Innovating the Interactive Society,"Uppsala, Sweden",2008,9781605583549.0,,https://doi.org/10.1145/1479190.1479193;http://dx.doi.org/10.1145/1479190.1479193,10.1145/1479190.1479193,"Tailoring systems development methods is a challenge. Both the research of method engineering and method-in-action have put much effort into this issue. State-of-the art Computer-Aided Method Engineering tools for situational method engineering often requires specific competences in meta modelling languages. Together with the tool investments they often become heavy weight solutions for small systems development companies that seek method tailoring support. In this paper a wiki-based approach to method tailoring, the Wiki Method Tool (WMT), has been evaluated during two systems development projects. The evaluation of this light weight tool has been carried out in a small systems development company in Sweden. The evaluation has been anchored in Activity Theory to focus on the collaborative actions on the situational methods using the WMT, such as changing work descriptions and templates. The WMT itself contributed with data about each documented change -- what the change looked like, by whom, and when it was made. Based on these data and subsequent interviews, we report on lessons learned and can conclude that all team members have contributed to the situational methods. It also means that they took a shared responsibility for the role as method engineer.","method rationale, wiki, evolutionary method engineering, method components, dynamic method tailoring",ICPW '08,,,
Journal Article,Neumann PG,Risks to the Public,SIGSOFT Softw. Eng. Notes,2016,41.0,4,18–24,Association for Computing Machinery,"New York, NY, USA",,,2016-08,,0163-5948,https://doi.org/10.1145/2967307.2967310;http://dx.doi.org/10.1145/2967307.2967310,10.1145/2967307.2967310,,,,,,
Journal Article,"Loughry J,Umphress DA",Information Leakage from Optical Emanations,ACM Trans. Inf. Syst. Secur.,2002,5.0,3,262–289,Association for Computing Machinery,"New York, NY, USA",,,2002-08,,1094-9224,https://doi.org/10.1145/545186.545189;http://dx.doi.org/10.1145/545186.545189,10.1145/545186.545189,"A previously unknown form of compromising emanations has been discovered. LED status indicators on data communication equipment, under certain conditions, are shown to carry a modulated optical signal that is significantly correlated with information being processed by the device. Physical access is not required; the attacker gains access to all data going through the device, including plaintext in the case of data encryption systems. Experiments show that it is possible to intercept data under realistic conditions at a considerable distance. Many different sorts of devices, including modems and Internet Protocol routers, were found to be vulnerable. A taxonomy of compromising optical emanations is developed, and design changes are described that will successfully block this kind of ""Optical Tempest"" attack.","COMSEC, EMSEC, fiber optics, communication, light emitting diode (LED), information displays, covert channel, compromising emanations, SIGINT, encryption, TEMPEST, COMINT",,,,
Conference Paper,"Lee J,Kim H,Park J,Shin I,Son S",Pride and Prejudice in Progressive Web Apps: Abusing Native App-like Features in Web Applications,,2018,,,1731–1746,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 2018 ACM SIGSAC Conference on Computer and Communications Security,"Toronto, Canada",2018,9781450356930.0,,https://doi.org/10.1145/3243734.3243867;http://dx.doi.org/10.1145/3243734.3243867,10.1145/3243734.3243867,"Progressive Web App (PWA) is a new generation of Web application designed to provide native app-like browsing experiences even when a browser is offline. PWAs make full use of new HTML5 features which include push notification, cache, and service worker to provide short-latency and rich Web browsing experiences. We conduct the first systematic study of the security and privacy aspects unique to PWAs. We identify security flaws in main browsers as well as design flaws in popular third-party push services, that exacerbate the phishing risk. We introduce a new side-channel attack that infers the victim's history of visited PWAs. The proposed attack exploits the offline browsing feature of PWAs using a cache. We demonstrate a cryptocurrency mining attack which abuses service workers. Defenses and recommendations to mitigate the identified security and privacy risks are suggested with in-depth understanding.","progressive web application, history sniffing, cryptocurrency mining, web push, phishing",CCS '18,,,
Journal Article,"Lilja DJ,Mirandola R,Sachs K",Paper Abstracts of the 2nd International Conferernce on Performance Engineering (ICPE 2011),SIGSOFT Softw. Eng. Notes,2011,36.0,5,36–53,Association for Computing Machinery,"New York, NY, USA",,,2011-09,,0163-5948,https://doi.org/10.1145/2020976.2069288;http://dx.doi.org/10.1145/2020976.2069288,10.1145/2020976.2069288,"Foreword This issue of SEN contains the abstracts of the papers, which were presented on the Second Joint WOSP/SIPEW International Conference (ICPE 2011), held in Karlsruhe, Germany, March 14-16, 2011, now established as a regular event known as ACM/SPEC International Conference on Performance Engineering (ICPE). The primary goal of this conference series is to bridge the gap between theory and practice in the field of computer systems performance engineering by providing a forum for sharing ideas and experiences between industry and academia. This years conference brought together researchers and industry practitioners to share and present their experiences, discuss challenges, and report on both state-of-the-art research and work-in-progress on performance engineering of software and systems, including performance measurement, modeling, benchmark design, and run-time performance management. The ICPE gives researchers and practitioners a unique opportunity to share their perspectives with others interested in the various aspects of computer systems performance engineering. The call for papers attracted 63 research and 24 industrial paper submissions from Europe, Asia, Africa, and North America. The program committees accepted 19 full research papers and 7 short papers together with 13 industrial papers. These papers cover a variety of topics, including performance modeling and techniques and measurement and benchmarking strategies for adaptive systems, power management, virtualized environments, and large-scale and distributed systems. We are confident that you will find the abstracts stimulating and that they will provide you with many new ideas and insights. The full paper are available at the ACM Digital Library. David J. Lilja: Program Co-Chair - Research Track Raffaela Mirandola: Program Co-Chair - Research Track Kai Sachs: Program Co-Chair - Industrial Track",,,,,
Journal Article,"Khan MM,Le HK,Ahmadi H,Abdelzaher TF,Han J",Troubleshooting Interactive Complexity Bugs in Wireless Sensor Networks Using Data Mining Techniques,ACM Trans. Sen. Netw.,2014,10.0,2,,Association for Computing Machinery,"New York, NY, USA",,,2014-01,,1550-4859,https://doi.org/10.1145/2530290;http://dx.doi.org/10.1145/2530290,10.1145/2530290,"This article presents a tool for uncovering bugs due to interactive complexity in networked sensing applications. Such bugs are not localized to one component that is faulty, but rather result from complex and unexpected interactions between multiple often individually nonfaulty components. Moreover, the manifestations of these bugs are often not repeatable, making them particularly hard to find, as the particular sequence of events that invokes the bug may not be easy to reconstruct. Because of the distributed nature of failure scenarios, our tool looks for sequences of events that may be responsible for faulty behavior, as opposed to localized bugs such as a bad pointer in a module. We identified several challenges in applying discriminative sequence mining for root cause analysis when the system fails to perform as expected and presented our solutions to those challenges. We also present two alternative schemes, namely, two-stage mining and the progressive discriminative sequence mining to address the scalability challenge. An extensible framework is developed where a front-end collects runtime data logs of the system being debugged and an offline back-end uses frequent discriminative pattern mining to uncover likely causes of failure. We provided several case studies where we applied our tool successfully to troubleshoot the cause of the problem. We uncovered a kernel-level race condition bug in the LiteOS operating system and a protocol design bug in the directed diffusion protocol. We also presented a case study of debugging a multichannel MAC protocol that was found to exhibit corner cases of poor performance (worse than single-channel MAC). The tool helped to uncover event sequences that lead to a highly degraded mode of operation. Fixing the problem significantly improved the performance of the protocol. We also evaluated the extensions presented in this article. Finally, we provided a detailed analysis of tool overhead in terms of memory requirements and impact on the running application.","Distributed protocol debugging, wireless sensor networks",,,,
Book Chapter,,Expert Modeling in OWL,,2020,,,,Association for Computing Machinery,"New York, NY, USA",,,2020,9781450376174.0,,https://doi.org/10.1145/3382097.3382114;http://dx.doi.org/10.1145/3382097.3382114,10.1145/3382097.3382114,"Enterprises have made amazing advances by taking advantage of data about their business to provide predictions and understanding of their customers, markets, and products. But as the world of business becomes more interconnected and global, enterprise data is no long a monolith; it is just a part of a vast web of data. Managing data on a world-wide scale is a key capability for any business today.The Semantic Web treats data as a distributed resource on the scale of the World Wide Web, and incorporates features to address the challenges of massive data distribution as part of its basic design. The aim of the first two editions was to motivate the Semantic Web technology stack from end-to-end; to describe not only what the Semantic Web standards are and how they work, but also what their goals are and why they were designed as they are. It tells a coherent story from beginning to end of how the standards work to manage a world-wide distributed web of knowledge in a meaningful way.The third edition builds on this foundation to bring Semantic Web practice to enterprise. Fabien Gandon joins Dean Allemang and Jim Hendler, bringing with him years of experience in global linked data, to open up the story to a modern view of global linked data. While the overall story is the same, the examples have been brought up to date and applied in a modern setting, where enterprise and global data come together as a living, linked network of data. Also included with the third edition, all of the data sets and queries are available online for study and experimentation at data.world/swwo.",,,,"Semantic Web for the Working Ontologist: Effective Modeling for Linked Data, RDFS, and OWL",
Conference Paper,"Zhao Y,Shi J,Zheng K,Wang H,Lin H,Shao L",Allocation Wall: A Limiting Factor of Java Applications on Emerging Multi-Core Platforms,,2009,,,361–376,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 24th ACM SIGPLAN Conference on Object Oriented Programming Systems Languages and Applications,"Orlando, Florida, USA",2009,9781605587660.0,,https://doi.org/10.1145/1640089.1640116;http://dx.doi.org/10.1145/1640089.1640116,10.1145/1640089.1640116,"Multi-core processors are widely used in computer systems. As the performance of microprocessors greatly exceeds that of memory, the memory wall becomes a limiting factor. It is important to understand how the large disparity of speed between processor and memory influences the performance and scalability of Java applications on emerging multi-core platforms.In this paper, we studied two popular Java benchmarks, SPECjbb2005 and SPECjvm2008, on multi-core platforms including Intel Clovertown and AMD Phenom. We focus on the ""partially scalable"" benchmark programs. With smaller number of CPU cores these programs scale perfectly, but when more cores and software threads are used, the slope of the scalability curve degrades dramatically.We identified a strong correlation between scalability, object allocation rate and memory bus write traffic in our experiments with our partially scalable programs. We find that these applications allocate large amounts of memory and consume almost all the memory write bandwidth in our hardware platforms. Because the write bandwidth is so limited, we propose the following hypothesis: the scalability and performance is limited by the object allocation on emerging multi-core platforms for those objects-allocation intensive Java applications, as if these applications are running into an ""allocation wall"".In order to verify this hypothesis, several experiments are performed, including measuring key architecture level metrics, composing a micro-benchmark program, and studying the effect of modifying some of the ""partially scalable"" programs. All the experiments strongly suggest the existence of the allocation wall.","allocation, java, scalability",OOPSLA '09,,,
Journal Article,"Zhao Y,Shi J,Zheng K,Wang H,Lin H,Shao L",Allocation Wall: A Limiting Factor of Java Applications on Emerging Multi-Core Platforms,SIGPLAN Not.,2009,44.0,10,361–376,Association for Computing Machinery,"New York, NY, USA",,,2009-10,,0362-1340,https://doi.org/10.1145/1639949.1640116;http://dx.doi.org/10.1145/1639949.1640116,10.1145/1639949.1640116,"Multi-core processors are widely used in computer systems. As the performance of microprocessors greatly exceeds that of memory, the memory wall becomes a limiting factor. It is important to understand how the large disparity of speed between processor and memory influences the performance and scalability of Java applications on emerging multi-core platforms.In this paper, we studied two popular Java benchmarks, SPECjbb2005 and SPECjvm2008, on multi-core platforms including Intel Clovertown and AMD Phenom. We focus on the ""partially scalable"" benchmark programs. With smaller number of CPU cores these programs scale perfectly, but when more cores and software threads are used, the slope of the scalability curve degrades dramatically.We identified a strong correlation between scalability, object allocation rate and memory bus write traffic in our experiments with our partially scalable programs. We find that these applications allocate large amounts of memory and consume almost all the memory write bandwidth in our hardware platforms. Because the write bandwidth is so limited, we propose the following hypothesis: the scalability and performance is limited by the object allocation on emerging multi-core platforms for those objects-allocation intensive Java applications, as if these applications are running into an ""allocation wall"".In order to verify this hypothesis, several experiments are performed, including measuring key architecture level metrics, composing a micro-benchmark program, and studying the effect of modifying some of the ""partially scalable"" programs. All the experiments strongly suggest the existence of the allocation wall.","scalability, java, allocation",,,,
Conference Paper,Smith RJ,Technology-Independent Circuit Layout,,1983,,,390–393,IEEE Press,"Miami Beach, Florida, USA",Proceedings of the 20th Design Automation Conference,,1983,9780818600265.0,,,,"As physical implementation of electronic circuits has evolved from discrete component printed circuit boards to very large scale integrated circuits, there has been increasing economic pressure to automate the layout design process. Early efforts to use computer-based layout focused on relatively simple PCB's, while recently developed layout tools address the demands of quite complex IC layout tasks. It has become evident that it is economically impractical to develop comprehensive layout systems that focus on a single circuit style or technology. Hence, increasing attention has been given to layout tools that are relatively technology independent. That is, they may be successfully applied to a diversity of layout problem styles having different floorplans, design rules, and component attributes.We would prefer to develop and support design tools that are “technology independent”; in the sense that for both printed circuit boards and integrated circuits having these diverse characteristics the software basis for these tools is common.",,DAC '83,,,
Journal Article,Neumann PG,Risks to the Public,SIGSOFT Softw. Eng. Notes,2013,38.0,3,21–28,Association for Computing Machinery,"New York, NY, USA",,,2013-05,,0163-5948,https://doi.org/10.1145/2464526.2464529;http://dx.doi.org/10.1145/2464526.2464529,10.1145/2464526.2464529,,,,,,
Conference Paper,"Lhoták O,Hendren L",Run-Time Evaluation of Opportunities for Object Inlining in Java,,2002,,,175–184,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 2002 Joint ACM-ISCOPE Conference on Java Grande,"Seattle, Washington, USA",2002,9781581135992.0,,https://doi.org/10.1145/583810.583830;http://dx.doi.org/10.1145/583810.583830,10.1145/583810.583830,"Object-oriented languages, such as Java, encourage the use of many small objects linked together by field references, instead of a few monolithic structures. While this practice is beneficial from a program design perspective, it can slow down program execution by incurring many pointer indirections. One solution to this problem is object inlining: when the compiler can safely do so, it fuses small objects together, thus removing the reads/writes to the removed field, saving the memory needed to store the field and object header, and reducing the number of object allocations.The objective of this paper is to measure the potential for object inlining by studying the run-time behaviour of a comprehensive set of Java programs. We study the traces of program executions in order to determine which fields behave like inlinable fields. Since we are using dynamic information instead of a static analysis, our results give an upper bound on what could be achieved via a static compiler-based approach. Our experimental results measure the potential improvements attainable with object inlining, including reductions in the numbers of field reads and writes, and reduced memory usage.Our study shows that some Java programs can benefit significantly from object inlining, with close to a 10% speedup. Somewhat to our surprise, our study found one case, the db benchmark, where the most important inlinable field was the result of unusual program design, and fixing this small flaw led to both better performance and clearer program design. However, the opportunities for object inlining are highly dependent on the individual program being considered, and are in many cases very limited. Furthermore, fields that are inlinable also have properties that make them potential candidates for other optimizations such as removing redundant memory accesses. The memory savings possible through object inlining are moderate.","Java, object inlining, compilers, optimization",JGI '02,,,
Conference Paper,Letouzey JL,The SQALE Method for Evaluating Technical Debt,,2012,,,31–36,IEEE Press,"Zurich, Switzerland",Proceedings of the Third International Workshop on Managing Technical Debt,,2012,9781467317498.0,,,,This paper presents the SQALE (Software Quality Assessment Based on Lifecycle Expectations) method. We describe its Quality Model and Analysis Model which is used to estimate the Quality and the Technical Debt of an application source code. We provide recommendations and guidelines for using the SQALE indicators in order to analyse the structure and the impact of the Technical Debt.,"technical debt, source code, quality model, quality, analysis model, SQALE",MTD '12,,,
Journal Article,"Lozi JP,David F,Thomas G,Lawall J,Muller G",Fast and Portable Locking for Multicore Architectures,ACM Trans. Comput. Syst.,2016,33.0,4,,Association for Computing Machinery,"New York, NY, USA",,,2016-01,,0734-2071,https://doi.org/10.1145/2845079;http://dx.doi.org/10.1145/2845079,10.1145/2845079,"The scalability of multithreaded applications on current multicore systems is hampered by the performance of lock algorithms, due to the costs of access contention and cache misses. The main contribution presented in this article is a new locking technique, Remote Core Locking (RCL), that aims to accelerate the execution of critical sections in legacy applications on multicore architectures. The idea of RCL is to replace lock acquisitions by optimized remote procedure calls to a dedicated server hardware thread. RCL limits the performance collapse observed with other lock algorithms when many threads try to acquire a lock concurrently and removes the need to transfer lock-protected shared data to the hardware thread acquiring the lock, because such data can typically remain in the server’s cache. Other contributions presented in this article include a profiler that identifies the locks that are the bottlenecks in multithreaded applications and that can thus benefit from RCL, and a reengineering tool that transforms POSIX lock acquisitions into RCL locks.Eighteen applications were used to evaluate RCL: the nine applications of the SPLASH-2 benchmark suite, the seven applications of the Phoenix 2 benchmark suite, Memcached, and Berkeley DB with a TPC-C client. Eight of these applications are unable to scale because of locks and benefit from RCL on an ×86 machine with four AMD Opteron processors and 48 hardware threads. By using RCL instead of Linux POSIX locks, performance is improved by up to 2.5 times on Memcached, and up to 11.6 times on Berkeley DB with the TPC-C client. On a SPARC machine with two Sun Ultrasparc T2+ processors and 128 hardware threads, three applications benefit from RCL. In particular, performance is improved by up to 1.3 times with respect to Solaris POSIX locks on Memcached, and up to 7.9 times on Berkeley DB with the TPC-C client.","locks, locality, synchronization, busy-waiting, reengineering, RPC, profiling, memory contention, Multicore",,,,
Journal Article,"McDaniel TL,Panchanathan S",A Visio-Haptic Wearable System for Assisting Individuals Who Are Blind,SIGACCESS Access. Comput.,2006,,86,12–15,Association for Computing Machinery,"New York, NY, USA",,,2006-09,,1558-2337,https://doi.org/10.1145/1196148.1196151;http://dx.doi.org/10.1145/1196148.1196151,10.1145/1196148.1196151,"Computer vision algorithms for visio-haptic information analysis, i.e., the conversion of visual data into haptic (tangible) features, can be utilized in wearable assistive devices for individuals who are blind. Touch is an important modality for individuals who are blind, but it is limited to the extent of one's reach. By estimating how an object feels from its visual image, we are able to overcome this limitation. This paper proposes a wearable assistive device to estimate haptic features from visual data to enable users to feel objects from a distance.",,,,,
Conference Paper,"Ravindran K,Adiththan A,Rabby M,Jose J",Autonomic Management of Replica Voting Based Data Collection Systems in Malicious Environments,,2015,,,19–28,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 11th ACM Symposium on QoS and Security for Wireless and Mobile Networks,"Cancun, Mexico",2015,9781450337571.0,,https://doi.org/10.1145/2815317.2815319;http://dx.doi.org/10.1145/2815317.2815319,10.1145/2815317.2815319,"We describe a model-based approach to QoS management in a replica voting based data collection system. The voting among replicated data collection devices achieves trusted data delivery to the end-user in a hostile environment: such as data corruptions by malicious devices and security & bandwidth attacks on the wireless data paths. How often an accurate data is delivered to the user in a timely manner depicts the QoS of data collection system. Aided by a computational model of the voting system, a situational assessment module macroscopically controls the voting system core based on the sensed external events. Our goal is the optimal use of system resources while enforcing an acceptable QoS. The paper describes the management methods for autonomic control of the degree of device replication and/or the algorithmic parameters (e.g., wireless bandwidth allocation) in response to the dynamically changing QoS needs and environment conditions. Our management methods are reusable across different systems, which lowers the software costs in the development of such complex systems.","fault-injection & modeling, situation-based control, sensor replication, adaptive fault-tolerance",Q2SWinet '15,,,
Journal Article,Neumann PG,Illustrative Risks to the Public in the Use of Computer Systems and Related Technology,SIGSOFT Softw. Eng. Notes,1994,19.0,1,16–29,Association for Computing Machinery,"New York, NY, USA",,,1994-01,,0163-5948,https://doi.org/10.1145/181610.181612;http://dx.doi.org/10.1145/181610.181612,10.1145/181610.181612,,,,,,
Conference Paper,"Medeiros H,Vilain P,Pereira VC",Reducing the Execution Time of Unit Tests of Smart Contracts in Blockchain Platforms,,2019,,,,Association for Computing Machinery,"New York, NY, USA",Proceedings of the XV Brazilian Symposium on Information Systems,"Aracaju, Brazil",2019,9781450372374.0,,https://doi.org/10.1145/3330204.3330225;http://dx.doi.org/10.1145/3330204.3330225,10.1145/3330204.3330225,"Smart Contracts are software code that resides within a blockchain, using its infrastructure as an advantage and guarantee of execution. Blockchain and smart contracts are enabling new business models and standards to information systems. However, a smart contract needs to be well tested before to be published in a blockchain, since it cannot be changed after being deployed. The execution time to deploy smart contracts and run their tests is considerable because all transactions must be mined before being added to a new block. This work proposes an approach to reuse the execution of the deployment and the setup of unit test in smart contracts to reduce the execution time of these tests. Experiments have shown a large reduction in the execution time of smart contract unit tests, without breaking the principle of test independency.","Test Automation, Smart Contracts Testing",SBSI'19,,,
Journal Article,"Lara J,Guerra E",Refactoring Multi-Level Models,ACM Trans. Softw. Eng. Methodol.,2018,27.0,4,,Association for Computing Machinery,"New York, NY, USA",,,2018-11,,1049-331X,https://doi.org/10.1145/3280985;http://dx.doi.org/10.1145/3280985,10.1145/3280985,"Multi-level modelling promotes flexibility in modelling by enabling the use of several meta-levels instead of just two, as is the case in mainstream two-level modelling approaches. While this approach leads to simpler models for some scenarios, it introduces an additional degree of freedom as designers can decide the meta-level where an element should reside, having to ascertain the suitability of such decisions.In this respect, model refactorings have been successfully applied in the context of two-level modelling to rearrange the elements of a model while preserving its meaning. Following this idea, we propose a catalogue of 17 novel refactorings specific to multi-level models. Their objective is to help designers in rearranging elements across and within meta-levels and exploring the consequences. In this article, we detail each refactoring in the catalogue, show a classification across different dimensions, and describe the support we provide in our MetaDepth tool. We present two experiments to assess two aspects of our refactorings. The first one validates the predicted semantic side effects of the refactorings on the basis of more than 210.000 refactoring applications. The second one measures the impact of refactorings on three quality attributes of multi-level models.","multi-level modelling, model refactoring, MetaDepth, Meta-modelling",,,,
Conference Paper,Olsen DR,Evaluating User Interface Systems Research,,2007,,,251–258,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 20th Annual ACM Symposium on User Interface Software and Technology,"Newport, Rhode Island, USA",2007,9781595936790.0,,https://doi.org/10.1145/1294211.1294256;http://dx.doi.org/10.1145/1294211.1294256,10.1145/1294211.1294256,"The development of user interface systems has languished with the stability of desktop computing. Future systems, however, that are off-the-desktop, nomadic or physical in nature will involve new devices and new software systems for creating interactive applications. Simple usability testing is not adequate for evaluating complex systems. The problems with evaluating systems work are explored and a set of criteria for evaluating new UI systems work is presented.",user interface systems evaluation,UIST '07,,,
Conference Paper,"Heilman S,Jagannath A,Naor A",Solution of the Propeller Conjecture in R3,,2012,,,269–276,Association for Computing Machinery,"New York, NY, USA",Proceedings of the Forty-Fourth Annual ACM Symposium on Theory of Computing,"New York, New York, USA",2012,9781450312455.0,,https://doi.org/10.1145/2213977.2214003;http://dx.doi.org/10.1145/2213977.2214003,10.1145/2213977.2214003,"It is shown that every measurable partition A1,..., Ak of R3 satisfies: ∑i=1k|intAi xe-1/2|x|22dx|22≤ 9π2. Let P1,P2,P3 be the partition of R2 into 120o sectors centered at the origin. The bound (1) is sharp, with equality holding if Ai=Pi x R for i∈ 1,2,3 and Ai=∅ for i∈ 4,...,k. This settles positively the 3-dimensional Propeller Conjecture of Khot and Naor (FOCS 2008). The proof of (1) reduces the problem to a finite set of numerical inequalities which are then verified with full rigor in a computer-assisted fashion. The main consequence (and motivation) of (1) is complexity-theoretic: the Unique Games hardness threshold of the Kernel Clustering problem with 4 x 4 centered and spherical hypothesis matrix equals 2π/3.","kernel clustering, semidefinite programming, unique games hardness, grothendieck inequalities",STOC '12,,,
Conference Paper,"Obermaier J,Hutle M",Analyzing the Security and Privacy of Cloud-Based Video Surveillance Systems,,2016,,,22–28,Association for Computing Machinery,"New York, NY, USA","Proceedings of the 2nd ACM International Workshop on IoT Privacy, Trust, and Security","Xi'an, China",2016,9781450342834.0,,https://doi.org/10.1145/2899007.2899008;http://dx.doi.org/10.1145/2899007.2899008,10.1145/2899007.2899008,"In the area of the Internet of Things, cloud-based camera surveillance systems are ubiquitously available for industrial and private environments. However, the sensitive nature of the surveillance use case imposes high requirements on privacy/confidentiality, authenticity, and availability of such systems. In this work, we investigate how currently available mass-market camera systems comply with these requirements. Considering two attacker models, we test the cameras for weaknesses and analyze for their implications. We reverse-engineered the security implementation and discovered several vulnerabilities in every tested system. These weaknesses impair the users' privacy and, as a consequence, may also damage the camera system manufacturer's reputation. We demonstrate how an attacker can exploit these vulnerabilities to blackmail users and companies by denial-of-service attacks, injecting forged video streams, and by eavesdropping private video data - even without physical access to the device. Our analysis shows that current systems lack in practice the necessary care when implementing security for IoT devices.","pentesting, surveillance systems, internet of things, security analysis, home automation, privacy, embedded security",IoTPTS '16,,,
Journal Article,"Sotiropoulos T,Chaliasos S,Mitropoulos D,Spinellis D",A Model for Detecting Faults in Build Specifications,Proc. ACM Program. Lang.,2020,4.0,OOPSLA,,Association for Computing Machinery,"New York, NY, USA",,,2020-11,,,https://doi.org/10.1145/3428212;http://dx.doi.org/10.1145/3428212,10.1145/3428212,"Incremental and parallel builds are crucial features of modern build systems. Parallelism enables fast builds by running independent tasks simultaneously, while incrementality saves time and computing resources by processing the build operations that were affected by a particular code change. Writing build definitions that lead to error-free incremental and parallel builds is a challenging task. This is mainly because developers are often unable to predict the effects of build operations on the file system and how different build operations interact with each other. Faulty build scripts may seriously degrade the reliability of automated builds, as they cause build failures, and non-deterministic and incorrect outputs. To reason about arbitrary build executions, we present BuildFS, a generally-applicable model that takes into account the specification (as declared in build scripts) and the actual behavior (low-level file system operation) of build operations. We then formally define different types of faults related to incremental and parallel builds in terms of the conditions under which a file system operation violates the specification of a build operation. Our testing approach, which relies on the proposed model, analyzes the execution of single full build, translates it into BuildFS, and uncovers faults by checking for corresponding violations. We evaluate the effectiveness, efficiency, and applicability of our approach by examining 612 Make and Gradle projects. Notably, thanks to our treatment of build executions, our method is the first to handle JVM-oriented build systems. The results indicate that our approach is (1) able to uncover several important issues (247 issues found in 47 open-source projects have been confirmed and fixed by the upstream developers), and (2) much faster than a state-of-the-art tool for Make builds (the median and average speedup is 39X and 74X respectively).","parallel builds, Make, incremental builds, JVM-based builds, Gradle",,,,
Conference Paper,"Zuo C,Wen H,Lin Z,Zhang Y",Automatic Fingerprinting of Vulnerable BLE IoT Devices with Static UUIDs from Mobile Apps,,2019,,,1469–1483,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 2019 ACM SIGSAC Conference on Computer and Communications Security,"London, United Kingdom",2019,9781450367479.0,,https://doi.org/10.1145/3319535.3354240;http://dx.doi.org/10.1145/3319535.3354240,10.1145/3319535.3354240,"Being an easy-to-deploy and cost-effective low power wireless solution, Bluetooth Low Energy (BLE) has been widely used by Internet-of-Things (IoT) devices. In a typical IoT scenario, an IoT device first needs to be connected with its companion mobile app which serves as a gateway for its Internet access. To establish a connection, a device first broadcasts advertisement packets with UUIDs to nearby smartphone apps. Leveraging these UUIDs, a companion app is able to identify the device, pairs and bonds with it, and allows further data communication. However, we show that there is a fundamental flaw in the current design and implementation of the communication protocols between a BLE device and its companion mobile app, which allows an attacker to precisely fingerprint a BLE device with static UUIDs from the apps. Meanwhile, we also discover that many BLE IoT devices adopt ""just works"" pairing, allowing attackers to actively connect with these devices if there is no app-level authentication. Even worse, this vulnerability can also be directly uncovered from mobile apps. Furthermore, we also identify that there is an alarming number of vulnerable app-level authentication apps, which means the devices connected by these apps can be directly controlled by attackers. To raise the public awareness of IoT device fingerprinting and also uncover these vulnerable BLE IoT devices before attackers, we develop an automated mobile app analysis tool BLESCOPE and evaluate it with all of the free BLE IoT apps in Google Play store. Our tool has identified 1,757 vulnerable mobile apps in total. We also performed a field test in a 1.28 square miles region, and identified 5,822 real BLE devices, among them 5,509 (94.6%) are fingerprintable by attackers, and 431 (7.4%) are vulnerable to unauthorized access. We have made responsible disclosures to the corresponding app developers, and also reported the fingerprinting issues to the Bluetooth Special Interest Group.","mobile app analysis, IoT security, device fingerprinting, bluetooth low energy",CCS '19,,,
Journal Article,Neumann PG,Illustrative Risks to the Public in the Use of Computer Systems and Related Technology,SIGSOFT Softw. Eng. Notes,1996,21.0,1,16–30,Association for Computing Machinery,"New York, NY, USA",,,1996-01,,0163-5948,https://doi.org/10.1145/381790.381797;http://dx.doi.org/10.1145/381790.381797,10.1145/381790.381797,,,,,,
Journal Article,"Leonard T,Hall-May M,Surridge M",Modelling Access Propagation in Dynamic Systems,ACM Trans. Inf. Syst. Secur.,2013,16.0,2,,Association for Computing Machinery,"New York, NY, USA",,,2013-09,,1094-9224,https://doi.org/10.1145/2516951.2516952;http://dx.doi.org/10.1145/2516951.2516952,10.1145/2516951.2516952,"Access control is a critical feature of many systems, including networks of services, processes within a computer, and objects within a running process. The security consequences of a particular architecture or access control policy are often difficult to determine, especially where some components are not under our control, where components are created dynamically, or where access policies are updated dynamically.The SERSCIS Access Modeller (SAM) takes a model of a system and explores how access can propagate through it. It can both prove defined safety properties and discover unwanted properties. By defining expected behaviours, recording the results as a baseline, and then introducing untrusted actors, SAM can discover a wide variety of design flaws.SAM is designed to handle dynamic systems (i.e., at runtime, new objects are created and access policies modified) and systems where some objects are not trusted. It extends previous approaches such as Scollar and Authodox to provide a programmer-friendly syntax for specifying behaviour, and allows modelling of services with mutually suspicious clients.Taking the Confused Deputy example from Authodox we show that SAM detects the attack automatically; using a web-based backup service, we show how to model RBAC systems, detecting a missing validation check; and using a proxy certificate system, we show how to extend it to model new access mechanisms. On discovering that a library fails to follow an RFC precisely, we re-evaluate our existing models under the new assumption and discover that the proxy certificate design is not safe with this library.","Object-capabilities, datalog, proxy certificates",,,,
Conference Paper,Boronat A,Structural Model Subtyping with OCL Constraints,,2017,,,194–205,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 10th ACM SIGPLAN International Conference on Software Language Engineering,"Vancouver, BC, Canada",2017,9781450355254.0,,https://doi.org/10.1145/3136014.3136026;http://dx.doi.org/10.1145/3136014.3136026,10.1145/3136014.3136026,"In model-driven engineering (MDE), models abstract the relevant features of software artefacts and model management operations, including model transformations, act on them automating large tasks of the development process. Flexible reuse of such operations is an important factor to improve productivity when developing and maintaining MDE solutions. In this work, we revisit the traditional notion of object subtyping based on subsumption, discarded by other approaches to model subtyping. We refine a type system for object-oriented programming, with multiple inheritance, to support model types in order to analyse its advantages and limitations with respect to reuse in MDE. Specifically, we extend type expressions with referential constraints and with OCL constraints. Our approach has been validated with a tool that extracts model types from (EMF) metamodels, paired with their OCL constraints, automatically and that exploits the extended subtyping relation to reuse model management operations. We show that structural model subtyping is expressive enough to support variants of model subtyping, including multiple, partial and dynamic model subtyping. The tool has received the ACM badge ""Artifacts Evaluated - Functional"".","type theory, OCL, Model subtyping, EMF",SLE 2017,,,
Journal Article,"Florence SP,Fetscher B,Flatt M,Temps WH,St-Amour V,Kiguradze T,West DP,Niznik C,Yarnold PR,Findler RB,Belknap SM",POP-PL: A Patient-Oriented Prescription Programming Language,ACM Trans. Program. Lang. Syst.,2018,40.0,3,,Association for Computing Machinery,"New York, NY, USA",,,2018-07,,0164-0925,https://doi.org/10.1145/3210256;http://dx.doi.org/10.1145/3210256,10.1145/3210256,"A medical prescription is a set of health care instructions that govern the plan of care for an individual patient, which may include orders for drug therapy, diet, clinical assessment, and laboratory testing. Clinicians have long used algorithmic thinking to describe and implement prescriptions but without the benefit of a formal programming language. Instead, medical algorithms are expressed using a natural language patois, flowcharts, or as structured data in an electronic medical record system. The lack of a prescription programming language inhibits expressiveness; results in prescriptions that are difficult to understand, hard to debug, and awkward to reuse; and increases the risk of fatal medical error.This article reports on the design and evaluation of Patient-Oriented Prescription Programming Language (POP-PL), a domain-specific programming language designed for expressing prescriptions. The language is based around the idea that programs and humans have complementary strengths that, when combined properly, can make for safer, more accurate performance of prescriptions. Use of POP-PL facilitates automation of certain low-level vigilance tasks, freeing up human cognition for abstract thinking, compassion, and human communication.We implemented this language and evaluated its design attempting to write prescriptions in the new language and evaluated its usability by assessing whether clinicians can understand and modify prescriptions written in the language. We found that some medical prescriptions can be expressed in a formal domain-specific programming language, and we determined that medical professionals can understand and correctly modify programs written in POP-PL. We also discuss opportunities for refining and further developing POP-PL.","medical programming languages, medical prescriptions, empirical evaluation, DSL design",,,,
Journal Article,"Börstler J,Hall MS,Nordström M,Paterson JH,Sanders K,Schulte C,Thomas L",An Evaluation of Object Oriented Example Programs in Introductory Programming Textbooks,SIGCSE Bull.,2010,41.0,4,126–143,Association for Computing Machinery,"New York, NY, USA",,,2010-01,,0097-8418,https://doi.org/10.1145/1709424.1709458;http://dx.doi.org/10.1145/1709424.1709458,10.1145/1709424.1709458,"Research shows that examples play an important role for cognitive skill acquisition. Students as well as teachers rank examples as important resources for learning to program. Therefore examples must be consistent with the principles and rules of the topics we are teaching.However, educators often struggle to find or develop objectoriented example programs of high quality. Common examples are often perceived as not fully faithful to all principles and guidelines of the object-oriented paradigm, or as not following general pedagogical principles and practices. Unless students are able to engage with good examples, they will not be able to tell desirable from undesirable properties in their own and others' programs.In this paper we report on a study in which experienced educators reviewed a wide range of object-oriented examples for novices from popular textbooks. This review was accomplished using an on-line checklist that elicited responses on 10 quality factors. Results show that the evaluation instrument provides a sufficiently consistent set of responses to distinguish examples.The paper then goes on to examine some of the characteristics of good and bad examples and how this study will influence the evolution of the evaluating instrument.","principles, assessment, examples, guidelines, textbooks, courseware, example programs, check list",,,,
Conference Paper,"Li S,Shah SA,Khan MA,Khayam SA,Sadeghi AR,Schmitz R",Breaking E-Banking CAPTCHAs,,2010,,,171–180,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 26th Annual Computer Security Applications Conference,"Austin, Texas, USA",2010,9781450301336.0,,https://doi.org/10.1145/1920261.1920288;http://dx.doi.org/10.1145/1920261.1920288,10.1145/1920261.1920288,"Many financial institutions have deployed CAPTCHAs to protect their services (e.g., e-banking) from automated attacks. In addition to CAPTCHAs for login, CAPTCHAs are also used to prevent malicious manipulation of e-banking transactions by automated Man-in-the-Middle (MitM) attackers. Despite serious financial risks, security of e-banking CAPTCHAs is largely unexplored. In this paper, we report the first comprehensive study on e-banking CAPTCHAs deployed around the world. A new set of image processing and pattern recognition techniques is proposed to break all e-banking CAPTCHA schemes that we found over the Internet, including three e-banking CAPTCHA schemes for transaction verification and 41 schemes for login. These broken e-banking CAPTCHA schemes are used by thousands of financial institutions worldwide, which are serving hundreds of millions of e-banking customers. The success rate of our proposed attacks are either equal to or close to 100%. We also discuss possible improvements to these e-banking CAPTCHA schemes and show essential difficulties of designing e-banking CAPTCHAs that are both secure and usable.","CAPTCHA, e-banking, electronic commerce, man-in-the-middle attack, malware",ACSAC '10,,,
Conference Paper,"Li H,Sanner S,Luo K,Wu G",A Ranking Optimization Approach to Latent Linear Critiquing for Conversational Recommender Systems,,2020,,,13–22,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 14th ACM Conference on Recommender Systems,"Virtual Event, Brazil",2020,9781450375832.0,,https://doi.org/10.1145/3383313.3412240;http://dx.doi.org/10.1145/3383313.3412240,10.1145/3383313.3412240,"Critiquing is a method for conversational recommendation that incrementally adapts recommendations in response to user preference feedback. Specifically, a user is iteratively provided with item recommendations and attribute descriptions for those items; the user may then either accept the recommendation or choose to critique an attribute to generate a new recommendation. A recent direction known as latent linear critiquing (LLC) takes a modern embedding-based approach that seeks to optimize the combination of user preference embeddings with embeddings of critiques based on subjective item descriptions (i.e., keyphrases from user reviews); LLC does so by exploiting the linear structure of the embeddings to efficiently optimize their weights in a linear programming (LP) formulation. In this paper, we revisit LLC and note that it’s score-based optimization approach inherently encourages extreme weightings in order to maximize predicted score gaps between preferred and non-preferred items. Noting that the overall end task objective in critiquing is to re-rank rather than re-score, in this paper we take a ranking optimization approach that seeks to optimize embedding weights based on observed rank violations from earlier critiquing iterations. We evaluate the proposed framework on two recommendation datasets containing user reviews. Empirical results demonstrate that ranking-based LLC generally outperforms scoring-based LLC and other baselines across a variety of datasets, critiquing styles, and both satisfaction and session-length performance metrics.","Conversational Recommendation, Critiquing",RecSys '20,,,
Journal Article,"Sergey I,Nagaraj V,Johannsen J,Kumar A,Trunov A,Hao KC",Safer Smart Contract Programming with Scilla,Proc. ACM Program. Lang.,2019,3.0,OOPSLA,,Association for Computing Machinery,"New York, NY, USA",,,2019-10,,,https://doi.org/10.1145/3360611;http://dx.doi.org/10.1145/3360611,10.1145/3360611,"The rise of programmable open distributed consensus platforms based on the blockchain technology has aroused a lot of interest in replicated stateful computations, aka smart contracts. As blockchains are used predominantly in financial applications, smart contracts frequently manage millions of dollars worth of virtual coins. Since smart contracts cannot be updated once deployed, the ability to reason about their correctness becomes a critical task. Yet, the de facto implementation standard, pioneered by the Ethereum platform, dictates smart contracts to be deployed in a low-level language, which renders independent audit and formal verification of deployed code infeasible in practice. We report an ongoing experiment held with an industrial blockchain vendor on designing, evaluating, and deploying Scilla, a new programming language for safe smart contracts. Scilla is positioned as an intermediate-level language, suitable to serve as a compilation target and also as an independent programming framework. Taking System F as a foundational calculus, Scilla offers strong safety guarantees by means of type soundness. It provides a clean separation between pure computational, state-manipulating, and communication aspects of smart contracts, avoiding many known pitfalls due to execution in a byzantine environment. We describe the motivation, design principles, and semantics of Scilla, and we report on Scilla use cases provided by the developer community. Finally, we present a framework for lightweight verification of Scilla programs, and showcase it with two domain-specific analyses on a suite of real-world use cases.","Blockchain, Smart Contracts, Static Analysis, Domain-Specific Languages",,,,
Journal Article,"Gardey JC,Garrido A,Firmenich S,Grigera J,Rossi G",UX-Painter: An Approach to Explore Interaction Fixes in the Browser,Proc.  ACM Hum. -Comput.  Interact.,2020,4.0,EICS,,Association for Computing Machinery,"New York, NY, USA",,,2020-06,,,https://doi.org/10.1145/3397877;http://dx.doi.org/10.1145/3397877,10.1145/3397877,"Usability and user interaction improvement is a central task in web development to guarantee the success of a web application. However, designers are barely able to keep up with the current development cycle because their practices are too costly, while interaction issues accumulate in applications that end-users keep suffering. In this work, we propose a method for designers to rapidly explore solutions through visual programming to the interaction problems of an application under development, even when it has been already deployed. The method is realized by a tool called UX-Painter, an exploratory tool for designers to apply quick fixes to interaction issues at the client-side of a web application without the need of any script programming knowledge. The palette of available fixes in UX-Painter are client-side web refactorings, i.e., changes to web page elements that solve specific user interaction problems without changing the underlying functionality. UX-Painter allows designers to quickly set up new versions of a web application by combining refactorings to create alternative designs for user testing or an inspection review. UX-Painter also provides the means to communicate design improvements, as a sequence of refactorings with clear semantics. We show the feedback provided by interviews with designers about UX-Painter's functionality and the results of a user test about its usability.","web refactoring, end-user programming, interaction design, web usability, user experience",,,,
Conference Paper,"Chabbi M,Mellor-Crummey J","Contention-Conscious, Locality-Preserving Locks",,2016,,,,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 21st ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming,"Barcelona, Spain",2016,9781450340922.0,,https://doi.org/10.1145/2851141.2851166;http://dx.doi.org/10.1145/2851141.2851166,10.1145/2851141.2851166,"Over the last decade, the growing use of cache-coherent NUMA architectures has spurred the development of numerous locality-preserving mutual exclusion algorithms. NUMA-aware locks such as HCLH, HMCS, and cohort locks exploit locality of reference among nearby threads to deliver high lock throughput under high contention. However, the hierarchical nature of these locality-aware locks increases latency, which reduces the throughput of uncontended or lightly-contended critical sections. To date, no lock design for NUMA systems has delivered both low latency under low contention and high throughput under high contention.In this paper, we describe the design and evaluation of an adaptive mutual exclusion scheme (AHMCS lock), which employs several orthogonal strategies---a hierarchical MCS (HMCS) lock for high throughput under high contention, Lamport's fast path approach for low latency under low contention, an adaptation mechanism that employs hysteresis to balance latency and throughput under moderate contention, and hardware transactional memory for lowest latency in the absence of contention. The result is a top performing lock that has most properties of an ideal mutual exclusion algorithm. AHMCS exploits the strengths of multiple contention management techniques to deliver high performance over a broad range of contention levels. Our empirical evaluations demonstrate the effectiveness of AHMCS over prior art.","NUMA, dynamic locks, hierarchical locks, spin locks",PPoPP '16,,,
Journal Article,"Chabbi M,Mellor-Crummey J","Contention-Conscious, Locality-Preserving Locks",SIGPLAN Not.,2016,51.0,8,,Association for Computing Machinery,"New York, NY, USA",,,2016-02,,0362-1340,https://doi.org/10.1145/3016078.2851166;http://dx.doi.org/10.1145/3016078.2851166,10.1145/3016078.2851166,"Over the last decade, the growing use of cache-coherent NUMA architectures has spurred the development of numerous locality-preserving mutual exclusion algorithms. NUMA-aware locks such as HCLH, HMCS, and cohort locks exploit locality of reference among nearby threads to deliver high lock throughput under high contention. However, the hierarchical nature of these locality-aware locks increases latency, which reduces the throughput of uncontended or lightly-contended critical sections. To date, no lock design for NUMA systems has delivered both low latency under low contention and high throughput under high contention.In this paper, we describe the design and evaluation of an adaptive mutual exclusion scheme (AHMCS lock), which employs several orthogonal strategies---a hierarchical MCS (HMCS) lock for high throughput under high contention, Lamport's fast path approach for low latency under low contention, an adaptation mechanism that employs hysteresis to balance latency and throughput under moderate contention, and hardware transactional memory for lowest latency in the absence of contention. The result is a top performing lock that has most properties of an ideal mutual exclusion algorithm. AHMCS exploits the strengths of multiple contention management techniques to deliver high performance over a broad range of contention levels. Our empirical evaluations demonstrate the effectiveness of AHMCS over prior art.","spin locks, NUMA, hierarchical locks, dynamic locks",,,,
Conference Paper,"Lilegdon WR,Talavage JJ",A MicroNET Application,,1983,,,497–508,IEEE Press,"Arlington, Virginia, USA",Proceedings of the 15th Conference on Winter Simulation - Volume 2,,1983,,,,,"MicroNET is a complete network simulation system designed specifically for microcomputers. Models of a wide variety of systems can be built, simulated, and analyzed using MicroNET's network approach. A network model is a graphical representation of a problem situation, that provides input data to the system and a means for describing the problem. Models are built by describing the problem situation graphically using MicroNET modeling symbols. These symbols are linked together and provide a graphic description of the problem situation. The MicroNET symbols translate directly into input statements that are recognized by the software. The MicroNET software uses a simulation procedure to analyze the network models. This procedure involves the generation of transactions, the processing of the transactions through the network, and the collection and reporting of statistical information.The MicroNET modeler interacts with the software through the system interface. The MicroNET system interface provides a structured approach to analyzing systems using simulation. Models are built, simulated, and analyzed interactively using this interface. MicroNET divides the tasks involved in solving problems using network modeling into three subsystems: CONTROL, DEVELOPMENT, and EXECUTION. For each subsystem MicroNET provides a corresponding set of commands that perform the required tasks. To initiate any action, the user responds to a prompt from the system by entering the command for the action desired.The CONTROL subsystem provides model management assistance. It allows models to be copied, deleted, and appended to one another. Using the CONTROL subsystem commands, the MicroNET system can be tailored to utilize additional memory and a parallel printer.The DEVELOPMENT subsystem supports the creation and modification of MicroNET network models with full editing capabilities. Commands are also included to display the contents of any MicroNET model on the screen or printer.Execution and analysis of MicroNET network models is performed using the EXECUTION subsystem. Commands are available to simulate a network model and display ongoing statistical information during model execution. Statistical reports may be prepared from this subsystem and presented at the screen or printer.To describe the MicroNET approach to analyzing systems using network models we will discuss an example application. The example system is an automated inspection area. We will briefly describe the system, discuss the transition from system to model, review the model and its outputs and discuss the performance and application of the MicroNET software.",,WSC '83,,,
Conference Paper,"Grubbs P,McPherson R,Naveed M,Ristenpart T,Shmatikov V",Breaking Web Applications Built On Top of Encrypted Data,,2016,,,1353–1364,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security,"Vienna, Austria",2016,9781450341394.0,,https://doi.org/10.1145/2976749.2978351;http://dx.doi.org/10.1145/2976749.2978351,10.1145/2976749.2978351,"We develop a systematic approach for analyzing client-server applications that aim to hide sensitive user data from untrusted servers. We then apply it to Mylar, a framework that uses multi-key searchable encryption (MKSE) to build Web applications on top of encrypted data.We demonstrate that (1) the Popa-Zeldovich model for MKSE does not imply security against either passive or active attacks; (2) Mylar-based Web applications reveal users' data and queries to passive and active adversarial servers; and (3) Mylar is generically insecure against active attacks due to system design flaws. Our results show that the problem of securing client-server applications against actively malicious servers is challenging and still unsolved.We conclude with general lessons for the designers of systems that rely on property-preserving or searchable encryption to protect data from untrusted servers.","provable security, leakage, application security, searchable encryption",CCS '16,,,
Conference Paper,"Ghosh AK,Aljallad Z,Badillo-Urquiola K,Wisniewski P","Carebit: A Privacy-Preserving ""Step"" Toward Remote Informal Caregiving",,2018,,,154–157,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 2018 ACM Conference on Supporting Groupwork,"Sanibel Island, Florida, USA",2018,9781450355629.0,,https://doi.org/10.1145/3148330.3154520;http://dx.doi.org/10.1145/3148330.3154520,10.1145/3148330.3154520,"Several tele-monitoring systems have been developed for in-home patient use. Unfortunately, many of these systems are cost prohibitive and privacy invasive to the patient. To overcome this problem, we designed a more affordable and lightweight solution called Carebit, an Android application that leverages the Fitbit API. We conducted two user studies to understand ways to improve our design. Overall, we found that the notifications feature is the most useful feature for users, and no concerns about privacy were mentioned. The goal of Carebit is improve informal caregiving.","family caregiving, wearable internet of things, privacy",GROUP '18,,,
Conference Paper,"Parikh R,Bertacco V",Formally Enhanced Runtime Verification to Ensure NoC Functional Correctness,,2011,,,410–419,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 44th Annual IEEE/ACM International Symposium on Microarchitecture,"Porto Alegre, Brazil",2011,9781450310536.0,,https://doi.org/10.1145/2155620.2155668;http://dx.doi.org/10.1145/2155620.2155668,10.1145/2155620.2155668,"As silicon technology scales, modern processors and embedded systems are rapidly shifting towards complex chip multi-processor (CMP) and system-on-chip (SoC) designs, comprising several processor cores and IP components communicating via a network-on-chip (NoC). As a side-effect of this trend, ensuring their correctness has become increasingly problematic. In particular, the network-on-chip often includes complex features and components to support the required communication bandwidth among the nodes in the system. In this landscape, it is no wonder that design errors in the NoC may go undetected and escape into the final silicon, with potential detrimental impact on the overall system.In this work, we propose ForEVeR, a solution that complements the use of formal methods and runtime verification to ensure functional correctness in NoCs. Formal verification, due to its scalability limitations, is used to verify the smaller modules, such as individual router components. We complete the protection against escaped design errors with a runtime technique, a network-level error detection and recovery solution, which monitors the traffic in the NoC and protects it against escaped functional bugs that affect the communication paths in the network. To this end, ForEVeR augments the baseline NoC with a lightweight checker network that alerts destination nodes of incoming packets ahead of time. If a bug is detected, flagged by missed packet arrivals, a recovery mechanism delivers the in-flight data safely to the intended destination via the checker network. ForEVeR's experimental evaluation shows that it can recover from NoC design errors at only 4.8% area cost for an 8x8 mesh interconnect, with a recovery performance cost of less than 30K cycles per functional bug manifestation. Additionally, it incurs no performance overhead in the absence of errors.","NoC, formal verification, network-on-chip, functional correctness, runtime verification",MICRO-44,,,
Conference Paper,"DiSalvo C,Jenkins T",Fruit Are Heavy: A Prototype Public IoT System to Support Urban Foraging,,2017,,,541–553,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 2017 Conference on Designing Interactive Systems,"Edinburgh, United Kingdom",2017,9781450349222.0,,https://doi.org/10.1145/3064663.3064748;http://dx.doi.org/10.1145/3064663.3064748,10.1145/3064663.3064748,"Smart cities are one area of interactive systems design where the technologies and services of the Internet of Things (IoT) have the potential to serve public interest. In this paper we present a design research project that explores the use of IoT technologies-specifically environmental sensing-to support urban foraging. We describe the design of a simple proof-of-concept sensing platform to monitor the relative ripeness of fruit in trees, and reflect upon its potential effectiveness for urban foraging. From the project, we draw out themes for designing in the context of smart cities, including questioning the presumed ""smartness"" of IoT systems, and highlight issues in designing to support diverse community economies.","public design, diverse economies, public iot, participatory design, community economies, digital civics, foraging, smart cities",DIS '17,,,
Conference Paper,Schaefer CF,Translation of VHDL to Ada,,1987,,,111–117,George Washington University,USA,Proceedings of the Joint Ada Conference Fifth National Conference on Ada Technology and Fourth Washington Ada Symposium,"Arlington, Virginia, USA",1987,,,,,,,WADAS '87,,,
Journal Article,"Kistowski J,Herbst N,Kounev S,Groenda H,Stier C,Lehrig S",Modeling and Extracting Load Intensity Profiles,ACM Trans. Auton. Adapt. Syst.,2017,11.0,4,,Association for Computing Machinery,"New York, NY, USA",,,2017-01,,1556-4665,https://doi.org/10.1145/3019596;http://dx.doi.org/10.1145/3019596,10.1145/3019596,"Today’s system developers and operators face the challenge of creating software systems that make efficient use of dynamically allocated resources under highly variable and dynamic load profiles, while at the same time delivering reliable performance. Autonomic controllers, for example, an advanced autoscaling mechanism in a cloud computing context, can benefit from an abstracted load model as knowledge to reconfigure on time and precisely. Existing workload characterization approaches have limited support to capture variations in the interarrival times of incoming work units over time (i.e., a variable load profile). For example, industrial and scientific benchmarks support constant or stepwise increasing load, or interarrival times defined by statistical distributions or recorded traces. These options show shortcomings either in representative character of load variation patterns or in abstraction and flexibility of their format.In this article, we present the Descartes Load Intensity Model (DLIM) approach addressing these issues. DLIM provides a modeling formalism for describing load intensity variations over time. A DLIM instance is a compact formal description of a load intensity trace. DLIM-based tools provide features for benchmarking, performance, and recorded load intensity trace analysis. As manually obtaining and maintaining DLIM instances becomes time consuming, we contribute three automated extraction methods and devised metrics for comparison and method selection. We discuss how these features are used to enhance system management approaches for adaptations during runtime, and how they are integrated into simulation contexts and enable benchmarking of elastic or adaptive behavior.We show that automatically extracted DLIM instances exhibit an average modeling error of 15.2% over 10 different real-world traces that cover between 2 weeks and 7 months. These results underline DLIM model expressiveness. In terms of accuracy and processing speed, our proposed extraction methods for the descriptive models are comparable to existing time series decomposition methods. Additionally, we illustrate DLIM applicability by outlining approaches of workload modeling in systems engineering that employ or rely on our proposed load intensity modeling formalism.","metamodeling, Load intensity variation, transformation, model extraction, load profile, open workloads",,,,
Conference Paper,"Kahol K,French J,Bratton L,Panchanathan S",Learning and Perceiving Colors Haptically,,2006,,,173–180,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 8th International ACM SIGACCESS Conference on Computers and Accessibility,"Portland, Oregon, USA",2006,9781595932907.0,,https://doi.org/10.1145/1168987.1169017;http://dx.doi.org/10.1145/1168987.1169017,10.1145/1168987.1169017,"Color is an integral part of spatial perception and there is a need to develop systems that render color information accessible to blind individuals. A novel system that allows learning, presentation and analysis of color information, designed in consultations with focus groups of individuals who are blind is proposed. Our system is based on a methodology that renders colors as textures through a haptic device. The aim of the proposed approach is to enable color perception and provide a basis for assessing color similarity. Initial testing of the system shows that both blind individuals and sighted individuals can recognize colors through our approach and further assess similarity between colors through the system. A space was obtained through multidimensional scaling performed on similarity scores between pairs of colors as presented through our system. This space obtained high congruency with the chromaticity diagram and the hue saturation color wheel which shows the validity of our system to allow color visualization. A realtime system based on the proposed mapping is designed to allow realtime color perception.","haptic user interfaces, color perception",Assets '06,,,
Conference Paper,"FitzRoy-Dale N,Kuz I",Towards Automatic Performance Optimisation of Componentised Systems,,2009,,,31–36,Association for Computing Machinery,"New York, NY, USA",Proceedings of the Second Workshop on Isolation and Integration in Embedded Systems,"Nuremburg, Germany",2009,9781605584645.0,,https://doi.org/10.1145/1519130.1519136;http://dx.doi.org/10.1145/1519130.1519136,10.1145/1519130.1519136,"Use of hardware-based memory protection to implement a componentised system is an effective way to enforce isolation between untrusted software components. Unfortunately this type of system design can lead to poor performance. Manual optimisation is error-prone and difficult. Instead, we describe a system to perform automatic optimisation of components, relying on three major functional units: a method to reconfigure the component system, simulations of each component in order to determine performance characteristics, and a system simulator that makes use of those characteristics to construct a ranking of optimisations. We start with a simple model and iteratively expand it until it is suitable for a wide variety of performance-measurement scenarios, and show that a small amount of information provided with each component allows for a wide variety of optimisation checks, such as scheduling, threading, and cache performance. We present our initial results with this system and discuss a number of interesting extensions.",,IIES '09,,,
Conference Paper,"Sheriff A,Sadan R,Keats Y,Zuckerman O",From Smart Homes to Smart Kids: Design Research for CataKit,,2017,,,159–169,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 2017 Conference on Interaction Design and Children,"Stanford, California, USA",2017,9781450349215.0,,https://doi.org/10.1145/3078072.3079729;http://dx.doi.org/10.1145/3078072.3079729,10.1145/3078072.3079729,"This paper presents the design research process of CataKit, a construction kit for children inspired by catapults, Rube-Goldberg chain reaction machines, and mechanical automata. We set out to promote children's initiative, positive risk-taking, and procedural thinking, all in the context of their bedrooms. Our motivation is to contrast the rising smart home movement in industry, which we fear may decrease children's initiative if children's bedrooms become too automated. We describe our design research process with six children followed by a low fidelity prototype design and evaluation. We present the qualitative analysis of children's reactions to the prototype and show support for our initial goals: encourage systematic exploration of mechanical concepts and initiative over automation. We hope that construction kits like Catakit will empower children to develop curiosity about the mechanical world around them, to think about risk taking as a potentially positive experience, and to think more critically about initiative in the smart home era.","positive risk-taking, learning, construction kit, children computational thinking",IDC '17,,,
Conference Paper,"Smith B,Austin A,Brown M,King JT,Lankford J,Meneely A,Williams L",Challenges for Protecting the Privacy of Health Information: Required Certification Can Leave Common Vulnerabilities Undetected,,2010,,,1–12,Association for Computing Machinery,"New York, NY, USA",Proceedings of the Second Annual Workshop on Security and Privacy in Medical and Home-Care Systems,"Chicago, Illinois, USA",2010,9781450300940.0,,https://doi.org/10.1145/1866914.1866916;http://dx.doi.org/10.1145/1866914.1866916,10.1145/1866914.1866916,"The use of electronic health record (EHR) systems by medical professionals enables the electronic exchange of patient data, yielding cost and quality of care benefits. The United States American Recovery and Reinvestment Act (ARRA) of 2009 provides up to $34 billion for meaningful use of certified EHR systems. But, will these certified EHR systems provide the infrastructure for secure patient data exchange? As a window into the ability of current and emerging certification criteria to expose security vulnerabilities, we performed exploratory security analysis on a proprietary and an open source EHR. We were able to exploit a range of common code-level and design-level vulnerabilities. These common vulnerabilities would have remained undetected by the 2011 security certification test scripts from the Certification Commission for Health Information Technology, the most widely used certification process for EHR systems. The consequences of these exploits included, but were not limited to: exposing all users' login information, the ability of any user to view or edit health records for any patient, and creating a denial of service for all users. Based upon our results, we suggest that an enhanced set of security test scripts be used as entry criteria to the EHR certification process. Before certification bodies spend the time to certify that an EHR application is functionally complete, they should have confidence that the software system meets a basic level of security competence.","security testing, exploit, medical records, attack, ehr, openemr, emr, white hat, sql injection, meaningful use, xss, dos, vulnerability, cchit, healthcare, man-in-the-middle, ethical hacking",SPIMACS '10,,,
Conference Paper,"Li K,Tang Y,Chen J,Yuan Z,Xu C,Xu J",Cost-Effective Data Feeds to Blockchains via Workload-Adaptive Data Replication,,2020,,,371–385,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 21st International Middleware Conference,"Delft, Netherlands",2020,9781450381536.0,,https://doi.org/10.1145/3423211.3425696;http://dx.doi.org/10.1145/3423211.3425696,10.1145/3423211.3425696,"Feeding external data to a blockchain, a.k.a. data feed, is an essential task to enable blockchain interoperability and support emerging cross-domain applications. Given the data-intensive nature of real-life feeds (e.g., high-frequency price updates) and the high cost of using blockchain, namely Gas, it is imperative to reduce the Gas cost of data feeds. Motivated by the constant-changing workloads in financial applications, this work aims at designing a dynamic, workload-aware approach for Gas cost optimization. This design space is understudied in existing blockchain research which has so far focused on static data placement.This work presents GRuB, a cost-effective data feed that dynamically replicates data between the blockchain and off-chain cloud storage. GRuB monitors the current workload and makes data-replication decisions in a workload-adaptive fashion. Online algorithms are proposed to bound the worstcase cost in Gas. GRuB's decision-making components run on the untrusted cloud off-chain for lower Gas, and employs a security protocol to authenticate the data transferred between the blockchain and cloud. We built a GRuB prototype on Ethereum and supported real financial applications. Using the workloads reconstructed from Ethereum transaction history, we evaluate GRuB's cost and show a Gas saving by 10% 74%, in comparison with the static baselines.","Blockchains, workload awareness, data feeds, data replication, DeFi, authenticated data structures",Middleware '20,,,
Conference Paper,"Garfinkel SL,Margrave D,Schiller JI,Nordlander E,Miller RC",How to Make Secure Email Easier to Use,,2005,,,701–710,Association for Computing Machinery,"New York, NY, USA",Proceedings of the SIGCHI Conference on Human Factors in Computing Systems,"Portland, Oregon, USA",2005,9781581139983.0,,https://doi.org/10.1145/1054972.1055069;http://dx.doi.org/10.1145/1054972.1055069,10.1145/1054972.1055069,"Cryptographically protected email has a justly deserved reputation of being difficult to use. Based on an analysis of the PEM, PGP and S/MIME standards and a survey of 470 merchants who sell products on Amazon.com, we argue that the vast majority of Internet users can start enjoying digitally signed email today. We present suggestions for the use of digitally signed mail in e-commerce and simple modifications to webmail systems that would significantly increase integrity, privacy and authorship guarantees that those systems make. We then show how to use the S/MIME standard to extend such protections Internet-wide. Finally, we argue that software vendors must make minor changes to the way that mail clients store email before unsophisticated users can safely handle mail that is sealed with encryption.","user interaction design, user studies, e-commerce",CHI '05,,,
Journal Article,"Anderson DP,Cobb J,Korpela E,Lebofsky M,Werthimer D",SETI@home: An Experiment in Public-Resource Computing,Commun. ACM,2002,45.0,11,56–61,Association for Computing Machinery,"New York, NY, USA",,,2002-11,,0001-0782,https://doi.org/10.1145/581571.581573;http://dx.doi.org/10.1145/581571.581573,10.1145/581571.581573,"Millions of computer owners worldwide contribute computer time to the search for extraterrestrial intelligence, performing the largest computation ever.",,,,,
Conference Paper,"Kanonov U,Wool A",Secure Containers in Android: The Samsung KNOX Case Study,,2016,,,3–12,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 6th Workshop on Security and Privacy in Smartphones and Mobile Devices,"Vienna, Austria",2016,9781450345644.0,,https://doi.org/10.1145/2994459.2994470;http://dx.doi.org/10.1145/2994459.2994470,10.1145/2994459.2994470,"Bring Your Own Device (BYOD) is a growing trend among enterprises, aiming to improve workers' mobility and productivity via their smartphones. The threats and dangers posed by the smartphones to the enterprise are also ever-growing. Such dangers can be mitigated by running the enterprise software inside a ""secure container"" on the smartphone. In our work we present a systematic assessment of security critical areas in design and implementation of a secure container for Android using reverse engineering and attacker-inspired methods. We do this through a case-study of Samsung KNOX, a real-world product deployed on millions of devices. Our research shows how KNOX security features work behind the scenes and lets us compare the vendor's public security claims against reality. Along the way we identified several design weaknesses and a few vulnerabilities that were disclosed to Samsung.","BYOD, mobile platform security, android",SPSM '16,,,
Conference Paper,"Zagieboylo D,Zaman KA",Cost-Efficient and Reliable Reporting of Highly Bursty Video Game Crash Data,,2017,,,201–212,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 8th ACM/SPEC on International Conference on Performance Engineering,"L'Aquila, Italy",2017,9781450344043.0,,https://doi.org/10.1145/3030207.3044529;http://dx.doi.org/10.1145/3030207.3044529,10.1145/3030207.3044529,"Video game crash events are characterized primarily by large media payloads and by highly bursty traffic patterns, with hundreds of thousands or millions of reports being issued in only a few minutes. These events are invaluable in quickly responding to game breaking issues that directly impact user experience. Even the slightest delay in capturing, processing and reporting these events can lead to user abandonment and significant financial cost.A traditional standalone RESTful service, backed by a vertically scaled SQL database is neither a reliable nor cost-effective solution to this problem. An architecture that decouples capture and persistence and uses a horizontally scalable NoSQL database is not only easier to provision, but also uses fewer cpu and memory resources to provide the same end to end latency and throughput.By replacing our RESTful implementation with one that takes advantage both of the aforementioned design and multi-tenant provisioning, we have reduced our dedicated cpu footprint by 63% and memory footprint by 59%. Additionally, we have decreased our data loss during spikes to essentially 0, maintained sub-second persistence latency and improved query latency in the average case by 54% with only a 3% sacrifice for worst case queries.","cost efficiency, cloud infrastructure, crash reporting, reliability, nosql",ICPE '17,,,
Conference Paper,"Ledo D,Houben S,Vermeulen J,Marquardt N,Oehlberg L,Greenberg S",Evaluation Strategies for HCI Toolkit Research,,2018,,,1–17,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems,"Montreal QC, Canada",2018,9781450356206.0,,https://doi.org/10.1145/3173574.3173610;http://dx.doi.org/10.1145/3173574.3173610,10.1145/3173574.3173610,"Toolkit research plays an important role in the field of HCI, as it can heavily influence both the design and implementation of interactive systems. For publication, the HCI community typically expects toolkit research to include an evaluation component. The problem is that toolkit evaluation is challenging, as it is often unclear what 'evaluating' a toolkit means and what methods are appropriate. To address this problem, we analyzed 68 published toolkit papers. From our analysis, we provide an overview of, reflection on, and discussion of evaluation methods for toolkit contributions. We identify and discuss the value of four toolkit evaluation strategies, including the associated techniques that each employs. We offer a categorization of evaluation strategies for toolkit researchers, along with a discussion of the value, potential limitations, and trade-offs associated with each strategy.","toolkits, user interfaces, design, evaluation, prototyping",CHI '18,,,
Conference Paper,Baumann P,Beyond Rasters: Introducing the New OGC Web Coverage Service 2.0,,2010,,,320–329,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 18th SIGSPATIAL International Conference on Advances in Geographic Information Systems,"San Jose, California",2010,9781450304283.0,,https://doi.org/10.1145/1869790.1869835;http://dx.doi.org/10.1145/1869790.1869835,10.1145/1869790.1869835,"In the classical triad of vector, raster, and meta data, it is the raster part which is not yet sufficiently supported in SDIs nowadays. Consequently, integration of earth observation imagery, LIDAR, legacy map scans, etc. into Spatial Data Infrastructures (SDIs) remains incomplete. In terms of standards, the OGC Web Coverage Service (WCS) Standard defines open interfaces for accessing and processing of raster data, more generally: coverages. In August 2010, the completely overhauled WCS 2.0 has been adopted by OGC. To make coverages interchangeable across all OGC-based services, WCS 2.0 has been based on Geography Markup Language (GML) 3.2.1, with a small, backwards compatible addition to achieve informational completeness. In parallel to specification writing, its reference implementation and an online demo are being pursued.WCS 2.0 offers several advantages over previous versions, such as: support for general n-D raster data and non-raster coverage types; crisp, modular, and easy to understand; flexible and adaptive; harmonized with GML and Sensor Web Enablement (SWE); improved testability; and allows for efficient and scalable implementations.In this paper we present WCS 2.0 and some central design rationales. Further, we inspect the reference implementation architecture discussing some features critical for scalability. Finally, we give an outlook on next steps, such as the planned WCS Earth Observation Application Profile.","standards, WCS, raster, grid, OGC, geo services, coverages, web coverage service",GIS '10,,,
Journal Article,Thorson M,Usenet Nuggets,SIGARCH Comput. Archit. News,1991,19.0,5,21–26,Association for Computing Machinery,"New York, NY, USA",,,1991-09,,0163-5964,https://doi.org/10.1145/379189.773720;http://dx.doi.org/10.1145/379189.773720,10.1145/379189.773720,,,,,,
Journal Article,"De Wael M,Marr S,De Fraine B,Van Cutsem T,De Meuter W",Partitioned Global Address Space Languages,ACM Comput. Surv.,2015,47.0,4,,Association for Computing Machinery,"New York, NY, USA",,,2015-05,,0360-0300,https://doi.org/10.1145/2716320;http://dx.doi.org/10.1145/2716320,10.1145/2716320,"The Partitioned Global Address Space (PGAS) model is a parallel programming model that aims to improve programmer productivity while at the same time aiming for high performance. The main premise of PGAS is that a globally shared address space improves productivity, but that a distinction between local and remote data accesses is required to allow performance optimizations and to support scalability on large-scale parallel architectures. To this end, PGAS preserves the global address space while embracing awareness of nonuniform communication costs.Today, about a dozen languages exist that adhere to the PGAS model. This survey proposes a definition and a taxonomy along four axes: how parallelism is introduced, how the address space is partitioned, how data is distributed among the partitions, and finally, how data is accessed across partitions. Our taxonomy reveals that today’s PGAS languages focus on distributing regular data and distinguish only between local and remote data access cost, whereas the distribution of irregular data and the adoption of richer data access cost models remain open challenges.","survey, HPC, data access, message passing, data distribution, one-sided communication, PGAS, Parallel programming",,,,
Conference Paper,"Billman D,Bier EA",Medical Sensemaking with Entity Workspace,,2007,,,229–232,Association for Computing Machinery,"New York, NY, USA",Proceedings of the SIGCHI Conference on Human Factors in Computing Systems,"San Jose, California, USA",2007,9781595935939.0,,https://doi.org/10.1145/1240624.1240662;http://dx.doi.org/10.1145/1240624.1240662,10.1145/1240624.1240662,"Knowledge workers making sense of a topic divide their time among activities including searching for information, reading, and taking notes. We have built a software system that supports and integrates these activities. To test its effectiveness, we conducted a study where subjects used it to perform medical question-answering tasks. Initial results indicate that subjects could use the system, but that the nature of this use depended on the subject's overall question-answering strategy. Two dominant strategies emerged that we call the Reader and Searcher strategies.","note-taking, highlighting, sensemaking, quick click, search",CHI '07,,,
Conference Paper,"Santos GE,Prates RO",Evaluating the PROMISE Framework for Trust in Sharing Economy System,,2018,,,,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 17th Brazilian Symposium on Human Factors in Computing Systems,"Belém, Brazil",2018,9781450366014.0,,https://doi.org/10.1145/3274192.3274212;http://dx.doi.org/10.1145/3274192.3274212,10.1145/3274192.3274212,"The PROMISE (Project Oriented towards Modeling trust Interactions in the Sharing Economy) framework was proposed to represent the users trust in the sharing economy. PROMISE'S proposal was grounded on the literature. In this paper we present a two-step analysis of the framework. To do so, we applied the Semiotic Inspection Method (SIM) in order to reconstruct the designer message to users, and a coding step was applied to evaluate the users experience with the sharing economy. As a result, we have identified some aspects that PROMISE was not able to represent well, and have proposed changes on the framework.","Trust, Sharing Economy, Coding, Semiotic Inspection Method, PROMISE",IHC 2018,,,
Conference Paper,Munnecke T,A Linguistic Comparison of MUMPS and COBOL,,1980,,,723–729,Association for Computing Machinery,"New York, NY, USA","Proceedings of the May 19-22, 1980, National Computer Conference","Anaheim, California",1980,9781450379236.0,,https://doi.org/10.1145/1500518.1500643;http://dx.doi.org/10.1145/1500518.1500643,10.1145/1500518.1500643,"There are endless discussions in data processing circles about which computer language is best. Not surprisingly, the arguments generally boil down to each participant saying: ""The language I know is best."" These dogmatic beliefs often lead to vigorous debates among programmers who use different languages.",,AFIPS '80,,,
Conference Paper,"Schuchart J,Niethammer C,Gracia J",Fibers Are Not (P)Threads: The Case for Loose Coupling of Asynchronous Programming Models and MPI Through Continuations,,2020,,,39–50,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 27th European MPI Users' Group Meeting,"Austin, TX, USA",2020,9781450388801.0,,https://doi.org/10.1145/3416315.3416320;http://dx.doi.org/10.1145/3416315.3416320,10.1145/3416315.3416320,"Asynchronous programming models (APM) are gaining more and more traction, allowing applications to expose the available concurrency to a runtime system tasked with coordinating the execution. While MPI has long provided support for multi-threaded communication and non-blocking operations, it falls short of adequately supporting APMs as correctly and efficiently handling MPI communication in different models is still a challenge. Meanwhile, new low-level implementations of light-weight, cooperatively scheduled execution contexts (fibers, aka user-level threads (ULT)) are meant to serve as a basis for higher-level APMs and their integration in MPI implementations has been proposed as a replacement for traditional POSIX thread support to alleviate these challenges. In this paper, we first establish a taxonomy in an attempt to clearly distinguish different concepts in the parallel software stack. We argue that the proposed tight integration of fiber implementations with MPI is neither warranted nor beneficial and instead is detrimental to the goal of MPI being a portable communication abstraction. We propose MPI Continuations as an extension to the MPI standard to provide callback-based notifications on completed operations, leading to a clear separation of concerns by providing a loose coupling mechanism between MPI and APMs. We show that this interface is flexible and interacts well with different APMs, namely OpenMP detached tasks, OmpSs-2, and Argobots.","Continuations, OmpSs, Tasks, MPI+X, TAMPI, OpenMP, Fiber, ULT",EuroMPI/USA '20,,,
Conference Paper,Heger C,Systematic Guidance in Solving Performance and Scalability Problems,,2013,,,7–12,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 18th International Doctoral Symposium on Components and Architecture,"Vancouver, British Columbia, Canada",2013,9781450321259.0,,https://doi.org/10.1145/2465498.2465500;http://dx.doi.org/10.1145/2465498.2465500,10.1145/2465498.2465500,"The performance of enterprise software systems affects business critical metrics like conversion rate (proportion of visitors who become customers) and total cost of ownership for the software system. Keeping such systems responsive and scalable with a growing user base is challenging for software engineers. Solving performance problems is an error-prone and time consuming task that requires deep expert knowledge about the system and performance evaluation. Existing approaches to support the resolution of performance problems mainly focus on the architecture level neglecting influences of the implementation. In this proposal paper, we introduce a novel approach in order to support software engineers in solving performance and scalability problems that leverages known solutions to common problems. The known solutions are evaluated in the context of the particular software system. As a result, a detailed plan is derived that helps and guides software engineers in resolving the problem. We plan to conduct an industrial case study at SAP.","software engineer support, feedback provisioning, systematic guidance, performance problem resolution",WCOP '13,,,
Conference Paper,"Bergner Y,Mund S,Chen O,Payne W",First Steps in Dance Data Science: Educational Design,,2019,,,,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 6th International Conference on Movement and Computing,"Tempe, AZ, USA",2019,9781450376549.0,,https://doi.org/10.1145/3347122.3347137;http://dx.doi.org/10.1145/3347122.3347137,10.1145/3347122.3347137,"We report results of a design-research effort to develop a culturally-relevant educational experience that can engage high school dancers in statistics and data science. In partnership with a local high school and members of its step team, we explore quantitative analysis of both visual and acoustic data captured from student dance. We describe prototype visualizations and interactive applications for evaluating pose precision, tempo, and timbre. With educational goals in mind, we have constrained our design to using only interpretable features and simple, accessible algorithms.","education, dance analytics, data science, motion capture",MOCO '19,,,
Journal Article,"Shen J,Cai Y,Ren Y,Yang X",A Universal Application Storage System Based on Smart Card,ACM Trans. Embed. Comput. Syst.,2016,15.0,4,,Association for Computing Machinery,"New York, NY, USA",,,2016-09,,1539-9087,https://doi.org/10.1145/2886116;http://dx.doi.org/10.1145/2886116,10.1145/2886116,"Nowadays, electronic commerce (e-commerce) has brought facilitation to people’s daily lives. Smart-card-based systems are widely used as an implementation, where smart cards act as a secure carrier for small-sized data. However, most of these systems are developed and managed by each service provider individually and repeatedly, which causes both unnecessary work and difficulties in future maintenance. Besides, advantages of smart card technology are not full-fledged for the lack of enough consideration in flexibility and security. To propose a solution, this article presents a Universal Application Storage System, including card side, terminal side, and back-end system. The card side provides a universal and secured infrastructure for data storage, where data are organized and stored in a card file system with several security mechanisms. In the terminal side, a framework for accessing various forms of secure element is presented to simplify the procedures involved in manipulating smart cards. Through this framework, the back-end system is able to establish a direct connection to the card, and performs authorized operations by exchanging commands in a secure channel. The validity of the proposed system is verified at the end of this article, illustrated by an e-coupon system.","file system, Smart card, java card, security, application protocol unit",,,,
Conference Paper,"Beck M,Haupt M,Hirschfeld R",NXTalk: Dynamic Object-Oriented Programming in a Constrained Environment,,2009,,,38–49,Association for Computing Machinery,"New York, NY, USA",Proceedings of the International Workshop on Smalltalk Technologies,"Brest, France",2009,9781605588995.0,,https://doi.org/10.1145/1735935.1735942;http://dx.doi.org/10.1145/1735935.1735942,10.1145/1735935.1735942,"Dynamic programming languages offer high expressiveness and flexibility, improving programmer productivity. Still, making dynamic programming languages available for embedded systems is challenging because such environments are often constrained in terms of memory or computational power. For this, it is necessary to reduce the size of language implementations (virtual machines, VMs) while at the same time retaining good performance and robustness. Automatic memory management deserves special attention because its performance and space overhead have noticeable impact on overall system usability. In this paper, we present NXTalk, a VM and programming environment for the Smalltalk programming language, making high-level object-oriented programming available on Lego Mindstorms NXT robots. We describe its VM implementation and evaluate its size and performance characteristics.","Smalltalk, embedded system, dynamic programming language, virtual machine, Lego Mindstorms NXT, resource-constrained device, Squeak",IWST '09,,,
Conference Paper,"Li S,Lu Y,Shu J,Hu Y,Li T",LocoFS: A Loosely-Coupled Metadata Service for Distributed File Systems,,2017,,,,Association for Computing Machinery,"New York, NY, USA","Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis","Denver, Colorado",2017,9781450351140.0,,https://doi.org/10.1145/3126908.3126928;http://dx.doi.org/10.1145/3126908.3126928,10.1145/3126908.3126928,"Key-Value stores provide scalable metadata service for distributed file systems. However, the metadata's organization itself, which is organized using a directory tree structure, does not fit the key-value access pattern, thereby limiting the performance. To address this issue, we propose a distributed file system with a loosely-coupled metadata service, LocoFS, to bridge the performance gap between file system metadata and key-value stores. LocoFS is designed to decouple the dependencies between different kinds of metadata with two techniques. First, LocoFS decouples the directory content and structure, which organizes file and directory index nodes in a flat space while reversely indexing the directory entries. Second, it decouples the file metadata to further improve the key-value access performance. Evaluations show that LocoFS with eight nodes boosts the metadata throughput by 5 times, which approaches 93% throughput of a single-node key-value store, compared to 18% in the state-of-the-art IndexFS.","key-value stores, distributed storage, file systems management, distributed architectures",SC '17,,,
Conference Paper,"Paccagnella R,Liao K,Tian D,Bates A",Logging to the Danger Zone: Race Condition Attacks and Defenses on System Audit Frameworks,,2020,,,1551–1574,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 2020 ACM SIGSAC Conference on Computer and Communications Security,"Virtual Event, USA",2020,9781450370899.0,,https://doi.org/10.1145/3372297.3417862;http://dx.doi.org/10.1145/3372297.3417862,10.1145/3372297.3417862,"For system logs to aid in security investigations, they must be beyond the reach of the adversary. Unfortunately, attackers that have escalated privilege on a host are typically able to delete and modify log events at will. In response to this threat, a variety of secure logging systems have appeared over the years that attempt to provide tamper-resistance (e.g., write once read many drives, remote storage servers) or tamper-evidence (e.g., cryptographic proofs) for system logs. These solutions expose an interface through which events are committed to a secure log, at which point they enjoy protection from future tampering. However, all proposals to date have relied on the assumption that an event's occurrence is concomitant with its commitment to the secured log.In this work, we challenge this assumption by presenting and validating a race condition attack on the integrity of audit frameworks. Our attack exploits the intrinsically asynchronous nature of I/O and IPC activity, demonstrating that an attacker can snatch events about their intrusion out of message buffers after they have occurred but before they are committed to the log, thus bypassing existing protections. We present a first step towards defending against our attack by introducing KennyLoggings, the first kernel- based tamper-evident logging system that satisfies the synchronous integrity property, meaning that it guarantees tamper-evidence of events upon their occurrence. We implement KennyLoggings on top of the Linux kernel and show that it imposes between 8% and 11% overhead on log-intensive application workloads.","forward security, Linux kernel, race conditions, tamper-evident logs, digital forensics, system auditing, operating systems",CCS '20,,,
Conference Paper,"Paci P,Mancini C,Price BA",The Role of Ethological Observation for Measuring Animal Reactions to Biotelemetry Devices,,2017,,,,Association for Computing Machinery,"New York, NY, USA",Proceedings of the Fourth International Conference on Animal-Computer Interaction,"Milton Keynes, United Kingdom",2017,9781450353649.0,,https://doi.org/10.1145/3152130.3152144;http://dx.doi.org/10.1145/3152130.3152144,10.1145/3152130.3152144,"This paper presents a methodological approach used to assess the wearability of biotelemetry devices in animals. A detailed protocol to gather quantitative and qualitative ethological observations was adapted and tested in an experimental study of 13 cat participants wearing two different GPS devices. The aim was twofold: firstly, to ascertain the potential interference generated by the devices on the animal body and behavior by quantifying and characterizing it; secondly, to individuate device features potentially responsible for the influence registered, and establish design requirements. This research contributes towards the development of a framework for evaluating the design of wearer-centered biotelemetry interventions for animals, consistent with values advocated by Animal-Computer Interaction researchers.","wearability, wearer-centered design, Biotelemetry, animal-computer interaction",ACI2017,,,
Conference Paper,"Liu D,Hua KA,Cheng H",Handle Local Optimum Traps in CBIR Systems,,2008,,,1202–1206,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 2008 ACM Symposium on Applied Computing,"Fortaleza, Ceara, Brazil",2008,9781595937537.0,,https://doi.org/10.1145/1363686.1363965;http://dx.doi.org/10.1145/1363686.1363965,10.1145/1363686.1363965,"Existing CBIR systems, designed around query refinement based on relevance feedback, suffer from local optimum traps. That is, when the user is examining a relevant cluster surrounded by less relevant images, essentially the same set of images will be returned for the user to provide relevance feedback. Since the user would select the same query images again, the relevance feedback process gets trapped in a local optimum. This local-optimum trap problem may severely impair the overall retrieval performance of today's CBIR systems. In this paper, we therefore propose a simulated annealing-based approach to address this important issue. When a stuck-at-a-local-optimum occurs, we employ a neighborhood search technique (i.e., simulated annealing) to escape from the local optimum. We also propose an index structure to speed up such neighborhood search. Our experimental study confirms that our approach can efficiently address the local-optimum trap problem, and therefore can improve the effectiveness of existing CBIR systems.","local optimum traps, content-based image retrieval, relevance feedback, query point movement techniques",SAC '08,,,
Conference Paper,Kopstein FF,Rational vs. Empirical Approaches to Job/Task Descriptions for COBOL Programmers,,1969,,,128–138,Association for Computing Machinery,"New York, NY, USA",Proceedings of the Seventh Annual Conference on SIGCPR,"Illinois, USA",1969,9781450374729.0,,https://doi.org/10.1145/800163.805189;http://dx.doi.org/10.1145/800163.805189,10.1145/800163.805189,"Perhaps the intent of this paper will stand out more clearly as figure when viewed against the background or context in which it first arose. While not directly relevant, the context is the development of a computer-administered instruction (CAI) course designed to produce competent COBOL programmers for the U.S. Army. To produce competent COBOL programmers one must specify what constitutes on-the-job competence and this concern led to the considerations to be set forth.Let it be noted clearly at the outset that these considerations obviously transcend COBOL as subject matter and training development as goal. Both are used here merely to provide a continuing illustrative framework. Because the primary concern here is with technique in the sense of practical (not formal) methodology, only simulated empirical data will be presented.",,SIGCPR '69,,,
Conference Paper,"Bastys I,Balliu M,Sabelfeld A",If This Then What? Controlling Flows in IoT Apps,,2018,,,1102–1119,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 2018 ACM SIGSAC Conference on Computer and Communications Security,"Toronto, Canada",2018,9781450356930.0,,https://doi.org/10.1145/3243734.3243841;http://dx.doi.org/10.1145/3243734.3243841,10.1145/3243734.3243841,"IoT apps empower users by connecting a variety of otherwise unconnected services. These apps (or applets ) are triggered by external information sources to perform actions on external information sinks. We demonstrate that the popular IoT app platforms, including IFTTT (If This Then That), Zapier, and Microsoft Flow are susceptible to attacks by malicious applet makers, including stealthy privacy attacks to exfiltrate private photos, leak user location, and eavesdrop on user input to voice-controlled assistants. We study a dataset of 279,828 IFTTT applets from more than 400 services, classify the applets according to the sensitivity of their sources, and find that 30% of the applets may violate privacy. We propose two countermeasures for short- and longterm protection: access control and information flow control. For short-term protection, we suggest that access control classifies an applet as either exclusively private or exclusively public, thus breaking flows from private sources to sensitive sinks. For longterm protection, we develop a framework for information flow tracking in IoT apps. The framework models applet reactivity and timing behavior, while at the same time faithfully capturing the subtleties of attacker observations caused by applet output. We show how to implement the approach for an IFTTT-inspired setting leveraging state-of-the-art information flow tracking techniques for JavaScript based on the JSFlow tool and evaluate its effectiveness on a collection of applets.","IoT apps, information flow, access control",CCS '18,,,
Conference Paper,"Novark G,Berger ED",DieHarder: Securing the Heap,,2010,,,573–584,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 17th ACM Conference on Computer and Communications Security,"Chicago, Illinois, USA",2010,9781450302456.0,,https://doi.org/10.1145/1866307.1866371;http://dx.doi.org/10.1145/1866307.1866371,10.1145/1866307.1866371,"Heap-based attacks depend on a combination of memory management error and an exploitable memory allocator. Many allocators include ad hoc countermeasures against particular exploits but their effectiveness against future exploits has been uncertain. This paper presents the first formal treatment of the impact of allocator design on security. It analyzes a range of widely-deployed memory allocators, including those used by Windows, Linux, FreeBSD and OpenBSD, and shows that they remain vulnerable to attack. It them presents DieHarder, a new allocator whose design was guided by this analysis. DieHarder provides the highest degree of security from heap-based attacks of any practical allocator of which we are aware while imposing modest performance overhead. In particular, the Firefox web browser runs as fast with DieHarder as with the Linux allocator.","dynamic memory allocation, memory errors, buffer overflow, dangling pointer",CCS '10,,,
Conference Paper,"Schümmer T,Haake JM,Stark W",Beyond Rational Design Patterns,,2014,,,,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 19th European Conference on Pattern Languages of Programs,"Irsee, Germany",2014,9781450334167.0,,https://doi.org/10.1145/2721956.2721984;http://dx.doi.org/10.1145/2721956.2721984,10.1145/2721956.2721984,"During the last decades, the pattern movement has been dominated by patterns describing the design of technology or matter, especially when it focused on the design of software or buildings. Positioned in the physical world, patterns are technical, full of formal instructions, and often building on a rational understanding of the world they act on. We argue that technical and formal pattern structures support the rational line of thought very well but also limit creativity of people applying the patterns. In contrast to this, a creative and performative account to design does at least not prevent the emergence of new structures not yet formalized in design patterns. In order to start a discussion about this distinction in the pattern community, this paper proposes eight theses that may serve as a research agenda towards a better understanding of the role of patterns for creative and performative design.","improvisation, christopher alexander, pattern breaking, rational design, design process, determinism, tacit knowledge, cultural values, design pattern, pattern language, explicit knowledge, performative design",EuroPLoP '14,,,
Conference Paper,"Kuznetsov S,Davis GN,Paulos E,Gross MD,Cheung JC","Red Balloon, Green Balloon, Sensors in the Sky",,2011,,,237–246,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 13th International Conference on Ubiquitous Computing,"Beijing, China",2011,9781450306300.0,,https://doi.org/10.1145/2030112.2030145;http://dx.doi.org/10.1145/2030112.2030145,10.1145/2030112.2030145,"Spectacle computing is a novel strategy for vibrantly projecting information into the public sphere using expressive and tangible media. We demonstrate an example of this computing meme with large, glowing balloons that change color based on input from attached air quality sensors (exhaust, diesel, or volatile organic compounds). In two public installations (city street and public park) and a deployment with six everyday citizens, we invited stakeholders to playfully explore and actively participate in visualizing surrounding air quality. We also created a do-it-yourself (DIY) kit that includes a printed circuit board, electronic parts and instructions for building the air quality balloons. In a workshop, six non-expert users successfully assembled functional balloons, validating our technology as a DIY tool for public air quality visualization. Our deployments and workshop highlight play and spectacle as essential elements for public participation and activism. We outline design guidelines for future spectacle computing projects that engage stakeholders with environmental data and empower them to transform urban landscapes.","spectacle computing, urban computing",UbiComp '11,,,
Book,,"ARRAY 2016: Proceedings of the 3rd ACM SIGPLAN International Workshop on Libraries, Languages, and Compilers for Array Programming",,2016,,,,Association for Computing Machinery,"New York, NY, USA",,"Santa Barbara, CA, USA",2016,9781450343848.0,,,,,,,Proceedings,,
Conference Paper,"Blume T,Richerby D,Scherp A",Incremental and Parallel Computation of Structural Graph Summaries for Evolving Graphs,,2020,,,75–84,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 29th ACM International Conference on Information & Knowledge Management,"Virtual Event, Ireland",2020,9781450368599.0,,https://doi.org/10.1145/3340531.3411878;http://dx.doi.org/10.1145/3340531.3411878,10.1145/3340531.3411878,"Graph summarization is the task of finding condensed representations of graphs such that a chosen set of (structural) subgraph features in the graph summary are equivalent to the input graph. Existing graph summarization algorithms are tailored to specific graph summary models, only support one-time batch computation, are designed and implemented for a specific task, or evaluated using static graphs. Our novel, incremental, parallel algorithm addresses all these shortcomings. We support various structural graph summary models defined in our formal language FLUID. All graph summaries defined with FLUID can be updated in time O(Δ · dk), where Δ is the number of additions, deletions, and modifications to the input graph, d is its maximum degree, and k is the maximum distance in the subgraphs considered. We empirically evaluate the performance of our algorithm on benchmark and real-world datasets. Our experiments show that, for commonly used summary models and datasets, the incremental summarization algorithm almost always outperforms their batch counterpart, even when about $50%$ of the graph database changes. The source code and the experimental results are openly available for reproducibility and extensibility.","evolving graphs, graph summarization, incremental algorithm",CIKM '20,,,
Conference Paper,Fong PW,Pluggable Verification Modules: An Extensible Protection Mechanism for the JVM,,2004,,,404–418,Association for Computing Machinery,"New York, NY, USA","Proceedings of the 19th Annual ACM SIGPLAN Conference on Object-Oriented Programming, Systems, Languages, and Applications","Vancouver, BC, Canada",2004,9781581138313.0,,https://doi.org/10.1145/1028976.1029010;http://dx.doi.org/10.1145/1028976.1029010,10.1145/1028976.1029010,"Through the design and implementation of a JVM that supports Pluggable Verification Modules (PVMs), the idea of an extensible protection mechanism is entertained. Link-time bytecode verification becomes a pluggable service that can be readily replaced, reconfigured and augmented. Application-specific verification services can be safely introduced into the dynamic linking process of the JVM. This feature is enabled by the adoption of a previously proposed modular verification architecture, Proof Linking [23, 24], which decouples bytecode verification from the dynamic linking process, rendering the verifier a replaceable module. The PVM mechanism has been implemented in an open source JVM, the Aegis VM [21]. To evaluate the software engineering and security engineering benefits of this extensible protection mechanism, an augmented type system JAC (Java Access Control) [37] has been successfully implemented as a PVM.","extensible protection mechanism, bytecode verification, proof linking, Java virtual machine, Aegis VM, pluggable verification modules, mobile code security, extensible systems",OOPSLA '04,,,
Journal Article,Fong PW,Pluggable Verification Modules: An Extensible Protection Mechanism for the JVM,SIGPLAN Not.,2004,39.0,10,404–418,Association for Computing Machinery,"New York, NY, USA",,,2004-10,,0362-1340,https://doi.org/10.1145/1035292.1029010;http://dx.doi.org/10.1145/1035292.1029010,10.1145/1035292.1029010,"Through the design and implementation of a JVM that supports Pluggable Verification Modules (PVMs), the idea of an extensible protection mechanism is entertained. Link-time bytecode verification becomes a pluggable service that can be readily replaced, reconfigured and augmented. Application-specific verification services can be safely introduced into the dynamic linking process of the JVM. This feature is enabled by the adoption of a previously proposed modular verification architecture, Proof Linking [23, 24], which decouples bytecode verification from the dynamic linking process, rendering the verifier a replaceable module. The PVM mechanism has been implemented in an open source JVM, the Aegis VM [21]. To evaluate the software engineering and security engineering benefits of this extensible protection mechanism, an augmented type system JAC (Java Access Control) [37] has been successfully implemented as a PVM.","pluggable verification modules, extensible protection mechanism, Aegis VM, mobile code security, bytecode verification, proof linking, extensible systems, Java virtual machine",,,,
Conference Paper,"Coblenz M,Aldrich J,Myers BA,Sunshine J",Interdisciplinary Programming Language Design,,2018,,,133–146,Association for Computing Machinery,"New York, NY, USA","Proceedings of the 2018 ACM SIGPLAN International Symposium on New Ideas, New Paradigms, and Reflections on Programming and Software","Boston, MA, USA",2018,9781450360319.0,,https://doi.org/10.1145/3276954.3276965;http://dx.doi.org/10.1145/3276954.3276965,10.1145/3276954.3276965,"Approaches for programming language design used commonly in the research community today center around theoretical and performance-oriented evaluation. Recently, researchers have been considering more approaches to language design, including the use of quantitative and qualitative user studies that examine how different designs might affect programmers. In this paper, we argue for an interdisciplinary approach that incorporates many different methods in the creation and evaluation of programming languages. We argue that the addition of user-oriented design techniques can be helpful at many different stages in the programming language design process.","programming language evaluation, programming language design, user-centered design",Onward! 2018,,,
Journal Article,"Feng J,Lazar J,Kumin L,Ozok A",Computer Usage by Children with Down Syndrome: Challenges and Future Research,ACM Trans. Access. Comput.,2010,2.0,3,,Association for Computing Machinery,"New York, NY, USA",,,2010-03,,1936-7228,https://doi.org/10.1145/1714458.1714460;http://dx.doi.org/10.1145/1714458.1714460,10.1145/1714458.1714460,"Children with Down syndrome, like neurotypical children, are growing up with extensive exposure to computer technology. Computers and computer-related devices have the potential to help these children in education, career development, and independent living. Our understanding of computer usage by this population is quite limited. Most of the software, games, and Web sites that children with Down syndrome interact with are designed without consideration of their special needs, making the applications less effective or completely inaccessible. We conducted a large-scale survey that collected computer usage information from the parents of approximately six hundred children with Down syndrome. This article reports the text responses collected in the survey and is intended as a step towards understanding the difficulties children with Down syndrome experience while using computers. The relationship between the age and the specific type of difficulties, as well as related design challenges are also reported. A number of potential research directions and hypotheses are identified for future studies. Due to limitations in survey methodology, the findings need to be further validated through hypothesis-driven, empirical studies.","Down syndrome, human-computer interaction, computer use, children",,,,
Journal Article,"Parikh R,Bertacco V",ForEVeR: A Complementary Formal and Runtime Verification Approach to Correct NoC Functionality,ACM Trans. Embed. Comput. Syst.,2014,13.0,3s,,Association for Computing Machinery,"New York, NY, USA",,,2014-03,,1539-9087,https://doi.org/10.1145/2514871;http://dx.doi.org/10.1145/2514871,10.1145/2514871,"As silicon technology scales, modern processor and embedded systems are rapidly shifting towards complex chip multi-processor (CMP) and system-on-chip (SoC) designs. As a side effect of complexity of these designs, ensuring their correctness has become increasingly problematic. Within these domains, Network-on-Chips (NoCs) are a de-facto choice to implement on-chip interconnect; their design is quickly becoming extremely complex in order to keep up with communication performance demands. As a result, design errors in the NoC may go undetected and escape into the final silicon.In this work, we propose ForEVeR, a solution that complements the use of formal methods and runtime verification to ensure functional correctness in NoCs. Formal verification, due to its scalability limitations, is used to verify smaller modules, such as individual router components. To deliver correctness guarantees for the complete network, we propose a network-level detection and recovery solution that monitors the traffic in the NoC and protects it against escaped functional bugs. To this end, ForEVeR augments the baseline NoC with a lightweight checker network that alerts destination nodes of incoming packets ahead of time. If a bug is detected, flagged by missed packet arrivals, our recovery mechanism delivers the in-flight data safely to the intended destination via the checker network. ForEVeR's experimental evaluation shows that it can recover from NoC design errors at only 4.9% area cost for an 8x8 mesh interconnect, over a time interval ranging from 0.5K to 30K cycles per recovery event, and it incurs no performance overhead in the absence of errors. ForEVeR can also protect NoC operations against soft-errors: a growing concern with the scaling of silicon. ForEVeR leverages the same monitoring hardware to detect soft-error manifestations, in addition to design-errors. Recovery of the soft-error affected packets is guaranteed by building resiliency features into our checker network. ForEVeR incurs minimal performance penalty up to a flit error rate of 0.01% in lightly loaded networks.","runtime verification, Network-on-chip, NoC, formal verification, functional correctness",,,,
Conference Paper,"Wan Z,Ren K,Preneel B",A Secure Privacy-Preserving Roaming Protocol Based on Hierarchical Identity-Based Encryption for Mobile Networks,,2008,,,62–67,Association for Computing Machinery,"New York, NY, USA",Proceedings of the First ACM Conference on Wireless Network Security,"Alexandria, VA, USA",2008,9781595938145.0,,https://doi.org/10.1145/1352533.1352544;http://dx.doi.org/10.1145/1352533.1352544,10.1145/1352533.1352544,"Roaming services in wireless networks provide people with preferable flexibility and convenience. However, such advantages should be offered with both security and privacy in mind. With consideration on privacy protection during roaming in wireless networks, we proposed a hierarchical ID-based roaming protocol in this paper. In our scheme, we use a 2-layer hierarchical ID-based cryptosystem in which a trusted party acts as the root authority, each domain server acts as the second-layer authority, and the roaming user is the end user. With the hierarchical ID-based cryptosystem, we can avoid involvement with home network, and keep the roaming the user's identity private. Furthermore, not only the root authority is relieved from management of a large amount of private/public key pairs, but the domain servers are free to generate key pairs for their registered users. At the same time, we use hash chains together with ID-based signatures to achieve non-repudiation for service payment.","mobile networks, privacy, roaming protocols",WiSec '08,,,
Journal Article,"Borchers J,Deussen O,Knörzer C",Getting It across: Layout Issues for Kiosk Systems,SIGCHI Bull.,1995,27.0,4,68–74,Association for Computing Machinery,"New York, NY, USA",,,1995-10,,0736-6906,https://doi.org/10.1145/214132.214170;http://dx.doi.org/10.1145/214132.214170,10.1145/214132.214170,"A clear and appealing screen layout is crucial to the success of on-line kiosk systems, public terminals that are connected to a network. This paper addresses the problem of developing such a layout, and provides several guidelines, drawn from traditional typography and Gestalt psychology as well as from hypertext authoring, and human-computer interaction. To identify how a kiosk system's primary task influences optimal layout, kiosk systems are classified into four basic types. The usability of HTML (Hypertext Markup Language) 2.0 and 3.0 to write documents for these systems is discussed, and some alternative existing environments are presented.",,,,,
Conference Paper,"Zhao J,Liu Z,Dontcheva M,Hertzmann A,Wilson A",MatrixWave: Visual Comparison of Event Sequence Data,,2015,,,259–268,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems,"Seoul, Republic of Korea",2015,9781450331456.0,,https://doi.org/10.1145/2702123.2702419;http://dx.doi.org/10.1145/2702123.2702419,10.1145/2702123.2702419,"Event sequence data analysis is common in many domains, including web and software development, transportation, and medical care. Few have investigated visualization techniques for comparative analysis of multiple event sequence datasets. Grounded in the real-world characteristics of web clickstream data, we explore visualization techniques for comparison of two clickstream datasets collected on different days or from users with different demographics. Through iterative design with web analysts, we designed MatrixWave, a matrix-based representation that allows analysts to get an overview of differences in traffic patterns and interactively explore paths through the website. We use color to encode differences and size to offer context over traffic volume. User feedback on MatrixWave is positive. Our study participants made fewer errors with MatrixWave and preferred it over the more familiar Sankey diagram.","information visualization, visual comparison, event sequences, matrix representation, sankey diagram",CHI '15,,,
Conference Paper,"Wu J,Shekh S,Sergiienko NY,Cazzolato BS,Ding B,Neumann F,Wagner M",Fast and Effective Optimisation of Arrays of Submerged Wave Energy Converters,,2016,,,1045–1052,Association for Computing Machinery,"New York, NY, USA",Proceedings of the Genetic and Evolutionary Computation Conference 2016,"Denver, Colorado, USA",2016,9781450342063.0,,https://doi.org/10.1145/2908812.2908844;http://dx.doi.org/10.1145/2908812.2908844,10.1145/2908812.2908844,"Renewable forms of energy are becoming increasingly important to consider, as the global energy demand continues to grow. Wave energy is one of these widely available forms, but it is largely unexploited. A common design for a wave energy converter is called a point absorber or buoy. The buoy typically floats on the surface or just below the surface of the water, and captures energy from the movement of the waves. It can use the motion of the waves to drive a pump to generate electricity and to create potable water. Since a single buoy can only capture a limited amount of energy, large-scale wave energy production necessitates the deployment of buoys in large numbers called arrays. However, the efficiency of arrays of buoys is affected by highly complex intra-buoy interactions. The contributions of this article are two-fold. First, we present an approximation of the buoy interactions model that results in a 350-fold computational speed-up to enable the use inside of iterative optimisation algorithms, Second, we study arrays of fully submerged three-tether buoys, with and without shared mooring points.","wave energy, evolutionary algorithm, renewable energy",GECCO '16,,,
Conference Paper,Denning DE,A New Paradigm for Trusted Systems,,1993,,,36–41,Association for Computing Machinery,"New York, NY, USA",Proceedings on the 1992-1993 Workshop on New Security Paradigms,"Little Compton, Rhode Island, USA",1993,9780818654305.0,,https://doi.org/10.1145/283751.283772;http://dx.doi.org/10.1145/283751.283772,10.1145/283751.283772,,,NSPW '92-93,,,
Conference Paper,"Michael E,Woos D,Anderson T,Ernst MD,Tatlock Z",Teaching Rigorous Distributed Systems With Efficient Model Checking,,2019,,,,Association for Computing Machinery,"New York, NY, USA",Proceedings of the Fourteenth EuroSys Conference 2019,"Dresden, Germany",2019,9781450362818.0,,https://doi.org/10.1145/3302424.3303947;http://dx.doi.org/10.1145/3302424.3303947,10.1145/3302424.3303947,"Writing correct distributed systems code is difficult, especially for novice programmers. The inherent asynchrony and need for fault-tolerance make errors almost inevitable. Industrial-strength testing and model checking have been shown to be effective at uncovering bugs, but they come at a cost --- in both time and effort --- that is far beyond what students can afford. To address this, we have developed an efficient model checking framework and visual debugger for distributed systems, with the goal of helping students find and fix bugs in near real-time. We identify two novel techniques for reducing the search state space to more efficiently find bugs in student implementations. We report our experiences using these tools to help over two hundred students build a correct, linearizable, fault-tolerant, dynamically-sharded key--value store.","distributed systems, model checking, education",EuroSys '19,,,
Conference Paper,"Niakanlahiji A,Jafarian JH",WebMTD: Defeating Web Code Injection Attacks Using Web Element Attribute Mutation,,2017,,,17–26,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 2017 Workshop on Moving Target Defense,"Dallas, Texas, USA",2017,9781450351768.0,,https://doi.org/10.1145/3140549.3140559;http://dx.doi.org/10.1145/3140549.3140559,10.1145/3140549.3140559,"Existing mitigation techniques for Web code injection attacks have not been widely adopted, primarily due to incurring impractical overheads on the developer, Web applications, or Web browsers. They either substantially increase Web server/client execution time, enforce restrictive coding practices on developers, fail to support legacy Web applications, demand browser code modification, or fail to provide browser backward compatibility. Moving Target Defense (MTD) is a novel proactive class of techniques that aim to defeat attacks by imposing uncertainty in attack reconnaissance and planning. This uncertainty is achieved by frequent and random mutation (randomization) of system configuration in a manner that is not traceable (predictable) by attackers. In this paper, we present WebMTD, a proactive moving target defense mechanism that thwarts a broad class of code injection attacks on Web applications, including cross-site scripting (XSS), HTML code injection, and server-side code injection attacks, in a manner that is transparent to developers, Web applications and browsers. Relying on built-in features of modern Web browsers, WebMTD randomizes certain attributes of Web elements to differentiate the application code from the injected code and disallow its execution; this is done without requiring Web developer involvement and browser code modification. Through rigorous evaluation, we show that WebMTD has very low performance overhead. Also, we argue that our technique outperforms all competing approaches due to its broad effectiveness, transparency, and low overhead. We claim that these qualities make WebMTD an ideal technique for defeating Web code injection attacks on real-world production Web applications.","web code injection attack, xss attack, moving target defense",MTD '17,,,
Journal Article,"Yamazaki T,Nakamaru T,Ichikawa K,Chiba S",Generating a Fluent API with Syntax Checking from an LR Grammar,Proc. ACM Program. Lang.,2019,3.0,OOPSLA,,Association for Computing Machinery,"New York, NY, USA",,,2019-10,,,https://doi.org/10.1145/3360560;http://dx.doi.org/10.1145/3360560,10.1145/3360560,"This paper proposes a fluent API generator for Scala, Haskell, and C++. It receives a grammar definition and generates a code skeleton of the library in the host programming language. The generated library is accessed through a chain of method calls; this style of API is called a fluent API. The library uses the host-language type checker to detect an invalid chain of method calls. Each method call is regarded as a lexical token in the embedded domain specific language implemented by that library. A sequence of the lexical tokens is checked and, if the sequence is not acceptable by the grammar, a type error is reported during compilation time. A contribution of this paper is to present an algorithm for generating the code-skeleton for a fluent API that reports a type error when a chain of method calls to the library does not match the given LR grammar. Our algorithm works in Scala, Haskell, and C++. To encode LR parsing, it uses the method/function overloading available in those languages. It does not need an advanced type system, or exponential compilation time or memory consumption. This paper also presents our implementation of the proposed generator.","LR parsing, fluent API, metaprogramming, library generation",,,,
Conference Paper,"Heller D,Krenzelok L,Orr J",Webtop: Realities in Designing a Web-Application Platform,,2003,,,1–15,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 2003 Conference on Designing for User Experiences,"San Francisco, California",2003,9781581137286.0,,https://doi.org/10.1145/997078.997115;http://dx.doi.org/10.1145/997078.997115,10.1145/997078.997115,"This case study focuses on the design process for a thin-client in a real world enterprise software environment, created for our own internal sales and marketing directives. This project became the basis of our biggest upgrade migration in four years. The project increased sales and earned our product industry awards, but was most successful for the paradigm shift in corporate culture; specifically, user experience has become a fundamental part of our development process, a challenge in most organizations. Developing this client as an interaction platform facilitated the design of applications based on the Documentum platform and increased client consistency throughout the Documentum product line.","design process, B2B, Web Content Management (WCM), interface design, thin-client, document management, workflow management, documentum, compliance, Web Publishing, library services, user experience, Enterprise Content Management (ECM), enterprise software, content management, B2All, Web applications, platform design, user interface, interaction design, accessibility",DUX '03,,,
Journal Article,Sites RL,"Benchmarking ""Hello, World!"": Six Different Views of the Execution of ""Hello, World!"" Show What is Often Missing in Today’s Tools",Queue,2018,16.0,5,54–80,Association for Computing Machinery,"New York, NY, USA",,,2018-10,,1542-7730,https://doi.org/10.1145/3291276.3291278;http://dx.doi.org/10.1145/3291276.3291278,10.1145/3291276.3291278,"As more and more software moves off the desktop and into data centers, and more and more cell phones use server requests as the other half of apps, observation tools for large-scale distributed transaction systems are not keeping up. This makes it tempting to look under the lamppost using simpler tools. You will waste a lot of high-pressure time following that path when you have a sudden complex performance crisis. Instead, know what each tool you use is blind to, know what information you need to understand a performance problem, and then look for tools that can actually observe that information directly.",,,,,
Journal Article,Arpaci-Dusseau RH,Run-Time Adaptation in River,ACM Trans. Comput. Syst.,2003,21.0,1,36–86,Association for Computing Machinery,"New York, NY, USA",,,2003-02,,0734-2071,https://doi.org/10.1145/592637.592639;http://dx.doi.org/10.1145/592637.592639,10.1145/592637.592639,"We present the design, implementation, and evaluation of run-time adaptation within the River dataflow programming environment. The goal of the River system is to provide adaptive mechanisms that allow database query-processing applications to cope with performance variations that are common in cluster platforms. We describe the system and its basic mechanisms, and carefully evaluate those mechanisms and their effectiveness. In our analysis, we answer four previously unanswered and important questions. Are the core run-time adaptive mechanisms effective, especially as compared to the ideal? What are the keys to making them work well? Can applications easily use these primitives? And finally, are there situations in which run-time adaptation is not sufficient? In performing our study, we utilize a three-pronged approach, comparing results from idealized models of system behavior, targeted simulations, and a prototype implementation. As well as providing insight on the positives and negatives of run-time adaptation both specifically in River and in a broader context, we also comment on the interplay of modeling, simulation, and implementation in system design.","clusters, Performance availability, robust performance, performance faults, parallel I/O, run-time adaptation",,,,
Conference Paper,"Cabral B,Marques P",Exception Handling: A Field Study in Java and .NET,,2007,,,151–175,Springer-Verlag,"Berlin, Heidelberg",Proceedings of the 21st European Conference on Object-Oriented Programming,"Berlin, Germany",2007,9783540735885.0,,,,"Most modern programming languages rely on exceptions for dealing with abnormal situations. Although exception handling was a significant improvement over other mechanisms like checking return codes, it is far from perfect. In fact, it can be argued that this mechanism is seriously limited, if not, flawed. This paper aims to contribute to the discussion by providing quantitative measures on how programmers are currently using exception handling. We examined 32 different applications, both for Java and .NET. The major conclusion for this work is that exceptions are not being correctly used as an error recovery mechanism. Exception handlers are not specialized enough for allowing recovery and, typically, programmers just do one of the following actions: logging, user notification and application termination. To our knowledge, this is the most comprehensive study done on exception handling to date, providing a quantitative measure useful for guiding the development of new error handling mechanisms.","programming languages, exception handling mechanisms",ECOOP'07,,,
Conference Paper,"Gisdakis S,Giannetsos T,Papadimitratos P",Android Privacy C(R)Ache: Reading Your External Storage and Sensors for Fun and Profit,,2016,,,1–10,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 1st ACM Workshop on Privacy-Aware Mobile Computing,"Paderborn, Germany",2016,9781450343466.0,,https://doi.org/10.1145/2940343.2940346;http://dx.doi.org/10.1145/2940343.2940346,10.1145/2940343.2940346,"Android's permission system empowers informed privacy decisions when installing third-party applications. However, examining the access permissions is not enough to assess privacy exposure; even seemingly harmless applications can severely expose user data. This is what we demonstrate here: an application with the common READ_EXTERNAL_STORAGE and the INTERNET permissions can be the basis of extracting and inferring a wealth of private information. What has been overlooked is that such a ""curious"" application can prey on data stored in the Android's commonly accessible external storage or on unprotected phone sensors. By accessing and stealthily extracting data thought to be unworthy of protection, we manage to access highly sensitive information: user identifiers and habits. Leveraging data-mining techniques, we explore a set of popular applications, establishing that there is a clear privacy danger for numerous users installing innocent-looking and but, possibly, ""curious"" applications.","monitoring, external storage, personal data leakage, profiling, Android permissions",PAMCO '16,,,
Journal Article,"Bombieri N,Fummi F,Vinco S",A Methodology to Recover RTL IP Functionality for Automatic Generation of SW Applications,ACM Trans. Des. Autom. Electron. Syst.,2015,20.0,3,,Association for Computing Machinery,"New York, NY, USA",,,2015-06,,1084-4309,https://doi.org/10.1145/2720019;http://dx.doi.org/10.1145/2720019,10.1145/2720019,"With the advent of heterogeneous multiprocessor system-on-chips (MPSoCs), hardware/software partitioning is again on the rise both in research and in product development. In this new scenario, implementing intellectual-property (IP) blocks as SW applications rather than dedicated HW is an increasing trend to fully exploit the computation power provided by the MPSoC CPUs. On the other hand, whole libraries of IP blocks are available as RTL descriptions, most of them without a corresponding high-level SW implementation. In this context, this article presents a methodology to automatically generate SW applications in C++, by starting from existing RTL IPs implemented in hardware description language (HDL). The methodology exploits an abstraction algorithm to eliminate implementation details typical of HW descriptions (such as cycle-accurate functionality and data types) to guarantee relevant performance of the generated code. The experimental results show that, in many cases, the C++ code automatically generated in a few seconds with the proposed methodology is as efficient as the corresponding code manually implemented from scratch.","IP reuse, RTL IP, embedded software generation",,,,
Journal Article,"Nehme RV,Rundensteiner EA,Bertino E",Tagging Stream Data for Rich Real-Time Services,Proc. VLDB Endow.,2009,2.0,1,73–84,VLDB Endowment,,,,2009-08,,2150-8097,https://doi.org/10.14778/1687627.1687637;http://dx.doi.org/10.14778/1687627.1687637,10.14778/1687627.1687637,"In recent years, data streams have become ubiquitous as technology is improving and the prices of portable devices are falling, e.g., sensor networks, location-based services. Most data streams transmit only data tuples based on which continuous queries are evaluated. In this paper, we propose to enrich data streams with a new type of metadata called streaming tags or short tick-tags. The fundamental premise of tagging is that users can label data using uncontrolled vocabulary, and these tags can be exploited in a wide variety of applications, such as data exploration, data search, and to produce ""enriched"" with additional semantics, thus more informative query results. In this paper we focus primarily on the problem of continuous query processing with streaming tags and tagged objects, and address the tick-tag semantic issues as well as efficiency concerns. Our main contributions are as follows. First, we specify a general and flexible Stream Tag Framework (or short STF) that supports a stream-centric approach to tagging, and where tick-tags, attached to streaming objects are treated as first-class citizens. Second, under STF, users can query tags explicitly as well as implicitly by outputting the tags of the base data together with query results. Finally, we have implemented STF in a prototype Data Stream Management System, and through a set of performance experiments, we show that the cost of stream tagging is small and the approach is scalable to a large percentage of tagged objects.",,,,,
Conference Paper,Rowley C,The LATEX Legacy: 2.09 and All That,,2001,,,17–25,Association for Computing Machinery,"New York, NY, USA",Proceedings of the Twentieth Annual ACM Symposium on Principles of Distributed Computing,"Newport, Rhode Island, USA",2001,9781581133837.0,,https://doi.org/10.1145/383962.383978;http://dx.doi.org/10.1145/383962.383978,10.1145/383962.383978,"The second edition of The Manual [23] begins: `LATEX is a system for typesetting documents. Its first widely available version, mysteriously numbered 2.09, appeared in 1985.'It is too early for a complete critical assessment of the impact of LATEX 2.09 because its world-wide effects on many aspects of many cultures, not least scientific publication, remain strong after 15 years—and that itself is significant in a technological world where a mere 15 months of fame can make and break an idea.Therefore this paper provides simply a review and evaluation of the relationship between TEX, LATEX and some of the major technical developments in the world of quality automated formatting since the publication of LATEX 2.09 in 1985. It is is neither definitive nor comprehensive but I hope it is informative.",,PODC '01,,,
Journal Article,"Pichler R,Savenkov V",DEMo: Data Exchange Modeling Tool,Proc. VLDB Endow.,2009,2.0,2,1606–1609,VLDB Endowment,,,,2009-08,,2150-8097,https://doi.org/10.14778/1687553.1687603;http://dx.doi.org/10.14778/1687553.1687603,10.14778/1687553.1687603,"Minimality is an important optimization criterion for solutions of data exchange problems, well captured by the notion of the core. Though tractability of core computation has been proved, it has not yet become a part of any industrial-strength system, still being highly computationally expensive. In this demonstration, we show how core computation can be used in a data exchange modeling tool, allowing data engineers to design more robust data transfer scenarios and better understand the sources of redundancy in the target database.",,,,,
Conference Paper,"Alice,Bob,Carol,Beznazwy J,Houmansadr A",How China Detects and Blocks Shadowsocks,,2020,,,111–124,Association for Computing Machinery,"New York, NY, USA",Proceedings of the ACM Internet Measurement Conference,"Virtual Event, USA",2020,9781450381383.0,,https://doi.org/10.1145/3419394.3423644;http://dx.doi.org/10.1145/3419394.3423644,10.1145/3419394.3423644,"Shadowsocks is one of the most popular circumvention tools in China. Since May 2019, there have been numerous anecdotal reports of the blocking of Shadowsocks from Chinese users. In this study, we reveal how the Great Firewall of China (GFW) detects and blocks Shadowsocks and its variants. Using measurement experiments, we find that the GFW uses the length and entropy of the first data packet in each connection to identify probable Shadowsocks traffic, then sends seven different types of active probes, in different stages, to the corresponding servers to test whether its guess is correct.We developed a prober simulator to analyze the effect of different types of probes on various Shadowsocks implementations, and used it to infer what vulnerabilities are exploited by the censor. We fingerprinted the probers and found differences relative to previous work on active probing. A network-level side channel reveals that the probers, which use thousands of IP addresses, are likely controlled by a set of centralized structures.Based on our gained understanding, we present a temporary workaround that successfully mitigates the traffic analysis attack by the GFW. We further discuss essential strategies to defend against active probing. We responsibly disclosed our findings and suggestions to Shadowsocks developers, which has led to more censorship-resistant tools.","Great Firewall of China, Shadowsocks, censorship circumvention, active probing",IMC '20,,,
Conference Paper,Vertesi J,"""Seeing like a Rover"": Embodied Experience on the Mars Exploration Rover Mission",,2008,,,2523–2532,Association for Computing Machinery,"New York, NY, USA",CHI '08 Extended Abstracts on Human Factors in Computing Systems,"Florence, Italy",2008,9781605580128.0,,https://doi.org/10.1145/1358628.1358709;http://dx.doi.org/10.1145/1358628.1358709,10.1145/1358628.1358709,"Although they work with two non-humanoid robots located several million miles away, the distributed team that operates the Mars Exploration Rovers demonstrates an uncanny sympathy for their robotic teammates. This paper examines not only how the Rovers are anthropomorphized by the human team, but also how the team takes on characteristics of the Rovers while conducting science and operations on Mars. Based on two years of ethnographic fieldwork with the Mars Rover mission, the paper places the configuration of the user in social context and probes the role of the machine as social resource, with implications for HCI.","anthropomorphism, social robotics, usability, embodied computing, human-robot interaction, affect",CHI EA '08,,,
Conference Paper,"Gidra L,Thomas G,Sopena J,Shapiro M,Nguyen N",NumaGiC: A Garbage Collector for Big Data on Big NUMA Machines,,2015,,,661–673,Association for Computing Machinery,"New York, NY, USA",Proceedings of the Twentieth International Conference on Architectural Support for Programming Languages and Operating Systems,"Istanbul, Turkey",2015,9781450328357.0,,https://doi.org/10.1145/2694344.2694361;http://dx.doi.org/10.1145/2694344.2694361,10.1145/2694344.2694361,"On contemporary cache-coherent Non-Uniform Memory Access (ccNUMA) architectures, applications with a large memory footprint suffer from the cost of the garbage collector (GC), because, as the GC scans the reference graph, it makes many remote memory accesses, saturating the interconnect between memory nodes. We address this problem with NumaGiC, a GC with a mostly-distributed design. In order to maximise memory access locality during collection, a GC thread avoids accessing a different memory node, instead notifying a remote GC thread with a message; nonetheless, NumaGiC avoids the drawbacks of a pure distributed design, which tends to decrease parallelism. We compare NumaGiC with Parallel Scavenge and NAPS on two different ccNUMA architectures running on the Hotspot Java Virtual Machine of OpenJDK 7. On Spark and Neo4j, two industry-strength analytics applications, with heap sizes ranging from 160GB to 350GB, and on SPECjbb2013 and SPECjbb2005, ourgc improves overall performance by up to 45% over NAPS (up to 94% over Parallel Scavenge), and increases the performance of the collector itself by up to 3.6x over NAPS (up to 5.4x over Parallel Scavenge).","garbage collection, NUMA, multicore",ASPLOS '15,,,
Journal Article,"Gidra L,Thomas G,Sopena J,Shapiro M,Nguyen N",NumaGiC: A Garbage Collector for Big Data on Big NUMA Machines,SIGPLAN Not.,2015,50.0,4,661–673,Association for Computing Machinery,"New York, NY, USA",,,2015-03,,0362-1340,https://doi.org/10.1145/2775054.2694361;http://dx.doi.org/10.1145/2775054.2694361,10.1145/2775054.2694361,"On contemporary cache-coherent Non-Uniform Memory Access (ccNUMA) architectures, applications with a large memory footprint suffer from the cost of the garbage collector (GC), because, as the GC scans the reference graph, it makes many remote memory accesses, saturating the interconnect between memory nodes. We address this problem with NumaGiC, a GC with a mostly-distributed design. In order to maximise memory access locality during collection, a GC thread avoids accessing a different memory node, instead notifying a remote GC thread with a message; nonetheless, NumaGiC avoids the drawbacks of a pure distributed design, which tends to decrease parallelism. We compare NumaGiC with Parallel Scavenge and NAPS on two different ccNUMA architectures running on the Hotspot Java Virtual Machine of OpenJDK 7. On Spark and Neo4j, two industry-strength analytics applications, with heap sizes ranging from 160GB to 350GB, and on SPECjbb2013 and SPECjbb2005, ourgc improves overall performance by up to 45% over NAPS (up to 94% over Parallel Scavenge), and increases the performance of the collector itself by up to 3.6x over NAPS (up to 5.4x over Parallel Scavenge).","NUMA, garbage collection, multicore",,,,
Journal Article,"Gidra L,Thomas G,Sopena J,Shapiro M,Nguyen N",NumaGiC: A Garbage Collector for Big Data on Big NUMA Machines,SIGARCH Comput. Archit. News,2015,43.0,1,661–673,Association for Computing Machinery,"New York, NY, USA",,,2015-03,,0163-5964,https://doi.org/10.1145/2786763.2694361;http://dx.doi.org/10.1145/2786763.2694361,10.1145/2786763.2694361,"On contemporary cache-coherent Non-Uniform Memory Access (ccNUMA) architectures, applications with a large memory footprint suffer from the cost of the garbage collector (GC), because, as the GC scans the reference graph, it makes many remote memory accesses, saturating the interconnect between memory nodes. We address this problem with NumaGiC, a GC with a mostly-distributed design. In order to maximise memory access locality during collection, a GC thread avoids accessing a different memory node, instead notifying a remote GC thread with a message; nonetheless, NumaGiC avoids the drawbacks of a pure distributed design, which tends to decrease parallelism. We compare NumaGiC with Parallel Scavenge and NAPS on two different ccNUMA architectures running on the Hotspot Java Virtual Machine of OpenJDK 7. On Spark and Neo4j, two industry-strength analytics applications, with heap sizes ranging from 160GB to 350GB, and on SPECjbb2013 and SPECjbb2005, ourgc improves overall performance by up to 45% over NAPS (up to 94% over Parallel Scavenge), and increases the performance of the collector itself by up to 3.6x over NAPS (up to 5.4x over Parallel Scavenge).","NUMA, multicore, garbage collection",,,,
Journal Article,"Howarth J,Andre TS,Hartson R",A Structured Process for Transforming Usability Data into Usability Information,J. Usability Studies,2007,3.0,1,7–23,Usability Professionals' Association,"Bloomingdale, IL",,,2007-11,,,,,"Much research has been devoted to developing usability evaluation methods that are used in evaluating interaction designs. More recently, however, research has shifted away from evaluation methods and comparisons of evaluation methods to issues of how to use the raw usability data generated by these methods. Associated with this focus is the assumption that the transformation of the raw usability data into usability information is relatively straightforward. We would argue that this assumption is incorrect, especially for novice usability practitioners. In this article, we present a structured process for transforming raw usability data into usability information that is based on a new way of thinking about usability problem data. The results of a study of this structured process indicate that it helps improve the effectiveness of novice usability practitioners.","usability engineering tools, usability evaluation, empirical findings, usability evaluation methods, usability problem instances",,,,
Conference Paper,"Yoon C,Kim K,Kim Y,Shin S,Son S",Doppelgängers on the Dark Web: A Large-Scale Assessment on Phishing Hidden Web Services,,2019,,,2225–2235,Association for Computing Machinery,"New York, NY, USA",The World Wide Web Conference,"San Francisco, CA, USA",2019,9781450366748.0,,https://doi.org/10.1145/3308558.3313551;http://dx.doi.org/10.1145/3308558.3313551,10.1145/3308558.3313551,"Anonymous network services on the World Wide Web have emerged as a new web architecture, called the Dark Web. The Dark Web has been notorious for harboring cybercriminals abusing anonymity. At the same time, the Dark Web has been a last resort for people who seek freedom of the press as well as avoid censorship. This anonymous nature allows website operators to conceal their identity and thereby leads users to have difficulties in determining the authenticity of websites. Phishers abuse this perplexing authenticity to lure victims; however, only a little is known about the prevalence of phishing attacks on the Dark Web. We conducted an in-depth measurement study to demystify the prevalent phishing websites on the Dark Web. We analyzed the text content of 28,928 HTTP Tor hidden services hosting 21 million dark webpages and confirmed 901 phishing domains. We also discovered a trend on the Dark Web in which service providers perceive dark web domains as their service brands. This trend exacerbates the risk of phishing for their service users who remember only a partial Tor hidden service address. Our work facilitates a better understanding of the phishing risks on the Dark Web and encourages further research on establishing an authentic and reliable service on the Dark Web.",,WWW '19,,,
Conference Paper,"Pullen EW,Shuttee DF",MUSE: A Tool for Testing and Debugging a Multi-Terminal Programming System,,1968,,,491–502,Association for Computing Machinery,"New York, NY, USA","Proceedings of the April 30--May 2, 1968, Spring Joint Computer Conference","Atlantic City, New Jersey",1968,9781450378970.0,,https://doi.org/10.1145/1468075.1468147;http://dx.doi.org/10.1145/1468075.1468147,10.1145/1468075.1468147,"Current literature on multi-terminal time sharing systems has a great deal to say about the macroscopic relationship of their parts and the statistics of their performance. However, very little is written about the efforts expended in putting them together or making them work efficiently and reliably, in the hands of the users. Traditionally checkout and testing of multi-terminal systems have assumed a method of an on-site marathon. Testing of the product outside the customer's shop has been virtually non-existent.",,AFIPS '68 (Spring),,,
Conference Paper,"Johnson R,Pandis I,Hardavellas N,Ailamaki A,Falsafi B",Shore-MT: A Scalable Storage Manager for the Multicore Era,,2009,,,24–35,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 12th International Conference on Extending Database Technology: Advances in Database Technology,"Saint Petersburg, Russia",2009,9781605584225.0,,https://doi.org/10.1145/1516360.1516365;http://dx.doi.org/10.1145/1516360.1516365,10.1145/1516360.1516365,"Database storage managers have long been able to efficiently handle multiple concurrent requests. Until recently, however, a computer contained only a few single-core CPUs, and therefore only a few transactions could simultaneously access the storage manager's internal structures. This allowed storage managers to use non-scalable approaches without any penalty. With the arrival of multicore chips, however, this situation is rapidly changing. More and more threads can run in parallel, stressing the internal scalability of the storage manager. Systems optimized for high performance at a limited number of cores are not assured similarly high performance at a higher core count, because unanticipated scalability obstacles arise.We benchmark four popular open-source storage managers (Shore, BerkeleyDB, MySQL, and PostgreSQL) on a modern multicore machine, and find that they all suffer in terms of scalability. We briefly examine the bottlenecks in the various storage engines. We then present Shore-MT, a multithreaded and highly scalable version of Shore which we developed by identifying and successively removing internal bottlenecks. When compared to other DBMS, Shore-MT exhibits superior scalability and 2--4 times higher absolute throughput than its peers. We also show that designers should favor scalability to single-thread performance, and highlight important principles for writing scalable storage engines, illustrated with real examples from the development of Shore-MT.",,EDBT '09,,,
Journal Article,"Benz S,Cool J",Using Regular Expressions to Locate Putative Zinc Finger Binding Sites,J. Comput. Sci. Coll.,2003,18.0,5,254–255,Consortium for Computing Sciences in Colleges,"Evansville, IN, USA",,,2003-05,,1937-4771,,,,,,,,
Conference Paper,Burch M,Mining and Visualizing Eye Movement Data,,2017,,,,Association for Computing Machinery,"New York, NY, USA",SIGGRAPH Asia 2017 Symposium on Visualization,"Bangkok, Thailand",2017,9781450354110.0,,https://doi.org/10.1145/3139295.3139304;http://dx.doi.org/10.1145/3139295.3139304,10.1145/3139295.3139304,"Eye movement data has a spatio-temporal nature which makes the design of suitable visualization techniques a challenging task. Moreover, eye movement data is typically recorded by tracking the eyes of various study participants in order to achieve significant results about applied visual task solution strategies. If we have to deal with vast amounts of eye movement data, a data preprocessing in form of data mining is useful since it can be applied to compute a set of rules. Those aggregate, filter, and hence reduce the original data to derive patterns in it. The generated rule sets are still large enough to serve as input data for a visual analytics system. In this paper we describe a visual analysis model for eye movement data combining data mining and visualization with the goal to get an impression about point-of-interest (POI) and area-of-interest (AOI) correlations in eye movement data on different levels of spatial and temporal granularities. Those correlations can support a data analyst to derive visual patterns that can be mapped to data patterns, i.e., visual scanning strategies with different probabilities of a group of eye tracked people. We show the usefulness of our data mining and visualization system by applying it to datasets recorded in a formerly conducted eye tracking experiment investigating the readability of metro maps.","data mining, spatio-temporal data, metro maps, eye movement data, visual analytics, rule mining, information visualization",SA '17,,,
Conference Paper,"Johnson R,Pandis I,Ailamaki A",Critical Sections: Re-Emerging Scalability Concerns for Database Storage Engines,,2008,,,35–40,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 4th International Workshop on Data Management on New Hardware,"Vancouver, Canada",2008,9781605581842.0,,https://doi.org/10.1145/1457150.1457157;http://dx.doi.org/10.1145/1457150.1457157,10.1145/1457150.1457157,"Critical sections in database storage engines impact performance and scalability more as the number of hardware contexts per chip continues to grow exponentially. With enough threads in the system, some critical section will eventually become a bottleneck. While algorithmic changes are the only long-term solution, they tend to be complex and costly to develop. Meanwhile, changes in enforcement of critical sections require much less effort. We observe that, in practice, many critical sections are so short that enforcing them contributes a significant or even dominating fraction of their total cost and tuning them directly improves database system performance. The contribution of this paper is two-fold: we (a) make a thorough performance comparison of the various synchronization primitives in the database system developer's toolbox and highlight the best ones for practical use, and (b) show that properly enforcing critical sections can delay the need to make algorithmic changes for a target number of processors.",,DaMoN '08,,,
Book,,SA '17: SIGGRAPH Asia 2017 Symposium on Visualization,,2017,,,,Association for Computing Machinery,"New York, NY, USA",,"Bangkok, Thailand",2017,9781450354110.0,,,,"The SIGGRAPH Asia Symposium on Visualization is an ideal platform for attendees to explore the opportunities and challenges of cutting-edge visualization techniques which facilitates human being to understand the data sets. The program aims to cover the development, technology, and demonstration of visualization techniques and their interactive applications.",,,Proceedings,,
Conference Paper,"Cornejo M,Ruhault S",Characterization of Real-Life PRNGs under Partial State Corruption,,2014,,,1004–1015,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 2014 ACM SIGSAC Conference on Computer and Communications Security,"Scottsdale, Arizona, USA",2014,9781450329576.0,,https://doi.org/10.1145/2660267.2660377;http://dx.doi.org/10.1145/2660267.2660377,10.1145/2660267.2660377,"Pseudo-random number generators (PRNGs) are widely used as a randomness source in cryptographic applications. It is essential for their security that the internal state, in which the entropy is accumulated, is kept secret. However, this assumption is unrealistic for PRNGs that are implemented in software, as the internal state can be partially corrupted through memory corruption bugs such as buffer overflows or through faults attacks. The recent Heartbleed bug gives us a concrete illustration of this vulnerability. In this work we study several widely used PRNGs from different popular providers, including OpenSSL, OpenJDK, Android, IBM and Bouncy Castle and we characterize how they handle their internal states. We formalize a framework based on the most recent and strongest security model called robustness of PRNGs to analyze these PRNGs and their implementations. With this framework we capture the notion of how much of the internal state must be corrupted in order to generate a predictable output. Using this framework, we determine the number of bits of the internal state that an attacker needs to corrupt in order to produce a predictable output. We also show that two of the PRNGs do not require state compromise to generate a non-random output. To the best of our knowledge, we present the first thorough characterization of an IBM implementation of a PRNG.","security models, randomness, android, openssl, java",CCS '14,,,
Conference Paper,"Heinl MP,Giehl A,Graif L",AntiPatterns Regarding the Application of Cryptographic Primitives by the Example of Ransomware,,2020,,,,Association for Computing Machinery,"New York, NY, USA","Proceedings of the 15th International Conference on Availability, Reliability and Security","Virtual Event, Ireland",2020,9781450388337.0,,https://doi.org/10.1145/3407023.3409182;http://dx.doi.org/10.1145/3407023.3409182,10.1145/3407023.3409182,"Cryptographic primitives are the basic building blocks for many cryptographic schemes and protocols. Implementing them incorrectly can lead to flaws, making a system or a product vulnerable to various attacks. As shown in the present paper, this statement also applies to ransomware. The paper surveys common errors occurring during the implementation of cryptographic primitives. Based on already existing research, it establishes a categorization framework to match selected ransomware samples by their respective vulnerabilities and assign them to the corresponding error categories. Subsequently, AntiPatterns are derived from the extracted error categories. These AntiPatterns are meant to support the field of software development by helping to detect and correct errors early during the implementation phase of cryptography.","Cryptography, AntiPatterns, Ransomware",ARES '20,,,
Conference Paper,"Bernabé RB,Navia IÁ,García-Peñalvo FJ",Faat: Freelance as a Team,,2015,,,687–694,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 3rd International Conference on Technological Ecosystems for Enhancing Multiculturality,"Porto, Portugal",2015,9781450334426.0,,https://doi.org/10.1145/2808580.2808685;http://dx.doi.org/10.1145/2808580.2808685,10.1145/2808580.2808685,"Agile methodologies are reliable engineering and management practices, capable of helping in the development of quality and successful software in business environments. However, most of these methodologies are centered on a development team and its internal communication. Moreover, for simplicity, a single product development is taken into account with its successive releases. There is another scenario: that of a single programmer working alone and often in much smaller projects and in several at the same time. Also in this scenario the client proximity is not as described by the agile environment ideal. In that case, the priorities and needs change, communication takes on another meaning and working mechanisms are not always comparable to that of a team. This paper introduces Faat (Freelance as a Team), a methodology specifically designed for those professionals. Integrating existing practices to the needs and possibilities of an individual programmer. However, it has been frequently considered the possible application of this methodology to small teams and/or other more general scenarios. This methodology has been tested in the web-based learning applications.","development process, personal software process, agile methodology",TEEM '15,,,
Journal Article,"Grosse P,Durand Y,Feautrier P",Methods for Power Optimization in SOC-Based Data Flow Systems,ACM Trans. Des. Autom. Electron. Syst.,2009,14.0,3,,Association for Computing Machinery,"New York, NY, USA",,,2009-06,,1084-4309,https://doi.org/10.1145/1529255.1529260;http://dx.doi.org/10.1145/1529255.1529260,10.1145/1529255.1529260,"Whereas the computing power of DSP or general-purpose processors was sufficient for 3G baseband telecommunication algorithms, stringent timing constraints of 4G wireless telecommunication systems require computing-intensive data-driven architectures. Managing the complexity of these systems within the energy constraints of a mobile terminal is becoming a major challenge for designers. System-level low-power policies have been widely explored for generic software-based systems, but data-flow architectures used for high data-rate telecommunication systems feature heterogeneous components that require specific configurations for power management. In this study, we propose an innovative power optimization scheme tailored to self-synchronized data-flow systems. Our technique, based on the synchronous data-flow modeling approach, takes advantage of the latest low-power techniques available for digital architectures. We illustrate our optimization method on a complete 4G telecommunication baseband modem and show the energy savings expected by this technique considering present and future silicon technologies.","data-driven SOC, 4G base-band modem, synchronous data-flow graph, Power optimization",,,,
Conference Paper,"Beaumont M,Hopkins B,Newby T",Hardware Trojan Resistant Computation Using Heterogeneous COTS Processors,,2013,,,97–106,"Australian Computer Society, Inc.",AUS,Proceedings of the Thirty-Sixth Australasian Computer Science Conference - Volume 135,"Adelaide, Australia",2013,9781921770203.0,,,,"Hardware Trojans pose a credible and increasing threat to computer security, with the potential to compromise the very electronics that ostensibly provide the security primitives underpinning various computer architectures.The discovery of stealthy Hardware Trojans within Integrated Circuits by current state-of-the-art pre-and post-manufacturing test and verification techniques cannot be guaranteed. Therefore electronic systems, especially those controlling safety or security critical systems should be designed to operate with integrity in the presence of any Hardware Trojans, and regardless of any Trojan activity.We present an architecture that fragments and replicates computation over a pool of Commercial-Off-The-Shelf processors with widely heterogeneous architectures. Processors are loosely synchronised through their use of a voted, architecture-independent message box mechanism to access a common memory space. A minimal Trusted Computing Base abstracts the processors as a single computational entity that can tolerate the effects of arbitrary Hardware Trojans within individual processors. The architecture provides integrity, data confidentiality, and availability for executing applications.",,ACSC '13,,,
Journal Article,"Frank EH,Sproull RF",Testing and Debugging Custom Integrated Circuits,ACM Comput. Surv.,1981,13.0,4,425–451,Association for Computing Machinery,"New York, NY, USA",,,1981-12,,0360-0300,https://doi.org/10.1145/356859.356863;http://dx.doi.org/10.1145/356859.356863,10.1145/356859.356863,,,,,,
Conference Paper,"Jensen T,Albayram Y,Khan MM,Fahim MA,Buck R,Coman E",The Apple Does Fall Far from the Tree: User Separation of a System from Its Developers in Human-Automation Trust Repair,,2019,,,1071–1082,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 2019 on Designing Interactive Systems Conference,"San Diego, CA, USA",2019,9781450358507.0,,https://doi.org/10.1145/3322276.3322349;http://dx.doi.org/10.1145/3322276.3322349,10.1145/3322276.3322349,"To promote safe and effective human-computer interactions, researchers have begun studying mechanisms for ""trust repair"" in response to automated system errors. The extent to which users distinguish between a system and the system's developers may be an important factor in the efficacy of trust repair messages. To investigate this, we conducted a 2 (reliability) x 3 (blame) between-group, factorial study. Participants interacted with a high or low reliability automated system that attributed blame for errors internally (""I was not able...""), pseudo-externally (""The developers were not able...""), or externally (""A third-party algorithm that I used was not able...""). We found that pseudo-external blame and internal blame influenced subjective trust differently, suggesting that the system and its developers represent distinct trustees. We discuss the implications of our findings for the design and study of human-automation trust repair.","attribution theory, human-automation trust, trust repair, blame",DIS '19,,,
Journal Article,"Miller Hillberg H,Levonian Z,Kluver D,Terveen L,Hecht B",What I See is What You Don't Get: The Effects of (Not) Seeing Emoji Rendering Differences across Platforms,Proc.  ACM Hum. -Comput.  Interact.,2018,2.0,CSCW,,Association for Computing Machinery,"New York, NY, USA",,,2018-11,,,https://doi.org/10.1145/3274393;http://dx.doi.org/10.1145/3274393,10.1145/3274393,"Emoji are popular in digital communication, but they are rendered differently on different viewing platforms (e.g., iOS, Android). It is unknown how many people are aware that emoji have multiple renderings, or whether they would change their emoji-bearing messages if they could see how these messages render on recipients' devices. We developed software to expose the multi-rendering nature of emoji and explored whether this increased visibility would affect how people communicate with emoji. Through a survey of 710 Twitter users who recently posted an emoji-bearing tweet, we found that at least 25% of respondents were unaware that the emoji they posted could appear differently to their followers. Additionally, after being shown how one of their tweets rendered across platforms, 20% of respondents reported that they would have edited or not sent the tweet. These statistics reflect millions of potentially regretful tweets shared per day because people cannot see emoji rendering differences across platforms. Our results motivate the development of tools that increase the visibility of emoji rendering differences across platforms, and we contribute our cross-platform emoji rendering software to facilitate this effort.","computer-mediated communication, rendering, invisibility of system status, cross-platform, emoji",,,,
Conference Paper,"Schmidt C,Gorman TJ,Bayor AA,Gary MS","Impact of Low-Cost, on-Demand, Information Access in a Remote Ghanaian Village",,2010,,,,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 4th ACM/IEEE International Conference on Information and Communication Technologies and Development,"London, United Kingdom",2010,9781450307871.0,,https://doi.org/10.1145/2369220.2369261;http://dx.doi.org/10.1145/2369220.2369261,10.1145/2369220.2369261,"Technology projects are finding ways to provide information to people living in rural poverty. However, using information to affect health or farming practices requires overcoming unique challenges including illiteracy and lack of electricity. We examine the effects of a low-cost audio computer (""Talking Book"")--a battery-powered, durable, handheld device that enables users to create and listen to recordings and copy recordings between devices-for improving learning opportunities and knowledge sharing in such environments. In northern Ghana, we studied the impact of giving rural, illiterate people on-demand access to guidance created by local experts. Our evaluation suggests that Talking Books can make a significant impact on learning and behavior change in villages with low literacy rates and no electricity.","information dissemination, agriculture production, audio, knowledge transfer, low-cost technology, illiteracy",ICTD '10,,,
Conference Paper,"Kaminski P,Litoiu M,Müller H",A Design Technique for Evolving Web Services,,2006,,,23–es,IBM Corp.,USA,Proceedings of the 2006 Conference of the Center for Advanced Studies on Collaborative Research,"Toronto, Ontario, Canada",2006,,,https://doi.org/10.1145/1188966.1188997;http://dx.doi.org/10.1145/1188966.1188997,10.1145/1188966.1188997,"In this paper, we define the problem of simultaneously deploying multiple versions of a web service in the face of independently developed unsupervised clients. We then propose a solution in the form of a design technique called Chain of Adapters and argue that this approach strikes a good balance between the various requirements. We recount our experiences in automating the application of the technique and provide an initial analysis of the performance degradations it may occasion. The Chain of Adapters technique is particularly suitable for self-managed systems since it makes many version-related reconfiguration tasks safe, and thus subject to automation.",,CASCON '06,,,
Journal Article,"Benford S,Calder M,Rodden T,Sevegnani M","On Lions, Impala, and Bigraphs: Modelling Interactions in Physical/Virtual Spaces",ACM Trans.  Comput. -Hum.  Interact.,2016,23.0,2,,Association for Computing Machinery,"New York, NY, USA",,,2016-05,,1073-0516,https://doi.org/10.1145/2882784;http://dx.doi.org/10.1145/2882784,10.1145/2882784,"While HCI has a long tradition of formally modelling task-based interactions with graphical user interfaces, there has been less progress in modelling emerging ubiquitous computing systems due in large part to their highly contextual nature and dependence on unreliable sensing systems. We present an exploration of modelling an example ubiquitous system, the Savannah game, using the mathematical formalism of bigraphs, which are based on a universal process algebra that encapsulates both dynamic and spatial behaviour of autonomous agents that interact and move among each other, or within each other. We establish a modelling approach based on four perspectives on ubiquitous systems—Computational, Physical, Human, and Technology—and explore how these interact with one another. We show how our model explains observed inconsistencies in user trials of Savannah, and then, how formal analysis reveals an incompleteness in design and guides extensions of the model and/or possible system re-design to resolve this.","mixed reality systems, Bigraphs, formal modelling",,,,
Conference Paper,"Nesbit KJ,Laudon J,Smith JE",Virtual Private Caches,,2007,,,57–68,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 34th Annual International Symposium on Computer Architecture,"San Diego, California, USA",2007,9781595937063.0,,https://doi.org/10.1145/1250662.1250671;http://dx.doi.org/10.1145/1250662.1250671,10.1145/1250662.1250671,"Virtual Private Machines (VPM) provide a framework for Quality of Service (QoS) in CMP-based computer systems. VPMs incorporate microarchitecture mechanisms that allow shares of hardware resources to be allocated to executing threads, thus providing applications with an upper bound on execution time regardless of other thread activity. Virtual Private Caches (VPCs) are an important element of VPMs. VPC hardware consists of two major components: the VPC Arbiter, which manages shared cache bandwidth, and the VPC Capacity Manager, which manages the cache storage. Both the VPC Arbiter and VPC Capacity Manager provide minimum service guarantees that, when combined, achieve QoS for the cache subsystem. Simulation-based evaluation shows that conventional cache bandwidth management policies allow concurrently executing threads to affect each other significantly in an uncontrollable manner. The evaluation targets cache bandwidth because the effects of cache capacity sharing have been studied elsewhere. In contrast with the conventional policies, the VPC Arbiter meets its QoS performance objectives on all workloads studied and over a range of allocated bandwidth levels. The VPC Arbiter’s fairness policy, which distributes leftover bandwidth, mitigates the effects of cache preemption latencies, thus ensuring threads a high-degree of performance isolation. Furthermore, the VPC Arbiter eliminates negative bandwidth interference which can improve aggregate throughput and resource utilization.","performance isolation, soft real-time, quality of service, shared caches, chip multiprocessor",ISCA '07,,,
Journal Article,"Nesbit KJ,Laudon J,Smith JE",Virtual Private Caches,SIGARCH Comput. Archit. News,2007,35.0,2,57–68,Association for Computing Machinery,"New York, NY, USA",,,2007-06,,0163-5964,https://doi.org/10.1145/1273440.1250671;http://dx.doi.org/10.1145/1273440.1250671,10.1145/1273440.1250671,"Virtual Private Machines (VPM) provide a framework for Quality of Service (QoS) in CMP-based computer systems. VPMs incorporate microarchitecture mechanisms that allow shares of hardware resources to be allocated to executing threads, thus providing applications with an upper bound on execution time regardless of other thread activity. Virtual Private Caches (VPCs) are an important element of VPMs. VPC hardware consists of two major components: the VPC Arbiter, which manages shared cache bandwidth, and the VPC Capacity Manager, which manages the cache storage. Both the VPC Arbiter and VPC Capacity Manager provide minimum service guarantees that, when combined, achieve QoS for the cache subsystem. Simulation-based evaluation shows that conventional cache bandwidth management policies allow concurrently executing threads to affect each other significantly in an uncontrollable manner. The evaluation targets cache bandwidth because the effects of cache capacity sharing have been studied elsewhere. In contrast with the conventional policies, the VPC Arbiter meets its QoS performance objectives on all workloads studied and over a range of allocated bandwidth levels. The VPC Arbiter’s fairness policy, which distributes leftover bandwidth, mitigates the effects of cache preemption latencies, thus ensuring threads a high-degree of performance isolation. Furthermore, the VPC Arbiter eliminates negative bandwidth interference which can improve aggregate throughput and resource utilization.","chip multiprocessor, soft real-time, shared caches, quality of service, performance isolation",,,,
Conference Paper,"Khmelevsky Y,Rinard M,Sidiroglou-Douskos S",A Source-to-Source Transformation Tool for Error Fixing,,2013,,,147–160,IBM Corp.,USA,Proceedings of the 2013 Conference of the Center for Advanced Studies on Collaborative Research,"Ontario, Canada",2013,,,,,"We present a methodology and a prototype of a source-to-source transformation tool for error fixing in C/C++ program source code for missing condition checks after a method call. The missing condition checks in a C program could lead to a program crash. This tool can be extended for other programming languages in addition to C/C++.The developed tool includes the ability to generate and apply a fix for a source code without human intervention. The tool can be run on different platforms, including MS Windows, Linux, MAC OS and other operating systems. We evaluate our technique by applying it to five widely used open source programs. Our results show that it is able to successfully detect and add the missing condition check or correct it after a method call in the program, and that our detection and error fixing technique is quite accurate in practice.",,CASCON '13,,,
Conference Paper,"Zhang Y,Salvaneschi G,Beightol Q,Liskov B,Myers AC",Accepting Blame for Safe Tunneled Exceptions,,2016,,,281–295,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 37th ACM SIGPLAN Conference on Programming Language Design and Implementation,"Santa Barbara, CA, USA",2016,9781450342612.0,,https://doi.org/10.1145/2908080.2908086;http://dx.doi.org/10.1145/2908080.2908086,10.1145/2908080.2908086,"Unhandled exceptions crash programs, so a compile-time check that exceptions are handled should in principle make software more reliable. But designers of some recent languages have argued that the benefits of statically checked exceptions are not worth the costs. We introduce a new statically checked exception mechanism that addresses the problems with existing checked-exception mechanisms. In particular, it interacts well with higher-order functions and other design patterns. The key insight is that whether an exception should be treated as a ""checked"" exception is not a property of its type but rather of the context in which the exception propagates. Statically checked exceptions can ""tunnel"" through code that is oblivious to their presence, but the type system nevertheless checks that these exceptions are handled. Further, exceptions can be tunneled without being accidentally caught, by expanding the space of exception identifiers to identify the exception-handling context. The resulting mechanism is expressive and syntactically light, and can be implemented efficiently. We demonstrate the expressiveness of the mechanism using significant codebases and evaluate its performance. We have implemented this new exception mechanism as part of the new Genus programming language, but the mechanism could equally well be applied to other programming languages.","exception handling, Exception tunneling, Genus",PLDI '16,,,
Journal Article,"Zhang Y,Salvaneschi G,Beightol Q,Liskov B,Myers AC",Accepting Blame for Safe Tunneled Exceptions,SIGPLAN Not.,2016,51.0,6,281–295,Association for Computing Machinery,"New York, NY, USA",,,2016-06,,0362-1340,https://doi.org/10.1145/2980983.2908086;http://dx.doi.org/10.1145/2980983.2908086,10.1145/2980983.2908086,"Unhandled exceptions crash programs, so a compile-time check that exceptions are handled should in principle make software more reliable. But designers of some recent languages have argued that the benefits of statically checked exceptions are not worth the costs. We introduce a new statically checked exception mechanism that addresses the problems with existing checked-exception mechanisms. In particular, it interacts well with higher-order functions and other design patterns. The key insight is that whether an exception should be treated as a ""checked"" exception is not a property of its type but rather of the context in which the exception propagates. Statically checked exceptions can ""tunnel"" through code that is oblivious to their presence, but the type system nevertheless checks that these exceptions are handled. Further, exceptions can be tunneled without being accidentally caught, by expanding the space of exception identifiers to identify the exception-handling context. The resulting mechanism is expressive and syntactically light, and can be implemented efficiently. We demonstrate the expressiveness of the mechanism using significant codebases and evaluate its performance. We have implemented this new exception mechanism as part of the new Genus programming language, but the mechanism could equally well be applied to other programming languages.","Genus, exception handling, Exception tunneling",,,,
Book,,SCALA 2016: Proceedings of the 2016 7th ACM SIGPLAN Symposium on Scala,,2016,,,,Association for Computing Machinery,"New York, NY, USA",,"Amsterdam, Netherlands",2016,9781450346481.0,,,,,,,Proceedings,,
Conference Paper,"Jain B,Tsai CC,John J,Porter DE",Practical Techniques to Obviate Setuid-to-Root Binaries,,2014,,,,Association for Computing Machinery,"New York, NY, USA",Proceedings of the Ninth European Conference on Computer Systems,"Amsterdam, The Netherlands",2014,9781450327046.0,,https://doi.org/10.1145/2592798.2592811;http://dx.doi.org/10.1145/2592798.2592811,10.1145/2592798.2592811,"Trusted, setuid-to-root binaries have been a substantial, long-lived source of privilege escalation vulnerabilities on Unix systems. Prior work on limiting privilege escalation has only considered privilege from the perspective of the administrator, neglecting the perspective of regular users---the primary reason for having setuid-to-root binaries.The paper presents a study of the current state of setuid-to-root binaries on Linux, focusing on the 28 most commonly deployed setuid binaries in the Debian and Ubuntu distributions. This study reveals several points where Linux kernel policies and abstractions are a poor fit for the policies desired by the administrator, and root privilege is used to create point solutions. The majority of these point solutions address 8 system calls that require administrator privilege, but also export functionality required by unprivileged users.This paper demonstrates how least privilege can be achieved on modern systems for non-administrator users. We identify the policies currently encoded in setuid-to-root binaries, and present a framework for expressing and enforcing these policy categories in the kernel. Our prototype, called Protego, deprivileges over 10,000 lines of code by changing only 715 lines of Linux kernel code. Protego also adds additional utilities to keep the kernel policy synchronized with legacy, policy-relevant configuration files, such as /etc/sudoers. Although some previously-privileged binaries may require changes, Protego provides users with the same functionality as Linux and introduces acceptable performance overheads. For instance, a Linux kernel compile incurs less than 2% overhead on Protego.",,EuroSys '14,,,
Conference Paper,"Waern A,Montola M,Stenros J",The Three-Sixty Illusion: Designing for Immersion in Pervasive Games,,2009,,,1549–1558,Association for Computing Machinery,"New York, NY, USA",Proceedings of the SIGCHI Conference on Human Factors in Computing Systems,"Boston, MA, USA",2009,9781605582467.0,,https://doi.org/10.1145/1518701.1518939;http://dx.doi.org/10.1145/1518701.1518939,10.1145/1518701.1518939,"Pervasive games are staged in reality and their main attractiveness is generated by using reality as a resource in the game. Yet, most pervasive games that use mobile and location-based technology use reality only in a weak sense, as the location for a computerized game.In this article we analyze two game practices, Nordic style live action role-playing (larp) and alternate reality games (ARG), that instead use reality as their main game resource. We analyze how they go about creating a believable game world and encourage the players to actively take part in this world. We present two example games that do the same with the support of technology, effectively realizing an immersive game world through a combination of physical play and technology-supported play.","mobile game, role-play, pervasive game, immersion",CHI '09,,,
Conference Paper,"Katsarakis A,Gavrielatos V,Katebzadeh MR,Joshi A,Dragojevic A,Grot B,Nagarajan V","Hermes: A Fast, Fault-Tolerant and Linearizable Replication Protocol",,2020,,,201–217,Association for Computing Machinery,"New York, NY, USA",Proceedings of the Twenty-Fifth International Conference on Architectural Support for Programming Languages and Operating Systems,"Lausanne, Switzerland",2020,9781450371025.0,,https://doi.org/10.1145/3373376.3378496;http://dx.doi.org/10.1145/3373376.3378496,10.1145/3373376.3378496,"Today's datacenter applications are underpinned by datastores that are responsible for providing availability, consistency, and performance. For high availability in the presence of failures, these datastores replicate data across several nodes. This is accomplished with the help of a reliable replication protocol that is responsible for maintaining the replicas strongly-consistent even when faults occur. Strong consistency is preferred to weaker consistency models that cannot guarantee an intuitive behavior for the clients. Furthermore, to accommodate high demand at real-time latencies, datastores must deliver high throughput and low latency.This work introduces Hermes, a broadcast-based reliable replication protocol for in-memory datastores that provides both high throughput and low latency by enabling local reads and fully-concurrent fast writes at all replicas. Hermes couples logical timestamps with cache-coherence-inspired invalidations to guarantee linearizability, avoid write serialization at a centralized ordering point, resolve write conflicts locally at each replica (hence ensuring that writes never abort) and provide fault-tolerance via replayable writes. Our implementation of Hermes over an RDMA-enabled reliable datastore with five replicas shows that Hermes consistently achieves higher throughput than state-of-the-art RDMA-based reliable protocols (ZAB and CRAQ) across all write ratios while also significantly reducing tail latency. At 5% writes, the tail latency of Hermes is 3.6X lower than that of CRAQ and ZAB.","availability, latency, replication, consistency, throughput, fault-tolerant, linearizability, rdma",ASPLOS '20,,,
Conference Paper,"Kelley PG,Cranor LF,Sadeh N",Privacy as Part of the App Decision-Making Process,,2013,,,3393–3402,Association for Computing Machinery,"New York, NY, USA",Proceedings of the SIGCHI Conference on Human Factors in Computing Systems,"Paris, France",2013,9781450318990.0,,https://doi.org/10.1145/2470654.2466466;http://dx.doi.org/10.1145/2470654.2466466,10.1145/2470654.2466466,"Smartphones have unprecedented access to sensitive personal information. While users report having privacy concerns, they may not actively consider privacy while downloading apps from smartphone application marketplaces. Currently, Android users have only the Android permissions display, which appears after they have selected an app to download, to help them understand how applications access their information. We investigate how permissions and privacy could play a more active role in app-selection decisions. We designed a short ""Privacy Facts' display, which we tested in a 20-participant lab study and a 366-participant online experiment. We found that by bringing privacy information to the user when they were making the decision and by presenting it in a clearer fashion, we could assist users in choosing applications that request fewer permissions.","interface, mobile, privacy, decision-making, android",CHI '13,,,
Conference Paper,"Wang P,Tyra J,Chan-Tin E,Malchow T,Kune DF,Hopper N,Kim Y",Attacking the Kad Network,,2008,,,,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 4th International Conference on Security and Privacy in Communication Netowrks,"Istanbul, Turkey",2008,9781605582412.0,,https://doi.org/10.1145/1460877.1460907;http://dx.doi.org/10.1145/1460877.1460907,10.1145/1460877.1460907,"The Kad network, an implementation of the Kademlia DHT protocol, supports the popular eDonkey peer-to-peer file sharing network and has over 1 million concurrent nodes. We describe several attacks that exploit critical design weaknesses in Kad to allow an attacker with modest resources to cause a significant fraction of all searches to fail. We measure the cost and effectiveness of these attacks against a set of 16,000 nodes connected to the operational Kad network. We also measure the cost of previously proposed, generic DHT attacks against the Kad network and find that our attacks are much more cost effective. Finally, we introduce and evaluate simple mechanisms to significantly increase the cost of these attacks.","security, attack, Kad, P2P",SecureComm '08,,,
Journal Article,"Russo D,Ciancarini P,Falasconi T,Tomasi M",A Meta-Model for Information Systems Quality: A Mixed Study of the Financial Sector,ACM Trans. Manage. Inf. Syst.,2018,9.0,3,,Association for Computing Machinery,"New York, NY, USA",,,2018-09,,2158-656X,https://doi.org/10.1145/3230713;http://dx.doi.org/10.1145/3230713,10.1145/3230713,"Information Systems Quality (ISQ) is a critical source of competitive advantages for organizations. In a scenario of increasing competition on digital services, ISQ is a competitive differentiation asset. In this regard, managing, maintaining, and evolving IT infrastructures have become a primary concern of organizations. Thus, a technical perspective on ISQ provides useful guidance to meet current challenges. The financial sector is paradigmatic, since it is a traditional business, with highly complex business-critical legacy systems, facing a tremendous change due to market and regulation drivers. We carried out a Mixed-Methods study, performing a Delphi-like study on the financial sector. We developed a specific research framework to pursue this vertical study. Data were collected in four phases starting with a high-level randomly stratified panel of 13 senior managers and then a target panel of 124 carefully selected and well-informed domain experts. We have identified and dealt with several quality factors; they were discussed in a comprehensive model inspired by the ISO 25010, 42010, and 12207 standards, corresponding to software quality, software architecture, and software process, respectively. Our results suggest that the relationship among quality, architecture, and process is a valuable technical perspective to explain the quality of an information system. Thus, we introduce and illustrate a novel meta-model, named SQuAP (Software Quality, Architecture, Process), which is intended to give a comprehensive picture of ISQ by abstracting and connecting detailed individual ISO models.","software process, mixed methods, Information systems quality, delphi study, software quality, software architecture, management information systems",,,,
Conference Paper,Lingel J,The Poetics of Socio-Technical Space: Evaluating the Internet of Things Through Craft,,2016,,,815–826,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems,"San Jose, California, USA",2016,9781450333627.0,,https://doi.org/10.1145/2858036.2858399;http://dx.doi.org/10.1145/2858036.2858399,10.1145/2858036.2858399,"Drawing on semi-structured interviews and cognitive mapping with 14 craftspeople, this paper analyzes the socio-technical arrangements of people and tools in the context of workspaces and productivity. Using actor-network theory and the concept of companionability, both of which emphasize the role of human and non-human actants in the socio-technical fabrics of everyday life, I analyze the relationships between people, productivity and technology through the following themes: embodiment, provenance, insecurity, flow and companionability. The discussion section develops these themes further through comparison with rhetoric surrounding the Internet of Things (IoT). By putting the experiences of craftspeople in conversation with IoT rhetoric, I suggest several policy interventions for understanding connectivity and inter-device operability as material, flexible and respectful of human agency.","craft, workspaces, internet of things",CHI '16,,,
Conference Paper,"Patelli A,Bencomo N,Ekárt A,Goldingay H,Lewis P",Two-B or Not Two-B? Design Patterns for Hybrid Metaheuristics,,2015,,,1269–1274,Association for Computing Machinery,"New York, NY, USA",Proceedings of the Companion Publication of the 2015 Annual Conference on Genetic and Evolutionary Computation,"Madrid, Spain",2015,9781450334884.0,,https://doi.org/10.1145/2739482.2768501;http://dx.doi.org/10.1145/2739482.2768501,10.1145/2739482.2768501,"Real world search problems, characterised by nonlinearity, noise and multidimensionality, are often best solved by hybrid algorithms. Techniques embodying different necessary features are triggered at specific iterations, in response to the current state of the problem space. In the existing literature, this alternation is managed either statically (through pre-programmed policies) or dynamically, at the cost of high coupling with algorithm inner representation. We extract two design patterns for hybrid metaheuristic search algorithms, the All-Seeing Eye and the Commentator patterns, which we argue should be replaced by the more flexible and loosely coupled Simple Black Box (Two-B) and Utility-based Black Box (Three-B) patterns that we propose here. We recommend the Two-B pattern for purely fitness based hybridisations and the Three-B pattern for more generic search quality evaluation based hybridisations.",,GECCO Companion '15,,,
Journal Article,"Eastman CM,Jansen BJ","Coverage, Relevance, and Ranking: The Impact of Query Operators on Web Search Engine Results",ACM Trans. Inf. Syst.,2003,21.0,4,383–411,Association for Computing Machinery,"New York, NY, USA",,,2003-10,,1046-8188,https://doi.org/10.1145/944012.944015;http://dx.doi.org/10.1145/944012.944015,10.1145/944012.944015,"Research has reported that about 10% of Web searchers utilize advanced query operators, with the other 90% using extremely simple queries. It is often assumed that the use of query operators, such as Boolean operators and phrase searching, improves the effectiveness of Web searching. We test this assumption by examining the effects of query operators on the performance of three major Web search engines. We selected one hundred queries from the transaction log of a Web search service. Each of these original queries contained query operators such as AND, OR, MUST APPEAR (+), or PHRASE ("" ""). We then removed the operators from these one hundred advanced queries. We submitted both the original and modified queries to three major Web search engines; a total of 600 queries were submitted and 5,748 documents evaluated. We compared the results from the original queries with the operators to the results from the modified queries without the operators. We examined the results for changes in coverage, relative precision, and ranking of relevant documents. The use of most query operators had no significant effect on coverage, relative precision, or ranking, although the effect varied depending on the search engine. We discuss implications for the effectiveness of searching techniques as currently taught, for future information retrieval system design, and for future research.","coverage, Relative precision, query operators, Web results, ranking, Boolean operators, search engines",,,,
Journal Article,"Murata M,Lee D,Mani M,Kawaguchi K",Taxonomy of XML Schema Languages Using Formal Language Theory,ACM Trans. Internet Technol.,2005,5.0,4,660–704,Association for Computing Machinery,"New York, NY, USA",,,2005-11,,1533-5399,https://doi.org/10.1145/1111627.1111631;http://dx.doi.org/10.1145/1111627.1111631,10.1145/1111627.1111631,"On the basis of regular tree grammars, we present a formal framework for XML schema languages. This framework helps to describe, compare, and implement such schema languages in a rigorous manner. Our main results are as follows: (1) a simple framework to study three classes of tree languages (local, single-type, and regular); (2) classification and comparison of schema languages (DTD, W3C XML Schema, and RELAX NG) based on these classes; (3) efficient document validation algorithms for these classes; and (4) other grammatical concepts and advanced validation algorithms relevant to an XML model (e.g., binarization, derivative-based validation).","interpretation, schema, tree automaton, validation, XML",,,,
Journal Article,"Bergstra JA,Middelburg CA",Thread Algebra for Strategic Interleaving,Form. Asp. Comput.,2007,19.0,4,445–474,Springer-Verlag,"Berlin, Heidelberg",,,2007-11,,0934-5043,https://doi.org/10.1007/s00165-007-0024-9;http://dx.doi.org/10.1007/s00165-007-0024-9,10.1007/s00165-007-0024-9,"We take a thread as the behavior of a sequential deterministic program under execution and multi-threading as the form of concurrency provided by contemporary programming languages such as Java and C#. We outline an algebraic theory about threads and multi-threading. In the case of multi-threading, some deterministic interleaving strategy determines how threads are interleaved. Interleaving operators for a number of plausible interleaving strategies are specified in a simple and concise way. By that, we show that it is essentially open-ended what counts as an interleaving strategy. We use deadlock freedom as an example to show that there are properties of multi-threaded programs that depend on the interleaving strategy used.","Deadlock freedom, Threads, Multi-threading, Interleaving strategies, Thread algebra, Services",,,,
Journal Article,"Oygür I,Epstein DA,Chen Y",Raising the Responsible Child: Collaborative Work in the Use of Activity Trackers for Children,Proc.  ACM Hum. -Comput.  Interact.,2020,4.0,CSCW2,,Association for Computing Machinery,"New York, NY, USA",,,2020-10,,,https://doi.org/10.1145/3415228;http://dx.doi.org/10.1145/3415228,10.1145/3415228,"Commercial activity trackers are increasingly being designed for children as young as 3 years old. However, we have limited understanding of family use practices around these trackers. To provide an overall view of how families naturally use activity trackers towards collaborative management of family health, we systematically identified 9 trackers designed for children available on 4 consumer electronics retailers. Our data is composed of 2,628 user reviews both from the consumer retailers (for the wearables) and mobile application stores (for the associated apps). Our findings indicate children's and parents' collaborative use of these technologies beyond health and wellness. Parents state that their children enjoy practicing independence and rewards while contributing to family health management and daily life requirements. Parents expect these devices to ease their life and to teach their children to become more responsible for their health, daily tasks, and schedule. However, the current designs give limited agency on child's side and require parents' active participation for wearable-app coordination. For these reasons, they do not fully address parents' expectations in decreasing their workload. On the other hand, they have the potential to facilitate family interaction with challenges structured around the data reported through trackers.","activity tracking, technology use in family life, family informatics, children, personal informatics",,,,
Conference Paper,"Perković T,Li S,Mumtaz A,Khayam SA,Javed Y,Čagalj M",Breaking Undercover: Exploiting Design Flaws and Nonuniform Human Behavior,,2011,,,,Association for Computing Machinery,"New York, NY, USA",Proceedings of the Seventh Symposium on Usable Privacy and Security,"Pittsburgh, Pennsylvania",2011,9781450309110.0,,https://doi.org/10.1145/2078827.2078834;http://dx.doi.org/10.1145/2078827.2078834,10.1145/2078827.2078834,"This paper reports two attacks on Undercover, a human authentication scheme against passive observers proposed at CHI 2008. The first attack exploits nonuniform human behavior in responding to authentication challenges and the second one is based on information leaked from authentication challenges or responses visible to the attacker. The second attack can be generalized to break two alternative Undercover designs presented at Pervasive 2009. All the attacks exploit design flaws of the Undercover implementations.Theoretical and experimental analyses show that both attacks can reveal the user's password with high probability with O(10) observed login sessions. Both attacks were verified by using the login data collected in a user study with 28 participants. We also propose some enhancements to make Undercover secure against the attacks reported in this paper.Our research in breaking and improving Undercover leads to two broader implications. First, it reemphasizes the principle of ""devil is in details"" for the design of security-related human-computer interface. Secondly, it reveals a subtle relationship between security and usability: human users may behave in an insecure way to compromise the security of a system. To design a secure human-computer interface, designers should pay special attention to possible negative influence of any detail of the interface including how human users interact with the system.","observation attack, timing attack, undercover, tactile device, intersection attack, passwords, audio channel",SOUPS '11,,,
Conference Paper,Zaytsev V,Language Design with Intent,,2017,,,45–52,IEEE Press,"Austin, Texas",Proceedings of the ACM/IEEE 20th International Conference on Model Driven Engineering Languages and Systems,,2017,9781538634929.0,,https://doi.org/10.1109/MODELS.2017.16;http://dx.doi.org/10.1109/MODELS.2017.16,10.1109/MODELS.2017.16,"Software languages have always been an essential component of model-driven engineering. Their importance and popularity has been on the rise thanks to language workbenches, language-oriented development and other methodologies that enable us to quickly and easily create new languages specific for each domain. Unfortunately, language design is largely a form of art and has resisted most attempts to turn it into a form of science or engineering. In this paper we borrow concepts, techniques and principles from the domain of persuasive technology, or wider yet, design with intent --- which was developed as a way to influence users behaviour for social and environmental benefit. Similarly, we claim, software language designers can make conscious choices in order to influence the behaviour of language users. The paper describes a process of extracting design components from 24 books of eight categories (dragon books, parsing techniques, compiler construction, compiler design, language implementation, language documentation, programming languages, software languages), as well as from the original set of Design with Intent cards and papers on DSL design. The resulting language design card toolkit can be used by DSL designers to cover important design decisions and make them with more confidence.",,MODELS '17,,,
Journal Article,"Dudley JJ,Vertanen K,Kristensson PO",Fast and Precise Touch-Based Text Entry for Head-Mounted Augmented Reality with Variable Occlusion,ACM Trans.  Comput. -Hum.  Interact.,2018,25.0,6,,Association for Computing Machinery,"New York, NY, USA",,,2018-12,,1073-0516,https://doi.org/10.1145/3232163;http://dx.doi.org/10.1145/3232163,10.1145/3232163,"We present the VISAR keyboard: An augmented reality (AR) head-mounted display (HMD) system that supports text entry via a virtualised input surface. Users select keys on the virtual keyboard by imitating the process of single-hand typing on a physical touchscreen display. Our system uses a statistical decoder to infer users’ intended text and to provide error-tolerant predictions. There is also a high-precision fall-back mechanism to support users in indicating which keys should be unmodified by the auto-correction process. A unique advantage of leveraging the well-established touch input paradigm is that our system enables text entry with minimal visual clutter on the see-through display, thus preserving the user’s field-of-view. We iteratively designed and evaluated our system and show that the final iteration of the system supports a mean entry rate of 17.75wpm with a mean character error rate less than 1%. This performance represents a 19.6% improvement relative to the state-of-the-art baseline investigated: A gaze-then-gesture text entry technique derived from the system keyboard on the Microsoft HoloLens. Finally, we validate that the system is effective in supporting text entry in a fully mobile usage scenario likely to be encountered in industrial applications of AR HMDs.","Augmented reality, text entry",,,,
Journal Article,"Jones AK,Schwarz P",Experience Using Multiprocessor Systems—A Status Report,ACM Comput. Surv.,1980,12.0,2,121–165,Association for Computing Machinery,"New York, NY, USA",,,1980-06,,0360-0300,https://doi.org/10.1145/356810.356813;http://dx.doi.org/10.1145/356810.356813,10.1145/356810.356813,,,,,,
Book,"Allemang D,Hendler J,Gandon F","Semantic Web for the Working Ontologist: Effective Modeling for Linked Data, RDFS, and OWL",,2020,33.0,,,Association for Computing Machinery,"New York, NY, USA",,,2020,9781450376174.0,,,,"Enterprises have made amazing advances by taking advantage of data about their business to provide predictions and understanding of their customers, markets, and products. But as the world of business becomes more interconnected and global, enterprise data is no long a monolith; it is just a part of a vast web of data. Managing data on a world-wide scale is a key capability for any business today.The Semantic Web treats data as a distributed resource on the scale of the World Wide Web, and incorporates features to address the challenges of massive data distribution as part of its basic design. The aim of the first two editions was to motivate the Semantic Web technology stack from end-to-end; to describe not only what the Semantic Web standards are and how they work, but also what their goals are and why they were designed as they are. It tells a coherent story from beginning to end of how the standards work to manage a world-wide distributed web of knowledge in a meaningful way.The third edition builds on this foundation to bring Semantic Web practice to enterprise. Fabien Gandon joins Dean Allemang and Jim Hendler, bringing with him years of experience in global linked data, to open up the story to a modern view of global linked data. While the overall story is the same, the examples have been brought up to date and applied in a modern setting, where enterprise and global data come together as a living, linked network of data. Also included with the third edition, all of the data sets and queries are available online for study and experimentation at data.world/swwo.",,,,,3.0
Conference Paper,"Knowles B,Blair L,Walker S,Coulton P,Thomas L,Mullagh L",Patterns of Persuasion for Sustainability,,2014,,,1035–1044,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 2014 Conference on Designing Interactive Systems,"Vancouver, BC, Canada",2014,9781450329026.0,,https://doi.org/10.1145/2598510.2598536;http://dx.doi.org/10.1145/2598510.2598536,10.1145/2598510.2598536,"Research into the values motivating unsustainable behavior has generated unique insight into how NGOs and environmental campaigns contribute toward successfully fostering significant and long-term behavior change, yet thus far this research has not been applied to the domain of sustainable HCI. We explore the implications of this research as it relates to the potential limitations of current approaches to persuasive technology, and what it means for designing higher impact interventions. As a means of communicating these implications to be readily understandable and implementable, we develop a set of antipatterns to describe persuasive technology approaches that values research suggests are unlikely to yield significant sustainability wins, and a complementary set of patterns to describe new guidelines for what may become persuasive technology best practice.","pattern language, values, persuasive technology, sustainability",DIS '14,,,
Conference Paper,"Blanton J,Leski S,Nicks B,Tirzaman T",Making SOA Work in a Healthcare Company,,2009,,,589–596,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 24th ACM SIGPLAN Conference Companion on Object Oriented Programming Systems Languages and Applications,"Orlando, Florida, USA",2009,9781605587684.0,,https://doi.org/10.1145/1639950.1639953;http://dx.doi.org/10.1145/1639950.1639953,10.1145/1639950.1639953,"Making SOA work in a large and diverse healthcare company is not just about bridging the gap between business and IT. It is also about bridging the gap between the technologies of yesterday, today and tomorrow. As Health Net has grown by acquiring other entities, we have acquired a landscape of diverse assets written with many languages, hosted on many platforms. These range from Java on WebLogic to .Net to RPG on iSeries to CICS on zSeries to COBOL on OpenVMS. Integrating these systems goes beyond simple business services. Successful integration ultimately requires elevating IT teams to the vision of a SOA enterprise as defined by an enterprise reference architecture. Educating our IT project teams in the fundamentals of SOA design and development has involved special approaches and a commitment to mentoring and continuous education in the enterprise. This discussion covers some of the challenges, successes, and lessons learned that we have encountered in bringing SOA to Health Net.","SOA, healthcare, ESB, services, health net, GSOAP",OOPSLA '09,,,
Book Chapter,,Good and Bad Modeling Practices,,2020,,,,Association for Computing Machinery,"New York, NY, USA",,,2020,9781450376174.0,,https://doi.org/10.1145/3382097.3382113;http://dx.doi.org/10.1145/3382097.3382113,10.1145/3382097.3382113,"Enterprises have made amazing advances by taking advantage of data about their business to provide predictions and understanding of their customers, markets, and products. But as the world of business becomes more interconnected and global, enterprise data is no long a monolith; it is just a part of a vast web of data. Managing data on a world-wide scale is a key capability for any business today.The Semantic Web treats data as a distributed resource on the scale of the World Wide Web, and incorporates features to address the challenges of massive data distribution as part of its basic design. The aim of the first two editions was to motivate the Semantic Web technology stack from end-to-end; to describe not only what the Semantic Web standards are and how they work, but also what their goals are and why they were designed as they are. It tells a coherent story from beginning to end of how the standards work to manage a world-wide distributed web of knowledge in a meaningful way.The third edition builds on this foundation to bring Semantic Web practice to enterprise. Fabien Gandon joins Dean Allemang and Jim Hendler, bringing with him years of experience in global linked data, to open up the story to a modern view of global linked data. While the overall story is the same, the examples have been brought up to date and applied in a modern setting, where enterprise and global data come together as a living, linked network of data. Also included with the third edition, all of the data sets and queries are available online for study and experimentation at data.world/swwo.",,,,"Semantic Web for the Working Ontologist: Effective Modeling for Linked Data, RDFS, and OWL",
Journal Article,"Meyrowitz N,van Dam A",Interactive Editing Systems: Part II,ACM Comput. Surv.,1982,14.0,3,353–415,Association for Computing Machinery,"New York, NY, USA",,,1982-09,,0360-0300,https://doi.org/10.1145/356887.356890;http://dx.doi.org/10.1145/356887.356890,10.1145/356887.356890,,,,,,
Conference Paper,"Loksa D,Ko AJ,Jernigan W,Oleson A,Mendez CJ,Burnett MM","Programming, Problem Solving, and Self-Awareness: Effects of Explicit Guidance",,2016,,,1449–1461,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems,"San Jose, California, USA",2016,9781450333627.0,,https://doi.org/10.1145/2858036.2858252;http://dx.doi.org/10.1145/2858036.2858252,10.1145/2858036.2858252,"More people are learning to code than ever, but most learning opportunities do not explicitly teach the problem solving skills necessary to succeed at open-ended programming problems. In this paper, we present a new approach to impart these skills, consisting of: 1) explicit instruction on programming problem solving, which frames coding as a process of translating mental representations of problems and solutions into source code, 2) a method of visualizing and monitoring progression through six problem solving stages, 3) explicit, on-demand prompts for learners to reflect on their strategies when seeking help from instructors, and 4) context-sensitive help embedded in a code editor that reinforces the problem solving instruction. We experimentally evaluated the effects of our intervention across two 2-week web development summer camps with 48 high school students, finding that the intervention increased productivity, independence, programming self-efficacy, metacognitive awareness, and growth mindset. We discuss the implications of these results on learning technologies and classroom instruction.","problem-solving, metacognition, programming, computer science education",CHI '16,,,
Conference Paper,"Bakke E,Karger DR",Expressive Query Construction through Direct Manipulation of Nested Relational Results,,2016,,,1377–1392,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 2016 International Conference on Management of Data,"San Francisco, California, USA",2016,9781450335317.0,,https://doi.org/10.1145/2882903.2915210;http://dx.doi.org/10.1145/2882903.2915210,10.1145/2882903.2915210,"Despite extensive research on visual query systems, the standard way to interact with relational databases remains to be through SQL queries and tailored form interfaces. We consider three requirements to be essential to a successful alternative: (1) query specification through direct manipulation of results, (2) the ability to view and modify any part of the current query without departing from the direct manipulation interface, and (3) SQL-like expressiveness. This paper presents the first visual query system to meet all three requirements in a single design. By directly manipulating nested relational results, and using spreadsheet idioms such as formulas and filters, the user can express a relationally complete set of query operators plus calculation, aggregation, outer joins, sorting, and nesting, while always remaining able to track and modify the state of the complete query. Our prototype gives the user an experience of responsive, incremental query building while pushing all actual query processing to the database layer. We evaluate our system with formative and controlled user studies on 28 spreadsheet users; the controlled study shows our system significantly outperforming Microsoft Access on the System Usability Scale.","visual query languages, visual query systems, report generation, hierarchical data models, spreadsheet interfaces, user studies, nested relations, direct manipulation",SIGMOD '16,,,
Conference Paper,"Walter B,Matuszyk B,Fontana FA",Including Structural Factors into the Metrics-Based Code Smells Detection,,2015,,,,Association for Computing Machinery,"New York, NY, USA",Scientific Workshop Proceedings of the XP2015,"Helsinki, Finland",2015,9781450334099.0,,https://doi.org/10.1145/2764979.2764990;http://dx.doi.org/10.1145/2764979.2764990,10.1145/2764979.2764990,"Code smells help to discover and describe deeper problems in software design. Several automated methods of smell detection are based the analysis of a combination of code-related metrics relevant for a given flaw. However, some smells reflect more complex issues and require a holistic perspective that woudl cover a number of different sources of data. In this paper we experimentally verify the usefulness of including structural factors into a metrics-based detection of God Class and Brain Class code smells.","detection strategies, code smell detection",XP '15 workshops,,,
Conference Paper,"Fontana FA,Ferme V,Spinelli S",Investigating the Impact of Code Smells Debt on Quality Code Evaluation,,2012,,,15–22,IEEE Press,"Zurich, Switzerland",Proceedings of the Third International Workshop on Managing Technical Debt,,2012,9781467317498.0,,,,"Different forms of technical debt exist that have to be carefully managed. In this paper we focus our attention on design debt, represented by code smells. We consider three smells that we detect in open source systems of different domains. Our principal aim is to give advice on which design debt has to be paid first, according to the three smells we have analyzed. Moreover, we discuss if the detection of these smells could be tailored to the specific application domain of a system.","code smell refactoring, design debt, software quality metrics",MTD '12,,,
Conference Paper,Chen Z,Helping Mobile Software Code Reviewers: A Study of Bug Repair and Refactoring Patterns,,2016,,,34–35,Association for Computing Machinery,"New York, NY, USA",Proceedings of the International Conference on Mobile Software Engineering and Systems,"Austin, Texas",2016,9781450341783.0,,https://doi.org/10.1145/2897073.2897130;http://dx.doi.org/10.1145/2897073.2897130,10.1145/2897073.2897130,"Mobile Developers commonly spend a significant amount of time and effort on conducting code reviews on newly introduced and domain-specific practices, such as platform-specific feature addition, quality of service anti-pattern refactorings, and battery-related bug fixes. To address these problems, we conducted a large empirical study over the software change history of 318 open source projects and investigated platform-dependent code changes from open source projects. Our analysis focuses on what types of changes mobile application developers typically make and how they perceive, recall, and communicate changed and affected code. Our study required the development of an automated strategy to examine open source repositories and categorize platform-related refactoring edits, bug repairs, and API updates, mining 1,961,990 commit changes. Our findings call for the need to develop a new recommendation system aimed at efficiently identifying required changes such as bug fixes and refactorings during mobile application code reviews.",,MOBILESoft '16,,,
Conference Paper,"Codabux Z,Dutchyn C",Profiling Developers Through the Lens of Technical Debt,,2020,,,,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 14th ACM / IEEE International Symposium on Empirical Software Engineering and Measurement (ESEM),"Bari, Italy",2020,9781450375801.0,,https://doi.org/10.1145/3382494.3422172;http://dx.doi.org/10.1145/3382494.3422172,10.1145/3382494.3422172,"Context: Technical Debt needs to be managed to avoid disastrous consequences, and investigating developers' habits concerning technical debt management is invaluable information in software development. Objective: This study aims to characterize how developers manage technical debt based on the code smells they induce and the refactorings they apply. Method: We mined a publicly-available Technical Debt dataset for Git commit information, code smells, coding violations, and refactoring activities for each developer of a selected project. Results: By combining this information, we profile developers to recognize prolific coders, highlight activities that discriminate among developer roles (reviewer, lead, architect), and estimate coding maturity and technical debt tolerance.","Developer Characterization, Code Smell, Mining Software Repositories, Technical Debt, Open Source Software, Refactoring",ESEM '20,,,
Conference Paper,"Pascarella L,Geiger FX,Palomba F,Di Nucci D,Malavolta I,Bacchelli A",Self-Reported Activities of Android Developers,,2018,,,144–155,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 5th International Conference on Mobile Software Engineering and Systems,"Gothenburg, Sweden",2018,9781450357128.0,,https://doi.org/10.1145/3197231.3197251;http://dx.doi.org/10.1145/3197231.3197251,10.1145/3197231.3197251,"To gain a deeper empirical understanding of how developers work on Android apps, we investigate self-reported activities of Android developers and to what extent these activities can be classified with machine learning techniques. To this aim, we firstly create a taxonomy of self-reported activities coming from the manual analysis of 5,000 commit messages from 8,280 Android apps. Then, we study the frequency of each category of self-reported activities identified in the taxonomy, and investigate the feasibility of an automated classification approach. Our findings can inform be used by both practitioners and researchers to take informed decisions or support other software engineering activities.","Android, empirical study, mining software repositories",MOBILESoft '18,,,
Conference Paper,"Yasir RM,Asad M,Galib AH,Ganguly KK,Siddik MS",GodExpo: An Automated God Structure Detection Tool for Golang,,2019,,,47–50,IEEE Press,"Montreal, Quebec, Canada",Proceedings of the 3rd International Workshop on Refactoring,,2019,,,https://doi.org/10.1109/IWoR.2019.00016;http://dx.doi.org/10.1109/IWoR.2019.00016,10.1109/IWoR.2019.00016,"God Class is a class that threatens maintainability and understandability of code by performing most of the work alone. Various tools exist that can detect God Class of Java or C++ programs, however, there is no existing tool for detecting God Class(Structure) in Golang. Although Golang is not an object-oriented language, it offers structures which are similar to classes in OOP as they can contain fields and methods. Unlike OOP, methods of a structure can be defined on any file in the package of Golang. This paper presents a tool entitled GodExpo to detect God Structures in Golang programs by calculating metrics namely Weighted Method Count, Tight Class Cohesion, and Access to Foreign Data. In addition, GodExpo can provide version wise result to observe the evolution of God structures. To evaluate GodExpo, an experiment has been conducted on several versions of two open source Golang projects and the tool successfully found God structures in all versions of those projects.","Golang, god class, OOP metrics, code smell",IWOR '19,,,
Conference Paper,"Bibiano AC,Soares V,Coutinho D,Fernandes E,Correia JL,Santos K,Oliveira A,Garcia A,Gheyi R,Fonseca B,Ribeiro M,Barbosa C,Oliveira D",How Does Incomplete Composite Refactoring Affect Internal Quality Attributes?,,2020,,,149–159,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 28th International Conference on Program Comprehension,"Seoul, Republic of Korea",2020,9781450379588.0,,https://doi.org/10.1145/3387904.3389264;http://dx.doi.org/10.1145/3387904.3389264,10.1145/3387904.3389264,"Program refactoring consists of code changes applied to improve the internal structure of a program and, as a consequence, its comprehensibility. Recent studies indicate that developers often perform composite refactorings, i.e., a set of two or more interrelated single refactorings. Recent studies also recommend certain patterns of composite refactorings to fully remove poor code structures, i.e, code smells, thus further improving the program comprehension. However, other recent studies report that composite refactorings often fail to fully remove code smells. Given their failure to achieve this purpose, these composite refactorings are considered incomplete, i.e, they are not able to entirely remove a smelly structure. Unfortunately, there is no study providing an in-depth analysis of the incompleteness nature of many composites and their possibly partial impact on improving, maybe decreasing, internal quality attributes. This paper identifies the most common forms of incomplete composites, and their effect on quality attributes, such as coupling and cohesion, which are known to have an impact on program comprehension. We analyzed 353 incomplete composite refactorings in 5 software projects, two common code smells (Feature Envy and God Class), and four internal quality attributes. Our results reveal that incomplete composite refactorings with at least one Extract Method are often (71%) applied without Move Methods on smelly classes. We have also found that most incomplete composite refactorings (58%) tended to at least maintain the internal structural quality of smelly classes, thereby not causing more harm to program comprehension. We also discuss the implications of our findings to the research and practice of composite refactoring.","Code refactoring, internal quality attribute, code smell, composite refactoring, incomplete composite, code metric, quantitative study",ICPC '20,,,
Conference Paper,"Aghajani E,Nagy C,Linares-Vásquez M,Moreno L,Bavota G,Lanza M,Shepherd DC",Software Documentation: The Practitioners' Perspective,,2020,,,590–601,Association for Computing Machinery,"New York, NY, USA",Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering,"Seoul, South Korea",2020,9781450371216.0,,https://doi.org/10.1145/3377811.3380405;http://dx.doi.org/10.1145/3377811.3380405,10.1145/3377811.3380405,"In theory, (good) documentation is an invaluable asset to any software project, as it helps stakeholders to use, understand, maintain, and evolve a system. In practice, however, documentation is generally affected by numerous shortcomings and issues, such as insufficient and inadequate content and obsolete, ambiguous information. To counter this, researchers are investigating the development of advanced recommender systems that automatically suggest high-quality documentation, useful for a given task. A crucial first step is to understand what quality means for practitioners and what information is actually needed for specific tasks.We present two surveys performed with 146 practitioners to investigate (i) the documentation issues they perceive as more relevant together with solutions they apply when these issues arise; and (ii) the types of documentation considered as important in different tasks. Our findings can help researchers in designing the next generation of documentation recommender systems.","empirical study, documentation",ICSE '20,,,
Conference Paper,"Nanthaamornphong A,Chaisutanon A",Empirical Evaluation of Code Smells in Open Source Projects: Preliminary Results,,2016,,,5–8,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 1st International Workshop on Software Refactoring,"Singapore, Singapore",2016,9781450345095.0,,https://doi.org/10.1145/2975945.2975947;http://dx.doi.org/10.1145/2975945.2975947,10.1145/2975945.2975947,"Open Source Software (OSS) now plays an important role in various industry domains. OSS is generally developed by highly experienced developers who have multiple perspectives. However, previous studies have indicated that OSS has quality limitations in software maintainability. In general, OSS developers typically focus on achieving the correct functionality. In contrast, in addition to focusing on building software functionality, software engineering practices also focus on the structure of the software and on its maintainability. Code with a well-designed structure is more likely to result in high quality software. To better understand how peer code review can reduce ``code smells"" in existing OSS projects, we examined comments from code reviewers that identified code smells in OSS projects. This paper is a proof-of-concept that presents the preliminary results from an analysis of comments we obtained for two OSS projects, OpenStack and WikiMedia, both of which use the code review data repository called Gerrit. The preliminary results of this ongoing research show that code reviewers comment on only a small number of code smells. The full-scale results would contribute to the empirical body of validated knowledge in the field of OSS quality and code review.","open source software, Software engineering, code smell",IWoR 2016,,,
Conference Paper,"De Wael M,Marr S,Van Cutsem T",Fork/Join Parallelism in the Wild: Documenting Patterns and Anti-Patterns in Java Programs Using the Fork/Join Framework,,2014,,,39–50,Association for Computing Machinery,"New York, NY, USA","Proceedings of the 2014 International Conference on Principles and Practices of Programming on the Java Platform: Virtual Machines, Languages, and Tools","Cracow, Poland",2014,9781450329262.0,,https://doi.org/10.1145/2647508.2647511;http://dx.doi.org/10.1145/2647508.2647511,10.1145/2647508.2647511,"Now that multicore processors are commonplace, developing parallel software has escaped the confines of high-performance computing and enters the mainstream. The Fork/Join framework, for instance, is part of the standard Java platform since version 7. Fork/Join is a high-level parallel programming model advocated to make parallelizing recursive divide-and-conquer algorithms particularly easy. While, in theory, Fork/Join is a simple and effective technique to expose parallelism in applications, it has not been investigated before whether and how the technique is applied in practice. We therefore performed an empirical study on a corpus of 120 open source Java projects that use the framework for roughly 362 different tasks.On the one hand, we confirm the frequent use of four best-practice patterns (Sequential Cutoff, Linked Subtasks, Leaf Tasks, and avoiding unnecessary forking) in actual projects. On the other hand, we also discovered three recurring anti-patterns that potentially limit parallel performance: sub-optimal use of Java collections when splitting tasks into subtasks as well as when merging the results of subtasks, and finally the inappropriate sharing of resources between tasks. We document these anti-patterns and study their impact on performance.","anti-patterns, empirical study, open source projects, fork/join, Java, patterns",PPPJ '14,,,
Conference Paper,"Roy D,Fakhoury S,Lee J,Arnaoudova V",A Model to Detect Readability Improvements in Incremental Changes,,2020,,,25–36,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 28th International Conference on Program Comprehension,"Seoul, Republic of Korea",2020,9781450379588.0,,https://doi.org/10.1145/3387904.3389255;http://dx.doi.org/10.1145/3387904.3389255,10.1145/3387904.3389255,"Identifying source code that has poor readability allows developers to focus maintenance efforts on problematic code. Therefore, the effort to develop models that can quantify the readability of a piece of source code has been an area of interest for software engineering researchers for several years. However, recent research questions the usefulness of these readability models in practice. When applying these models to readability improvements that are made in practice, i.e., commits, they are unable to capture these incremental improvements, despite a clear perceived improvement by the developers. This results in a discrepancy between the models we have built to measure readability, and the actual perception of readability in practice.In this work, we propose a model that is able to detect incremental readability improvements made by developers in practice with an average precision of 79.2% and an average recall of 67% on an unseen test set. We then investigate the metrics that our model associates with developer perceived readability improvements as well as non-readability changes. Finally, we compare our model to existing state-of-the-art readability models, which our model outperforms by at least 23% in terms of precision and 42% in terms of recall.","Source code readability, Code quality, Machine learning",ICPC '20,,,
Conference Paper,Petrić J,Using Different Characteristics of Machine Learners to Identify Different Defect Families,,2016,,,,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 20th International Conference on Evaluation and Assessment in Software Engineering,"Limerick, Ireland",2016,9781450336918.0,,https://doi.org/10.1145/2915970.2915979;http://dx.doi.org/10.1145/2915970.2915979,10.1145/2915970.2915979,"Background: Software defect prediction has been an active area of research for the last few decades. Many models have been developed with aim to find locations in code likely to contain defects. As of yet, these prediction models are of limited use and rarely used in the software industry.Problem: Current modelling techniques are too coarse grained and fail in finding some defects. Most of the prediction models do not look for targeted defect characteristics, but rather treat them as a black box and homogeneous. No study has investigated in greater detail how well certain defect characteristics work with different prediction modelling techniques.Methodology: This PhD will address three major tasks. First, the relation among software defects, prediction models and static code metrics will be analysed. Second, the possibility of a mapping function between prediction models and defect characteristics shall be investigated. Third, an optimised ensemble model that searches for targeted defects will be developed.Contribution: A few contributions will yield from this work. Characteristics of defects will be identified, allowing other researchers to build on this work to produce more efficient prediction models in future. New modelling techniques that better suit state-of-the-art knowledge in defect prediction shall be designed. Such prediction models should be transformed in a tool that can be used by our industrial collaborator in the real industry environment.","prediction modeling, software defect prediction, machine learning",EASE '16,,,
Conference Paper,"Palomba F,Nucci DD,Tufano M,Bavota G,Oliveto R,Poshyvanyk D,De Lucia A",Landfill: An Open Dataset of Code Smells with Public Evaluation,,2015,,,482–485,IEEE Press,"Florence, Italy",Proceedings of the 12th Working Conference on Mining Software Repositories,,2015,9780769555942.0,,,,"Code smells are symptoms of poor design and implementation choices that may hinder code comprehension and possibly increase change- and fault-proneness of source code. Several techniques have been proposed in the literature for detecting code smells. These techniques are generally evaluated by comparing their accuracy on a set of detected candidate code smells against a manually-produced oracle. Unfortunately, such comprehensive sets of annotated code smells are not available in the literature with only few exceptions. In this paper we contribute (i) a dataset of 243 instances of five types of code smells identified from 20 open source software projects, (ii) a systematic procedure for validating code smell datasets, (iii) Landfill, a Web-based platform for sharing code smell datasets, and (iv) a set of APIs for programmatically accessing Landfill's contents. Anyone can contribute to Landfill by (i) improving existing datasets (e.g., adding missing instances of code smells, flagging possibly incorrectly classified instances), and (ii) sharing and posting new datasets. Landfill is available at www.sesa.unisa.it/landfill/, while the video demonstrating its features in action is available at http://www.sesa.unisa.it/tools/landfill.jsp.",,MSR '15,,,
Conference Paper,Anwar H,Towards Greener Android Application Development,,2020,,,170–173,Association for Computing Machinery,"New York, NY, USA",Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering: Companion Proceedings,"Seoul, South Korea",2020,9781450371223.0,,https://doi.org/10.1145/3377812.3381390;http://dx.doi.org/10.1145/3377812.3381390,10.1145/3377812.3381390,"Empirical studies have shown that mobile applications that do not drain battery usually get good ratings from users. To make mobile application energy efficient many studies have been published that present refactoring guidelines and tools to optimize the code. However, these guidelines cannot be generalized w.r.t energy efficiency, as there is not enough energy related data for every context. Existing energy enhancement tools/profilers are mostly prototypes applicable to only a small subset of energy related problems. In addition, the existing guidelines and tools mostly address the energy issues once they have already been introduced. My goal is to add to the existing energy related data by evaluating the energy consumption of various code smell refactorings and third-party libraries used in Android development. Data from such evaluations could provide generalized contextual guidelines that could be used during application development to prevent the introduction of energy related problems. I also aim to develop a support tool for the Android Studio IDE that could give meaningful recommendations to developers during development to make the application code more energy efficient.","third-party libraries, green software engineering, energy consumption, green Android development, code smell refactoring",ICSE '20,,,
Conference Paper,"Souza PP,Sousa BL,Ferreira KA,Bigonha MA",Applying Software Metric Thresholds for Detection of Bad Smells,,2017,,,,Association for Computing Machinery,"New York, NY, USA","Proceedings of the 11th Brazilian Symposium on Software Components, Architectures, and Reuse","Fortaleza, Ceará, Brazil",2017,9781450353250.0,,https://doi.org/10.1145/3132498.3134268;http://dx.doi.org/10.1145/3132498.3134268,10.1145/3132498.3134268,"Software metrics can be an effective measurement tool to assess the quality of software. In the literature, there are a lot of software metrics applicable to systems implemented in different paradigms like Objects Oriented Programming (OOP). To guide the use of these metrics in the evaluation of the quality of software systems, it is important to define their thresholds. The aim of this study is to investigate the effectiveness of the thresholds in the evaluation of the quality of object oriented software. To do that, we used a threshold catalog of 18 software metrics derived from 100 software systems to define detection strategies for five bad smells. They are: Large Class, Long Method, Data Class, Feature Envy and Refused Bequest. We investigate the effectiveness of the thresholds in detection analysis of 12 software systems using these strategies. The results obtained by the proposed strategies were compared with the results obtained by the tools JDeodorant and JSPiRIT, used to identify bad smells. This study shows that the metric thresholds were significantly effective in supporting the detection of bad smells.","software metrics, thresholds, bad smells detection, software quality",SBCARS '17,,,
Conference Paper,"Kambli A,Modi S",Fuzzy Neuro Approach to Water Management Systems,,2019,,,215–220,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 3rd International Conference on Machine Learning and Soft Computing,"Da Lat, Viet Nam",2019,9781450366120.0,,https://doi.org/10.1145/3310986.3311026;http://dx.doi.org/10.1145/3310986.3311026,10.1145/3310986.3311026,"This paper addresses the need for intelligent water management and distribution system in smart cities to ensure optimal consumption and distribution of water for drinking and sanitation purposes using two mostly widely used particular types of data driven models, namely recurrent neural networks (RNN) and fuzzy logic-based models.. The objective of this paper is to review the principles of various types and architectures of neural network and fuzzy adaptive systems and their applications to integrated water resources management. Final goal of the review is to expose and formulate progressive direction of their applicability and further research of the AI-related and data-driven techniques application and to demonstrate applicability of the neural networks, fuzzy systems and other machine learning techniques in the practical issues of the regional water management. Apart from this the paper will deal with water storage, using RNN to find optimum reservoir level and predicting peak daily demands.","Water Management and Distribution, Peak Daily Demand Prediction, Neural Networks, Fuzzy Systems",ICMLSC 2019,,,
Conference Paper,Ebert C,Experiences with Criticality Predictions in Software Development,,1997,,,278–293,Springer-Verlag,"Berlin, Heidelberg",Proceedings of the 6th European SOFTWARE ENGINEERING Conference Held Jointly with the 5th ACM SIGSOFT International Symposium on Foundations of Software Engineering,"Zurich, Switzerland",1997,9783540635314.0,,https://doi.org/10.1145/267895.267916;http://dx.doi.org/10.1145/267895.267916,10.1145/267895.267916,,"software metrics, data analysis, eomplexity, classification, quality models, criticality prediction",ESEC '97/FSE-5,,,
Journal Article,Ebert C,Experiences with Criticality Predictions in Software Development,SIGSOFT Softw. Eng. Notes,1997,22.0,6.0,278–293,Association for Computing Machinery,"New York, NY, USA",,,1997-11,,0163-5948,https://doi.org/10.1145/267896.267916;http://dx.doi.org/10.1145/267896.267916,10.1145/267896.267916,,"quality models, criticality prediction, eomplexity, classification, data analysis, software metrics",,,,
Conference Paper,"Boutaib S,Bechikh S,Coello CA,Hung CC,Said LB",Handling Uncertainty in Code Smells Detection Using a Possibilistic SBSE Approach,,2020,,,303–304,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 2020 Genetic and Evolutionary Computation Conference Companion,"Cancún, Mexico",2020,9781450371278.0,,https://doi.org/10.1145/3377929.3389948;http://dx.doi.org/10.1145/3377929.3389948,10.1145/3377929.3389948,"Code smells, also known as anti-patterns, are indicators of bad design solutions. However, two different experts may have different opinions not only about the smelliness of a particular software class but also about the smell type. This causes an uncertainty problem that should be taken into account. Unfortunately, existing works reject uncertain data that correspond to software classes with doubtful labels. Uncertain data rejection could cause a significant loss of information that could considerably degrade the performance of the detection process. Motivated by this observation and the good performance of the possibilistic K-NN classifier in handling uncertain data, we propose in this paper a new evolutionary detection method, named ADIPOK (Anti-pattern Detection and Identification using Possibilistic Optimized K-NN), that is able to cope with the uncertainty factor using the possibility theory. The comparative experimental results reveal the merits of our proposal with respect to four relevant state-of-the-art approaches.","code smells detection, evolutionary algorithm, uncertain class labels, possibilistic K-NN",GECCO '20,,,
Conference Paper,"Müller S,Würsch M,Fritz T,Gall HC",An Approach for Collaborative Code Reviews Using Multi-Touch Technology,,2012,,,93–99,IEEE Press,"Zurich, Switzerland",Proceedings of the 5th International Workshop on Co-Operative and Human Aspects of Software Engineering,,2012,9781467318242.0,,,,"Code reviews are an effective mechanism to improve software quality, but often fall short in the development of software. To improve the desirability and ease of code reviews, we introduce an approach that explores how multi-touch interfaces can support code reviews and can make them more collaborative. Our approach provides users with features to collaboratively find and investigate code smells, annotate source code and generate review reports using gesture recognition and a Microsoft Surface Table. In a preliminary evaluation, subjects generally liked the prototypical implementation of our approach for performing code review tasks.","software metrics, multi-touch, code review, code smell, collaboration, gesture",CHASE '12,,,
Conference Paper,"Rahman A,Parnin C,Williams L",The Seven Sins: Security Smells in Infrastructure as Code Scripts,,2019,,,164–175,IEEE Press,"Montreal, Quebec, Canada",Proceedings of the 41st International Conference on Software Engineering,,2019,,,https://doi.org/10.1109/ICSE.2019.00033;http://dx.doi.org/10.1109/ICSE.2019.00033,10.1109/ICSE.2019.00033,"Practitioners use infrastructure as code (IaC) scripts to provision servers and development environments. While developing IaC scripts, practitioners may inadvertently introduce security smells. Security smells are recurring coding patterns that are indicative of security weakness and can potentially lead to security breaches. The goal of this paper is to help practitioners avoid insecure coding practices while developing infrastructure as code (IaC) scripts through an empirical study of security smells in IaC scripts.We apply qualitative analysis on 1,726 IaC scripts to identify seven security smells. Next, we implement and validate a static analysis tool called Security Linter for Infrastructure as Code scripts (SLIC) to identify the occurrence of each smell in 15,232 IaC scripts collected from 293 open source repositories. We identify 21,201 occurrences of security smells that include 1,326 occurrences of hard-coded passwords. We submitted bug reports for 1,000 randomly-selected security smell occurrences. We obtain 212 responses to these bug reports, of which 148 occurrences were accepted by the development teams to be fixed. We observe security smells can have a long lifetime, e.g., a hard-coded secret can persist for as long as 98 months, with a median lifetime of 20 months.","static analysis, smell, puppet, infrastructure as code, empirical study, devsecops, devops",ICSE '19,,,
Conference Paper,"Aljarallah S,Lock R",An Exploratory Study of Software Sustainability Dimensions and Characteristics: End User Perspectives in the Kingdom of Saudi Arabia (KSA),,2018,,,,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 12th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement,"Oulu, Finland",2018,9781450358231.0,,https://doi.org/10.1145/3239235.3239240;http://dx.doi.org/10.1145/3239235.3239240,10.1145/3239235.3239240,"Background: Sustainability has become an important topic globally and the focus on ICT sustainability is increasing. However, issues exist, including vagueness and complexity of the concept itself, in addition to immaturity of the Software Engineering (SE) field. Aims: The study surveys respondents on software sustainability dimensions and characteristics from their perspectives, and seeks to derive rankings for their priority. Method: An exploratory study was conducted to quantitatively investigate Saudi Arabian (KSA) software user's perceptions with regard to the concept itself, the dimensions and characteristics of the software sustainability. Survey data was gathered from 906 respondents. Results: The results highlight key dimensions for sustainability and their priorities to users. The results also indicate that the characteristics perceived to be the most significant, were security, usability, reliability, maintainability, extensibility and portability, whereas respondents were relatively less concerned with computer ethics (e.g. privacy and trust), functionality, efficiency and reusability. A key finding was that females considered the environmental dimension to be more important than males. Conclusions: The dimensions and characteristics identified here can be used as a means of providing valuable feedback for the planning and implementation of future development of sustainable software.","sustainability dimensions, empirical study, software sustainability",ESEM '18,,,
Conference Paper,"Souza IS,Machado I,Seaman C,Gomes G,Chavez C,de Almeida ES,Masiero P",Investigating Variability-Aware Smells in SPLs: An Exploratory Study,,2019,,,367–376,Association for Computing Machinery,"New York, NY, USA",Proceedings of the XXXIII Brazilian Symposium on Software Engineering,"Salvador, Brazil",2019,9781450376518.0,,https://doi.org/10.1145/3350768.3350774;http://dx.doi.org/10.1145/3350768.3350774,10.1145/3350768.3350774,"Variability-aware smell is a concept referring to artifact shortcomings in the context of highly-configurable systems that can degrade aspects such as program comprehension, maintainability, and evolvability. To the best of our knowledge, there is very little evidence that variability-aware smells exist in Software Product Lines (SPLs). This work presents an exploratory study that investigated (I) evidence that variability-aware smells exist in SPLs and (II) new types of variability-aware smell not yet documented in the literature based on a quantitative study with open source SPL projects. We collected quantitative data to generate reliable research evidence, by performing feature model and source code inspections on eleven open-source SPL projects. Our findings revealed that (1) instances of variability-aware smells exist in open-source SPL projects and (2) feature information presented significant associations with variability-aware smells. Furthermore, (3) the study presented six new types of variability-aware smells.","Variability-Aware Smells, Exploratory Study, Software Product Lines, Empirical Study",SBES '19,,,
Conference Paper,"Kurbatova Z,Veselov I,Golubev Y,Bryksin T",Recommendation of Move Method Refactoring Using Path-Based Representation of Code,,2020,,,315–322,Association for Computing Machinery,"New York, NY, USA",Proceedings of the IEEE/ACM 42nd International Conference on Software Engineering Workshops,"Seoul, Republic of Korea",2020,9781450379632.0,,https://doi.org/10.1145/3387940.3392191;http://dx.doi.org/10.1145/3387940.3392191,10.1145/3387940.3392191,"Software refactoring plays an important role in increasing code quality. One of the most popular refactoring types is the Move Method refactoring. It is usually applied when a method depends more on members of other classes than on its own original class. Several approaches have been proposed to recommend Move Method refactoring automatically. Most of them are based on heuristics and have certain limitations (e.g., they depend on the selection of metrics and manually-defined thresholds). In this paper, we propose an approach to recommend Move Method refactoring based on a path-based representation of code called code2vec that is able to capture the syntactic structure and semantic information of a code fragment. We use this code representation to train a machine learning classifier suggesting to move methods to more appropriate classes. We evaluate the approach on two publicly available datasets: a manually compiled dataset of well-known open-source projects and a synthetic dataset with automatically injected code smell instances. The results show that our approach is capable of recommending accurate refactoring opportunities and outperforms JDeodorant and JMove, which are state of the art tools in this field.","Path-based Representation, Feature Envy, Move Method Refactoring, Code Smells, Automatic Refactoring Recommendation",ICSEW'20,,,
Conference Paper,"Feng Q,Cai Y,Kazman R,Cui D,Liu T,Fang H",Active Hotspot: An Issue-Oriented Model to Monitor Software Evolution and Degradation,,2020,,,986–997,IEEE Press,"San Diego, California",Proceedings of the 34th IEEE/ACM International Conference on Automated Software Engineering,,2020,9781728125084.0,,https://doi.org/10.1109/ASE.2019.00095;http://dx.doi.org/10.1109/ASE.2019.00095,10.1109/ASE.2019.00095,"Architecture degradation has a strong negative impact on software quality and can result in significant losses. Severe software degradation does not happen overnight. Software evolves continuously, through numerous issues, fixing bugs and adding new features, and architecture flaws emerge quietly and largely unnoticed until they grow in scope and significance when the system becomes difficult to maintain. Developers are largely unaware of these flaws or the accumulating debt as they are focused on their immediate tasks of address individual issues. As a consequence, the cumulative impacts of their activities, as they affect the architecture, go unnoticed. To detect these problems early and prevent them from accumulating into severe ones we propose to monitor software evolution by tracking the interactions among files revised to address issues. In particular, we propose and show how we can automatically detect active hotspots, to reveal architecture problems. We have studied hundreds of hotspots along the evolution timelines of 21 open source projects and showed that there exist just a few dominating active hotspots per project at any given time. Moreover, these dominating active hotspots persist over long time periods, and thus deserve special attention. Compared with state-of-the-art design and code smell detection tools we report that, using active hotspots, it is possible to detect signs of software degradation both earlier and more precisely.","architecture debt, software evolution",ASE '19,,,
Conference Paper,"Lavallée M,Robillard PN",Causes of Premature Aging during Software Development: An Observational Study,,2011,,,61–70,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 12th International Workshop on Principles of Software Evolution and the 7th Annual ERCIM Workshop on Software Evolution,"Szeged, Hungary",2011,9781450308489.0,,https://doi.org/10.1145/2024445.2024458;http://dx.doi.org/10.1145/2024445.2024458,10.1145/2024445.2024458,"Much work has been done on the subject of what happens to software architecture during maintenance activities. There seems to be a consensus that it degrades during the evolution of the software. More recent work shows that this degradation occurs even during development activities: design decisions are either adjusted or forgotten. Some studies have looked into the causes of this degradation, but these have mostly done so at a very high level. This study examines three projects at code level. Three architectural pre-implementation designs are compared with their post-implementation design counterparts, with special attention paid to the causes of the changes. We found many negative changes causing anti-patterns, at the package, class, and method levels. After analysis of the code, we were able to find the specific reasons for the poor design decisions. Although the underlying causes are varied, they can be grouped into three basic categories: knowledge problems, artifact problems, and management problems. This categorization shows that anti-pattern causes are varied and are not all due to the developers. The main conclusion is that promoting awareness of anti-patterns to developers is insufficient to prevent them since some of the causes escape their grasp.","design erosion, post-implementation design, pre-implementation design, software aging",IWPSE-EVOL '11,,,
Conference Paper,"Rubin J,Henniche AN,Moha N,Bouguessa M,Bousbia N",Sniffing Android Code Smells: An Association Rules Mining-Based Approach,,2019,,,123–127,IEEE Press,"Montreal, Quebec, Canada",Proceedings of the 6th International Conference on Mobile Software Engineering and Systems,,2019,,,,,"Interest in mobile applications (mobile apps) has grown significantly in recent years and has become an important part of the software development market. Indeed, mobile apps become more and more complex and evolve constantly, while their development time decreases. This complexity and time pressure might lead developers to adopt bad design and implementation choices, which are known as code smells. Code smells in mobile apps could lead to performance issues such as overconsumption of hardware resources (CPU, RAM, battery) or even downtime and crashes. Some tools have been proposed for the detection of code smells in Android apps, such as Paprika or a Doctor tools. These tools rely on metrics-based detection rules, which are defined manually according to code smell definitions. However, manually defined rules might be inaccurate and subjective because they are based on user interpretations. In this paper, we present a tool-based approach, called Fakie, which allows the automatic inference of detection rules by analysing code smells data using an association rules algorithm: FP-Growth. We validated Fakie by applying it on a manually analysed validation dataset of 48 opensource mobile apps. We were able to generate detection rules for a dozen code smells, with an average F-measure of 0.95. After all of that, we performed an empirical study by applying Fakie on 2,993 apps downloaded from AndroZoo, a repository of mobile apps.","detection, code smells, Android, association rules, mobile applications",MOBILESoft '19,,,
Conference Paper,"Cardoso B,Figueiredo E",Co-Occurrence of Design Patterns and Bad Smells in Software Systems: An Exploratory Study,,2015,,,347–354,Brazilian Computer Society,"Porto Alegre, BRA",Proceedings of the Annual Conference on Brazilian Symposium on Information Systems: Information Systems: A Computer Socio-Technical Perspective - Volume 1,"Goiania, Goias, Brazil",2015,,,,,"A design pattern is a general reusable solution to a recurring problem in software design. Bad smells are symptoms that may indicate something wrong in the system design or code. Therefore, design patterns and bad smells represent antagonistic structures. They are subject of recurring research and typically appear in software systems. Although design patterns represent good design, their use is often inadequate because their implementation is not always trivial or they may be unnecessarily employed. The inadequate use of design patterns may lead to a bad smell. Therefore, this paper performs an exploratory study in order to identify instances of co-occurrences of design patterns and bad smells. This study is performed over five systems and discovers some co-occurrences between design patterns and bad smells. For instance, we observed the co-occurrences of Command with God Class and Template Method with Duplicated Code. The results of this study make it possible to understand in which situations design patterns are misused or overused and establish guidelines for their better use.","Design Patterns, Bad Smells",SBSI 2015,,,
Conference Paper,"Mannan UA,Ahmed I,Almurshed RA,Dig D,Jensen C",Understanding Code Smells in Android Applications,,2016,,,225–234,Association for Computing Machinery,"New York, NY, USA",Proceedings of the International Conference on Mobile Software Engineering and Systems,"Austin, Texas",2016,9781450341783.0,,https://doi.org/10.1145/2897073.2897094;http://dx.doi.org/10.1145/2897073.2897094,10.1145/2897073.2897094,"Code smells are associated with poor coding practices that cause long-term maintainability problems and mask bugs. Despite mobile being a fast growing software sector, code smells in mobile applications have been understudied. We do not know how code smells in mobile applications compare to those in desktop applications, and how code smells are affecting the design of mobile applications. Without such knowledge, application developers, tool builders, and researchers cannot improve the practice and state of the art of mobile development.We first reviewed the literature on code smells in Android applications and found that there is a significant gap between the most studied code smells in literature and most frequently occurring code smells in real world applications. Inspired by this finding, we conducted a large scale empirical study to compare the type, density, and distribution of code smells in mobile vs. desktop applications. We analyze an open-source corpus of 500 Android applications (total of 6.7M LOC) and 750 desktop Java applications (total of 16M LOC), and compare 14,553 instances of code smells in Android applications to 117,557 instances of code smells in desktop applications. We find that, despite mobile applications having different structure and workflow than desktop applications, the variety and density of code smells is similar. However, the distribution of code smells is different - some code smells occur more frequently in mobile applications. We also found that different categories of Android applications have different code smell distributions. We highlight several implications of our study for application developers, tool builders, and researchers.",,MOBILESoft '16,,,
Conference Paper,"Ferreira KA,Moreira RC,Bigonha MA,Bigonha RS",The Evolving Structures of Software Systems,,2012,,,28–34,IEEE Press,"Zurich, Switzerland",Proceedings of the 3rd International Workshop on Emerging Trends in Software Metrics,,2012,9781467317627.0,,,,"Software maintenance is an important problem because software is an evolving complex system. To make software maintenance viable, it is important to know the real nature of the systems we have to deal with. Little House is a model that provides a macroscopic view of software systems. According to Little House, a software system can be modeled as a graph with five components. This model is intended to be an approach to improve the understanding and the analysis of software structures. However, to achieve this aim, it is necessary to determine its characteristics and its implications. This paper presents the results of an empirical study aiming to characterize software evolution by means of Little House and software metrics. We analyzed several versions of 13 open source software systems, which have been developed over nearly 10 years. The results of the study show that there are two main components of Little House which suffer substantial degradation as the software system evolves. This finding indicates that those components should be carefully taken in consideration when maintenance tasks are performed in the system.","software metrics, software evolution, complex networks",WETSoM '12,,,
Conference Paper,"Dintyala P,Narechania A,Arulraj J",SQLCheck: Automated Detection and Diagnosis of SQL Anti-Patterns,,2020,,,2331–2345,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data,"Portland, OR, USA",2020,9781450367356.0,,https://doi.org/10.1145/3318464.3389754;http://dx.doi.org/10.1145/3318464.3389754,10.1145/3318464.3389754,"The emergence of database-as-a-service platforms has made deploying database applications easier than before. Now, developers can quickly create scalable applications. However, designing performant, maintainable, and accurate applications is challenging. Developers may unknowingly introduce anti-patterns in the application's SQL statements. These anti-patterns are design decisions that are intended to solve a problem but often lead to other problems by violating fundamental design principles. In this paper, we present SQLCheck, a holistic toolchain for automatically finding and fixing anti-patterns in database applications. We introduce techniques for automatically (1) detecting anti-patterns with high precision and recall, (2) ranking the anti-patterns based on their impact on performance, maintainability, and accuracy of applications, and (3) suggesting alternative queries and changes to the database design to fix these anti-patterns. We demonstrate the prevalence of these anti-patterns in a large collection of queries and databases collected from open-source repositories. We introduce an anti-pattern detection algorithm that augments query analysis with data analysis. We present a ranking model for characterizing the impact of frequently occurring anti-patterns. We discuss how SQLCheck suggests fixes for high-impact anti-patterns using rule-based query refactoring techniques. Our experiments demonstrate that SQLCheck enables developers to create more performant, maintainable, and accurate applications.","anti-patterns, database applications",SIGMOD '20,,,
Journal Article,"Pantiuchina J,Zampetti F,Scalabrino S,Piantadosi V,Oliveto R,Bavota G,Penta MD",Why Developers Refactor Source Code: A Mining-Based Study,ACM Trans. Softw. Eng. Methodol.,2020,29.0,4.0,,Association for Computing Machinery,"New York, NY, USA",,,2020-09,,1049-331X,https://doi.org/10.1145/3408302;http://dx.doi.org/10.1145/3408302,10.1145/3408302,"Refactoring aims at improving code non-functional attributes without modifying its external behavior. Previous studies investigated the motivations behind refactoring by surveying developers. With the aim of generalizing and complementing their findings, we present a large-scale study quantitatively and qualitatively investigating why developers perform refactoring in open source projects. First, we mine 287,813 refactoring operations performed in the history of 150 systems. Using this dataset, we investigate the interplay between refactoring operations and process (e.g., previous changes/fixes) and product (e.g., quality metrics) metrics. Then, we manually analyze 551 merged pull requests implementing refactoring operations and classify the motivations behind the implemented refactorings (e.g., removal of code duplication). Our results led to (i) quantitative evidence of the relationship existing between certain process/product metrics and refactoring operations and (ii) a detailed taxonomy, generalizing and complementing the ones existing in the literature, of motivations pushing developers to refactor source code.","Refactoring, empirical software engineering",,,,
Conference Paper,"Fontana FA,Ferme V,Zanoni M",Filtering Code Smells Detection Results,,2015,,,803–804,IEEE Press,"Florence, Italy",Proceedings of the 37th International Conference on Software Engineering - Volume 2,,2015,,,,,"Many tools for code smell detection have been developed, providing often different results. This is due to the informal definition of code smells and to the subjective interpretation of them. Usually, aspects related to the domain, size, and design of the system are not taken into account when detecting and analyzing smells. These aspects can be used to filter out the noise and achieve more relevant results. In this paper, we propose different filters that we have identified for five code smells. We provide two kind of filters, Strong and Weak Filters, that can be integrated as part of a detection approach.",,ICSE '15,,,
Conference Paper,"Lenarduzzi V,Saarimäki N,Taibi D",The Technical Debt Dataset,,2019,,,2–11,Association for Computing Machinery,"New York, NY, USA",Proceedings of the Fifteenth International Conference on Predictive Models and Data Analytics in Software Engineering,"Recife, Brazil",2019,9781450372336.0,,https://doi.org/10.1145/3345629.3345630;http://dx.doi.org/10.1145/3345629.3345630,10.1145/3345629.3345630,"Technical Debt analysis is increasing in popularity as nowadays researchers and industry are adopting various tools for static code analysis to evaluate the quality of their code. Despite this, empirical studies on software projects are expensive because of the time needed to analyze the projects. In addition, the results are difficult to compare as studies commonly consider different projects. In this work, we propose the Technical Debt Dataset, a curated set of project measurement data from 33 Java projects from the Apache Software Foundation. In the Technical Debt Dataset, we analyzed all commits from separately defined time frames with SonarQube to collect Technical Debt information and with Ptidej to detect code smells. Moreover, we extracted all available commit information from the git logs, the refactoring applied with Refactoring Miner, and fault information reported in the issue trackers (Jira). Using this information, we executed the SZZ algorithm to identify the fault-inducing and -fixing commits. We analyzed 78K commits from the selected 33 projects, detecting 1.8M SonarQube issues, 62K code smells, 28K faults and 57K refactorings. The project analysis took more than 200 days. In this paper, we describe the data retrieval pipeline together with the tools used for the analysis. The dataset is made available through CSV files and an SQLite database to facilitate queries on the data. The Technical Debt Dataset aims to open up diverse opportunities for Technical Debt research, enabling researchers to compare results on common projects.","Software Quality, Technical Debt, SZZ, SonarQube, Faults, Mining Software Repository, Dataset",PROMISE'19,,,
Conference Paper,"de Macedo CM,Ruela AS,Delgado KV",Application of Clustering Algorithms for Discovering Bug Patterns in JavaScript Software,,2019,,,,Association for Computing Machinery,"New York, NY, USA",Proceedings of the XV Brazilian Symposium on Information Systems,"Aracaju, Brazil",2019,9781450372374.0,,https://doi.org/10.1145/3330204.3330230;http://dx.doi.org/10.1145/3330204.3330230,10.1145/3330204.3330230,"Applications developed with JavaScript language are increasing every day, not only for client-side, but also for server-side and for mobile devices. In this context, the existence of tools to identify faults is fundamental in order to assist developers during the evolution of their applications. Different tools and approaches have been proposed over the years, however they have limitations to evolve over time, becoming obsolete quickly. The reason for this is the use of a fixed list of pre-defined faults that are searched in the code. The BugAID tool implements a semiautomatic strategy for discovering bug patterns by grouping the changes made during the project development. The objective of this work is to contribute to the BugAID tool, extending this tool with improvements in the extraction of characteristics to be used by the clustering algorithm. The extended module of the BugAID extraction module (BE) that extracts the characteristics is called BE+. Additionally, an evaluation of the clustering algorithms used for discovering fault patterns in JavaScript software is performed. The results show that the DBScan and Optics algorithms with BE+ presented the best results for the Rand, Jaccard and Adjusted Rand indexes, while HDBScan with BE and BE+ presented the worst result.","Bug Discovery, Machine Learning, Software Quality, Data Mining, Pattern Recognition",SBSI'19,,,
Conference Paper,"Lam W,Muşlu K,Sajnani H,Thummalapenta S",A Study on the Lifecycle of Flaky Tests,,2020,,,1471–1482,Association for Computing Machinery,"New York, NY, USA",Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering,"Seoul, South Korea",2020,9781450371216.0,,https://doi.org/10.1145/3377811.3381749;http://dx.doi.org/10.1145/3377811.3381749,10.1145/3377811.3381749,"During regression testing, developers rely on the pass or fail outcomes of tests to check whether changes broke existing functionality. Thus, flaky tests, which nondeterministically pass or fail on the same code, are problematic because they provide misleading signals during regression testing. Although flaky tests are the focus of several existing studies, none of them study (1) the reoccurrence, runtimes, and time-before-fix of flaky tests, and (2) flaky tests in-depth on proprietary projects.This paper fills this knowledge gap about flaky tests and investigates whether prior categorization work on flaky tests also apply to proprietary projects. Specifically, we study the lifecycle of flaky tests in six large-scale proprietary projects at Microsoft. We find, as in prior work, that asynchronous calls are the leading cause of flaky tests in these Microsoft projects. Therefore, we propose the first automated solution, called Flakiness and Time Balancer (FaTB), to reduce the frequency of flaky-test failures caused by asynchronous calls. Our evaluation of five such flaky tests shows that FaTB can reduce the running times of these tests by up to 78% without empirically affecting the frequency of their flaky-test failures. Lastly, our study finds several cases where developers claim they ""fixed"" a flaky test but our empirical experiments show that their changes do not fix or reduce these tests' frequency of flaky-test failures. Future studies should be more cautious when basing their results on changes that developers claim to be ""fixes"".","empirical study, lifecycle, flaky test",ICSE '20,,,
Conference Paper,"Hozano M,Garcia A,Antunes N,Fonseca B,Costa E",Smells Are Sensitive to Developers! On the Efficiency of (Un)Guided Customized Detection,,2017,,,110–120,IEEE Press,"Buenos Aires, Argentina",Proceedings of the 25th International Conference on Program Comprehension,,2017,9781538605356.0,,https://doi.org/10.1109/ICPC.2017.32;http://dx.doi.org/10.1109/ICPC.2017.32,10.1109/ICPC.2017.32,"Code smells indicate poor implementation choices that may hinder program comprehension and maintenance. Their informal definition allows developers to follow different heuristics to detect smells in their projects. Machine learning has been used to customize smell detection according to the developer's perception. However, such customization is not guided (i.e. constrained) to consider alternative heuristics used by developers when detecting smells. As a result, their customization might not be efficient, requiring a considerable effort to reach high effectiveness. In fact, there is no empirical knowledge yet about the efficiency of such unguided approaches for supporting developer-sensitive smell detection. This paper presents Histrategy, a guided customization technique to improve the efficiency on smell detection. Histrategy considers a limited set of detection strategies, produced from different detection heuristics, as input of a customization process. The output of the customization process consists of a detection strategy tailored to each developer. The technique was evaluated in an experimental study with 48 developers and four types of code smells. The results showed that Histrategy is able to outperform six widely adopted machine learning algorithms - used in unguided approaches - both in effectiveness and efficiency. It was also confirmed that most developers benefit from using alternative heuristics to: (i) build their tailored detection strategies, and (ii) achieve efficient smell detection.",,ICPC '17,,,
Journal Article,Doernhoefer M,Surfing the Net for Software Engineering Notes,SIGSOFT Softw. Eng. Notes,2012,37.0,6.0,10–18,Association for Computing Machinery,"New York, NY, USA",,,2012-11,,0163-5948,https://doi.org/10.1145/2382756.2382780;http://dx.doi.org/10.1145/2382756.2382780,10.1145/2382756.2382780,,,,,,
Conference Paper,Hussain S,Threshold Analysis of Design Metrics to Detect Design Flaws: Student Research Abstract,,2016,,,1584–1585,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 31st Annual ACM Symposium on Applied Computing,"Pisa, Italy",2016,9781450337397.0,,https://doi.org/10.1145/2851613.2852013;http://dx.doi.org/10.1145/2851613.2852013,10.1145/2851613.2852013,"Detection of design flaws at different granularity levels of software can help the software engineer to reduce the testing efforts and maintenance cost. In the context of metric-based analysis, current state of art for the quality assurance tools is to extract the metrics from the source code and analyzed the design complexity. But in case of legacy systems, a software engineer needs to pass through the re-engineering process. In this study, I propose a methodology to investigate the threshold effect of software design metrics in order to detect design flaws and its effect over the granularity level of software. Moreover, I will use some statistical methods and machine learning techniques to derive and validate the effect of thresholds over the NASA and open source datasets retrieve from the PROMISE repository.",,SAC '16,,,
Conference Paper,"Silva D,Tsantalis N,Valente MT",Why We Refactor? Confessions of GitHub Contributors,,2016,,,858–870,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering,"Seattle, WA, USA",2016,9781450342186.0,,https://doi.org/10.1145/2950290.2950305;http://dx.doi.org/10.1145/2950290.2950305,10.1145/2950290.2950305,"Refactoring is a widespread practice that helps developers to improve the maintainability and readability of their code. However, there is a limited number of studies empirically investigating the actual motivations behind specific refactoring operations applied by developers. To fill this gap, we monitored Java projects hosted on GitHub to detect recently applied refactorings, and asked the developers to explain the reasons behind their decision to refactor the code. By applying thematic analysis on the collected responses, we compiled a catalogue of 44 distinct motivations for 12 well-known refactoring types. We found that refactoring activity is mainly driven by changes in the requirements and much less by code smells. Extract Method is the most versatile refactoring operation serving 11 different purposes. Finally, we found evidence that the IDE used by the developers affects the adoption of automated refactoring tools.","software evolution, code smells, Refactoring, GitHub",FSE 2016,,,
Conference Paper,"Arcelli D,Cortellessa V,Trubiani C",Antipattern-Based Model Refactoring for Software Performance Improvement,,2012,,,33–42,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 8th International ACM SIGSOFT Conference on Quality of Software Architectures,"Bertinoro, Italy",2012,9781450313469.0,,https://doi.org/10.1145/2304696.2304704;http://dx.doi.org/10.1145/2304696.2304704,10.1145/2304696.2304704,"Identifying and removing the causes of poor performance in software systems are complex problems due to a variety of factors to take into account. Nowadays these problems are usually tackled after the software deployment only with human-based means, which frequently boil down to developer skills and previous experiences. Performance antipatterns can be used to cope with these problems since they capture typical design patterns that are known leading to performance problems, as well as refactoring actions that can be taken to remove them.The goal of this paper is to introduce an approach that allows the refactoring of architectural models, based on antipatterns, that aims at providing performance improvement. To this end, we use a Role-Based Modeling Language to represent: (i) antipattern problems as Source Role Models (SRMs), and (ii) antipattern solutions as Target Role Models (TRMs). Hence, SRM-TRM pairs represent new instruments in the hands of developers to achieve architectural model refactorings aimed at removing sources of performance problems. Model refactoring for antipattern removal can be in fact obtained by replacing an SRM with the corresponding TRM. This approach has been applied to a case study in the e-commerce domain, whose experimental results demonstrate its effectiveness.","model refactoring, roles, software performance, performance antipatterns",QoSA '12,,,
Conference Paper,"Le D,Medvidovic N",Architectural-Based Speculative Analysis to Predict Bugs in a Software System,,2016,,,807–810,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 38th International Conference on Software Engineering Companion,"Austin, Texas",2016,9781450342056.0,,https://doi.org/10.1145/2889160.2889260;http://dx.doi.org/10.1145/2889160.2889260,10.1145/2889160.2889260,"Over time, a software system's code and its underlying design tend to decay steadily and, in turn, to complicate the system's maintenance. In order to address that phenomenon, many researchers tried to help engineers predict parts of a system that are most likely to create problems while or even before they are modifying the system. Problems that creep into a system may manifest themselves as bugs, in which case engineers have no choice but to fix them or develop workarounds. However, these problems may also be more subtle, such as code clones, circular dependencies among system elements, very large APIs, individual elements that implement multiple diffuse concerns, etc. Even though such architectural and code ""smells"" may not crash a system outright, they impose real costs in terms of engineers' time and effort, as well as system correctness and performance. Along the time, implicit problems may be revealed as explicit problems. However, most current techniques predict explicit problems of a system only based on explicit problems themselves. Our research takes a further step by using implicit problems, e.g., architectural- and code-smells, in combination with explicit problems to provide an accurate, systematic and in depth approach to predict potential system problems, particularly bugs.","bug prediction, architectural decay, speculative analysis, architectural-based analysis",ICSE '16,,,
Conference Paper,"Sousa L,Oliveira R,Garcia A,Lee J,Conte T,Oizumi W,de Mello R,Lopes A,Valentim N,Oliveira E,Lucena C",How Do Software Developers Identify Design Problems? A Qualitative Analysis,,2017,,,54–63,Association for Computing Machinery,"New York, NY, USA",Proceedings of the XXXI Brazilian Symposium on Software Engineering,"Fortaleza, CE, Brazil",2017,9781450353267.0,,https://doi.org/10.1145/3131151.3131168;http://dx.doi.org/10.1145/3131151.3131168,10.1145/3131151.3131168,"When a software design decision has a negative impact on one or more quality attributes, we call it a design problem. For example, the Fat Interface problem indicates that an interface exposes non-cohesive services Thus, clients and implementations of this interface may have to handle with services that they are not interested. A design problem such as this hampers the extensibility and maintainability of a software system. As illustrated by the example, a single design problem often affects several elements in the program. Despite its harmfulness, it is difficult to identify a design problem in a system. It is even more challenging to identify design problems when the source code is the only available artifact. In particular, no study has observed what strategy(ies) developers use in practice to identify design problems when the design documentation is unavailable. In order to address this gap, we conducted a qualitative analysis on how developers identify design problems in two different scenarios: when they are either familiar (Scenario 1) or unfamiliar (Scenario 2) with the analyzed systems. Developers familiar with the systems applied a diverse set of strategies during the identification of each design problem. Some strategies were frequently used to locate code elements for analysis, and other strategies were frequently used to confirm design problems in these elements. Developers unfamiliar with the systems relied only on the use of code smells along the task. Despite some differences among the subjects from both scenarios, we noticed that developers often search for multiple indicators during the identification of each design problem.","symptoms, software design, design problem, strategy",SBES '17,,,
Conference Paper,"Pérez J,Crespo Y",Computation of Refactoring Plans from Refactoring Strategies Using HTN Planning,,2012,,,24–31,Association for Computing Machinery,"New York, NY, USA",Proceedings of the Fifth Workshop on Refactoring Tools,"Rapperswil, Switzerland",2012,9781450315005.0,,https://doi.org/10.1145/2328876.2328880;http://dx.doi.org/10.1145/2328876.2328880,10.1145/2328876.2328880,"Complex refactoring processes, such as applying big refactorings or removing design smells are difficult to perform in practice. The complexity of these processes is partly due to their heuristic nature and to the constraints imposed by preconditions on the applicability of the individual refactorings. We introduce refactoring strategies as heuristic-based, automation-suitable specifications of a complex refactoring process. They allow us to specify correction strategies and big refactorings more formally than it is done in current catalogues. Refactoring strategies can be instantiated, for each particular case, into refactoring plans. We define refactoring plans as sequences of refactorings that are immediately applicable over the current system source code. We have developed an approach for instantiating refactoring strategies into refactoring plans that uses Hierarchical Task Network (HTN) planning. This paper describes this approach and presents a case study, in order to evaluate and characterise it.","refactoring plans, refactoring strategies, design smell correction, HTN planning, prototype, case study, big refactorings, refactoring",WRT '12,,,
Conference Paper,"Ferenc R,Tóth Z,Ladányi G,Siket I,Gyimóthy T",A Public Unified Bug Dataset for Java,,2018,,,12–21,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 14th International Conference on Predictive Models and Data Analytics in Software Engineering,"Oulu, Finland",2018,9781450365932.0,,https://doi.org/10.1145/3273934.3273936;http://dx.doi.org/10.1145/3273934.3273936,10.1145/3273934.3273936,"Background: Bug datasets have been created and used by many researchers to build bug prediction models.Aims: In this work we collected existing public bug datasets and unified their contents.Method: We considered 5 public datasets which adhered to all of our criteria. We also downloaded the corresponding source code for each system in the datasets and performed their source code analysis to obtain a common set of source code metrics. This way we produced a unified bug dataset at class and file level that is suitable for further research (e.g. to be used in the building of new bug prediction models). Furthermore, we compared the metric definitions and values of the different bug datasets.Results: We found that (i) the same metric abbreviation can have different definitions or metrics calculated in the same way can have different names, (ii) in some cases different tools give different values even if the metric definitions coincide because (iii) one tool works on source code while the other calculates metrics on bytecode, or (iv) in several cases the downloaded source code contained more files which influenced the afferent metric values significantly.Conclusions: Apart from all these imprecisions, we think that having a common metric set can help in building better bug prediction models and deducing more general conclusions. We made the unified dataset publicly available for everyone. By using a public dataset as an input for different bug prediction related investigations, researchers can make their studies reproducible, thus able to be validated and verified.","static code analysis, code metrics, Bug dataset",PROMISE'18,,,
Conference Paper,"Liu B,Zhang H,Yang L,Dong L,Shen H,Song K",An Experimental Evaluation of Imbalanced Learning and Time-Series Validation in the Context of CI/CD Prediction,,2020,,,21–30,Association for Computing Machinery,"New York, NY, USA",Proceedings of the Evaluation and Assessment in Software Engineering,"Trondheim, Norway",2020,9781450377317.0,,https://doi.org/10.1145/3383219.3383222;http://dx.doi.org/10.1145/3383219.3383222,10.1145/3383219.3383222,"Background: Machine Learning (ML) has been widely used as a powerful tool to support Software Engineering (SE). The fundamental assumptions of data characteristics required for specific ML methods have to be carefully considered prior to their applications in SE. Within the context of Continuous Integration (CI) and Continuous Deployment (CD) practices, there are two vital characteristics of data prone to be violated in SE research. First, the logs generated during CI/CD for training are imbalanced data, which is contrary to the principles of common balanced classifiers; second, these logs are also time-series data, which violates the assumption of cross-validation. Objective: We aim to systematically study the two data characteristics and further provide a comprehensive evaluation for predictive CI/CD with the data from real projects. Method: We conduct an experimental study that evaluates 67 CI/CD predictive models using both cross-validation and time-series-validation. Results: Our evaluation shows that cross-validation makes the evaluation of the models optimistic in most cases, there are a few counter-examples as well. The performance of the top 10 imbalanced models are better than the balanced models in the predictions of failed builds, even for balanced data. The degree of data imbalance has a negative impact on prediction performance. Conclusion: In research and practice, the assumptions of the various ML methods should be seriously considered for the validity of research. Even if it is used to compare the relative performance of models, cross-validation may not be applicable to the problems with time-series features. The research community need to revisit the evaluation results reported in some existing research.","imbalanced learning, time-series-validation, cross-validation, continuous integration, continuous deployment",EASE '20,,,
Conference Paper,"Dugan RF,Glinert EP,Shokoufandeh A",The Sisyphus Database Retrieval Software Performance Antipattern,,2002,,,10–16,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 3rd International Workshop on Software and Performance,"Rome, Italy",2002,9781581135633.0,,https://doi.org/10.1145/584369.584372;http://dx.doi.org/10.1145/584369.584372,10.1145/584369.584372,"In this paper we propose the Sisyphus database retrieval software performance antipattern. The antipattern occurs in application designs that process large, frequently accessed lists stored in a relational database, but display only a small subset to the user. Software Performance Engineering (SPE) techniques are used to analyze the antipattern. Four solutions are evaluated: rownum and index, upper/lower bound, sequence numbering, and caching. We discuss the real world challenges of correcting this antipattern early in the application life cycle.","patterns, antipatterns",WOSP '02,,,
Conference Paper,"Langelier G,Sahraoui H,Poulin P",Visualization-Based Analysis of Quality for Large-Scale Software Systems,,2005,,,214–223,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 20th IEEE/ACM International Conference on Automated Software Engineering,"Long Beach, CA, USA",2005,9781581139938.0,,https://doi.org/10.1145/1101908.1101941;http://dx.doi.org/10.1145/1101908.1101941,10.1145/1101908.1101941,"We propose an approach for complex software analysis based on visualization. Our work is motivated by the fact that in spite of years of research and practice, software development and maintenance are still time and resource consuming, and high-risk activities. The most important reason in our opinion is the complexity of many phenomena related to software, such as its evolution and its reliability. In fact, there is very little theory explaining them. Today, we have a unique opportunity to empirically study these phenomena, thanks to large sets of software data available through open-source programs and open repositories. Automatic analysis techniques, such as statistics and machine learning, are usually limited when studying phenomena with unknown or poorly-understood influence factors. We claim that hybrid techniques that combine automatic analysis with human expertise through visualization are excellent alternatives to them. In this paper, we propose a visualization framework that supports quality analysis of large-scale software systems. We circumvent the problem of size by exploiting perception capabilities of the human visual system.","metrics, quality assessment, software visualization",ASE '05,,,
Conference Paper,"Habchi S,Rouvoy R,Moha N",On the Survival of Android Code Smells in the Wild,,2019,,,87–98,IEEE Press,"Montreal, Quebec, Canada",Proceedings of the 6th International Conference on Mobile Software Engineering and Systems,,2019,,,,,"The success of smartphones and app stores have contributed to the explosion of the number of mobile apps proposed to end-users. In this very competitive market, developers are rushed to regularly release new versions of their apps in order to retain users. Under such pressure, app developers may be tempted to adopt bad design or implementation choices, leading to the introduction of code smells. Mobile-specific code smells represent a real concern in mobile software engineering. Many studies have proposed tools to automatically detect their presence and quantify their impact on performance. However, there remains---so far---no evidence about the lifespan of these code smells in the history of mobile apps. In this paper, we present the first large-scale empirical study that investigates the survival of Android code smells. This study covers 8 types of Android code smells, 324 Android apps, 255k commits, and the history of 180k code smell instances. Our study reports that while in terms of time Android code smells can remain in the codebase for years before being removed, it only takes 34 effective commits to remove 75% of them. Also, Android code smells disappear faster in bigger projects with higher releasing trends. Finally, we observed that code smells that are detected and prioritised by linters tend to disappear before other code smells.","code smells, Android, mobile apps",MOBILESoft '19,,,
Conference Paper,"Gîrba T,Ducasse S,Kuhn A,Marinescu R,Daniel R",Using Concept Analysis to Detect Co-Change Patterns,,2007,,,83–89,Association for Computing Machinery,"New York, NY, USA",Ninth International Workshop on Principles of Software Evolution: In Conjunction with the 6th ESEC/FSE Joint Meeting,"Dubrovnik, Croatia",2007,9781595937223.0,,https://doi.org/10.1145/1294948.1294970;http://dx.doi.org/10.1145/1294948.1294970,10.1145/1294948.1294970,"Software systems need to change over time to cope with new requirements, and due to design decisions, the changes happen to crosscut the system's structure. Understanding how changes appear in the system can reveal hidden dependencies between different entities of the system. We propose the usage of concept analysis to identify groups of entities that change in the same way and in the same time. We apply our approach at different levels of abstraction (i.e., method, class, package) and we detect fine grained changes (i.e., statements were added in a class, but no method was added there). Concept analysis is a technique that identifies entities that have the same properties, but it requires manual inspection due to the large number of candidates it detects. We propose a heuristic that dramatically eliminate the false positives. We apply our approach on two case studies and we show how we can identify hidden dependencies and detect bad smells.","concept analysis, evolution analysis, co-change analysis",IWPSE '07,,,
Conference Paper,"Tosun A,Ahmed M,Turhan B,Juristo N",On the Effectiveness of Unit Tests in Test-Driven Development,,2018,,,113–122,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 2018 International Conference on Software and System Process,"Gothenburg, Sweden",2018,9781450364591.0,,https://doi.org/10.1145/3202710.3203153;http://dx.doi.org/10.1145/3202710.3203153,10.1145/3202710.3203153,"Background: Writing unit tests is one of the primary activities in test-driven development. Yet, the existing reviews report few evidence supporting or refuting the effect of this development approach on test case quality. Lack of ability and skills of developers to produce sufficiently good test cases are also reported as limitations of applying test-driven development in industrial practice. Objective: We investigate the impact of test-driven development on the effectiveness of unit test cases compared to an incremental test last development in an industrial context. Method: We conducted an experiment in an industrial setting with 24 professionals. Professionals followed the two development approaches to implement the tasks. We measure unit test effectiveness in terms of mutation score. We also measure branch and method coverage of test suites to compare our results with the literature. Results: In terms of mutation score, we have found that the test cases written for a test-driven development task have a higher defect detection ability than test cases written for an incremental test-last development task. Subjects wrote test cases that cover more branches on a test-driven development task compared to the other task. However, test cases written for an incremental test-last development task cover more methods than those written for the second task. Conclusion: Our findings are different from previous studies conducted at academic settings. Professionals were able to perform more effective unit testing with test-driven development. Furthermore, we observe that the coverage measure preferred in academic studies reveal different aspects of a development approach. Our results need to be validated in larger industrial contexts.","unit testing, empirical study, mutation score, code coverage, test-driven development",ICSSP '18,,,
Conference Paper,"Habchi S,Moha N,Rouvoy R",The Rise of Android Code Smells: Who is to Blame?,,2019,,,445–456,IEEE Press,"Montreal, Quebec, Canada",Proceedings of the 16th International Conference on Mining Software Repositories,,2019,,,https://doi.org/10.1109/MSR.2019.00071;http://dx.doi.org/10.1109/MSR.2019.00071,10.1109/MSR.2019.00071,"The rise of mobile apps as new software systems led to the emergence of new development requirements regarding performance. Development practices that do not respect these requirements can seriously hinder app performances and impair user experience, they qualify as code smells. Mobile code smells are generally associated with inexperienced developers who lack knowledge about the framework guidelines. However, this assumption remains unverified and there is no evidence about the role played by developers in the accrual of mobile code smells. In this paper, we therefore study the contributions of developers related to Android code smells. To support this study, we propose SNIFFER, an open-source toolkit that mines Git repositories to extract developers contributions as code smell histories. Using SNIFFER, we analysed 255k commits from the change history of 324 Android apps. We found that the ownership of code smells is spread across developers regardless of their seniority. There are no distinct groups of code smell introducers and removers. Developers who introduce and remove code smells are mostly the same.","history mining, Android, mobile apps, code smells",MSR '19,,,
Conference Paper,"Tan SH,Yoshida H,Prasad MR,Roychoudhury A",Anti-Patterns in Search-Based Program Repair,,2016,,,727–738,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering,"Seattle, WA, USA",2016,9781450342186.0,,https://doi.org/10.1145/2950290.2950295;http://dx.doi.org/10.1145/2950290.2950295,10.1145/2950290.2950295,"Search-based program repair automatically searches for a program fix within a given repair space. This may be accomplished by retrofitting a generic search algorithm for program repair as evidenced by the GenProg tool, or by building a customized search algorithm for program repair as in SPR. Unfortunately, automated program repair approaches may produce patches that may be rejected by programmers, because of which past works have suggested using human-written patches to produce templates to guide program repair. In this work, we take the position that we will not provide templates to guide the repair search because that may unduly restrict the repair space and attempt to overfit the repairs into one of the provided templates. Instead, we suggest the use of a set of anti-patterns --- a set of generic forbidden transformations that can be enforced on top of any search-based repair tool. We show that by enforcing our anti-patterns, we obtain repairs that localize the correct lines or functions, involve less deletion of program functionality, and are mostly obtained more efficiently. Since our set of anti-patterns are generic, we have integrated them into existing search based repair tools, including GenProg and SPR, thereby allowing us to obtain higher quality program patches with minimal effort.","fault localization, and repair, Debugging",FSE 2016,,,
Journal Article,"Rathee A,Chhabra JK",Restructuring of Object-Oriented Software Through Cohesion Improvement Using Frequent Usage Patterns,SIGSOFT Softw. Eng. Notes,2017,42.0,3,1–8,Association for Computing Machinery,"New York, NY, USA",,,2017-09,,0163-5948,https://doi.org/10.1145/3127360.3127370;http://dx.doi.org/10.1145/3127360.3127370,10.1145/3127360.3127370,"Due to wide adoption of object-oriented programming in software development, there is always a requirement to produce well-designed software systems, so that the overall software maintenance cost is reduced and reusability of the component is increased. But, due to prolonged maintenance activities, the internal structure of software system deteriorates. In this situation, restructuring is a widely used solution to improve the overall internal structure of the system without changing its external behavior. As, it is known that, one technique to perform restructuring is to use refactoring on the existing source code to alter its internal structure without modifying its external functionality. However, the refactoring solely depends on our ability to identify various code smells present in the system. Refactoring aims at improving cohesion and reducing coupling in the software system. So, in this paper, a restructuring approach based on refactoring is proposed through improvement in cohesion. This paper focuses on improving the cohesion of different classes of object-oriented software using a newly proposed similarity metric based on Frequent Usage Patterns (FUP). The proposed similarity metric measure the relatedness among member functions of the classes. The metric makes use of FUPs used by member functions. The FUP consists of unordered sequences of member variables accessed by member function in performing its task. The usage pattern includes both direct and indirect usages based on sub-function calls within a member function. Based on the values of the similarity metric, we performed hierarchical agglomerative clustering using complete linkage strategy to cluster member functions. Finally, based on the clusters obtained, the source code of the software is refactored using proposed refactoring algorithm. The applicability of our proposed approach is tested using two java projects related to different domains of real life. The result obtained encourages the applicability of proposed approach in the restructuring of a software system.patterns, refactoring, hierarchical clustering, maintainability. usage patterns, refactoring, hierarchical clustering, maintainability.","refactoring, maintainability, hierarchical clustering, frequent usage patterns, Cohesion",,,,
Conference Paper,"Arcelli D,Berardinelli L,Trubiani C",Performance Antipattern Detection through FUML Model Library,,2015,,,23–28,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 2015 Workshop on Challenges in Performance Methods for Software Development,"Austin, Texas, USA",2015,9781450333405.0,,https://doi.org/10.1145/2693561.2693565;http://dx.doi.org/10.1145/2693561.2693565,10.1145/2693561.2693565,"Identifying performance problems is critical in the software design, mostly because the results of performance analysis (i.e., mean values, variances, and probability distributions) are difficult to be interpreted for providing feedback to software designers. Performance antipatterns support the interpretation of performance analysis results and help to fill the gap between numbers and design alternatives.In this paper, we present a model-driven framework that enables an early detection of performance antipatterns, i.e., without generating performance models. Specific design features (e.g., the number of sent messages) are monitored while simulating the specified software model, in order to point out the model elements that most likely contribute for performance flaws. To this end, we propose to use fUML models instrumented with a reusable library that provides data structures (as Classes) and algorithms (as Activities) to detect performance antipatterns while simulating the fUML model itself. A case study is provided to show our framework at work, its current capabilities and future challenges.","performance antipatterns, foundational uml, design feedback",WOSP '15,,,
Conference Paper,"Uludağ Ö,Matthes F",Large-Scale Agile Development Patterns for Enterprise and Solution Architects,,2020,,,,Association for Computing Machinery,"New York, NY, USA",Proceedings of the European Conference on Pattern Languages of Programs 2020,"Virtual Event, Germany",2020,9781450377690.0,,https://doi.org/10.1145/3424771.3424895;http://dx.doi.org/10.1145/3424771.3424895,10.1145/3424771.3424895,"Over the past decades, the emergence of agile development approaches has transformed the way software is developed. Even though systems are getting more and more complex, companies have to develop and release software faster and at the same time increase software quality. Due to the proven success of agile approaches, companies also try to make use of these benefits in large-scale software development projects with over 50 people and over 6 agile teams. However, this represents a risk and is often associated with challenges such as managing silos, complex functional dependencies between systems, and establishing an agile way of working for multiple teams. Especially enterprise and solution architects face a large number of problems in large-scale software development projects. Regardless of their importance for large-scale agile endeavors, there is a lack of research on their typical concerns and best practices. Based on mixed-methods research design, we provide an overview of typical concerns of enterprise and solution architects in large-scale agile development and present one principle: (1) Simplest Working Architecture, three patterns: (2) Lunch Talks, (3) Solution Space, (4) Principle-Based Intentional Architecture, and one anti-pattern: (5) Don't bw a PowerPoint Architect for addressing them.","enterprise architects, patterns, concerns, large-scale agile development, solution architects",EuroPLoP '20,,,
Journal Article,"Fontana FA,Chatzigeorgiou A,Trumler W,Izurieta C,Avgeriou P,Nord RL",Technical Debt in Agile Development: Report on the Ninth Workshop on Managing Technical Debt (MTD 2017),SIGSOFT Softw. Eng. Notes,2017,42.0,3,18–21,Association for Computing Machinery,"New York, NY, USA",,,2017-09,,0163-5948,https://doi.org/10.1145/3127360.3127372;http://dx.doi.org/10.1145/3127360.3127372,10.1145/3127360.3127372,"We report on the Ninth International Workshop on Managing Technical Debt, collocated with the 18th International Conference on Agile Software Development (XP 2017) in Cologne. The technical debt research community continues to expand through collaborations of industry, tool vendors, and academia. The theme of this year's workshop was on technical debt in agile development. Presentations and discussion centered on the topics: technical debt at the code level, architectural technical debt assessment, agile approaches and their impact on technical debt management, and selling the business case of technical debt management.","software quality, software analytics, software economics, software evolution, agile development, Technical debt",,,,
Conference Paper,"Bursztein E,Martin M,Mitchell J",Text-Based CAPTCHA Strengths and Weaknesses,,2011,,,125–138,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 18th ACM Conference on Computer and Communications Security,"Chicago, Illinois, USA",2011,9781450309486.0,,https://doi.org/10.1145/2046707.2046724;http://dx.doi.org/10.1145/2046707.2046724,10.1145/2046707.2046724,"We carry out a systematic study of existing visual CAPTCHAs based on distorted characters that are augmented with anti-segmentation techniques. Applying a systematic evaluation methodology to 15 current CAPTCHA schemes from popular web sites, we find that 13 are vulnerable to automated attacks. Based on this evaluation, we identify a series of recommendations for CAPTCHA designers and attackers, and possible future directions for producing more reliable human/computer distinguishers.","machine learning, vision, CAPTCHA, human interaction proof",CCS '11,,,
Conference Paper,"Muse BA,Rahman MM,Nagy C,Cleve A,Khomh F,Antoniol G","On the Prevalence, Impact, and Evolution of SQL Code Smells in Data-Intensive Systems",,2020,,,327–338,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 17th International Conference on Mining Software Repositories,"Seoul, Republic of Korea",2020,9781450375177.0,,https://doi.org/10.1145/3379597.3387467;http://dx.doi.org/10.1145/3379597.3387467,10.1145/3379597.3387467,"Code smells indicate software design problems that harm software quality. Data-intensive systems that frequently access databases often suffer from SQL code smells besides the traditional smells. While there have been extensive studies on traditional code smells, recently, there has been a growing interest in SQL code smells. In this paper, we conduct an empirical study to investigate the prevalence and evolution of SQL code smells in open-source, data-intensive systems. We collected 150 projects and examined both traditional and SQL code smells in these projects. Our investigation delivers several important findings. First, SQL code smells are indeed prevalent in data-intensive software systems. Second, SQL code smells have a weak co-occurrence with traditional code smells. Third, SQL code smells have a weaker association with bugs than that of traditional code smells. Fourth, SQL code smells are more likely to be introduced at the beginning of the project lifetime and likely to be left in the code without a fix, compared to traditional code smells. Overall, our results show that SQL code smells are indeed prevalent and persistent in the studied data-intensive software systems. Developers should be aware of these smells and consider detecting and refactoring SQL code smells and traditional code smells separately, using dedicated tools.","database access, SQL code smells, Code smells, data-intensive systems",MSR '20,,,
Conference Paper,"Islam MJ,Nguyen G,Pan R,Rajan H",A Comprehensive Study on Deep Learning Bug Characteristics,,2019,,,510–520,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering,"Tallinn, Estonia",2019,9781450355728.0,,https://doi.org/10.1145/3338906.3338955;http://dx.doi.org/10.1145/3338906.3338955,10.1145/3338906.3338955,"Deep learning has gained substantial popularity in recent years. Developers mainly rely on libraries and tools to add deep learning capabilities to their software. What kinds of bugs are frequently found in such software? What are the root causes of such bugs? What impacts do such bugs have? Which stages of deep learning pipeline are more bug prone? Are there any antipatterns? Understanding such characteristics of bugs in deep learning software has the potential to foster the development of better deep learning platforms, debugging mechanisms, development practices, and encourage the development of analysis and verification frameworks. Therefore, we study 2716 high-quality posts from Stack Overflow and 500 bug fix commits from Github about five popular deep learning libraries Caffe, Keras, Tensorflow, Theano, and Torch to understand the types of bugs, root causes of bugs, impacts of bugs, bug-prone stage of deep learning pipeline as well as whether there are some common antipatterns found in this buggy software. The key findings of our study include: data bug and logic bug are the most severe bug types in deep learning software appearing more than 48% of the times, major root causes of these bugs are Incorrect Model Parameter (IPS) and Structural Inefficiency (SI) showing up more than 43% of the times.We have also found that the bugs in the usage of deep learning libraries have some common antipatterns.","Empirical Study of Bugs, Bugs, Deep learning software, Q&A forums, Deep learning bugs",ESEC/FSE 2019,,,
Conference Paper,"Vale G,Albuquerque D,Figueiredo E,Garcia A",Defining Metric Thresholds for Software Product Lines: A Comparative Study,,2015,,,176–185,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 19th International Conference on Software Product Line,"Nashville, Tennessee",2015,9781450336130.0,,https://doi.org/10.1145/2791060.2791078;http://dx.doi.org/10.1145/2791060.2791078,10.1145/2791060.2791078,"A software product line (SPL) is a set of software systems that share a common and variable set of features. Software metrics provide basic means to quantify several modularity aspects of SPLs. However, the effectiveness of the SPL measurement process is directly dependent on the definition of reliable thresholds. If thresholds are not properly defined, it is difficult to actually know whether a given metric value indicates a potential problem in the feature implementation. There are several methods to derive thresholds for software metrics. However, there is little understanding about their appropriateness for the SPL context. This paper aims at comparing three methods to derive thresholds based on a benchmark of 33 SPLs. We assess to what extent these methods derive appropriate values for four metrics used in product-line engineering. These thresholds were used for guiding the identification of a typical anomaly found in features' implementation, named God Class. We also discuss the lessons learned on using such methods to derive thresholds for SPLs.","software product lines, metrics, thresholds",SPLC '15,,,
Conference Paper,"Min H,Li Ping Z",Survey on Software Clone Detection Research,,2019,,,9–16,Association for Computing Machinery,"New York, NY, USA","Proceedings of the 2019 3rd International Conference on Management Engineering, Software Engineering and Service Sciences","Wuhan, China",2019,9781450361897.0,,https://doi.org/10.1145/3312662.3312707;http://dx.doi.org/10.1145/3312662.3312707,10.1145/3312662.3312707,"In order to improve the efficiency of software development, developers often copy-paste code. It is found that the clone code may affect the quality of the software system, especially the maintenance and comprehension of the software, so it is necessary to find and locate it. Many clone detection techniques and tools have been proposed in the search for clone code. How to make better use of these detection techniques and tools will be very important. This paper describes the clone code and general process of clone code detection; introduces different clone code detection methods and related technologies; then conducts a summary analysis, the challenges and development direction faced by clone detection technology.","Software maintenance, Clone Management, Clone Code, Clone Type, Clone Detection",ICMSS 2019,,,
Conference Paper,"Mello R,Uchôa A,Oliveira R,Oliveira D,Oizumi W,Souza J,Fonseca B,Garcia A",Investigating the Social Representations of the Identification of Code Smells by Practitioners and Students from Brazil,,2019,,,457–466,Association for Computing Machinery,"New York, NY, USA",Proceedings of the XXXIII Brazilian Symposium on Software Engineering,"Salvador, Brazil",2019,9781450376518.0,,https://doi.org/10.1145/3350768.3351794;http://dx.doi.org/10.1145/3350768.3351794,10.1145/3350768.3351794,"Context: The identification of code smells is one of the most subjective tasks in software engineering. A key reason is the influence of collective aspects of communities working on this task, such as their beliefs regarding the relevance of certain smells. However, collective aspects are often neglected in the context of smell identification. For this purpose, we can use the social representations theory. Social representations comprise the set of values, behaviors, and practices of communities associated with a social object, such as the task of identifying smells. Aim: To characterize the social representations behind smell identification. Method: We conducted an empirical study on the social representations of smell identification by two communities. One community is composed of postgraduate students from different Brazilian universities. The other community is composed of practitioners located in Brazilian companies, having different levels of experience in code reviews. We analyzed the associations made by the study participants about smell identification, i.e., what immediately comes to their minds when they think about this task. Results: One of the key findings is that the community of students and practitioners have stronger associations with different types of code smells. Students share a strong belief that smell identification is a matter of measurement, while practitioners focus on the structure of the source code and its semantics. Besides, we found that only practitioners frequently associate the task with individual skills. This finding suggests research directions on code smells may be revisited. Conclusion: We found evidence that social representations theory allows identifying research gaps and opportunities by looking beyond the borders of formal knowledge and individual opinions. Therefore, this theory can be considered an important resource for conducting qualitative studies in software engineering.","qualitative research, code smells, Social representations",SBES '19,,,
Conference Paper,"Louis A,Dash SK,Barr ET,Ernst MD,Sutton C",Where Should I Comment My Code? A Dataset and Model for Predicting Locations That Need Comments,,2020,,,21–24,Association for Computing Machinery,"New York, NY, USA",Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering: New Ideas and Emerging Results,"Seoul, South Korea",2020,9781450371261.0,,https://doi.org/10.1145/3377816.3381736;http://dx.doi.org/10.1145/3377816.3381736,10.1145/3377816.3381736,"Programmers should write code comments, but not on every line of code. We have created a machine learning model that suggests locations where a programmer should write a code comment. We trained it on existing commented code to learn locations that are chosen by developers. Once trained, the model can predict locations in new code. Our models achieved precision of 74% and recall of 13% in identifying comment-worthy locations. This first success opens the door to future work, both in the new where-to-comment problem and in guiding comment generation. Our code and data is available at http://groups.inf.ed.ac.uk/cup/comment-locator/.","NLP, comments, natural language processing",ICSE-NIER '20,,,
Conference Paper,"Araújo CW,Zapalowski V,Nunes I",Using Code Quality Features to Predict Bugs in Procedural Software Systems,,2018,,,122–131,Association for Computing Machinery,"New York, NY, USA",Proceedings of the XXXII Brazilian Symposium on Software Engineering,"Sao Carlos, Brazil",2018,9781450365031.0,,https://doi.org/10.1145/3266237.3266271;http://dx.doi.org/10.1145/3266237.3266271,10.1145/3266237.3266271,"A wide range of metrics have been used as features to build bug (or fault) predictors. However, most of the existing predictors focus mostly on object-oriented (OO) systems, either because they rely on OO metrics or were evaluated mainly with OO systems. Procedural software systems (PSS), less addressed in bug prediction research, often suffer from maintainability problems because they typically consist of low-level applications, using for example preprocessors to cope with variability. Previous work evaluated sets of features (composed of static code metrics) proposed in existing approaches in the PSS context. However, explored metrics are limited to those that are part of traditional metric suites, being often associated with structural code properties. A type of information explored to a smaller extent in this context is the output of code quality tools that statically analyse source code, providing hints of code problems. In this paper, we investigate the use of information collected from quality tools to build bug predictors dedicated to PSS. We specify four features derived from code quality tools or associated with poor programming practices and evaluate the effectiveness of these features. Our evaluation shows that our proposed features improve bug predictors in our investigated context.","code metrics, procedural languanges, bug prediction",SBES '18,,,
Conference Paper,"Azadi U,Fontana FA,Taibi D",Architectural Smells Detected by Tools: A Catalogue Proposal,,2019,,,88–97,IEEE Press,"Montreal, Quebec, Canada",Proceedings of the Second International Conference on Technical Debt,,2019,,,https://doi.org/10.1109/TechDebt.2019.00027;http://dx.doi.org/10.1109/TechDebt.2019.00027,10.1109/TechDebt.2019.00027,"Architectural smells can negatively impact on different software qualities and can represent a relevant source of architectural debt. Several architectural smells have been defined by different researchers. Moreover, both academia and industry proposed several tools for software quality analysis, but it is not always clear to understand which tools provide also support for architectural smells detection and if the tools developed for this specific purpose are effectively available or not. In this paper we propose a catalogue of architectural smells for which, at least one tool able to detect the smell exists. We outline the main differences in the detection techniques exploited by the tools and we propose a classification of these architectural smells according to the violation of three design principles.","architectural debt, architectural smells detection, architectural smells catalogue, architectural smells",TechDebt '19,,,
Journal Article,"Moha N,Guéhéneuc YG,Meur AF,Duchien L,Tiberghien A",From a Domain Analysis to the Specification and Detection of Code and Design Smells,Form. Asp. Comput.,2010,22.0,3,345–361,Springer-Verlag,"Berlin, Heidelberg",,,2010-05,,0934-5043,https://doi.org/10.1007/s00165-009-0115-x;http://dx.doi.org/10.1007/s00165-009-0115-x,10.1007/s00165-009-0115-x,"Code and design smells are recurring design problems in software systems that must be identified to avoid their possible negative consequences on development and maintenance. Consequently, several smell detection approaches and tools have been proposed in the literature. However, so far, they allow the detection of predefined smells but the detection of new smells or smells adapted to the context of the analysed systems is possible only by implementing new detection algorithms manually. Moreover, previous approaches do not explain the transition from specifications of smells to their detection. Finally, the validation of the existing approaches and tools has been limited on few proprietary systems and on a reduced number of smells. In this paper, we introduce an approach to automate the generation of detection algorithms from specifications written using a domain-specific language. This language is defined from a thorough domain analysis. It allows the specification of smells using high-level domain-related abstractions. It allows the adaptation of the specifications of smells to the context of the analysed systems. We specify 10 smells, generate automatically their detection algorithms using templates, and validate the algorithms in terms of precision and recall on Xerces v2.7.0 and GanttProject v1.10.2, two open-source object-oriented systems. We also compare the detection results with those of a previous approach, iPlasma.","Java, Detection, Algorithm generation, Antipatterns, Design smells, Domain-specific language, Code smells",,,,
Journal Article,"Avgeriou P,Ernst NA,Nord RL,Kruchten P",Technical Debt: Broadening Perspectives Report on the Seventh Workshop on Managing Technical Debt (MTD 2015),SIGSOFT Softw. Eng. Notes,2016,41.0,2,38–41,Association for Computing Machinery,"New York, NY, USA",,,2016-05,,0163-5948,https://doi.org/10.1145/2894784.2894800;http://dx.doi.org/10.1145/2894784.2894800,10.1145/2894784.2894800,"Increasingly software engineers use the metaphor of technical debt to communicate issues related to the growing cost of change. In this article, we report on the Seventh Workshop on Managing Technical Debt (MTD 2015), held in Bremen, Germany, on October 2, 2015, collocated with the International Conference on Software Maintenance and Evolution (ICSME). The 30 workshop participants from industry and academia engaged in lively discussions, which helped clarify issues, refine questions, and promote common understanding about technical debt in software.",,,,,
Conference Paper,"Soltanifar B,Akbarinasaji S,Caglayan B,Bener AB,Filiz A,Kramer BM",Software Analytics in Practice: A Defect Prediction Model Using Code Smells,,2016,,,148–155,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 20th International Database Engineering & Applications Symposium,"Montreal, QC, Canada",2016,9781450341189.0,,https://doi.org/10.1145/2938503.2938553;http://dx.doi.org/10.1145/2938503.2938553,10.1145/2938503.2938553,"In software engineering, maintainability is related to investigating the defects and their causes, correcting the defects and modifying the system to meet customer requirements. Maintenance is a time consuming activity within the software life cycle. Therefore, there is a need for efficiently organizing the software resources in terms of time, cost and personnel for maintenance activity. One way of efficiently managing maintenance resources is to predict defects that may occur after the deployment. Many researchers so far have built defect prediction models using different sets of metrics such as churn and static code metrics. However, hidden causes of defects such as code smells have not been investigated thoroughly. In this study we propose using data science and analytics techniques on software data to build defect prediction models. In order to build the prediction model we used code smells metrics, churn metrics and combination of churn and code smells metrics. The results of our experiments on two different software companies show that code smells is a good indicator of defect proneness of the software product. Therefore, we recommend that code smells metrics should be used to train a defect prediction model to guide the software maintenance team.","Defect Prediction Model, Mining software repositories, Code Smells",IDEAS '16,,,
Conference Paper,"Shepherd D,Pollock L,Vijay-Shanker K",Case Study: Supplementing Program Analysis with Natural Language Analysis to Improve a Reverse Engineering Task,,2007,,,49–54,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 7th ACM SIGPLAN-SIGSOFT Workshop on Program Analysis for Software Tools and Engineering,"San Diego, California, USA",2007,9781595935953.0,,https://doi.org/10.1145/1251535.1251544;http://dx.doi.org/10.1145/1251535.1251544,10.1145/1251535.1251544,"Software maintainers often use reverse engineering tools to aid in the extremely difficult task of understanding unfamiliar code, especially within large, complex software systems. While traditional program analysis can provide detailed information for reverse engineering, often this information is not sufficient to assist the user with high-level program understanding tasks. To bridge the gap between current reverse engineering tools and the high-level questions that software maintainers want answered, we propose supplementing traditional program analysis with natural language analysis of program source code. This paper presents a case study where we have augmented an existing reverse engineering tool, an aspect miner, to complement the existing traditional program analysis-based miner with natural language analysis of method names, class names, and comments. Our quantitative and qualitative results strongly suggest that supplementing traditional program analysis with natural language analysis is a promising approach to raising the level of effectiveness of reverse engineering tools.","program analysis, natural language, aspect mining",PASTE '07,,,
Conference Paper,"Palomba F,Tamburri DA,Serebrenik A,Zaidman A,Fontana FA,Oliveto R",How Do Community Smells Influence Code Smells?,,2018,,,240–241,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 40th International Conference on Software Engineering: Companion Proceeedings,"Gothenburg, Sweden",2018,9781450356633.0,,https://doi.org/10.1145/3183440.3194950;http://dx.doi.org/10.1145/3183440.3194950,10.1145/3183440.3194950,"Code smells reflect sub-optimal patterns of code that often lead to critical software flaws or failure. In the same way, community smells reflect sub-optimal organisational and socio-technical patterns in the organisational structure of the software community.To understand the relation between the community smells and code smells we start by surveying 162 developers of nine open-source systems. Then we look deeper into this connection by conducting an empirical study of 117 releases from these systems.Our results indicate that community-related factors are intuitively perceived by most developers as causes of the persistence of code smells. Inspired by this observation we design a community-aware prediction model for code smells and show that it outperforms a model that does not consider community factors.","code smells, community smells, organisational structure",ICSE '18,,,
Conference Paper,"Khandelwal S,Sripada SK,Reddy YR",Impact of Gamification on Code Review Process: An Experimental Study,,2017,,,122–126,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 10th Innovations in Software Engineering Conference,"Jaipur, India",2017,9781450348560.0,,https://doi.org/10.1145/3021460.3021474;http://dx.doi.org/10.1145/3021460.3021474,10.1145/3021460.3021474,"Researchers have supported the idea of gamification to enhance students' interest in activities like code reviews, change management, knowledge management, issue tracking, etc. which might otherwise be repetitive and monotonous. We performed an experimental study consisting of nearly 180+ participants to measure the impact of gamification on code review process using 5 different code review tools, including one gamified code review instance from our extensible architectural framework. We assess the impact of gamification based on the code smells and bugs identified in a gamified and non-gamified environment as per code inspection report. Further, measurement and comparison of the quantity and usefulness of code review comments was done using machine learning techniques.","Architectural Framework, Text Analysis, Gamification, Evaluation, Classification, Code Reviews",ISEC '17,,,
Conference Paper,"Oizumi W,Bibiano AC,Cedrim D,Oliveira A,Sousa L,Garcia A,Oliveira D",Recommending Composite Refactorings for Smell Removal: Heuristics and Evaluation,,2020,,,72–81,Association for Computing Machinery,"New York, NY, USA",Proceedings of the XXXIV Brazilian Symposium on Software Engineering,"Natal, Brazil",2020,9781450387538.0,,https://doi.org/10.1145/3422392.3422423;http://dx.doi.org/10.1145/3422392.3422423,10.1145/3422392.3422423,"Structural degradation is the process in which quality attributes of a system are negatively impacted. When due attention is not paid to structural degradation, the source code may also become difficult to change. Code smells are recurring structures in the source code that may represent structural degradation. Hence, there are many catalogs and techniques for supporting the removal of code smells through refactoring recommendations, which usually consist of single refactorings such as a Move Method or an Extract Method. However, single refactorings are often not enough for completely removing certain smell occurrences. Moreover, recent studies show that developers most often apply composite refactorings - i.e., sequences of two or more refactorings - for removing code smells. Despite showing the importance of performing composite refactorings, most studies do not provide information on which composite refactoring patterns are recurrent in practice. In this context, a previous study identified 35 smell removal patterns that are frequent across multiple open source systems. However, such study has not explored how the removal patterns could help developers to apply effective composite refactorings. Thus, in this work, we propose a suite of new recommendation heuristics to help developers in applying effective composite refactorings. These heuristics are intended to remove three code smell types, namely Complex Class, Feature Envy, and God Class. After designing the heuristics, we evaluated their effectiveness through a quasi-experiment. This evaluation was conducted with 12 software developers and 9 smelly Java classes. Results indicate that developers considered our heuristics effective or partially effective in more than 93% of the cases. In addition, the evaluation helped us to identify multiple factors that contribute to the acceptance or rejection of the refactoring recommendations. Based on these factors, we defined new guidelines for the effective recommendation of smell-removal composite refactorings.","code smells, refactoring recommendations, refactoring heuristics, refactoring, composite refactoring patterns, composite refactoring",SBES '20,,,
Conference Paper,"Tsantalis N,Mansouri M,Eshkevari LM,Mazinanian D,Dig D",Accurate and Efficient Refactoring Detection in Commit History,,2018,,,483–494,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 40th International Conference on Software Engineering,"Gothenburg, Sweden",2018,9781450356381.0,,https://doi.org/10.1145/3180155.3180206;http://dx.doi.org/10.1145/3180155.3180206,10.1145/3180155.3180206,"Refactoring detection algorithms have been crucial to a variety of applications: (i) empirical studies about the evolution of code, tests, and faults, (ii) tools for library API migration, (iii) improving the comprehension of changes and code reviews, etc. However, recent research has questioned the accuracy of the state-of-the-art refactoring detection tools, which poses threats to the reliability of their application. Moreover, previous refactoring detection tools are very sensitive to user-provided similarity thresholds, which further reduces their practical accuracy. In addition, their requirement to build the project versions/revisions under analysis makes them inapplicable in many real-world scenarios.To reinvigorate a previously fruitful line of research that has stifled, we designed, implemented, and evaluated RMiner, a technique that overcomes the above limitations. At the heart of RMiner is an AST-based statement matching algorithm that determines refactoring candidates without requiring user-defined thresholds. To empirically evaluate RMiner, we created the most comprehensive oracle to date that uses triangulation to create a dataset with considerably reduced bias, representing 3,188 refactorings from 185 open-source projects. Using this oracle, we found that RMiner has a precision of 98% and recall of 87%, which is a significant improvement over the previous state-of-the-art.","Oracle, abstract syntax tree, accuracy, commit, refactoring, Git",ICSE '18,,,
Conference Paper,"Hecht G,Benomar O,Rouvoy R,Moha N,Duchien L",Tracking the Software Quality of Android Applications along Their Evolution,,2015,,,236–247,IEEE Press,"Lincoln, Nebraska",Proceedings of the 30th IEEE/ACM International Conference on Automated Software Engineering,,2015,9781509000241.0,,https://doi.org/10.1109/ASE.2015.46;http://dx.doi.org/10.1109/ASE.2015.46,10.1109/ASE.2015.46,"Mobile apps are becoming complex software systems that must be developed quickly and evolve continuously to fit new user requirements and execution contexts. However, addressing these requirements may result in poor design choices, also known as antipatterns, which may incidentally degrade software quality and performance. Thus, the automatic detection and tracking of antipatterns in this apps are important activities in order to ease both maintenance and evolution. Moreover, they guide developers to refactor their applications and thus, to improve their quality. While antipatterns are well-known in object-oriented applications, their study in mobile applications is still in its infancy. In this paper, we analyze the evolution of mobile apps quality on 3,568 versions of 106 popular Android applications downloaded from the Google Play Store. For this purpose, we use a tooled approach, called Paprika, to identify 3 object-oriented and 4 Android-specific antipatterns from binaries of mobile apps, and to analyze their quality along evolutions.","Android, antipattern, mobile app, software quality",ASE '15,,,
Conference Paper,"Kiefer C,Bernstein A,Tappolet J",Mining Software Repositories with ISPAROL and a Software Evolution Ontology,,2007,,,10,IEEE Computer Society,USA,Proceedings of the Fourth International Workshop on Mining Software Repositories,,2007,9780769529509.0,,https://doi.org/10.1109/MSR.2007.21;http://dx.doi.org/10.1109/MSR.2007.21,10.1109/MSR.2007.21,"One of the most important decisions researchers face when analyzing the evolution of software systems is the choice of a proper data analysis/exchange format. Most existing formats have to be processed with special programs written specifically for that purpose and are not easily extendible. Most scientists, therefore, use their own database( s) requiring each of them to repeat the work of writing the import/export programs to their format. We present EvoOnt, a software repository data exchange format based on the Web Ontology Language (OWL). EvoOnt includes software, release, and bug-related information. Since OWL describes the semantics of the data, EvoOnt is (1) easily extendible, (2) comes with many existing tools, and (3) allows to derive assertions through its inherent Description Logic reasoning capabilities. The paper also shows iSPARQL -- our SPARQL-based Semantic Web query engine containing similarity joins. Together with EvoOnt, iSPARQL can accomplish a sizable number of tasks sought in software repository mining projects, such as an assessment of the amount of change between versions or the detection of bad code smells. To illustrate the usefulness of EvoOnt (and iSPARQL), we perform a series of experiments with a real-world Java project. These show that a number of software analyses can be reduced to simple iSPARQL queries on an EvoOnt dataset.",,MSR '07,,,
Conference Paper,Cedrim D,Context-Sensitive Identification of Refactoring Opportunities,,2016,,,827–830,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 38th International Conference on Software Engineering Companion,"Austin, Texas",2016,9781450342056.0,,https://doi.org/10.1145/2889160.2889266;http://dx.doi.org/10.1145/2889160.2889266,10.1145/2889160.2889266,"Refactoring is a popular procedure for improving the internal structure of software systems. Refactoring is widely practiced by developers, and considerable development effort has been invested in refactoring tooling support. The identification of refactoring opportunities usually is based on subjective perceptions. At best, identification of refactoring opportunities is based on observation of code smells, which is also subjective and error-prone. As a consequence, developers struggle to identify when large systems should be refactored and, once the identification is complete, they still need to choose the types of refactoring to be performed. To overcome these problems, this PhD research aims at: (i) improving the state-of-the-art by introducing a novel model for identifying refactoring opportunities; and (ii) indicating which types of refactorings should be performed in order to improve the code internal structure.","code smells, refactoring, refactoring opportunities",ICSE '16,,,
Conference Paper,"Palomba F,Zaidman A,Oliveto R,De Lucia A",An Exploratory Study on the Relationship between Changes and Refactoring,,2017,,,176–185,IEEE Press,"Buenos Aires, Argentina",Proceedings of the 25th International Conference on Program Comprehension,,2017,9781538605356.0,,https://doi.org/10.1109/ICPC.2017.38;http://dx.doi.org/10.1109/ICPC.2017.38,10.1109/ICPC.2017.38,"Refactoring aims at improving the internal structure of a software system without changing its external behavior. Previous studies empirically assessed, on the one hand, the benefits of refactoring in terms of code quality and developers' productivity, and on the other hand, the underlying reasons that push programmers to apply refactoring. Results achieved in the latter investigations indicate that besides personal motivation such as the responsibility concerned with code authorship, refactoring is mainly performed as a consequence of changes in the requirements rather than driven by software quality. However, these findings have been derived by surveying developers, and therefore no software repository study has been carried out to corroborate the achieved findings. To bridge this gap, we provide a quantitative investigation on the relationship between different types of code changes (i.e., Fault Repairing Modification, Feature Introduction Modification, and General Maintenance Modification) and 28 different refactoring types coming from 3 open source projects. Results showed that developers tend to apply a higher number of refactoring operations aimed at improving maintainability and comprehensibility of the source code when fixing bugs. Instead, when new features are implemented, more complex refactoring operations are performed to improve code cohesion. Most of the times, the underlying reasons behind the application of such refactoring operations are represented by the presence of duplicate code or previously introduced self-admitted technical debts.","empirical studies, code changes, refactoring",ICPC '17,,,
Conference Paper,"Rocha J,Melo H,Coelho R,Sena B",Towards a Catalogue of Java Exception Handling Bad Smells and Refactorings,,2020,,,,The Hillside Group,USA,Proceedings of the 25th Conference on Pattern Languages of Programs,"Portland, Oregon",2020,,,,,"Software is made by humans for human use and, for that reason, it is bound to fail. As language designers accepted failure as an inevitable factor, mechanisms had to be created to deal with it. Java was designed with an elaborate built-in exception handling mechanism which allowed programmers to anticipate failures and prepare the application to deal with them from a high level point of view. However, the exception handling code designed to make a system more robust often works the other way around and become a burden programmers have to cope with.Some guidelines on how to better cope with the exception handling code have been proposed, papers have been written on this topic and tools have been built, nevertheless, such pieces of information are spread and structured in different ways. This paper aims to collect such guidelines on good and bad practices from different sources and compile it as a catalogue of bad smells and associated refactorings as a way to help new and experienced developers improve the exception handling code of Java programs.","patterns, Java, exception handling",PLoP '18,,,
Conference Paper,Rama GM,A Desiderata for Refactoring-Based Software Modularity Improvement,,2010,,,93–102,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 3rd India Software Engineering Conference,"Mysore, India",2010,9781605589220.0,,https://doi.org/10.1145/1730874.1730893;http://dx.doi.org/10.1145/1730874.1730893,10.1145/1730874.1730893,"There exists number of large business critical software systems of recent vintage that are becoming increasingly difficult to maintain. These systems, written in newer languages such as C and Java are fast becoming legacy and showing the same symptoms of modularity deterioration reminiscent of older legacy systems written in Cobol and PL1. However, this problem has not received much attention from the software modularization community. In this paper, we argue that the modularization needs of these relatively newer systems, which generally have some modular structure, is significantly different from the needs of older, mainly monolithic, legacy systems. We emphasize the need for incrementally improving modularity and propose a software refactoring based approach to solve this problem, thereby, uniting hitherto two disparate threads of research - software modularization and software refactoring. As part of this refactoring based modularity improvement approach, we identify a set of modularity smells and specify how to detect these smells in poorly modularized software systems. A validation of these modularity smells is carried out using several open source systems.","software modularity, software refactoring, software maintenance, preventive maintenance, code smells",ISEC '10,,,
Conference Paper,"Gama E,Freire S,Mendonça M,Spínola RO,Paixao M,Cortés MI",Using Stack Overflow to Assess Technical Debt Identification on Software Projects,,2020,,,730–739,Association for Computing Machinery,"New York, NY, USA",Proceedings of the XXXIV Brazilian Symposium on Software Engineering,"Natal, Brazil",2020,9781450387538.0,,https://doi.org/10.1145/3422392.3422429;http://dx.doi.org/10.1145/3422392.3422429,10.1145/3422392.3422429,"Context. The accumulation of technical debt (TD) items can lead to risks in software projects, such a gradual decrease in product quality, difficulties in their maintenance, and ultimately the cancellation of the project. To mitigate these risks, developers need means to identify TD items, which enable better documentation and improvements in TD management. Recent literature has proposed different indicator-based strategies for TD identification. However, there is limited empirical evidence to support that developers use these indicators to identify TD in practice. In this context, data from Q&A websites, such as Stack Overflow (SO), have been extensively leveraged in recent studies to investigate software engineering practices from a developers' point of view. Goal. This paper seeks to investigate, from the point of view of practitioners, how developers commonly identify TD items in their projects. Method. We mined, curated, and selected a total of 140 TD-related discussions on SO, from which we performed both quantitative and qualitative analyses. Results. We found that SO's practitioners commonly discuss TD identification, revealing 29 different low-level indicators for recognizing TD items on code, infrastructure, architecture, and tests. We grouped low-level indicators based on their themes, producing an aggregated set of 13 distinct high-level indicators. We then classified all low- and high-level indicators into three different categories according to which type of debt each of them is meant to identify. Conclusions. We organize the empirical evidence on the low- and high-level indicators and their relationship to types of TD in a conceptual framework, which may assist developers and serve as guidance for future research, shedding new light on TD identification state-of-practice.","Indicators, Stack Overflow, Technical Debt, Mining Software Repositories",SBES '20,,,
Journal Article,"Ouni A,Kessentini M,Sahraoui H,Inoue K,Deb K",Multi-Criteria Code Refactoring Using Search-Based Software Engineering: An Industrial Case Study,ACM Trans. Softw. Eng. Methodol.,2016,25.0,3,,Association for Computing Machinery,"New York, NY, USA",,,2016-06,,1049-331X,https://doi.org/10.1145/2932631;http://dx.doi.org/10.1145/2932631,10.1145/2932631,"One of the most widely used techniques to improve the quality of existing software systems is refactoring—the process of improving the design of existing code by changing its internal structure without altering its external behavior. While it is important to suggest refactorings that improve the quality and structure of the system, many other criteria are also important to consider, such as reducing the number of code changes, preserving the semantics of the software design and not only its behavior, and maintaining consistency with the previously applied refactorings. In this article, we propose a multi-objective search-based approach for automating the recommendation of refactorings. The process aims at finding the optimal sequence of refactorings that (i) improves the quality by minimizing the number of design defects, (ii) minimizes code changes required to fix those defects, (iii) preserves design semantics, and (iv) maximizes the consistency with the previously code changes. We evaluated the efficiency of our approach using a benchmark of six open-source systems, 11 different types of refactorings (move method, move field, pull up method, pull up field, push down method, push down field, inline class, move class, extract class, extract method, and extract interface) and six commonly occurring design defect types (blob, spaghetti code, functional decomposition, data class, shotgun surgery, and feature envy) through an empirical study conducted with experts. In addition, we performed an industrial validation of our technique, with 10 software engineers, on a large project provided by our industrial partner. We found that the proposed refactorings succeed in preserving the design coherence of the code, with an acceptable level of code change score while reusing knowledge from recorded refactorings applied in the past to similar contexts.","Search-based software engineering, software maintenance, multi-objective optimization, refactoring, software evolution",,,,
Conference Paper,"Cedrim D,Garcia A,Mongiovi M,Gheyi R,Sousa L,de Mello R,Fonseca B,Ribeiro M,Chávez A",Understanding the Impact of Refactoring on Smells: A Longitudinal Study of 23 Software Projects,,2017,,,465–475,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 2017 11th Joint Meeting on Foundations of Software Engineering,"Paderborn, Germany",2017,9781450351058.0,,https://doi.org/10.1145/3106237.3106259;http://dx.doi.org/10.1145/3106237.3106259,10.1145/3106237.3106259,"Code smells in a program represent indications of structural quality problems, which can be addressed by software refactoring. However, refactoring intends to achieve different goals in practice, and its application may not reduce smelly structures. Developers may neglect or end up creating new code smells through refactoring. Unfortunately, little has been reported about the beneficial and harmful effects of refactoring on code smells. This paper reports a longitudinal study intended to address this gap. We analyze how often commonly-used refactoring types affect the density of 13 types of code smells along the version histories of 23 projects. Our findings are based on the analysis of 16,566 refactorings distributed in 10 different types. Even though 79.4% of the refactorings touched smelly elements, 57% did not reduce their occurrences. Surprisingly, only 9.7% of refactorings removed smells, while 33.3% induced the introduction of new ones. More than 95% of such refactoring-induced smells were not removed in successive commits, which suggest refactorings tend to more frequently introduce long-living smells instead of eliminating existing ones. We also characterized and quantified typical refactoring-smell patterns, and observed that harmful patterns are frequent, including: (i) approximately 30% of the Move Method and Pull Up Method refactorings induced the emergence of God Class, and (ii) the Extract Superclass refactoring creates the smell Speculative Generality in 68% of the cases.","Structural Quality, Refactoring, Code Smells",ESEC/FSE 2017,,,
Conference Paper,"Saarimäki N,Lenarduzzi V,Taibi D",On the Diffuseness of Code Technical Debt in Java Projects of the Apache Ecosystem,,2019,,,98–107,IEEE Press,"Montreal, Quebec, Canada",Proceedings of the Second International Conference on Technical Debt,,2019,,,https://doi.org/10.1109/TechDebt.2019.00028;http://dx.doi.org/10.1109/TechDebt.2019.00028,10.1109/TechDebt.2019.00028,"Background. Companies commonly invest major effort into removing, respectively not introducing, technical debt issues detected by static analysis tools such as SonarQube, Cast, or Coverity. These tools classify technical debt issues into categories according to severity, and developers commonly pay attention to not introducing issues with a high level of severity that could generate bugs or make software maintenance more difficult.Objective. In this work, we aim to understand the diffuseness of Technical Debt (TD) issues and the speed with which developers remove them from the code if they introduced such an issue. The goal is to understand which type of TD is more diffused and how much attention is paid by the developers, as well as to investigate whether TD issues with a higher level of severity are resolved faster than those with a lower level of severity. We conducted a case study across 78K commits of 33 Java projects from the Apache Software Foundation Ecosystem to investigate the distribution of 1.4M TD items.Results. TD items introduced into the code are mostly related to code smells (issues that can increase the maintenance effort). Moreover, developers commonly remove the most severe issues faster than less severe ones. However, the time needed to resolve issues increases when the level of severity increases (minor issues are removed faster that blocker ones).Conclusion. One possible answer to the unexpected issue of resolution time might be that severity is not correctly defined by the tools. Another possible answer is that the rules at an intermediate severity level could be the ones that technically require more time to be removed. The classification of TD items, including their severity and type, require thorough investigation from a research point of view.","sonarqube, technical debt issues, violations",TechDebt '19,,,
Conference Paper,"Fu X,Xu Y",Recursive Autoencoder with HowNet Lexicon for Sentence-Level Sentiment Analysis,,2015,,,,Association for Computing Machinery,"New York, NY, USA",Proceedings of the ASE BigData & SocialInformatics 2015,"Kaohsiung, Taiwan",2015,9781450337359.0,,https://doi.org/10.1145/2818869.2818908;http://dx.doi.org/10.1145/2818869.2818908,10.1145/2818869.2818908,"Semantic word representations have been very useful but usually ignore the syntactic relationship. In the task of sentiment analysis, compositional vector representations require more structure information from natural language text and richer supervised training for more accuracy predictions. However, labeled data are generally expensive to acquire in reality. To remedy this, we propose a new method that train our model based on fully labeled parse tree using supervised learning without manual annotation. Our method not only significantly reduces the burden of manual labeling, but also allows the compositionality to capture syntactic and semantic information jointly. We show the effectiveness of this model on the task of sentence-level sentiment classification and conduct preliminary experiments to investigate its performance. Lastly, it can accurately predict the sentiment distribution and outperforms other approaches.","Data Mining, Deep Learning, HowNet Lexicon, Word Embedding, Parse Tree, Sentiment Label, Sentiment Analysis",ASE BD&SI '15,,,
Conference Paper,"Kessentini M,Ouni A",Detecting Android Smells Using Multi-Objective Genetic Programming,,2017,,,122–132,IEEE Press,"Buenos Aires, Argentina",Proceedings of the 4th International Conference on Mobile Software Engineering and Systems,,2017,9781538626696.0,,https://doi.org/10.1109/MOBILESoft.2017.29;http://dx.doi.org/10.1109/MOBILESoft.2017.29,10.1109/MOBILESoft.2017.29,"The evolution rate of mobile applications is much higher than regular software applications having shorter release deadlines and smaller code base. Mobile applications tend to be evolved quickly by developers to meet several new customer requirements and fix discovered bugs. However, evolving the existing features and design may introduce bad design practices, also called code smells, which can highly decrease the maintainability and performance of these mobile applications. However, unlike the area of object-oriented software systems, the detection of code smells in mobile applications received a very little of attention. Recent, few studies defined a set of quality metrics for Android applications and proposed a support to manually write a set of rules to detect code smells by combining these quality metrics. However, finding the best combination of metrics and their thresholds to identify code smells is left to the developer as a manual process. In this paper, we propose to automatically generate rules for the detection of code smells in Android applications using a multi-objective genetic programming algorithm (MOGP). The MOGP algorithm aims at finding the best set of rules that cover a set of code smell examples of Android applications based on two conflicting objective functions of precision and recall. We evaluate our approach on 184 Android projects with source code hosted in GitHub. The statistical test of our results show that the generated detection rules identified 10 Android smell types on these mobile applications with an average correctness higher than 82% and an average relevance of 77% based on the feedback of active developers of mobile apps.","quality, Android apps, search-based software engineering",MOBILESoft '17,,,
Conference Paper,"Akbarinasaji S,Bener A,Neal A",A Heuristic for Estimating the Impact of Lingering Defects: Can Debt Analogy Be Used as a Metric?,,2017,,,36–42,IEEE Press,"Buenos Aires, Argentina",Proceedings of the 8th Workshop on Emerging Trends in Software Metrics,,2017,9781538628072.0,,,,"Background: Due to tight scheduling and limited budget, it may not be possible to resolve all the existing bugs in a current release of a software product. The accumulation of the deferred bugs in the issue tracking system are obligations (liabilities) of the software team similar to financial analogy of 'debt'. Defect debt is known as latent defects which are not resolved in the current release. Aim: In order to manage the defect debt, software managers need to be aware of the amount of debt (principal) as well as the price of the credit (interest) in their system. There are no studies in the literature to measure the principal or interest of defect debt. In this study, we propose a novel approach to identify the interest of defect debt. Methodology: We developed a heuristic to specify the interest based on three metrics: PageRank index, customer feedback and bug fixing duration. In order to investigate the feasibility of our heuristic, we employ it to two datasets that are extracted from both open source and commercial software products. We validate the heuristic using two metrics: the severity/priority of bugs, and the duration of bug fixing time. Result: The results show that 24% and 18% of the deferred bugs are high and medium impact bugs in project 1 and project 2, respectively.","lingering defect, PageRank, software metrics, graph theory, interest",WETSoM '17,,,
Conference Paper,"Wert A,Oehler M,Heger C,Farahbod R",Automatic Detection of Performance Anti-Patterns in Inter-Component Communications,,2014,,,3–12,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 10th International ACM Sigsoft Conference on Quality of Software Architectures,"Marcq-en-Bareul, France",2014,9781450325769.0,,https://doi.org/10.1145/2602576.2602579;http://dx.doi.org/10.1145/2602576.2602579,10.1145/2602576.2602579,"Performance problems such as high response times in software applications have a significant effect on the customer's satisfaction. In enterprise applications, performance problems are frequently manifested in inefficient or unnecessary communication patterns between software components originating from poor architectural design or implementation. Due to high manual effort, thorough performance analysis is often neglected, in practice. In order to overcome this problem, automated engineering approaches are required for the detection of performance problems. In this paper, we introduce several heuristics for measurement-based detection of well-known performance anti-patterns in inter-component communications. The detection heuristics comprise load and instrumentation descriptions for performance tests as well as corresponding detection rules. We integrate these heuristics with Dynamic Spotter, a framework for automatic detection of performance problems. We evaluate our heuristics on four evaluation scenarios based on an e-commerce benchmark (TPC-W) where the heuristics detect the expected communication performance anti-patterns and pinpoint their root causes.","performance testing, performance anti-patterns, performance problem diagnostics",QoSA '14,,,
Conference Paper,"Tufano M,Palomba F,Bavota G,Oliveto R,Di Penta M,De Lucia A,Poshyvanyk D",When and Why Your Code Starts to Smell Bad,,2015,,,403–414,IEEE Press,"Florence, Italy",Proceedings of the 37th International Conference on Software Engineering - Volume 1,,2015,9781479919345.0,,,,"In past and recent years, the issues related to managing technical debt received significant attention by researchers from both industry and academia. There are several factors that contribute to technical debt. One of these is represented by code bad smells, i.e., symptoms of poor design and implementation choices. While the repercussions of smells on code quality have been empirically assessed, there is still only anecdotal evidence on when and why bad smells are introduced. To fill this gap, we conducted a large empirical study over the change history of 200 open source projects from different software ecosystems and investigated when bad smells are introduced by developers, and the circumstances and reasons behind their introduction. Our study required the development of a strategy to identify smell-introducing commits, the mining of over 0.5M commits, and the manual analysis of 9,164 of them (i.e., those identified as smell-introducing). Our findings mostly contradict common wisdom stating that smells are being introduced during evolutionary tasks. In the light of our results, we also call for the need to develop a new generation of recommendation systems aimed at properly planning smell refactoring activities.",,ICSE '15,,,
Conference Paper,"AlOmar EA,Mkaouer MW,Ouni A",Can Refactoring Be Self-Affirmed? An Exploratory Study on How Developers Document Their Refactoring Activities in Commit Messages,,2019,,,51–58,IEEE Press,"Montreal, Quebec, Canada",Proceedings of the 3rd International Workshop on Refactoring,,2019,,,https://doi.org/10.1109/IWoR.2019.00017;http://dx.doi.org/10.1109/IWoR.2019.00017,10.1109/IWoR.2019.00017,"Refactoring is a critical task in software maintenance and is usually performed to enforce best design practices, or to cope with design defects. Previous studies heavily rely on defining a set of keywords to identify refactoring commits from a list of general commits extracted from a small set of software systems. All approaches thus far consider all commits without checking whether refactorings had actually happened or not. In this paper, we aim at exploring how developers document their refactoring activities during the software life cycle. We call such activity Self-Affirmed Refactoring, which is an indication of the developer-related refactoring events in the commit messages. Our approach relies on text mining refactoring-related change messages and identifying refactoring patterns by only considering refactoring commits. We found that (1) developers use a variety of patterns to purposefully target refactoring-related activities; (2) developers tend to explicitly mention the improvement of specific quality attributes and code smells; and (3) commit messages with self-affirmed refactoring patterns tend to have more significant refactoring activity than those without.","mining software repositories, self-affirmed refactoring, software quality",IWOR '19,,,
Journal Article,"Hall T,Zhang M,Bowes D,Sun Y",Some Code Smells Have a Significant but Small Effect on Faults,ACM Trans. Softw. Eng. Methodol.,2014,23.0,4,,Association for Computing Machinery,"New York, NY, USA",,,2014-09,,1049-331X,https://doi.org/10.1145/2629648;http://dx.doi.org/10.1145/2629648,10.1145/2629648,"We investigate the relationship between faults and five of Fowler et al.'s least-studied smells in code: Data Clumps, Switch Statements, Speculative Generality, Message Chains, and Middle Man. We developed a tool to detect these five smells in three open-source systems: Eclipse, ArgoUML, and Apache Commons. We collected fault data from the change and fault repositories of each system. We built Negative Binomial regression models to analyse the relationships between smells and faults and report the McFadden effect size of those relationships. Our results suggest that Switch Statements had no effect on faults in any of the three systems; Message Chains increased faults in two systems; Message Chains which occurred in larger files reduced faults; Data Clumps reduced faults in Apache and Eclipse but increased faults in ArgoUML; Middle Man reduced faults only in ArgoUML, and Speculative Generality reduced faults only in Eclipse. File size alone affects faults in some systems but not in all systems. Where smells did significantly affect faults, the size of that effect was small (always under 10 percent). Our findings suggest that some smells do indicate fault-prone code in some circumstances but that the effect that these smells have on faults is small. Our findings also show that smells have different effects on different systems. We conclude that arbitrary refactoring is unlikely to significantly reduce fault-proneness and in some cases may increase fault-proneness.","defects, Software code smells",,,,
Conference Paper,"Ordiales Coscia JL,Crasso M,Mateos C,Zunino A",An Approach to Improve Code-First Web Services Discoverability at Development Time,,2012,,,638–643,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 27th Annual ACM Symposium on Applied Computing,"Trento, Italy",2012,9781450308571.0,,https://doi.org/10.1145/2245276.2245400;http://dx.doi.org/10.1145/2245276.2245400,10.1145/2245276.2245400,"Previous efforts towards simplifying Web Service discovery have shown that avoiding some well-known WSDL specification anti-patterns yield quite good results in making more discoverable services. The anti-patterns, however, have been studied with contract-first Web Services, a service construction methodology that is much less popular in the software industry compared to code-first. We study a number of source code refactorings that can be applied at service development time to reduce the presence of anti-patterns in code-first WSDL documents. The cornerstone of these refactorings is a statistical correlation between common object-oriented (OO) metrics and the anti-patterns computed by using a data-set of real Web Services. We quantify the impact of the refactorings on Web Service discovery and show that more clear WSDL documents are generated and service discovery is greatly improved.","OO metrics, web services, web service discovery, code-first, WSDL anti-patterns",SAC '12,,,
Journal Article,"Bajczi L,Vörös A,Molnár V",Will My Program Break on This Faulty Processor? Formal Analysis of Hardware Fault Activations in Concurrent Embedded Software,ACM Trans. Embed. Comput. Syst.,2019,18.0,5s,,Association for Computing Machinery,"New York, NY, USA",,,2019-10,,1539-9087,https://doi.org/10.1145/3358238;http://dx.doi.org/10.1145/3358238,10.1145/3358238,"Formal verification is approaching a point where it will be reliably applicable to embedded software. Even though formal verification can efficiently analyze multi-threaded applications, multi-core processors are often considered too dangerous to use in critical systems, despite the many benefits they can offer. One reason is the advanced memory consistency model of such CPUs. Nowadays, most software verifiers assume strict sequential consistency, which is also the naïve view of programmers. Modern multi-core processors, however, rarely guarantee this assumption by default. In addition, complex processor architectures may easily contain design faults. Thanks to the recent advances in hardware verification, these faults are increasingly visible and can be detected even in existing processors, giving an opportunity to compensate for the problem in software. In this paper, we propose a generic approach to consider inconsistent behavior of the hardware in the analysis of software. Our approach is based on formal methods and can be used to detect the activation of existing hardware faults on the application level and facilitate their mitigation in software. The approach relies heavily on recent results of model checking and hardware verification and offers new, integrative research directions. We propose a partial solution based on existing model checking tools to demonstrate feasibility and evaluate their performance in this context.","memory consistency model, Fault, analysis, concurrent, litmus test",,,,
Conference Paper,"Sedano T,Ralph P,Péraire C",Sustainable Software Development through Overlapping Pair Rotation,,2016,,,,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 10th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement,"Ciudad Real, Spain",2016,9781450344272.0,,https://doi.org/10.1145/2961111.2962590;http://dx.doi.org/10.1145/2961111.2962590,10.1145/2961111.2962590,"Context: Conventional wisdom says that team disruptions (like team churn) should be avoided. However, we have observed software development projects that succeed despite high disruption.Objective: The purpose of this paper is to understand how to develop software effectively, even in the face of team disruption.Method: We followed Constructivist Grounded Theory. The primary researcher conducted participant-observation of several projects at Pivotal (a software development company), and interviewed 21 software engineers, interaction designers, and product managers. The researcher iteratively sampled and analyzed the collected data until achieving theoretical saturation.Results: This paper introduces a descriptive theory of Sustainable Software Development. The theory encompasses principles, policies, and practices aiming at removing knowledge silos and improving code quality (including discoverability and readability), hence leading to development sustainability.Limitations: While the results are highly relevant to the observed projects at Pivotal, the outcomes may not be transferable to other software development organizations with different software development cultures.Conclusion: The theory refines and extends the understanding of Extreme Programming by adding a few principles, policies, and practices (like the unique Overlapping Pair Rotation practice) and aligning these principles, policies, and practices towards the business goal of sustainability.","Code ownership, Extreme Programming, Sustainable software development, Grounded Theory",ESEM '16,,,
Conference Paper,"Linares-Vásquez M,Klock S,McMillan C,Sabané A,Poshyvanyk D,Guéhéneuc YG","Domain Matters: Bringing Further Evidence of the Relationships among Anti-Patterns, Application Domains, and Quality-Related Metrics in Java Mobile Apps",,2014,,,232–243,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 22nd International Conference on Program Comprehension,"Hyderabad, India",2014,9781450328791.0,,https://doi.org/10.1145/2597008.2597144;http://dx.doi.org/10.1145/2597008.2597144,10.1145/2597008.2597144,"Some previous work began studying the relationship between application domains and quality, in particular through the prevalence of code and design smells (e.g., anti-patterns). Indeed, it is generally believed that the presence of these smells degrades quality but also that their prevalence varies across domains. Though anecdotal experiences and empirical evidence gathered from developers and researchers support this belief, there is still a need to further deepen our understanding of the relationship between application domains and quality. Consequently, we present a large-scale study that investigated the systematic relationships between the presence of smells and quality-related metrics computed over the bytecode of 1,343 Java Mobile Edition applications in 13 different application domains. Although, we did not find evidence of a correlation between smells and quality- related metrics, we found (1) that larger differences exist between metric values of classes exhibiting smells and classes without smells and (2) that some smells are commonly present in all the domains while others are most prevalent in certain domains","Anti-patterns, Domain categories, Software quality, Internal metrics, Java Mobile Edition",ICPC 2014,,,
Conference Paper,"Higo Y,Kusumoto S",How Should We Measure Functional Sameness from Program Source Code? An Exploratory Study on Java Methods,,2014,,,294–305,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 22nd ACM SIGSOFT International Symposium on Foundations of Software Engineering,"Hong Kong, China",2014,9781450330565.0,,https://doi.org/10.1145/2635868.2635886;http://dx.doi.org/10.1145/2635868.2635886,10.1145/2635868.2635886,"Program source code is one of the main targets of software engineering research. A wide variety of research has been conducted on source code, and many studies have leveraged structural, vocabulary, and method signature similarities to measure the functional sameness of source code. In this research, we conducted an empirical study to ascertain how we should use three similarities to measure functional sameness. We used two large datasets and measured the three similarities between all the method pairs in the datasets, each of which included approximately 15 million Java method pairs. The relationships between the three similarities were analyzed to determine how we should use each to detect functionally similar code. The results of our study revealed the following. (1) Method names are not always useful for detecting functionally similar code. Only if there are a small number of methods having a given name, the methods are likely to include functionally similar code. (2) Existing file-level, method-level, and block-level clone detection techniques often miss functionally similar code generated by copy-and-paste operations between different projects. (3) In the cases we use structural similarity for detecting functionally similar code, we obtained many false positives. However, we can avoid detecting most false positives by using a vocabulary similarity in addition to a structural one. (4) Using a vocabulary similarity to detect functionally similar code is not suitable for method pairs in the same file because such method pairs use many of the same program elements such as private methods or private fields.","Structural similarity, Method name similarity, Functionally similar code, Clone Detection, Vocabulary similarity",FSE 2014,,,
Conference Paper,"Catolino G,Palomba F,De Lucia A,Ferrucci F,Zaidman A",Developer-Related Factors in Change Prediction: An Empirical Assessment,,2017,,,186–195,IEEE Press,"Buenos Aires, Argentina",Proceedings of the 25th International Conference on Program Comprehension,,2017,9781538605356.0,,https://doi.org/10.1109/ICPC.2017.19;http://dx.doi.org/10.1109/ICPC.2017.19,10.1109/ICPC.2017.19,"Predicting the areas of the source code having a higher likelihood to change in the future is a crucial activity to allow developers to plan preventive maintenance operations such as refactoring or peer-code reviews. In the past the research community was active in devising change prediction models based on structural metrics extracted from the source code. More recently, Elish et al. showed how evolution metrics can be more efficient for predicting change-prone classes. In this paper, we aim at making a further step ahead by investigating the role of different developer-related factors, which are able to capture the complexity of the development process under different perspectives, in the context of change prediction. We also compared such models with existing change-prediction models based on evolution and code metrics. Our findings reveal the capabilities of developer-based metrics in identifying classes of a software system more likely to be changed in the future. Moreover, we observed interesting complementarities among the experimented prediction models, that may possibly lead to the definition of new combined models exploiting developer-related factors as well as product and evolution metrics.","mining software repositories, change prediction, empirical studies",ICPC '17,,,
Journal Article,"Vidal S,Berra I,Zulliani S,Marcos C,Pace JA",Assessing the Refactoring of Brain Methods,ACM Trans. Softw. Eng. Methodol.,2018,27.0,1,,Association for Computing Machinery,"New York, NY, USA",,,2018-04,,1049-331X,https://doi.org/10.1145/3191314;http://dx.doi.org/10.1145/3191314,10.1145/3191314,"Code smells are a popular mechanism for identifying structural design problems in software systems. Several tools have emerged to support the detection of code smells and propose some refactorings. However, existing tools do not guarantee that a smell will be automatically fixed by means of refactorings. This article presents Bandago, an automated approach to fix a specific type of code smell called Brain Method. A Brain Method centralizes the intelligence of a class and manifests itself as a long and complex method that is difficult to understand and maintain by developers. For each Brain Method, Bandago recommends several refactoring solutions to remove the smell using a search strategy based on simulated annealing. Our approach has been evaluated with several open-source Java applications, and the results show that Bandago can automatically fix more than 60% of Brain Methods. Furthermore, we conducted a survey with 35 industrial developers that showed evidence about the usefulness of the refactorings proposed by Bandago. Also, we compared the performance of the Bandago against that of a third-party refactoring tool.","Code smells, long method, brain method, refactoring",,,,
Conference Paper,"Benelallam A,Harrand N,Soto-Valero C,Baudry B,Barais O",The Maven Dependency Graph: A Temporal Graph-Based Representation of Maven Central,,2019,,,344–348,IEEE Press,"Montreal, Quebec, Canada",Proceedings of the 16th International Conference on Mining Software Repositories,,2019,,,https://doi.org/10.1109/MSR.2019.00060;http://dx.doi.org/10.1109/MSR.2019.00060,10.1109/MSR.2019.00060,"The Maven Central Repository provides an extraordinary source of data to understand complex architecture and evolution phenomena among Java applications. As of September 6, 2018, this repository includes 2.8M artifacts (compiled piece of code implemented in a JVM-based language), each of which is characterized with metadata such as exact version, date of upload and list of dependencies towards other artifacts.Today, one who wants to analyze the complete ecosystem of Maven artifacts and their dependencies faces two key challenges: (i) this is a huge data set; and (ii) dependency relationships among artifacts are not modeled explicitly and cannot be queried. In this paper, we present the Maven Dependency Graph. This open source data set provides two contributions: a snapshot of the whole Maven Central taken on September 6, 2018, stored in a graph database in which we explicitly model all dependencies; an open source infrastructure to query this huge dataset.","maven central, temporal graph, dataset, mining",MSR '19,,,
Conference Paper,"Zhou Y,Gu R,Chen T,Huang Z,Panichella S,Gall H",Analyzing APIs Documentation and Code to Detect Directive Defects,,2017,,,27–37,IEEE Press,"Buenos Aires, Argentina",Proceedings of the 39th International Conference on Software Engineering,,2017,9781538638682.0,,https://doi.org/10.1109/ICSE.2017.11;http://dx.doi.org/10.1109/ICSE.2017.11,10.1109/ICSE.2017.11,"Application Programming Interface (API) documents represent one of the most important references for API users. However, it is frequently reported that the documentation is inconsistent with the source code and deviates from the API itself. Such inconsistencies in the documents inevitably confuse the API users hampering considerably their API comprehension and the quality of software built from such APIs. In this paper, we propose an automated approach to detect defects of API documents by leveraging techniques from program comprehension and natural language processing. Particularly, we focus on the directives of the API documents which are related to parameter constraints and exception throwing declarations. A first-order logic based constraint solver is employed to detect such defects based on the obtained analysis results. We evaluate our approach on parts of well documented JDK 1.8 APIs. Experiment results show that, out of around 2000 API usage constraints, our approach can detect 1158 defective document directives, with a precision rate of 81.6%, and a recall rate of 82.0%, which demonstrates its practical feasibility.","API documentation, static analysis, natural language processing",ICSE '17,,,
Conference Paper,"Alizadeh V,Kessentini M",Reducing Interactive Refactoring Effort via Clustering-Based Multi-Objective Search,,2018,,,464–474,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 33rd ACM/IEEE International Conference on Automated Software Engineering,"Montpellier, France",2018,9781450359375.0,,https://doi.org/10.1145/3238147.3238217;http://dx.doi.org/10.1145/3238147.3238217,10.1145/3238147.3238217,"Refactoring is nowadays widely adopted in the industry because bad design decisions can be very costly and extremely risky. On the one hand, automated refactoring does not always lead to the desired design. On the other hand, manual refactoring is error-prone, time-consuming and not practical for radical changes. Thus, recent research trends in the field focused on integrating developers feedback into automated refactoring recommendations because developers understand the problem domain intuitively and may have a clear target design in mind. However, this interactive process can be repetitive, expensive, and tedious since developers must evaluate recommended refactorings, and adapt them to the targeted design especially in large systems where the number of possible strategies can grow exponentially. In this paper, we propose an interactive approach combining the use of multi-objective and unsupervised learning to reduce the developer's interaction effort when refactoring systems. We generate, first, using multi-objective search different possible refactoring strategies by finding a trade-off between several conflicting quality attributes. Then, an unsupervised learning algorithm clusters the different trade-off solutions, called the Pareto front, to guide the developers in selecting their region of interests and reduce the number of refactoring options to explore. The feedback from the developer, both at the cluster and solution levels, are used to automatically generate constraints to reduce the search space in the next iterations and focus on the region of developer preferences. We selected 14 active developers to manually evaluate the effectiveness our tool on 5 open source projects and one industrial system. The results show that the participants found their desired refactorings faster and more accurate than the current state of the art.","Search based software engineering, multi-objective search, clustering, refactoring",ASE 2018,,,
Conference Paper,"Liu H,Xu Z,Zou Y",Deep Learning Based Feature Envy Detection,,2018,,,385–396,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 33rd ACM/IEEE International Conference on Automated Software Engineering,"Montpellier, France",2018,9781450359375.0,,https://doi.org/10.1145/3238147.3238166;http://dx.doi.org/10.1145/3238147.3238166,10.1145/3238147.3238166,"Software refactoring is widely employed to improve software quality. A key step in software refactoring is to identify which part of the software should be refactored. To facilitate the identification, a number of approaches have been proposed to identify certain structures in the code (called code smells) that suggest the possibility of refactoring. Most of such approaches rely on manually designed heuristics to map manually selected source code metrics to predictions. However, it is challenging to manually select the best features, especially textual features. It is also difficult to manually construct the optimal heuristics. To this end, in this paper we propose a deep learning based novel approach to detecting feature envy, one of the most common code smells. The key insight is that deep neural networks and advanced deep learning techniques could automatically select features (especially textual features) of source code for feature envy detection, and could automatically build the complex mapping between such features and predictions. We also propose an automatic approach to generating labeled training data for the neural network based classifier, which does not require any human intervention. Evaluation results on open-source applications suggest that the proposed approach significantly improves the state-of-the-art in both detecting feature envy smells and recommending destinations for identified smelly methods.","Software Refactoring, Deep Learning, Code Smells, Feature Envy",ASE 2018,,,
Conference Paper,"Robbes R,Janes A",Leveraging Small Software Engineering Data Sets with Pre-Trained Neural Networks,,2019,,,29–32,IEEE Press,"Montreal, Quebec, Canada",Proceedings of the 41st International Conference on Software Engineering: New Ideas and Emerging Results,,2019,,,https://doi.org/10.1109/ICSE-NIER.2019.00016;http://dx.doi.org/10.1109/ICSE-NIER.2019.00016,10.1109/ICSE-NIER.2019.00016,"Many software engineering data sets, particularly those that demand manual labelling for classification, are necessarily small. As a consequence, several recent software engineering papers have cast doubt on the effectiveness of deep neural networks for classification tasks, when applied to these data sets. We provide initial evidence that recent advances in Natural Language Processing, that allow neural networks to leverage large amount of unlabelled data in a pre-training phase, can significantly improve performance.","deep learning, transfer learning, data sets",ICSE-NIER '19,,,
Conference Paper,"Jain B,Tsai CC,Porter DE",A Clairvoyant Approach to Evaluating Software (In)Security,,2017,,,62–68,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 16th Workshop on Hot Topics in Operating Systems,"Whistler, BC, Canada",2017,9781450350686.0,,https://doi.org/10.1145/3102980.3102991;http://dx.doi.org/10.1145/3102980.3102991,10.1145/3102980.3102991,"Nearly all modern software has security flaws---either known or unknown by the users. However, metrics for evaluating software security (or lack thereof) are noisy at best. Common evaluation methods include counting the past vulnerabilities of the program, or comparing the size of the Trusted Computing Base (TCB), measured in lines of code (LoC) or binary size. Other than deleting large swaths of code from project, it is difficult to assess whether a code change decreased the likelihood of a future security vulnerability. Developers need a practical, constructive way of evaluating security.This position paper argues that we actually have all the tools needed to design a better, empirical method of security evaluation. We discuss related work that estimates the severity and vulnerability of certain attack vectors based on code properties that can be determined via static analysis. This paper proposes a grand, unified model that can predict the risk and severity of vulnerabilities in a program. Our prediction model uses machine learning to correlate these code features of open-source applications with the history of vulnerabilities reported in the CVE (Common Vulnerabilities and Exposures) database. Based on this model, one can incorporate an analysis into the standard development cycle that predicts whether the code is becoming more or less prone to vulnerabilities.",,HotOS '17,,,
Conference Paper,"Pérez J,Crespo Y",Perspectives on Automated Correction of Bad Smells,,2009,,,99–108,Association for Computing Machinery,"New York, NY, USA",Proceedings of the Joint International and Annual ERCIM Workshops on Principles of Software Evolution (IWPSE) and Software Evolution (Evol) Workshops,"Amsterdam, The Netherlands",2009,9781605586786.0,,https://doi.org/10.1145/1595808.1595827;http://dx.doi.org/10.1145/1595808.1595827,10.1145/1595808.1595827,"Keeping a software system conformant with a desired architecture and consistent with good design principles is a recurring task during the software evolution process. Deviations from good design principles can manifest in the form of bad smells: problems in the system's structure that can negatively affect software quality factors.Many authors have worked in identifying bad smells and in removing them with refactorings: tools have been built to suggest refactorings; successful approaches to detect bad smells have been developed, etc.. We present a comprehensive and historical review on this subject, in order to model the current state of the art and to identify the open challenges, current trends and research opportunities.We also propose a technique based on automated planning, aimed at taking one step forward in the automatic improvement of a system's structure. This proposal will allow computing complex refactoring sequences which can be directed to the achievement of a certain objective, such as the correction of bad smells.","automated planning, refactoring, bad smells",IWPSE-Evol '09,,,
Conference Paper,"Chen B,Jiang ZM",Characterizing and Detecting Anti-Patterns in the Logging Code,,2017,,,71–81,IEEE Press,"Buenos Aires, Argentina",Proceedings of the 39th International Conference on Software Engineering,,2017,9781538638682.0,,https://doi.org/10.1109/ICSE.2017.15;http://dx.doi.org/10.1109/ICSE.2017.15,10.1109/ICSE.2017.15,"Snippets of logging code are output statements (e.g., LOG.info or System.out.println) that developers insert into a software system. Although more logging code can provide more execution context of the system's behavior during runtime, it is undesirable to instrument the system with too much logging code due to maintenance overhead. Furthermore, excessive logging may cause unexpected side-effects like performance slow-down or high disk I/O bandwidth. Recent studies show that there are no well-defined coding guidelines for performing effective logging. Previous research on the logging code mainly tackles the problems of where-to-log and what-to-log. There are very few works trying to address the problem of how-to-log (developing and maintaining high-quality logging code).In this paper, we study the problem of how-to-log by characterizing and detecting the anti-patterns in the logging code. As the majority of the logging code is evolved together with the feature code, the remaining set of logging code changes usually contains the fixes to the anti-patterns. We have manually examined 352 pairs of independently changed logging code snippets from three well-maintenance open source systems: ActiveMQ, Hadoop and Maven. Our analysis has resulted in six different anti-patterns in the logging code. To demonstrate the value of our findings, we have encoded these anti-patterns into a static code analysis tool, LCAnalyzer. Case studies show that LCAnalyzer has an average recall of 95% and precision of 60% and can be used to automatically detect previously unknown anti-patterns in the source code. To gather feedback, we have filed 64 representative instances of the logging code anti-patterns from the most recent releases of ten open source software systems. Among them, 46 instances (72%) have already been accepted by their developers.","logging practices, logging code, software maintenance, empirical studies, anti-patterns",ICSE '17,,,
Conference Paper,"Oliveira A,Sousa L,Oizumi W,Garcia A","On the Prioritization of Design-Relevant Smelly Elements: A Mixed-Method, Multi-Project Study",,2019,,,83–92,Association for Computing Machinery,"New York, NY, USA","Proceedings of the XIII Brazilian Symposium on Software Components, Architectures, and Reuse","Salvador, Brazil",2019,9781450376372.0,,https://doi.org/10.1145/3357141.3357142;http://dx.doi.org/10.1145/3357141.3357142,10.1145/3357141.3357142,"Software systems are likely to face what is called design problems. Given the typical lack of design documentation, developers have to rely on implementation-level symptoms, the so-called code smells, to identify and remove design problems. A code smell is a microstructure in the program that can indicate the presence of a design problem. Large programs have hundreds or thousands of program elements (e.g., classes) in which a significant proportion may be affected by smells. Consequently, due to time constraints and the large number of elements, developers have to prioritize the designrelevant program elements, i.e., locate a shortlist of elements that are suspects of having design-relevant smells. However, this task is hard and time-consuming. Unfortunately, the literature fails to provide developers with effective heuristics that automate such prioritization task. The objective of this paper is to propose heuristics that effectively locate a shortlist of design-relevant smelly program elements. For this purpose, we report two studies. In the first one, we investigated the criteria that developers used in practice to accurately prioritize design-relevant smelly elements. Based on these criteria, we derived a preliminary suite of prioritization heuristics. Since we do not know if the heuristics are suitable for an effective prioritization across multiple projects, we performed a second study to evaluate the proposed heuristics. We found that two out of nine heuristics reached an average precision higher than 75% for the four projects we analyzed. Thus, our findings suggest these heuristics are promising to support developers in prioritizing design-relevant smelly elements.","heuristics, design problems, prioritization",SBCARS '19,,,
Conference Paper,"Hemel A,Kalleberg KT,Vermaas R,Dolstra E",Finding Software License Violations through Binary Code Clone Detection,,2011,,,63–72,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 8th Working Conference on Mining Software Repositories,"Waikiki, Honolulu, HI, USA",2011,9781450305747.0,,https://doi.org/10.1145/1985441.1985453;http://dx.doi.org/10.1145/1985441.1985453,10.1145/1985441.1985453,"Software released in binary form frequently uses third-party packages without respecting their licensing terms. For instance, many consumer devices have firmware containing the Linux kernel, without the suppliers following the requirements of the GNU General Public License. Such license violations are often accidental, e.g., when vendors receive binary code from their suppliers with no indication of its provenance. To help find such violations, we have developed the Binary Analysis Tool (BAT), a system for code clone detection in binaries. Given a binary, such as a firmware image, it attempts to detect cloning of code from repositories of packages in source and binary form. We evaluate and compare the effectiveness of three of BAT's clone detection techniques: scanning for string literals, detecting similarity through data compression, and detecting similarity by computing binary deltas.","binary analysis, firmware, code clone detection, repository mining",MSR '11,,,
Conference Paper,"Lavazza L,Morasca S,Tosi D",Technical Debt as an External Software Attribute,,2018,,,21–30,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 2018 International Conference on Technical Debt,"Gothenburg, Sweden",2018,9781450357135.0,,https://doi.org/10.1145/3194164.3194168;http://dx.doi.org/10.1145/3194164.3194168,10.1145/3194164.3194168,"Background: Technical debt is currently receiving increasing attention from practitioners and researchers. Several metaphors, concepts, and indications concerning technical debt have been introduced, but no agreement exists about a solid definition of technical debt.Objective: We aim at providing a solid basis to the definition of technical debt and the way it should be quantified.Method: We view technical debt as a software quality attribute and therefore we use Measurement Theory, the general reference framework for the quantification of attributes, to define technical debt and its characteristics in a rigorous way.Results: We show that technical debt should be defined as an external software quality attribute. Therefore, it should be quantified via statistical and machine-learning models whose independent variables are internal software quality attributes. Different models may exist, depending on the specific needs and goals of the software product and development environment. Also, technical debt is a multifaceted concept, so different kinds of technical debt exist, related to different quality attributes, such as performance, usability, and maintainability. These different kinds of technical debt should be evaluated individually, so one can better focus on the specific quality issues that need to be addressed.Conclusions: We show that, to provide it with a rigorous basis, technical debt should be considered and measured as an external software attribute. Researchers and practitioners should build models for technical debt and use them to (1) assess the extent of the technical debt and (2) investigate and assess different ways of modifying software to repay technical debt.","software quality, technical debt",TechDebt '18,,,
Conference Paper,"Gao X,Mechtaev S,Roychoudhury A",Crash-Avoiding Program Repair,,2019,,,8–18,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 28th ACM SIGSOFT International Symposium on Software Testing and Analysis,"Beijing, China",2019,9781450362245.0,,https://doi.org/10.1145/3293882.3330558;http://dx.doi.org/10.1145/3293882.3330558,10.1145/3293882.3330558,"Existing program repair systems modify a buggy program so that the modified program passes given tests. The repaired program may not satisfy even the most basic notion of correctness, namely crash-freedom. In other words, repair tools might generate patches which over-fit the test data driving the repair, and the automatically repaired programs may even introduce crashes or vulnerabilities. We propose an integrated approach for detecting and discarding crashing patches. Our approach fuses test and patch generation into a single process, in which patches are generated with the objective of passing existing tests, and new tests are generated with the objective of filtering out over-fitted patches by distinguishing candidate patches in terms of behavior. We use crash-freedom as the oracle to discard patch candidates which crash on the new tests. In its core, our approach defines a grey-box fuzzing strategy that gives higher priority to new tests that separate patches behaving equivalently on existing tests. This test generation strategy identifies semantic differences between patch candidates, and reduces over-fitting in program repair. We evaluated our approach on real-world vulnerabilities and open-source subjects from the Google OSS-Fuzz infrastructure. We found that our tool Fix2Fit (implementing patch space directed test generation), produces crash-avoiding patches. While we do not give formal guarantees about crash-freedom, cross-validation with fuzzing tools and their sanitizers provides greater confidence about the crash-freedom of our suggested patches.","Automated program repair, Overfitting, Fuzzing",ISSTA 2019,,,
Journal Article,"Goues CL,Pradel M,Roychoudhury A",Automated Program Repair,Commun. ACM,2019,62.0,12.0,56–65,Association for Computing Machinery,"New York, NY, USA",,,2019-11,,0001-0782,https://doi.org/10.1145/3318162;http://dx.doi.org/10.1145/3318162,10.1145/3318162,Automated program repair can relieve programmers from the burden of manually fixing the ever-increasing number of programming mistakes.,,,,,
Conference Paper,"Li S,Niu X,Jia Z,Wang J,He H,Wang T",Logtracker: Learning Log Revision Behaviors Proactively from Software Evolution History,,2018,,,178–188,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 26th Conference on Program Comprehension,"Gothenburg, Sweden",2018,9781450357142.0,,https://doi.org/10.1145/3196321.3196328;http://dx.doi.org/10.1145/3196321.3196328,10.1145/3196321.3196328,"Log statements are widely used for postmortem debugging. Despite the importance of log messages, it is difficult for developers to establish good logging practices. There are two main reasons for this. First, there are no rigorous specifications or systematic processes to guide the practices of software logging. Second, logging code co-evolves with bug fixes or feature updates. While previous works on log enhancement have successfully focused on the first problem, they are hard to solve the latter. For taking the first step towards solving the second problem, this paper is inspired by code clones and assumes that logging code with similar context is pervasive in software and deserves similar modifications. To verify our assumptions, we conduct an empirical study on eight open-source projects. Based on the observation, we design and implement LogTracker, an automatic tool that can predict log revisions by mining the correlation between logging context and modifications. With an enhanced modeling of logging context, LogTracker is able to guide more intricate log revisions that cannot be covered by existing tools. We evaluate the effectiveness of LogTracker by applying it to the latest version of subject projects. The results of our experiments show that LogTracker can detect 199 instances of log revisions. So far, we have reported 25 of them, and 6 have been accepted.","software evolution, log revision, failure diagnose",ICPC '18,,,
Conference Paper,"Douziech PE,Curtis B","Cross-Technology, Cross-Layer Defect Detection in IT Systems: Challenges and Achievements",,2015,,,21–26,IEEE Press,"Florence, Italy",Proceedings of the First International Workshop on Complex FaUlts and Failures in LargE Software Systems,,2015,,,,,"Although critical for delivering resilient, secure, efficient, and easily changed IT systems; cross-technology, cross-layer quality defect detection in IT systems still faces hurdles. Two hurdles involve the absence of an absolute target architecture and the difficulty of apprehending multi-component anti-patterns. However, Static analysis and measurement technologies are now able to both consume contextual input and detect system-level anti-patterns. This paper will provide several examples of the information required to detect system-level anti-patterns using examples from the Common Weakness Enumeration repository maintained by MITRE Corp.","software architecture, CWE, software quality measures, software anti-patterns, software pattern detection, structural quality, IT systems",COUFLESS '15,,,
Conference Paper,Jaafar F,On the Analysis of Evolution of Software Artefacts and Programs,,2012,,,1563–1566,IEEE Press,"Zurich, Switzerland",Proceedings of the 34th International Conference on Software Engineering,,2012,9781467310673.0,,,,"The literature describes several approaches to identify the artefacts of programs that evolve together to reveal the (hidden) dependencies among these artefacts and to infer and describe their evolution trends. We propose the use of biological methods to group artefacts, to detect co-evolution among them, and to construct their phylogenic trees to express their evolution trends. First, we introduced the novel concepts of macro co-changes (MCCs), i.e., of artefacts that co-change within a large time interval and of dephase macro co-changes (DMCCs), i.e., macro co-changes that always happen with the same shifts in time. We developped an approach, Macocha, to identify these new patterns of artefacts co-evolution in large programs. Now, we are analysing the evolution of classes playing roles in design pattern and–or anti-patterns. In parallel to previous work, we are detecting what classes are in macro co-change or in dephase macro co-change with the design motifs. Results trend to show that classes plying roles in design motifs have specifics evolution trends. Finally, we are implementing an approach, Profilo, to achieve the analysis of the evolution of artefacts and versions of large object-oriented programs. Profilo create a phylogenic tree of different versions of program that describes versions evolution and the relation among versions and programs. We will also evaluate the usefulness of our tools using lab and field studies.",,ICSE '12,,,
Journal Article,"Ouni A,Wang H,Kessentini M,Bouktif S,Inoue K",A Hybrid Approach for Improving the Design Quality of Web Service Interfaces,ACM Trans. Internet Technol.,2018,19.0,1.0,,Association for Computing Machinery,"New York, NY, USA",,,2018-12,,1533-5399,https://doi.org/10.1145/3226593;http://dx.doi.org/10.1145/3226593,10.1145/3226593,"A key success of a Web service is to appropriately design its interface to make it easy to consume and understand. In the context of service-oriented computing (SOC), the service’s interface is the main source of interaction with the consumers to reuse the service functionality in real-world applications. The SOC paradigm provides a collection of principles and guidelines to properly design services to provide best practice of third-party reuse. However, recent studies showed that service designers tend to pay little care to the design of their service interfaces, which often lead to several side effects known as antipatterns. One of the most common Web service interface antipatterns is to expose a large number of semantically unrelated operations, implementing different abstractions, in one single interface. Such bad design practices may have a significant impact on the service reusability, understandability, as well as the development and run-time characteristics. To address this problem, in this article, we propose a hybrid approach to improve the design quality of Web service interfaces and fix antipatterns as a combination of both deterministic and heuristic-based approaches. The first step consists of a deterministic approach using a graph partitioning-based technique to split the operations of a large service interface into more cohesive interfaces, each one representing a distinct abstraction. Then, the produced interfaces will be checked using a heuristic-based approach based on the non-dominated sorting genetic algorithm (NSGA-II) to correct potential antipatterns while reducing the interface design deviation to avoid taking the service away from its original design. To evaluate our approach, we conduct an empirical study on a benchmark of 26 real-world Web services provided by Amazon and Yahoo. Our experiments consist of a quantitative evaluation based on design quality metrics, as well as a qualitative evaluation with developers to assess its usefulness in practice. The results show that our approach significantly outperforms existing approaches and provides more meaningful results from a developer’s perspective.","search-based software engineering, Web service design, antipattern, Web services, service-oriented computing",,,,
Conference Paper,"Izurieta C,Vetrò A,Zazworka N,Cai Y,Seaman C,Shull F",Organizing the Technical Debt Landscape,,2012,,,23–26,IEEE Press,"Zurich, Switzerland",Proceedings of the Third International Workshop on Managing Technical Debt,,2012,9781467317498.0,,,,"To date, several methods and tools for detecting source code and design anomalies have been developed. While each method focuses on identifying certain classes of source code anomalies that potentially relate to technical debt (TD), the overlaps and gaps among these classes and TD have not been rigorously demonstrated. We propose to construct a seminal technical debt landscape as a way to visualize and organize research on the subject.","landscape, design debt, technical debt, code smells",MTD '12,,,
Conference Paper,"Bartoszuk C,Dąbrowski R,Stencel K,Timoszuk G",On Quick Comprehension and Assessment of Software,,2013,,,161–168,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 14th International Conference on Computer Systems and Technologies,"Ruse, Bulgaria",2013,9781450320214.0,,https://doi.org/10.1145/2516775.2516806;http://dx.doi.org/10.1145/2516775.2516806,10.1145/2516775.2516806,"By an architecture of a software system we mean the fundamental organization of the system embodied in its components, their relationships to one another and to the system's environment. It also encompasses principles governing the system's design and evolution. Architectures of complex systems are obviously complex as well. The goal of our research is to harness this complexity. In this paper we focus on providing software architects with ability to quickly comprehend the complexity and assess the quality of software. The essential tools we use are: (1) a graph-based repository for collecting information on software artefacts, accompanied by (2) tools to perform software intelligence tasks, like analyzing dependencies among those artefacts, calculating their importance, and quality. On top of those tools we implement visualization methods that render the relative importance using size and the quality using colours. By means of such methods a software architect can at glance comprehend and assess the software, He/she can (1) find the starting points to dig into a complex system; (2) judge the cohesion and coupling of system components; and (3) assess the overall quality. We demonstrate this method using selected open-source projects of various sizes and qualities.","intelligence, graph, architecture, software, quality, metrics",CompSysTech '13,,,
Conference Paper,Mantyla MV,Developing New Approaches for Software Design Quality Improvement Based on Subjective Evaluations,,2004,,,48–50,IEEE Computer Society,USA,Proceedings of the 26th International Conference on Software Engineering,,2004,9780769521633.0,,,,This research abstract presents two approaches forutilizing the developersý subjective design qualityevaluations during the software lifecycle. In process-basedapproach developers study and improve theirsystemýs structure at fixed intervals. Tool-based approachuses subjective evaluations as input to tool analysis.These approaches or their combination are expected toimprove software design and promote organizationallearning about software design.,,ICSE '04,,,
Conference Paper,"Ghari S,Hadian M,Rasolroveicy M,Fokaefs M",A Multi-Dimensional Quality Analysis of Android Applications,,2019,,,34–43,IBM Corp.,USA,Proceedings of the 29th Annual International Conference on Computer Science and Software Engineering,"Toronto, Ontario, Canada",2019,,,,,"Quality is a multi-faceted aspect of software. As described by international standards, the process of quality assurance is concerned not only with the functionality of the software, but also with performance, security, maintainability, reliability and others. However, during the maintenance phase of software development, developers usually focus on one aspect at a time, for example improving design or fixing bugs, either due to time constraints or because of specific priorities. In this work, we present a study to show that quality issues do not occur in isolation during development. We study the source code from 10 Android applications and we explore problems around reliability, maintainability and security. In addition, we study the impact of maintenance activities around these problems on other quality aspects like performance and energy consumption. Our first objective is to find if quality problems of different types occur together and if there is correlation between specific types. Secondly, we want to see if fixing problems always has monotonically positive effect on the general quality or special attention needs to be taken when fixing specific problems. Our long-term goal is to create tool support for the multi-dimensional analysis and assurance of software quality.","mobile computing, software quality, empirical software engineering",CASCON '19,,,
Journal Article,Long J,Software Reuse Antipatterns,SIGSOFT Softw. Eng. Notes,2001,26.0,4.0,68–76,Association for Computing Machinery,"New York, NY, USA",,,2001-07,,0163-5948,https://doi.org/10.1145/505482.505492;http://dx.doi.org/10.1145/505482.505492,10.1145/505482.505492,"Software reuse is a productivity technique attempted by many development organizations, with mixed success. In analyzing reuse failures, a number of antipatterns emerge. Antipatterns are obvious, but wrong, solutions to recurring problems. This article outlines a number of reuse antipatterns that have been observed within the software industry.","antipattern, repository, pattern, reuse, asset, library, component",,,,
Conference Paper,"Fontana FA,Spinelli S",Impact of Refactoring on Quality Code Evaluation,,2011,,,37–40,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 4th Workshop on Refactoring Tools,"Waikiki, Honolulu, HI, USA",2011,9781450305792.0,,https://doi.org/10.1145/1984732.1984741;http://dx.doi.org/10.1145/1984732.1984741,10.1145/1984732.1984741,"Code smells are characteristics of the software that may indicate a code or design problem that can make software hard to understand, to evolve and maintain. Detecting code smells in the code and consequently applying the right refactoring steps, when necessary, is very important for improving the quality of the code. In this paper, according to well known metrics proposed to evaluate the code and design quality of a system, we analyze the impact of refactoring, applied to remove code smells, on the quality evaluation of the system.","code smells, refactoring, metrics, quality evaluation",WRT '11,,,
Conference Paper,"Yang J,Zhikhartsev A,Liu Y,Tan L",Better Test Cases for Better Automated Program Repair,,2017,,,831–841,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 2017 11th Joint Meeting on Foundations of Software Engineering,"Paderborn, Germany",2017,9781450351058.0,,https://doi.org/10.1145/3106237.3106274;http://dx.doi.org/10.1145/3106237.3106274,10.1145/3106237.3106274,"Automated generate-and-validate program repair techniques (G&V techniques) suffer from generating many overfitted patches due to in-capabilities of test cases. Such overfitted patches are incor- rect patches, which only make all given test cases pass, but fail to fix the bugs. In this work, we propose an overfitted patch detec- tion framework named Opad (Overfitted PAtch Detection). Opad helps improve G&V techniques by enhancing existing test cases to filter out overfitted patches. To enhance test cases, Opad uses fuzz testing to generate new test cases, and employs two test or- acles (crash and memory-safety) to enhance validity checking of automatically-generated patches. Opad also uses a novel metric (named O-measure) for deciding whether automatically-generated patches overfit. Evaluated on 45 bugs from 7 large systems (the same benchmark used by GenProg and SPR), Opad filters out 75.2% (321/427) over- fitted patches generated by GenProg/AE, Kali, and SPR. In addition, Opad guides SPR to generate correct patches for one more bug (the original SPR generates correct patches for 11 bugs). Our analysis also shows that up to 40% of such automatically-generated test cases may further improve G&V techniques if empowered with better test oracles (in addition to crash and memory-safety oracles employed by Opad).","Testing, Patch validation, Overfitting in automated program repair",ESEC/FSE 2017,,,
Conference Paper,"MacGregor E,Hsieh Y,Kruchten P",Cultural Patterns in Software Process Mishaps: Incidents in Global Projects,,2005,,,1–5,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 2005 Workshop on Human and Social Factors of Software Engineering,"St. Louis, Missouri",2005,9781595931207.0,,https://doi.org/10.1145/1083106.1083116;http://dx.doi.org/10.1145/1083106.1083116,10.1145/1083106.1083116,"This paper describes a current and ongoing research project being conducted at the University of British Columbia, Canada. The paper begins by briefly describing past anthropological and sociological culture research. This research will inform our current exploration into the issues surrounding culture and its role in Global Software Development efforts. It then clarifies why this research is particularly important. The paper continues with a description of the current phase of this research, which is an exploratory qualitative approach rooted in Grounded Theory, and of the next phase, which will be a more quantitative approach looking at specific ""problem areas"" that were identified during the first phase.","global software development, culture, software engineering, cultural patterns",HSSE '05,,,
Journal Article,"MacGregor E,Hsieh Y,Kruchten P",Cultural Patterns in Software Process Mishaps: Incidents in Global Projects,SIGSOFT Softw. Eng. Notes,2005,30.0,4.0,1–5,Association for Computing Machinery,"New York, NY, USA",,,2005-05,,0163-5948,https://doi.org/10.1145/1082983.1083116;http://dx.doi.org/10.1145/1082983.1083116,10.1145/1082983.1083116,"This paper describes a current and ongoing research project being conducted at the University of British Columbia, Canada. The paper begins by briefly describing past anthropological and sociological culture research. This research will inform our current exploration into the issues surrounding culture and its role in Global Software Development efforts. It then clarifies why this research is particularly important. The paper continues with a description of the current phase of this research, which is an exploratory qualitative approach rooted in Grounded Theory, and of the next phase, which will be a more quantitative approach looking at specific ""problem areas"" that were identified during the first phase.","cultural patterns, global software development, culture, software engineering",,,,
Conference Paper,"Peruma A,Almalki K,Newman CD,Mkaouer MW,Ouni A,Palomba F",TsDetect: An Open Source Test Smells Detection Tool,,2020,,,1650–1654,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering,"Virtual Event, USA",2020,9781450370431.0,,https://doi.org/10.1145/3368089.3417921;http://dx.doi.org/10.1145/3368089.3417921,10.1145/3368089.3417921,"The test code, just like production source code, is subject to bad design and programming practices, also known as smells. The presence of test smells in a software project may affect the quality, maintainability, and extendability of test suites making them less effective in finding potential faults and quality issues in the project's production code. In this paper, we introduce tsDetect, an automated test smell detection tool for Java software systems that uses a set of detection rules to locate existing test smells in test code. We evaluate the effectiveness of tsDetect on a benchmark of 65 unit test files containing instances of 19 test smell types. Results show that tsDetect achieves a high detection accuracy with an average precision score of 96% and an average recall score of 97%. tsDetect is publicly available, with a demo video, at: https://testsmells.github.io/","Test Smells, Software Quality, Detection Tool",ESEC/FSE 2020,,,
Conference Paper,"Hedegaard S,Simonsen JG",Mining until It Hurts: Automatic Extraction of Usability Issues from Online Reviews Compared to Traditional Usability Evaluation,,2014,,,157–166,Association for Computing Machinery,"New York, NY, USA","Proceedings of the 8th Nordic Conference on Human-Computer Interaction: Fun, Fast, Foundational","Helsinki, Finland",2014,9781450325424.0,,https://doi.org/10.1145/2639189.2639211;http://dx.doi.org/10.1145/2639189.2639211,10.1145/2639189.2639211,"Large amounts of data available on the web, for example reviews, tweets, and forum postings, contain user narratives on interaction with products. Finding usability issues in such user narratives offers an interesting alternative to traditional usability testing. To leverage such data for identifying usability issues, we (I) devise a methodology for building automated extraction tools for usability issues; (II) perform empirical assessment of such tools by training a number of classifiers to extract sentences describing usability issues for two digital cameras and a children's tablet; (III) perform quantitative and qualitative comparisons between the usability issues identified by the classifiers and those identified and assessed by two traditional methods: heuristic evaluation and think aloud testing. Our results show that it is possible to build and train algorithms for extracting actionable usability issues, but raise serious concerns about the practical future prospects for supplementing traditional evaluation methods with automated extraction algorithms.","user experience, usability, natural language processing, machine learning",NordiCHI '14,,,
Journal Article,"Lutteri E,Russo B,Succi G",Report of the 4th International Symposium on Empirical Software Engineering and Measurement ESEM 2010,SIGSOFT Softw. Eng. Notes,2011,36.0,2.0,28–34,Association for Computing Machinery,"New York, NY, USA",,,2011-03,,0163-5948,https://doi.org/10.1145/1943371.1943393;http://dx.doi.org/10.1145/1943371.1943393,10.1145/1943371.1943393,"This report summarizes the research works, in particular the full and short papers, presented at the 4th International Symposium on Empirical Software Engineering and Measurement (ESEM 2010), held the 16th and 17th of September in Bolzano-Bozen, Italy. The program provided thirty full papers, twenty six short papers and three invited talks held by Bertrand Meyer, Steve Fraser and Carlo Ghezzi.","software engineering, software measurement, ESEM, empirical software engineering",,,,
Conference Paper,"Bavota G,Panichella S,Tsantalis N,Di Penta M,Oliveto R,Canfora G",Recommending Refactorings Based on Team Co-Maintenance Patterns,,2014,,,337–342,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 29th ACM/IEEE International Conference on Automated Software Engineering,"Vasteras, Sweden",2014,9781450330138.0,,https://doi.org/10.1145/2642937.2642948;http://dx.doi.org/10.1145/2642937.2642948,10.1145/2642937.2642948,"Refactoring aims at restructuring existing source code when undisciplined development activities have deteriorated its comprehensibility and maintainability. There exist various approaches for suggesting refactoring opportunities, based on different sources of information, e.g., structural, semantic, and historical. In this paper we claim that an additional source of information for identifying refactoring opportunities, sometimes orthogonal to the ones mentioned above, is team development activity. When the activity of a team working on common modules is not aligned with the current design structure of a system, it would be possible to recommend appropriate refactoring operations---e.g., extract class/method/package---to adjust the design according to the teams' activity patterns. Results of a preliminary study---conducted in the context of extract class refactoring---show the feasibility of the approach, and also suggest that this new refactoring dimension can be complemented with others to build better refactoring recommendation tools.","teams, developers, refactoring",ASE '14,,,
Conference Paper,"Fengrong Z,Liping Z,Junqi Z",Research on the Tools of Clone Code Refactoring,,2019,,,27–31,Association for Computing Machinery,"New York, NY, USA","Proceedings of the 2019 3rd International Conference on Management Engineering, Software Engineering and Service Sciences","Wuhan, China",2019,9781450361897.0,,https://doi.org/10.1145/3312662.3312693;http://dx.doi.org/10.1145/3312662.3312693,10.1145/3312662.3312693,"Clone code is the code fragment that is identical or similar in syntax or semantics, which has great impact on software development and maintenance. According to the large amount of clone code in software and its complex changes, researchers have proposed many methods to eliminate the harmful clone code, in which refactoring is an effective measure. In this paper, concepts of clone code and refactoring are introduced firstly. And then the main methods of current clone code refactoring are compared and analyzed, so the related tools of clone code refactoring can be elaborated and their advantages and disadvantages are summarized. At last, the shortcomings and limitations of the clone code refactoring are discussed.","Refactoring, Clone Code, Refactorability Analysis",ICMSS 2019,,,
Conference Paper,"Johnson B,Pandita R,Smith J,Ford D,Elder S,Murphy-Hill E,Heckman S,Sadowski C",A Cross-Tool Communication Study on Program Analysis Tool Notifications,,2016,,,73–84,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering,"Seattle, WA, USA",2016,9781450342186.0,,https://doi.org/10.1145/2950290.2950304;http://dx.doi.org/10.1145/2950290.2950304,10.1145/2950290.2950304,"Program analysis tools use notifications to communicate with developers, but previous research suggests that developers encounter challenges that impede this communication. This paper describes a qualitative study that identifies 10 kinds of challenges that cause notifications to miscommunicate with developers. Our resulting notification communication theory reveals that many challenges span multiple tools and multiple levels of developer experience. Our results suggest that, for example, future tools that model developer experience could improve communication and help developers build more accurate mental models.","human factors, communication, program analysis tools",FSE 2016,,,
Conference Paper,"Destefanis G,Qaderi S,Bowes D,Petrić J,Ortu M",A Longitudinal Study of Anti Micro Patterns in 113 Versions of Tomcat,,2018,,,90–93,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 14th International Conference on Predictive Models and Data Analytics in Software Engineering,"Oulu, Finland",2018,9781450365932.0,,https://doi.org/10.1145/3273934.3273945;http://dx.doi.org/10.1145/3273934.3273945,10.1145/3273934.3273945,"Background: Micro patterns represent design decisions in code. They are similar to design patterns and can be detected automatically. These micro structures can be helpful in identifying portions of code which should be improved (anti-micro patterns), or other well-designed parts which need to be preserved. The concepts expressed in these design decisions are defined at class-level; therefore the primary goal is to detect and provide information related to a specific granularity level. Aim: this paper aims to present preliminary results about a longitudinal study performed on anti-micro pattern distributions over 113 versions of Tomcat. Method: we first extracted the micro patterns from the 113 versions of Tomcat, then found the percentage of classes matching each of the six anti-micro pattern considered for this analysis, and studied correlations among the obtained time series after testing for stationarity, randomness and seasonality. Results: results show that the time series are stationary, not random (except for Function Pointer), and that additional studied are needed for studying seasonality. Regarding correlations, only the Pool and Record time series presented a correlation of 0.69, while moderate correlation has been found between Function Pointer and Function Object (0.58) and between Cobol Like and Pool (0.44).","software engineering, time series analysis, micro patterns",PROMISE'18,,,
Conference Paper,"Correia R,Adachi E",Detecting Design Violations in Django-Based Web Applications,,2019,,,33–42,Association for Computing Machinery,"New York, NY, USA","Proceedings of the XIII Brazilian Symposium on Software Components, Architectures, and Reuse","Salvador, Brazil",2019,9781450376372.0,,https://doi.org/10.1145/3357141.3357600;http://dx.doi.org/10.1145/3357141.3357600,10.1145/3357141.3357600,"If on one hand frameworks allow programmers to reuse well-known architectural solutions, on the other hand they can make programmers unaware of important design decisions that should be followed during software construction, maintenance and evolution. And if programmers are unaware of these design decisions, there is a high risk of introducing design violations in the source code, and the accumulation of these violations might hinder software maintainability and evolvability. The use of static analysis tools might be employed to mitigate these problems by assisting the detection of recurring design violations in a given architectural pattern. In this work, we present MTV-Checker, a tool to assist the automatic detection of 5 design violations in Django-based web applications. We also conducted an empirical study in the context of the SUAP system, a large-scale Django-based information system with more than 175.000 lines of Python code currently deployed in more than 30 Brazilian institutions. Our results present the most recurrent violations, how they evolve along software evolution, and the opinions and experiences of software architects regarding these violations.","Design Violations, Python Applications, Web-Applications, Code Smells, Software Architecture, Software Design",SBCARS '19,,,
Conference Paper,"Li B,Vendome C,Linares-Vásquez M,Poshyvanyk D",Aiding Comprehension of Unit Test Cases and Test Suites with Stereotype-Based Tagging,,2018,,,52–63,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 26th Conference on Program Comprehension,"Gothenburg, Sweden",2018,9781450357142.0,,https://doi.org/10.1145/3196321.3196339;http://dx.doi.org/10.1145/3196321.3196339,10.1145/3196321.3196339,"Techniques to automatically identify the stereotypes of different software artifacts (e.g., classes, methods, commits) were previously presented. Those approaches utilized the techniques to support comprehension of software artifacts, but those stereotype-based approaches were not designed to consider the structure and purpose of unit tests, which are widely used in software development to increase the quality of source code. Moreover, unit tests are different than production code, since they are designed and written by following different principles and workflows.In this paper, we present a novel approach, called TeStereo, for automated tagging of methods in unit tests. The tagging is based on an original catalog of stereotypes that we have designed to improve the comprehension and navigation of unit tests in a large test suite. The stereotype tags are automatically selected by using static control-flow, data-flow, and API call based analyses. To evaluate the benefits of the stereotypes and the tagging reports, we conducted a study with 46 students and another survey with 25 Apache developers to (i) validate the accuracy of the inferred stereotypes, (ii) measure the usefulness of the stereotypes when writing/understanding unit tests, and (iii) collect feedback on the usefulness of the generated tagging reports.","maintaining software, unit test cases, program comprehension",ICPC '18,,,
Conference Paper,"Kouroshfar E,Mirakhorli M,Bagheri H,Xiao L,Malek S,Cai Y",A Study on the Role of Software Architecture in the Evolution and Quality of Software,,2015,,,246–257,IEEE Press,"Florence, Italy",Proceedings of the 12th Working Conference on Mining Software Repositories,,2015,9780769555942.0,,,,"Conventional wisdom suggests that a software system's architecture has a significant impact on its evolution. Prior research has studied the evolution of software using the information of how its files have changed together in their revision history. No prior study, however, has investigated the impact of architecture on the evolution of software from its change history. This is mainly because most open-source software systems do not document their architectures. We have overcome this challenge using several architecture recovery techniques. We used the recovered models to examine if co-changes spanning multiple architecture modules are more likely to introduce bugs than co-changes that are within modules. The results show that the co-changes that cross architectural module boundaries are more correlated with defects than co-changes within modules, implying that, to improve accuracy, bug predictors should also take the software architecture of the system into consideration.","software repositories, defects, software architecture",MSR '15,,,
Conference Paper,"Demeyer S,Guéhéneuc YG,Keller A,Lange CF,Mens K,Kuhn A,Kuhlemann M",Object-Oriented Reengineering: Report on the Workshop WOOR'07 at ECOOP 2007 10th Anniversary Edition,,2007,,,142–153,Springer-Verlag,"Berlin, Heidelberg",Proceedings of the 2007 Conference on Object-Oriented Technology,"Berlin, Germany",2007,9783540781943.0,,,,"The ability to reengineer object-oriented legacy systems has become a vital matter in today's software industry. Early adopters of the object-oriented programming paradigm are now facing the problem of transforming their object-oriented ""legacy"" systems into full-fledged frameworks. To address this problem, a series of workshops has been organised to set up a forum for exchanging experiences, discussing solutions, and exploring new ideas. Typically, these workshops were organised as satellite events of major software engineering conferences, such as ECOOP [1,2,3,4,5,6,7,8,9,10,11] and ESEC/FSE [12,13,14]. During the past 10 years, participants of this workshop series have been actively contributing to the state-of-the-art on reengineering of object-oriented systems. This special 10th anniversary edition was no exception and this report summarises the key discussions and outcome of that workshop.",,ECOOP'07,,,
Book,,ASE 2016: Proceedings of the 31st IEEE/ACM International Conference on Automated Software Engineering,,2016,,,,Association for Computing Machinery,"New York, NY, USA",,"Singapore, Singapore",2016,9781450338455.0,,,,,,,Proceedings,,
Conference Paper,"Perez A,Abreu R,van Deursen A",A Test-Suite Diagnosability Metric for Spectrum-Based Fault Localization Approaches,,2017,,,654–664,IEEE Press,"Buenos Aires, Argentina",Proceedings of the 39th International Conference on Software Engineering,,2017,9781538638682.0,,https://doi.org/10.1109/ICSE.2017.66;http://dx.doi.org/10.1109/ICSE.2017.66,10.1109/ICSE.2017.66,"Current metrics for assessing the adequacy of a test-suite plainly focus on the number of components (be it lines, branches, paths) covered by the suite, but do not explicitly check how the tests actually exercise these components and whether they provide enough information so that spectrum-based fault localization techniques can perform accurate fault isolation. We propose a metric, called DDU, aimed at complementing adequacy measurements by quantifying a test-suite's diagnosability, i.e., the effectiveness of applying spectrum-based fault localization to pinpoint faults in the code in the event of test failures. Our aim is to increase the value generated by creating thorough test-suites, so they are not only regarded as error detection mechanisms but also as effective diagnostic aids that help widely-used fault-localization techniques to accurately pinpoint the location of bugs in the system. Our experiments show that optimizing a test suite with respect to DDU yields a 34% gain in spectrum-based fault localization report accuracy when compared to the standard branch-coverage metric.","testing, diagnosability, coverage",ICSE '17,,,
Conference Paper,"Foucault M,Palyart M,Falleri JR,Blanc X",Computing Contextual Metric Thresholds,,2014,,,1120–1125,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 29th Annual ACM Symposium on Applied Computing,"Gyeongju, Republic of Korea",2014,9781450324694.0,,https://doi.org/10.1145/2554850.2554997;http://dx.doi.org/10.1145/2554850.2554997,10.1145/2554850.2554997,"Software metrics have been developed to measure the quality of software systems. A proper use of metrics requires thresholds to determine whether the value of a metric is acceptable or not. Many approaches propose to define thresholds based on large analyses of software systems. However it has been shown that thresholds depend greatly on the context of the project. Thus there is a need for an approach that computes thresholds by taking into account this context. In this paper we propose such approach with the objective to reach a trade-off between representativeness of the threshold and computation cost. Our approach is based on an unbiased selection of software entities and makes no assumptions on the statistical properties of the software metrics values. It can therefore be used by anyone, ranging from developer to manager, for computing a representative metric threshold tailored to their context.",,SAC '14,,,
Conference Paper,"de Pádua GB,Shang W",Studying the Relationship between Exception Handling Practices and Post-Release Defects,,2018,,,564–575,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 15th International Conference on Mining Software Repositories,"Gothenburg, Sweden",2018,9781450357166.0,,https://doi.org/10.1145/3196398.3196435;http://dx.doi.org/10.1145/3196398.3196435,10.1145/3196398.3196435,"Modern programming languages, such as Java and C#, typically provide features that handle exceptions. These features separate error-handling code from regular source code and aim to assist in the practice of software comprehension and maintenance. Nevertheless, their misuse can still cause reliability degradation or even catastrophic software failures. Prior studies on exception handling revealed the suboptimal practices of the exception handling flows and the prevalence of their anti-patterns. However, little is known about the relationship between exception handling practices and software quality. In this work, we investigate the relationship between software quality (measured by the probability of having post-release defects) and: (i) exception flow characteristics and (ii) 17 exception handling anti-patterns. We perform a case study on three Java and C# open-source projects. By building statistical models of the probability of post-release defects using traditional software metrics and metrics that are associated with exception handling practice, we study whether exception flow characteristics and exception handling anti-patterns have a statistically significant relationship with post-release defects. We find that exception flow characteristics in Java projects have a significant relationship with post-release defects. In addition, although the majority of the exception handing anti-patterns are not significant in the models, there exist anti-patterns that can provide significant explanatory power to the probability of post-release defects. Therefore, development teams should consider allocating more resources to improving their exception handling practices and avoid the anti-patterns that are found to have a relationship with post-release defects. Our findings also highlight the need for techniques that assist in handling exceptions in the software development practice.","empirical software engineering, exception handling, software quality",MSR '18,,,
Conference Paper,"Sotiropoulos T,Mitropoulos D,Spinellis D",Practical Fault Detection in Puppet Programs,,2020,,,26–37,Association for Computing Machinery,"New York, NY, USA",Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering,"Seoul, South Korea",2020,9781450371216.0,,https://doi.org/10.1145/3377811.3380384;http://dx.doi.org/10.1145/3377811.3380384,10.1145/3377811.3380384,"Puppet is a popular computer system configuration management tool. By providing abstractions that model system resources it allows administrators to set up computer systems in a reliable, predictable, and documented fashion. Its use suffers from two potential pitfalls. First, if ordering constraints are not correctly specified whenever a Puppet resource depends on another, the non-deterministic application of resources can lead to race conditions and consequent failures. Second, if a service is not tied to its resources (through the notification construct), the system may operate in a stale state whenever a resource gets modified. Such faults can degrade a computing infrastructure's availability and functionality.We have developed an approach that identifies these issues through the analysis of a Puppet program and its system call trace. Specifically, a formal model for traces allows us to capture the interactions of Puppet resources with the file system. By analyzing these interactions we identify (1) resources that are related to each other (e.g., operate on the same file), and (2) resources that should act as notifiers so that changes are correctly propagated. We then check the relationships from the trace's analysis against the program's dependency graph: a representation containing all the ordering constraints and notifications declared in the program. If a mismatch is detected, our system reports a potential fault.We have evaluated our method on a large set of popular Puppet modules, and discovered 92 previously unknown issues in 33 modules. Performance benchmarking shows that our approach can analyze in seconds real-world configurations with a magnitude measured in thousands of lines and millions of system calls.","system calls, ordering relationships, notifiers, program analysis, puppet",ICSE '20,,,
Conference Paper,"Cedrim D,Sousa L,Garcia A,Gheyi R",Does Refactoring Improve Software Structural Quality? A Longitudinal Study of 25 Projects,,2016,,,73–82,Association for Computing Machinery,"New York, NY, USA",Proceedings of the XXX Brazilian Symposium on Software Engineering,"Maringá, Brazil",2016,9781450342018.0,,https://doi.org/10.1145/2973839.2973848;http://dx.doi.org/10.1145/2973839.2973848,10.1145/2973839.2973848,"Code smells in a program represent indications of structural quality problems, which can be addressed by software refactoring. Refactoring is widely practiced by developers, and considerable development effort has been invested in refactoring tooling support. There is an explicit assumption that software refactoring improves the structural quality of a program by reducing its density of code smells. However, little has been reported about whether and to what extent developers successfully remove code smells through refactoring. This paper reports a first longitudinal study intended to address this gap. We analyze how often the commonly-used refactoring types affect the density of 5 types of code smells along the version histories of 25 projects. Our findings are based on the analysis of 2,635 refactorings distributed in 11 different types. Surprisingly, 2,506 refactorings (95.1%) did not reduce or introduce code smells. Thus, these findings suggest that refactorings lead to smell reduction less often than what has been reported. According to our data, only 2.24% of refactoring changes removed code smells and 2.66% introduced new ones. Moreover, several smells induced by refactoring tended to live long, i.e., 146 days on average. These smells were only eventually removed when smelly elements started to exhibit poor structural quality and, as a consequence, started to be more costly to get rid of.","Refactoring, Code Smells, Structural Quality",SBES '16,,,
Conference Paper,"Jiang J,Xiong Y,Zhang H,Gao Q,Chen X",Shaping Program Repair Space with Existing Patches and Similar Code,,2018,,,298–309,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 27th ACM SIGSOFT International Symposium on Software Testing and Analysis,"Amsterdam, Netherlands",2018,9781450356992.0,,https://doi.org/10.1145/3213846.3213871;http://dx.doi.org/10.1145/3213846.3213871,10.1145/3213846.3213871,"Automated program repair (APR) has great potential to reduce bug-fixing effort and many approaches have been proposed in recent years. APRs are often treated as a search problem where the search space consists of all the possible patches and the goal is to identify the correct patch in the space. Many techniques take a data-driven approach and analyze data sources such as existing patches and similar source code to help identify the correct patch. However, while existing patches and similar code provide complementary information, existing techniques analyze only a single source and cannot be easily extended to analyze both. In this paper, we propose a novel automatic program repair approach that utilizes both existing patches and similar code. Our approach mines an abstract search space from existing patches and obtains a concrete search space by differencing with similar code snippets. Then we search within the intersection of the two search spaces. We have implemented our approach as a tool called SimFix, and evaluated it on the Defects4J benchmark. Our tool successfully fixed 34 bugs. To our best knowledge, this is the largest number of bugs fixed by a single technology on the Defects4J benchmark. Furthermore, as far as we know, 13 bugs fixed by our approach have never been fixed by the current approaches.","code differencing, code adaptation, Automated program repair",ISSTA 2018,,,
Conference Paper,"Cruz L,Abreu R",Performance-Based Guidelines for Energy Efficient Mobile Applications,,2017,,,46–57,IEEE Press,"Buenos Aires, Argentina",Proceedings of the 4th International Conference on Mobile Software Engineering and Systems,,2017,9781538626696.0,,https://doi.org/10.1109/MOBILESoft.2017.19;http://dx.doi.org/10.1109/MOBILESoft.2017.19,10.1109/MOBILESoft.2017.19,"Mobile and wearable devices are nowadays the de facto personal computers, while desktop computers are becoming less popular. Therefore, it is important for companies to deliver efficient mobile applications. As an example, Google has published a set of best practices to optimize the performance of Android applications. However, these guidelines fall short to address energy consumption. As mobile software applications operate in resource-constrained environments, guidelines to build energy efficient applications are of utmost importance. In this paper, we studied whether or not eight best performance-based practices have an impact on the energy consumed by Android applications. In an experimental study with six popular mobile applications, we observed that the battery of the mobile device can last up to approximately an extra hour if the applications are developed with energy-aware practices. This work paves the way for a set of guidelines for energy-aware automatic refactoring techniques.","anti patterns, green computing, mobile computing",MOBILESoft '17,,,
Conference Paper,"Wong S,Cai Y,Kim M,Dalton M",Detecting Software Modularity Violations,,2011,,,411–420,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 33rd International Conference on Software Engineering,"Waikiki, Honolulu, HI, USA",2011,9781450304450.0,,https://doi.org/10.1145/1985793.1985850;http://dx.doi.org/10.1145/1985793.1985850,10.1145/1985793.1985850,"This paper presents Clio, an approach that detects modularity violations, which can cause software defects, modularity decay, or expensive refactorings. Clio computes the discrepancies between how components should change together based on the modular structure, and how components actually change together as revealed in version history. We evaluated Clio using 15 releases of Hadoop Common and 10 releases of Eclipse JDT. The results show that hundreds of violations identified using Clio were indeed recognized as design problems or refactored by the developers in later versions. The identified violations exhibit multiple symptoms of poor design, some of which are not easily detectable using existing approaches.","design structure matrix, bad code smells, refactoring, modularity violation detection",ICSE '11,,,
Conference Paper,"Habchi S,Hecht G,Rouvoy R,Moha N",Code Smells in IOS Apps: How Do They Compare to Android?,,2017,,,110–121,IEEE Press,"Buenos Aires, Argentina",Proceedings of the 4th International Conference on Mobile Software Engineering and Systems,,2017,9781538626696.0,,https://doi.org/10.1109/MOBILESoft.2017.11;http://dx.doi.org/10.1109/MOBILESoft.2017.11,10.1109/MOBILESoft.2017.11,"With billions of app downloads, the Apple App Store and Google Play Store succeeded to conquer mobile devices. However, this success also challenges app developers to publish high-quality apps to keep attracting and satisfying end-users. In particular, taming the ever-growing complexity of mobile apps to cope with maintenance and evolution tasks under such a pressure may lead to bad development choices. While these bad choices, a.k.a. code smells, are widely studied in object-oriented software, their study in the context of mobile apps, and in particular iOS apps, remains in its infancy.Therefore, in this paper, we consider the presence of object-oriented and iOS-specific code smells by analyzing 279 open-source iOS apps. As part of this empirical study, we extended the Paprika toolkit, which was previously designed to analyze Android apps, in order to support the analysis of iOS apps developed in Objective-C or Swift. We report on the results of this analysis as well as a comparison between iOS and Android apps. We comment our findings related to the quality of apps in these two ecosystems. Interestingly, we observed that iOS apps tend to contain the same proportions of code smells regardless of the development language, but they seem to be less prone to code smells compared to Android apps.","code smells, iOS, Android, mobile apps, software quality",MOBILESoft '17,,,
Conference Paper,"Jovanovikj I,Yigitbas E,Nagaraj A,Anjorin A,Sauer S,Engels G",Validating Test Case Migration via Mutation Analysis,,2020,,,31–40,Association for Computing Machinery,"New York, NY, USA",Proceedings of the IEEE/ACM 1st International Conference on Automation of Software Test,"Seoul, Republic of Korea",2020,9781450379571.0,,https://doi.org/10.1145/3387903.3389319;http://dx.doi.org/10.1145/3387903.3389319,10.1145/3387903.3389319,"Testing plays an important role in the context of software migration as it is used to validate and ensure functional equivalence as a key requirement. As developing new test cases of the migrated system is typically a costly and time-consuming activity, migrating existing test cases for the old system is thus an attractive alternative. Considering that migrated test cases are relied upon to validate an implemented system migration, it is clear that validating the corresponding test case migration is indeed crucial. The solution space involved in validating test case migration, however, is currently not well researched. In this paper, therefore, we analyze the application of mutation analysis as a validation technique for test case migration. Depending on what is mutated, we identify six scenarios which can be used as mutation patterns depending on the situation and the user's requirements. For each scenario, we provide a discussion of implied assumptions, indications, and limitations in each case. We also present an initial evaluation of the identified mutation analysis scenarios performed in a real-world migration setting.","mutation analysis, software migration, software testing",AST '20,,,
Conference Paper,Ouni A,"Search Based Software Engineering: Challenges, Opportunities and Recent Applications",,2020,,,1114–1146,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 2020 Genetic and Evolutionary Computation Conference Companion,"Cancún, Mexico",2020,9781450371278.0,,https://doi.org/10.1145/3377929.3389887;http://dx.doi.org/10.1145/3377929.3389887,10.1145/3377929.3389887,,,GECCO '20,,,
Conference Paper,"Pantiuchina J,Bavota G,Tufano M,Poshyvanyk D",Towards Just-in-Time Refactoring Recommenders,,2018,,,312–315,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 26th Conference on Program Comprehension,"Gothenburg, Sweden",2018,9781450357142.0,,https://doi.org/10.1145/3196321.3196365;http://dx.doi.org/10.1145/3196321.3196365,10.1145/3196321.3196365,"Empirical studies have provided ample evidence that low code quality is generally associated with lower maintainability. For this reason, tools have been developed to automatically detect design flaws (e.g., code smells). However, these tools are not able to prevent the introduction of design flaws. This means that the code has to experience a quality decay (with a consequent increase of maintenance/evolution costs) before state-of-the-art tools can be applied to identify and refactor the design flaws.Our goal is to develop a new generation of refactoring recommenders aimed at preventing, via refactoring operations, the introduction of design flaws rather than fixing them once they already affect the system. We refer to such a novel perspective on software refactoring as just-in-time refactoring. In this paper, we make a first step towards this direction, presenting an approach able to predict which classes will be affected in the future by code smells.","code smells, refactoring",ICPC '18,,,
Conference Paper,"Almeida D,Campos JC,Saraiva J,Silva JC",Towards a Catalog of Usability Smells,,2015,,,175–181,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 30th Annual ACM Symposium on Applied Computing,"Salamanca, Spain",2015,9781450331968.0,,https://doi.org/10.1145/2695664.2695670;http://dx.doi.org/10.1145/2695664.2695670,10.1145/2695664.2695670,"This paper presents a catalog of smells in the context of interactive applications. These so-called usability smells are indicators of poor design on an application's user interface, with the potential to hinder not only its usability but also its maintenance and evolution. To eliminate such usability smells we discuss a set of program/usability refactorings. In order to validate the presented usability smells catalog, and the associated refactorings, we present a preliminary empirical study with software developers in the context of a real open source hospital management application. Moreover, a tool that computes graphical user interface behavior models, giving the applications' source code, is used to automatically detect usability smells at the model level.","graphical user interfaces, empirical studies, code smells",SAC '15,,,
Conference Paper,"Molnar AJ,Motogna S",Long-Term Evaluation of Technical Debt in Open-Source Software,,2020,,,,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 14th ACM / IEEE International Symposium on Empirical Software Engineering and Measurement (ESEM),"Bari, Italy",2020,9781450375801.0,,https://doi.org/10.1145/3382494.3410673;http://dx.doi.org/10.1145/3382494.3410673,10.1145/3382494.3410673,"Background: A consistent body of research and practice have identified that technical debt provides valuable and actionable insight into the design and implementation deficiencies of complex software systems. Existing software tools enable characterizing and measuring the amount of technical debt at selective granularity levels; by providing a computational model, they enable stakeholders to measure and ultimately control this phenomenon. Aims: In this paper we aim to study the evolution and characteristics of technical debt in open-source software. For this, we carry out a longitudinal study that covers the entire development history of several complex applications. The goal is to improve our understanding of how the amount and composition of technical debt changes in evolving software. We also study how new technical debt is introduced in software, as well as identify how developers handle its accumulation over the long term. Method: We carried out our evaluation using three complex, open-source Java applications. All 110 released versions, covering more than 10 years of development history for each application were analyzed using SonarQube. We studied how the amount, composition and history of technical debt changed during development, compared our results across the studied applications and present our most important findings. Results: For each application, we identified key versions during which large amounts of technical debt were added, removed or both. This had significantly more impact when compared to the lines of code or class count increases that generally occurred during development. However, within each version, we found high correlation between file lines of code and technical debt. We observed that the Pareto principle was satisfied for the studied applications, as 20% of issue types generated around 80% of total technical debt. Interestingly, there was a large degree of overlap between the issues that generated most of the debt across the studied applications. Conclusions: Early application versions showed greater fluctuation in the amount of existing technical debt. We found application size to be an unreliable predictor for the quantity of technical debt. Most debt was introduced in applications as part of milestone releases that expanded their feature set; likewise, we identified releases where extensive refactoring significantly reduced the level of debt. We also discovered that technical debt issues persist for a long time in source code, and their removal did not appear to be prioritized according to type or severity.","technical debt, software evolution, software maintenance, static analysis, open-source",ESEM '20,,,
Conference Paper,"Novozhilov E,Veselov I,Pravilov M,Bryksin T",Evaluation of Move Method Refactorings Recommendation Algorithms: Are We Doing It Right?,,2019,,,23–26,IEEE Press,"Montreal, Quebec, Canada",Proceedings of the 3rd International Workshop on Refactoring,,2019,,,https://doi.org/10.1109/IWoR.2019.00012;http://dx.doi.org/10.1109/IWoR.2019.00012,10.1109/IWoR.2019.00012,"Previous studies introduced various techniques for detecting Move Method refactoring opportunities. However, different authors have different evaluations, which leads to the fact that results reported by different papers do not correlate with each other and it is almost impossible to understand which algorithm works better in practice. In this paper, we provide an overview of existing evaluation approaches for Move Method refactoring recommendation algorithms, as well as discuss their advantages and disadvantages. We propose a tool that can be used for generating large synthetic datasets suitable for both algorithms evaluation and building complex machine learning models for Move Method refactoring recommendation.","automatic refactoring recommendation, move method refactoring, algorithms evaluation, feature envy, code smells, dataset generation",IWOR '19,,,
Conference Paper,"Bird C,Zimmermann T",Assessing the Value of Branches with What-If Analysis,,2012,,,,Association for Computing Machinery,"New York, NY, USA",Proceedings of the ACM SIGSOFT 20th International Symposium on the Foundations of Software Engineering,"Cary, North Carolina",2012,9781450316149.0,,https://doi.org/10.1145/2393596.2393648;http://dx.doi.org/10.1145/2393596.2393648,10.1145/2393596.2393648,"Branches within source code management systems (SCMs) allow a software project to divide work among its teams for concurrent development by isolating changes. However, this benefit comes with several costs: increased time required for changes to move through the system and pain and error potential when integrating changes across branches. In this paper, we present the results of a survey to characterize how developers use branches in a large industrial project and common problems that they face. One of the major problems mentioned was the long delay that it takes changes to move from one team to another, which is often caused by having too many branches (branchmania). To monitor branch health, we introduce a novel what-if analysis to assess alternative branch structures with respect to two properties, isolation and liveness. We demonstrate with several scenarios how our what-if analysis can support branch decisions. By removing high-cost-low-benefit branches in Windows based on our what-if analysis, changes would each have saved 8.9 days of delay and only introduced 0.04 additional conflicts on average.","coordination, what-if analysis, concurrent development, branch refactoring, teams, branches",FSE '12,,,
Conference Paper,"Siqueira SW,Carvalho ST","Session Details: Main Track - Software Requirements, Architecture and Design for Information Systems",,2015,,,,Brazilian Computer Society,"Porto Alegre, BRA",Proceedings of the Annual Conference on Brazilian Symposium on Information Systems: Information Systems: A Computer Socio-Technical Perspective - Volume 1,"Goiania, Goias, Brazil",2015,,,,,,,SBSI 2015,,,
Conference Paper,"Abidi M,Khomh F,Guéhéneuc YG",Anti-Patterns for Multi-Language Systems,,2019,,,,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 24th European Conference on Pattern Languages of Programs,"Irsee, Germany",2019,9781450362061.0,,https://doi.org/10.1145/3361149.3364227;http://dx.doi.org/10.1145/3361149.3364227,10.1145/3361149.3364227,"Multi-language systems are common nowadays because most of the systems are developed using components written in different programming languages. These systems could arise from three different reasons: (1) to leverage the strengths and take benefits of each language, (2) to reduce the cost by reusing code written in other languages, (3) to include and accommodate legacy code. However, they also introduce additional challenges, including the increase in the complexity and the need for proper interfaces and interactions between the different languages. To address these challenges, the software-engineering research community, as well as the industry, should describe and provide common guidelines, idioms, and patterns to support the development, maintenance, and evolution of these systems. These patterns are an effective means of improving the quality of multi-language systems. They capture good practices to adopt and bad practices to avoid. In order to help to improve the quality of multi-language systems, we analysed open-source systems, developers' documentation, bug reports, and programming language specifications to extract bad practices of multi-language systems usage. We encoded and cataloged these practices in the form of design anti-patterns. We report here six anti-patterns. These results could help not only researchers but also professional developers considering the use of more than one programming language.","multi-language systems, code analysis, software quality, anti-patterns",EuroPLop '19,,,
Conference Paper,"de Sousa DB,Maia PH,Rocha LS,Viana W",Analysing the Evolution of Exception Handling Anti-Patterns in Large-Scale Projects: A Case Study,,2018,,,73–82,Association for Computing Machinery,"New York, NY, USA","Proceedings of the VII Brazilian Symposium on Software Components, Architectures, and Reuse","Sao Carlos, Brazil",2018,9781450365543.0,,https://doi.org/10.1145/3267183.3267191;http://dx.doi.org/10.1145/3267183.3267191,10.1145/3267183.3267191,"Previous studies have shown that exception handling bad practices may impact the overall software quality. We believe that quality of exception handling code is directly affected by (i) an absence, or lack of awareness, of an explicit exception handling policy; and (ii) a silent rising and spreading of exception handling anti-patterns. To investigate such phenomenon, we conducted a case study in a large-scale Java Web system, trying to better understand the relationship between (i) and (ii). The study takes into account technical and human aspects. We surveyed 21 developers regarding their perception about exception handling in the system's institution. Next, we analyse the evolution of exception handling anti-patterns across 15 releases of the target system. Finally, we conduct a semi-structured interview with three senior software architects. Our finds beneficiated the system's institution by making it aware of these problems and enabling it to take actions towards to combat them.","Exception Handling Anti-Patterns, Case Study, Exception Handling",SBCARS '18,,,
Journal Article,"Canfora G,Concas G,Marchesi M,Tempero E,Zhang H",2010 ICSE Workshop on Emerging Trends in Software Metrics,SIGSOFT Softw. Eng. Notes,2010,35.0,5.0,51–53,Association for Computing Machinery,"New York, NY, USA",,,2010-10,,0163-5948,https://doi.org/10.1145/1838687.1838700;http://dx.doi.org/10.1145/1838687.1838700,10.1145/1838687.1838700,"This paper reports on the 2010 ICSE Workshop on Emerging Trends in Software Metrics (WETSoM 2010) held on Tuesday 4 May 2010 in Cape Town, South Africa, as part of the ICSE workshop series. The goal of this workshop was to bring together researchers and practitioners to discuss the progress of software metrics.","metric validation, complex network theory, software metrics",,,,
Journal Article,"John M,Maurer F,Tessem B",Human and Social Factors of Software Engineering: Workshop Summary,SIGSOFT Softw. Eng. Notes,2005,30.0,4.0,1–6,Association for Computing Machinery,"New York, NY, USA",,,2005-07,,0163-5948,https://doi.org/10.1145/1082983.1083000;http://dx.doi.org/10.1145/1082983.1083000,10.1145/1082983.1083000,"Software is developed for people and by people. Human and social factors have a very strong impact on the success of software development endeavours and the resulting system. Surprisingly, much of software engineering research in the last decade is technical, quantitative and deemphasizes the people aspect. The workshop on Human and Social Factors in Software Engineering has been picking up on the some of the soft aspects in software development that was highlighted in the early days of software engineering. It also follows a recent trend in the software industry, namely the introduction of agile methods, and provides a scientific perspective on these. Including and combining approaches of software engineering with social science, the workshop looked at software engineering from a number of perspectives, including those of agile methods and communication theory, in order to point out solutions and conditions for human-centred software engineering.","decision support, interaction theory, agile methods, software engineering, qualitative studies, group dynamics, knowledge dissemination and reuse, human-centric collaboration support, management, social software, communication architectures",,,,
Conference Paper,"Xiong Y,Wang J,Yan R,Zhang J,Han S,Huang G,Zhang L",Precise Condition Synthesis for Program Repair,,2017,,,416–426,IEEE Press,"Buenos Aires, Argentina",Proceedings of the 39th International Conference on Software Engineering,,2017,9781538638682.0,,https://doi.org/10.1109/ICSE.2017.45;http://dx.doi.org/10.1109/ICSE.2017.45,10.1109/ICSE.2017.45,"Due to the difficulty of repairing defect, many research efforts have been devoted into automatic defect repair. Given a buggy program that fails some test cases, a typical automatic repair technique tries to modify the program to make all tests pass. However, since the test suites in real world projects are usually insufficient, aiming at passing the test suites often leads to incorrect patches. This problem is known as weak test suites or overfitting.In this paper we aim to produce precise patches, that is, any patch we produce has a relatively high probability to be correct. More concretely, we focus on condition synthesis, which was shown to be able to repair more than half of the defects in existing approaches. Our key insight is threefold. First, it is important to know what variables in a local context should be used in an ""if"" condition, and we propose a sorting method based on the dependency relations between variables. Second, we observe that the API document can be used to guide the repair process, and propose document analysis technique to further filter the variables. Third, it is important to know what predicates should be performed on the set of variables, and we propose to mine a set of frequently used predicates in similar contexts from existing projects.Based on the insight, we develop a novel program repair system, ACS, that could generate precise conditions at faulty locations. Furthermore, given the generated conditions are very precise, we can perform a repair operation that is previously deemed to be too overfitting: directly returning the test oracle to repair the defect. Using our approach, we successfully repaired 18 defects on four projects of Defects4J, which is the largest number of fully automatically repaired defects reported on the dataset so far. More importantly, the precision of our approach in the evaluation is 78.3%, which is significantly higher than previous approaches, which are usually less than 40%.",,ICSE '17,,,
Conference Paper,"Song W,Zhang J,Huang J",ServDroid: Detecting Service Usage Inefficiencies in Android Applications,,2019,,,362–373,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering,"Tallinn, Estonia",2019,9781450355728.0,,https://doi.org/10.1145/3338906.3338950;http://dx.doi.org/10.1145/3338906.3338950,10.1145/3338906.3338950,"Services in Android applications are frequently-used components for performing time-consuming operations in the background. While services play a crucial role in the app performance, our study shows that service uses in practice are not as efficient as expected, e.g., they tend to cause unnecessary resource occupation and/or energy consumption. Moreover, as service usage inefficiencies do not manifest with immediate failures, e.g., app crashes, existing testing-based approaches fall short in finding them. In this paper, we identify four anti-patterns of such service usage inefficiency bugs, including premature create, late destroy, premature destroy, and service leak, and present a static analysis technique, ServDroid, to automatically and effectively detect them based on the anti-patterns. We have applied ServDroid to a large collection of popular real-world Android apps. Our results show that, surprisingly, service usage inefficiencies are prevalent and can severely impact the app performance.","service usage inefficiency, static analysis, Android app",ESEC/FSE 2019,,,
Conference Paper,"Mkaouer MW,Kessentini M,Bechikh S,Tauritz DR",Preference-Based Multi-Objective Software Modelling,,2013,,,61–66,IEEE Press,"San Francisco, California",Proceedings of the 1st International Workshop on Combining Modelling and Search-Based Software Engineering,,2013,9781467362849.0,,,,"In this paper, we propose the use of preference-based evolutionary multi-objective optimization techniques (P-EMO) to address various software modelling challenges. P-EMO allows the incorporation of decision maker (i.e., designer) preferences (e.g., quality, correctness, etc.) in multi-objective optimization techniques by restricting the Pareto front to a region of interest easing the decision making task. We discuss the different challenges and potential benefits of P-EMO in software modelling. We report experiments on the use of P-EMO on a well-known modeling problem where very promising results are obtained.","multi-objective optimization, user-preferences, search-based software engineering, modelling, evolutionary computation",CMSBSE '13,,,
Conference Paper,"de Pádua GB,Shang W",Studying the Prevalence of Exception Handling Anti-Patterns,,2017,,,328–331,IEEE Press,"Buenos Aires, Argentina",Proceedings of the 25th International Conference on Program Comprehension,,2017,9781538605356.0,,https://doi.org/10.1109/ICPC.2017.1;http://dx.doi.org/10.1109/ICPC.2017.1,10.1109/ICPC.2017.1,"Modern programming languages, such as Java and C#, typically provide features that handle exceptions. These features separate error-handling code from regular source code and are proven to enhance the practice of software reliability, comprehension, and maintenance. Having acknowledged the advantages of exception handling features, the misuse of them can still cause catastrophic software failures, such as application crash. Prior studies suggested anti-patterns of exception handling; while little knowledge was shared about the prevalence of these anti-patterns. In this paper, we investigate the prevalence of exception-handling anti-patterns. We collected a thorough list of exception anti-patterns from 16 open-source Java and C# libraries and applications using an automated exception flow analysis tool. We found that although exception handling anti-patterns widely exist in all of our subjects, only a few anti-patterns (e.g. Unhandled Exceptions, Catch Generic, Unreachable Handler, Over-catch, and Destructive Wrapping) can be commonly identified. On the other hand, we find that the prevalence of anti-patterns illustrates differences between C# and Java. Our results call for further in-depth analyses on the exception handling practices across different languages.",,ICPC '17,,,
Conference Paper,"Janes A,Lenarduzzi V,Stan AC",A Continuous Software Quality Monitoring Approach for Small and Medium Enterprises,,2017,,,97–100,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 8th ACM/SPEC on International Conference on Performance Engineering Companion,"L'Aquila, Italy",2017,9781450348997.0,,https://doi.org/10.1145/3053600.3053618;http://dx.doi.org/10.1145/3053600.3053618,10.1145/3053600.3053618,"Context: SMEs cannot always afford the effort required for software quality assurance, and therefore there is the need of easy and affordable practices to prevent issues in the software they develop.Object: In this paper we propose an approach to allow SMEs to access SQA practices, using an SQA approach based on a continuous issue and error monitoring and a recommendation system that will suggest quality practices, recommending a set of quality actions based on the issues that previously created errors, so as to help SMEs to maintain quality above a minimum threshold. Method: First, we aim to identify a set of SQA practices applicable in SMEs, based on the main constraints of SMEs and a set of tools and practices to fulfill a complete DevOps pipeline. Second, we aim to define a recommendation system to provide software quality feedback to micro-teams, suggesting which action(s) they should take to maintain a certain quality level and allowing them to remove the most severe issues with the lowest possible effort. Our approach will be validated by a set of local SMEs. Moreover, the tools developed will be published as Open Source.","continuous quality assurance, software maintenance, code smells, software monitoring, anti-patterns",ICPE '17 Companion,,,
Conference Paper,"Johnson M,Stevens P",Confidentiality in the Process of (Model-Driven) Software Development,,2018,,,1–8,Association for Computing Machinery,"New York, NY, USA","Conference Companion of the 2nd International Conference on Art, Science, and Engineering of Programming","Nice, France",2018,9781450355131.0,,https://doi.org/10.1145/3191697.3191714;http://dx.doi.org/10.1145/3191697.3191714,10.1145/3191697.3191714,"Much is now understood about how to develop software that will have good security properties in use. We claim that a topic which needs more attention, in particular from the Bx community, is security, especially confidentiality, in the software development process itself. What is then at issue is not what particular users of the software may be allowed to know, but rather, what particular developers of the software may be allowed to know. How can software development processes guarantee to respect confidentiality without compromising effective development? The question is of general interest across software engineering, but model-driven development (MDD) seems a particularly promising arena in which to address it, because of MDD's focus on separation of concerns. In MDD, different people work with separate models, where (ideally) each model records all and only the information necessary to those who work with it. When necessary, the models are reconciled by bidirectional transformations, which automate a process which would otherwise have to be undertaken manually by the groups of experts meeting and studying both their models in order to bring them back into consistency. In model-driven development confidentiality issues become particularly clear and tractable, and bidirectional transformations have a key technical role. We hope to encourage the community to take up this challenge, and in this paper we begin our own analysis of a selection of the issues, focusing particularly on developing a threat model and some examples of secure restoration of consistency.","Cospan, Model-driven software development, Security, Confidentiality",Programming'18 Companion,,,
Journal Article,"Bavota G,Gethers M,Oliveto R,Poshyvanyk D,Lucia A",Improving Software Modularization via Automated Analysis of Latent Topics and Dependencies,ACM Trans. Softw. Eng. Methodol.,2014,23.0,1.0,,Association for Computing Machinery,"New York, NY, USA",,,2014-02,,1049-331X,https://doi.org/10.1145/2559935;http://dx.doi.org/10.1145/2559935,10.1145/2559935,"Oftentimes, during software maintenance the original program modularization decays, thus reducing its quality. One of the main reasons for such architectural erosion is suboptimal placement of source-code classes in software packages. To alleviate this issue, we propose an automated approach to help developers improve the quality of software modularization. Our approach analyzes underlying latent topics in source code as well as structural dependencies to recommend (and explain) refactoring operations aiming at moving a class to a more suitable package. The topics are acquired via Relational Topic Models (RTM), a probabilistic topic modeling technique. The resulting tool, coined as R3 (Rational Refactoring via RTM), has been evaluated in two empirical studies. The results of the first study conducted on nine software systems indicate that R3 provides a coupling reduction from 10% to 30% among the software modules. The second study with 62 developers confirms that R3 is able to provide meaningful recommendations (and explanations) for move class refactoring. Specifically, more than 70% of the recommendations were considered meaningful from a functional point of view.","relational topic modeling, Software modularization, empirical studies, recommendation system, refactoring",,,,
Conference Paper,"Alexandru CV,Gall HC","Rapid Multi-Purpose, Multi-Commit Code Analysis",,2015,,,635–638,IEEE Press,"Florence, Italy",Proceedings of the 37th International Conference on Software Engineering - Volume 2,,2015,,,,,"Existing code- and software evolution studies typically operate on the scale of a few revisions of a small number of projects, mostly because existing tools are unsuited for performing large-scale studies. We present a novel approach, which can be used to analyze an arbitrary number of revisions of a software project simultaneously and which can be adapted for the analysis of mixed-language projects. It lays the foundation for building high-performance code analyzers for a variety of scenarios. We show that for one particular scenario, namely code metric computation, our prototype outperforms existing tools by multiple orders of magnitude when analyzing thousands of revisions.",,ICSE '15,,,
Conference Paper,"Cho CY,Babi ć D,Shin EC,Song D",Inference and Analysis of Formal Models of Botnet Command and Control Protocols,,2010,,,426–439,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 17th ACM Conference on Computer and Communications Security,"Chicago, Illinois, USA",2010,9781450302456.0,,https://doi.org/10.1145/1866307.1866355;http://dx.doi.org/10.1145/1866307.1866355,10.1145/1866307.1866355,"We propose a novel approach to infer protocol state machines in the realistic high-latency network setting, and apply it to the analysis of botnet Command and Control (C &C) protocols. Our proposed techniques enable an order of magnitude reduction in the number of queries and time needed to learn a botnet C &C protocol compared to classic algorithms (from days to hours for inferring the MegaD C &C protocol). We also show that the computed protocol state machines enable formal analysis for botnet defense, including finding the weakest links in a protocol, uncovering protocol design flaws, inferring the existence of unobservable communication back-channels among botnet servers, and finding deviations of protocol implementations which can be used for fingerprinting. We validate our technique by inferring the protocol state-machine from Postfix's SMTP implementation and comparing the inferred state-machine to the SMTP standard. Further, our experimental results offer new insights into MegaD's C &C, showing our technique can be used as a powerful tool for defense against botnets.","protocol model inference and analysis, response prediction",CCS '10,,,
Conference Paper,"Tufano M,Palomba F,Bavota G,Di Penta M,Oliveto R,De Lucia A,Poshyvanyk D",An Empirical Investigation into the Nature of Test Smells,,2016,,,4–15,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 31st IEEE/ACM International Conference on Automated Software Engineering,"Singapore, Singapore",2016,9781450338455.0,,https://doi.org/10.1145/2970276.2970340;http://dx.doi.org/10.1145/2970276.2970340,10.1145/2970276.2970340,"Test smells have been defined as poorly designed tests and, as reported by recent empirical studies, their presence may negatively affect comprehension and maintenance of test suites. Despite this, there are no available automated tools to support identification and repair of test smells. In this paper, we firstly investigate developers' perception of test smells in a study with 19 participants. The results show that developers generally do not recognize (potentially harmful) test smells, highlighting that automated tools for identifying such smells are much needed. However, to build effective tools, deeper insights into the test smells phenomenon are required. To this aim, we conducted a large-scale empirical investigation aimed at analyzing (i) when test smells occur in source code, (ii) what their survivability is, and (iii) whether their presence is associated with the presence of design problems in production code (code smells). The results indicate that test smells are usually introduced when the corresponding test code is committed in the repository for the first time, and they tend to remain in a system for a long time. Moreover, we found various unexpected relationships between test and code smells. Finally, we show how the results of this study can be used to build effective automated tools for test smell detection and refactoring.","Software Evolution, Test Smells, Mining Software Repositories",ASE 2016,,,
Journal Article,"Ozkaya I,Nord RL,Koziolek H,Avgeriou P","Toward Simpler, Not Simplistic, Quantification of Software Architecture and Metrics: Report on the Second International Workshop on Software Architecture and Metrics",SIGSOFT Softw. Eng. Notes,2015,40.0,5.0,43–46,Association for Computing Machinery,"New York, NY, USA",,,2015-09,,0163-5948,https://doi.org/10.1145/2815021.2815037;http://dx.doi.org/10.1145/2815021.2815037,10.1145/2815021.2815037,"Architects of complex software systems face the challenge of how best to assess the achievement of quality attributes and other key system drivers, how to reveal issues and risks early, and how to make decisions about architecture improvement. Software architecture quality has a large impact on this effort, but it is usually not assessed with quantitative measures. A software architecture metric quantifies architecture quality, value, and cost. While it is highly desirable to improve feedback between development and deployment through measurable means for intrinsic quality, value, and cost, efforts in software architecture quality measurement have lagged behind the body of work focusing on code quality. The goal of the Second International Workshop on Software Architecture and Metrics was to discuss progress on architecture and metrics, measurement, and analysis; to gather empirical evidence on the use and effectiveness of metrics; and to identify priorities for a research agenda.","empirical software engineering, qualitative methods, software maintenance and evolution, technical debt, metrics, software analytics, software quality, Software architecture",,,,
Conference Paper,"Namrud Z,Kpodjedo S,Talhi C",AndroVul: A Repository for Android Security Vulnerabilities,,2019,,,64–71,IBM Corp.,USA,Proceedings of the 29th Annual International Conference on Computer Science and Software Engineering,"Toronto, Ontario, Canada",2019,,,,,"Security issues in mobile apps are increasingly relevant as these software have become part of the daily life for billions. As the dominant OS, Android is a primary target for ill-intentioned programmers willing to exploit vulnerabilities and spread malwares. Significant research has been devoted to the identification of these malwares. The current paper aims to contribute to that research effort, with a focus on providing an additional benchmark of Android vulnerabilities to be used in the detection of malwares. Our proposal is AndroVul, a repository for Android security vulnerabilities, including dangerous permissions, security code smells and dangerous shell commands. Our work builds on AndroZoo, a well known Android app dataset, and proposes data on vulnerabilities for a representative sample of about 16,000 Android apps. Moreover, we briefly describe and make available the scripts we wrote to automate the extraction of security vulnerabilities, given a list of apps; this allows any researcher to readily recreate a custom repository build from his or her apps of interest. Finally, we propose preliminary findings on the effectiveness of the vulnerability metrics present in our dataset, with respect to the detection of malicious apps. Our results show that the collected metrics, as input to even basic classifiers, are enough to achieve competitive results with respect to some recent malware detection works. Overall, Androvul, with its scripts and datasets, is intended as a starting package for mobile security researchers interested in jump-starting their investigations.","mobile security, mobile computing, static analysis, reverse engineering",CASCON '19,,,
Conference Paper,"Behnamghader P,Meemeng P,Fostiropoulos I,Huang D,Srisopha K,Boehm B",A Scalable and Efficient Approach for Compiling and Analyzing Commit History,,2018,,,,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 12th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement,"Oulu, Finland",2018,9781450358231.0,,https://doi.org/10.1145/3239235.3239237;http://dx.doi.org/10.1145/3239235.3239237,10.1145/3239235.3239237,"Background: Researchers oftentimes measure quality metrics only in the changed files when analyzing software evolution over commit-history. This approach is not suitable for compilation and using program analysis techniques that require byte-code. At the same time, compiling the whole software not only is costly but may also leave us with many uncompilable and unanalyzed revisions. Aims: We intend to demonstrate if analyzing changes in a module results in achieving a high compilation ratio and a better understanding of software quality evolution. Method: We conduct a large-scale multi-perspective empirical study on 37838 distinct revisions of the core module of 68 systems across Apache, Google, and Netflix to assess their compilability and identify when the software is uncompilable as a result of a developer's fault. We study the characteristics of uncompilable revisions and analyze compilable ones to understand the impact of developers on software quality. Results: We achieve high compilation ratios: 98.4% for Apache, 99.0% for Google, and 94.3% for Netflix. We identify 303 sequences of uncompile commits and create a model to predict uncompilability based on commit metadata with an F1-score of 0.89 and an AUC of 0.96. We identify statistical differences between the impact of affiliated and external developers of organizations. Conclusions: Focusing on a module results in a more complete and accurate software evolution analysis, reduces the cost and complexity, and facilitates manual inspection.","software maintainbaility evolution, software technical debt, mining software repositories, software compilability",ESEM '18,,,
Conference Paper,"El-Dahshan KA,Elsayed EK,Ghannam NE",Comparative Study for Detecting Mobile Application's Anti-Patterns,,2019,,,1–8,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 2019 8th International Conference on Software and Information Engineering,"Cairo, Egypt",2019,9781450361057.0,,https://doi.org/10.1145/3328833.3328834;http://dx.doi.org/10.1145/3328833.3328834,10.1145/3328833.3328834,"Software design has a main impact in the quality of the software systems. Anti-patterns are shortcomings exist in the software designs and impact negatively software quality. Mobile applications (apps) with anti-patterns have bad quality and short lifetime. Many empirical studies have assessed that the anti-patterns have a negative impact on change-proneness, fault-proneness, memory consumption and energy efficiency. In addition to that, many studies showed that there was an improvement in the user interface and memory performance of mobile apps when correcting Android anti-patterns. The aim of our research is choosing the suitable UML modeling environment to detect Mobile applications' anti-patterns via reverse engineering. So, in this research, first we present a comparative study between nine UML tools for determining the tools that have the functionality for (reverse, forward) engineering and have the ability for validating the model against the anti-patterns. Second, we apply our proposed method to generate the class diagram model of the apps through decoding the Java source code and detects the design anti-patterns in the model. For validating the proposed method, we applied it in twenty-nine Mobile apps which were downloaded from APKmirror. The proposed method detects and treats ten anti-patterns which have appeared 749 times in the twenty-nine apps.","Mobile Applications, Reverse Engineering, Anti-patterns, Modelio, UML",ICSIE '19,,,
Conference Paper,"Paternò F,Schiavone AG,Conti A",Customizable Automatic Detection of Bad Usability Smells in Mobile Accessed Web Applications,,2017,,,,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 19th International Conference on Human-Computer Interaction with Mobile Devices and Services,"Vienna, Austria",2017,9781450350754.0,,https://doi.org/10.1145/3098279.3098558;http://dx.doi.org/10.1145/3098279.3098558,10.1145/3098279.3098558,"Remote usability evaluation enables the possibility of analysing users' behaviour in their daily settings. We present a method and an associated tool able to identify potential usability issues through the analysis of client-side logs of mobile Web interactions. Such log analysis is based on the identification of specific usability smells. We describe an example set of bad usability smells, and how they are detected. The tool also allows evaluators to add new usability smells not included in the original set. We also report on the tool use in analysing the usability of a real, widely used application accessed by forty people through their smartphones whenever and wherever they wanted.","usability bad smells, web mobile application log analysis, remote usability evaluation",MobileHCI '17,,,
Conference Paper,"Yongting Y,Dongsheng L,Liping Z",Detection Technology and Application of Clone Refactoring,,2018,,,128–133,Association for Computing Machinery,"New York, NY, USA","Proceedings of the 2018 2nd International Conference on Management Engineering, Software Engineering and Service Sciences","Wuhan, China",2018,9781450354318.0,,https://doi.org/10.1145/3180374.3181332;http://dx.doi.org/10.1145/3180374.3181332,10.1145/3180374.3181332,"Clone code is a similar part of code, such code may seriously affect the maintainability of the software and reduce the quality of code. In order to eliminate the negative impact of clone code, the researchers offer many methods to eliminate clone code and refactoring is an important part in them. In this paper, several different methods and tools are introduced, the advantages and disadvantages of various methods and tools are summarized, apart from this, we also describe the important application of clone refactoring in maintaining the quality of code, at last, we discussed the challenges of the current clone refactoring detection.","Maintainability, Refactoring, Clone Code, Software Quality",ICMSS 2018,,,
Conference Paper,"Destefanis G,Tonelli R,Concas G,Marchesi M",An Analysis of Anti-Micro-Patterns Effects on Fault-Proneness in Large Java Systems,,2012,,,1251–1253,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 27th Annual ACM Symposium on Applied Computing,"Trento, Italy",2012,9781450308571.0,,https://doi.org/10.1145/2245276.2231972;http://dx.doi.org/10.1145/2245276.2231972,10.1145/2245276.2231972,"Micro patterns are similar to design patterns, but are at a lower level of abstraction, closer to the implementation. Anti patterns are micro patterns not respecting the prescriptions of good Object Oriented programming practices. In this paper, we use the definitions introduced by Arcelli and Maggioni [3] in order to study the evolution of five particular micro patterns (anti patterns) in different releases of the Eclipse and NetBeans systems, and the correlations between anti patterns and faults. Our analysis confirms previous findings regarding the high coverage of micro patterns onto the system classes, and show that anti patterns not only represent bad Object Oriented programming practices, but may also be associated to the production of lower quality software, since they present a fault proneness significantly enhanced.","anti patterns, object-oriented programming, micro patterns, software faults, metrics",SAC '12,,,
Conference Paper,"Schulze S,Kuhlemann M,Rosenmüller M",Towards a Refactoring Guideline Using Code Clone Classification,,2008,,,,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 2nd Workshop on Refactoring Tools,"Nashville, Tennessee",2008,9781605583396.0,,https://doi.org/10.1145/1636642.1636648;http://dx.doi.org/10.1145/1636642.1636648,10.1145/1636642.1636648,"Evolution of software often decreases desired properties like readability and maintainability of the evolved code. The process of refactoring aims at increasing the same desired properties by restructuring the code. New paradigms like AOP allow aspect-oriented refactorings as counterparts of object-oriented refactoring with the same aim. However, it is not obvious to the user, when to use which paradigm for achieving certain goals. In this paper we present an approach of code clone classification, which advises the developer when to use a respective refactoring technique or concept.","refactoring, object-oriented programming, aspect-oriented programming, clone detection",WRT '08,,,
Conference Paper,"Sheppard JW,Simpson WR",Functional Path Analysis: An Approach to Software Verification,,1988,,,266–272,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 1988 ACM Sixteenth Annual Conference on Computer Science,"Atlanta, Georgia, USA",1988,9780897912600.0,,https://doi.org/10.1145/322609.322652;http://dx.doi.org/10.1145/322609.322652,10.1145/322609.322652,"A widely accepted approach to software development involves successive refinement of design and requirements specifications from a top-level description of the system down to the code level. As the system is refined, it is verified at each phase of development before proceeding to the next phase. In the past, several tools and techniques have been developed to assist in the development and verification process.Tools have been developed and have been in use for many years to examine the testability and system information flow in hardware systems. These problems are approached as a knowledge base verification and validation problem. Several strong analogues exist between hardware and software systems. However, several fundamental differences exist which affect the approach to modeling and verifying the system.This paper briefly describes past efforts in verifying hardware and software systems and then presents a preliminary synthesis and extension of these past efforts to the software verification problem. We then conclude with an assessment of our current status and note future directions and recommendations for research in this area.",,CSC '88,,,
Conference Paper,"Efstathiou V,Chatzilenas C,Spinellis D",Word Embeddings for the Software Engineering Domain,,2018,,,38–41,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 15th International Conference on Mining Software Repositories,"Gothenburg, Sweden",2018,9781450357166.0,,https://doi.org/10.1145/3196398.3196448;http://dx.doi.org/10.1145/3196398.3196448,10.1145/3196398.3196448,"The software development process produces vast amounts of textual data expressed in natural language. Outcomes from the natural language processing community have been adapted in software engineering research for leveraging this rich textual information; these include methods and readily available tools, often furnished with pre-trained models. State of the art pre-trained models however, capture general, common sense knowledge, with limited value when it comes to handling data specific to a specialized domain. There is currently a lack of domain-specific pre-trained models that would further enhance the processing of natural language artefacts related to software engineering. To this end, we release a word2vec model trained over 15GB of textual data from Stack Overflow posts. We illustrate how the model disambiguates polysemous words by interpreting them within their software engineering context. In addition, we present examples of fine-grained semantics captured by the model, that imply transferability of these results to diverse, targeted information retrieval tasks in software engineering and motivate for further reuse of the model.","word2vec, stack overflow, skip-gram, natural language processing",MSR '18,,,
Conference Paper,"Zhou Z,Xu J,Balasubramanian A,Porter DE",A Survey of Patterns for Adapting Smartphone App UIs to Smart Watches,,2020,,,,Association for Computing Machinery,"New York, NY, USA",22nd International Conference on Human-Computer Interaction with Mobile Devices and Services,"Oldenburg, Germany",2020,9781450375160.0,,https://doi.org/10.1145/3379503.3403564;http://dx.doi.org/10.1145/3379503.3403564,10.1145/3379503.3403564,"Wearable devices, such as smart watches and fitness trackers are growing in popularity, creating a need for application developers to adapt or extend a UI, typically from a smartphone, onto these devices. Wearables generally have a smaller form factor than a phone; thus, porting an app to the watch necessarily involves reworking the UI. An open problem is identifying best practices for adapting UIs to wearable devices. This paper contributes a study and data set of the state of practice in UI adaptation for wearables. We automatically extract UI designs from a set of 101 popular Android apps that have both a phone and watch version, and manually label how each UI element, as well as how screens in the app, are translated from the phone to the wearable. The paper identifies trends in adaptation strategies and presents design guidelines. We expect that the UI adaptation strategies identified in this paper can have wide-ranging impacts for future research and identifying best practices in this space, such as grounding future user studies that evaluate which strategies improve user satisfaction or automatically adapting UIs.","smartphones, dataset, wearable devices, UI design patterns, smartwatches",MobileHCI '20,,,
Conference Paper,"Wagner I,Bertacco V,Austin T",Shielding against Design Flaws with Field Repairable Control Logic,,2006,,,344–347,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 43rd Annual Design Automation Conference,"San Francisco, CA, USA",2006,9781595933812.0,,https://doi.org/10.1145/1146909.1146998;http://dx.doi.org/10.1145/1146909.1146998,10.1145/1146909.1146998,"Correctness is a paramount attribute of any microprocessor design; however, without novel technologies to tame the increasing complexity of design verification, the amount of bugs that escape into silicon will only grow in the future. In this paper, we propose a novel hardware patching mechanism that can detect design errors which escaped the verification process, and can correct them directly in the field. We accomplish this goal through a simple field-programmable state matcher, which can identify erroneous configurations in the processor's control state and switch the processor into formally-verified degraded performance mode, once a ""match"" occurs. When the instructions exposing the design flaw are committed, the processor is switched back to normal mode. We show that our approach can detect and correct infrequently-occurring errors with almost no performance impact and has approximately 2% area overhead.","hardware patching, processor verification",DAC '06,,,
Conference Paper,"Becker C,Fagerholm F,Mohanani R,Chatzigeorgiou A",Temporal Discounting in Technical Debt: How Do Software Practitioners Discount the Future?,,2019,,,23–32,IEEE Press,"Montreal, Quebec, Canada",Proceedings of the Second International Conference on Technical Debt,,2019,,,https://doi.org/10.1109/TechDebt.2019.00011;http://dx.doi.org/10.1109/TechDebt.2019.00011,10.1109/TechDebt.2019.00011,"Technical Debt management decisions always imply a trade-off among outcomes at different points in time. In such intertemporal choices, distant outcomes are often valued lower than close ones, a phenomenon known as temporal discounting. Technical Debt research largely develops prescriptive approaches for how software engineers should make such decisions. Few have studied how they actually make them. This leaves open central questions about how software practitioners make decisions.This paper investigates how software practitioners discount uncertain future outcomes and whether they exhibit temporal discounting. We adopt experimental methods from intertemporal choice, an active area of research. We administered an online questionnaire to 33 developers from two companies in which we presented choices between developing a feature and making a longer-term investment in architecture. The results show wide-spread temporal discounting with notable differences in individual behavior. The results are consistent with similar studies in consumer behavior and raise a number of questions about the causal factors that influence temporal discounting in software engineering. As the first empirical study on intertemporal choice in SE, the paper establishes an empirical basis for understanding how software developers approach intertemporal choice and provides a blueprint for future studies.","questionnaire, technical debt, psychology, intertemporal choice, decision making, behavioral software engineering, temporal discounting, technical debt management",TechDebt '19,,,
Conference Paper,"Barcelona-Pons D,Sánchez-Artigas M,París G,Sutra P,García-López P",On the FaaS Track: Building Stateful Distributed Applications with Serverless Architectures,,2019,,,41–54,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 20th International Middleware Conference,"Davis, CA, USA",2019,9781450370097.0,,https://doi.org/10.1145/3361525.3361535;http://dx.doi.org/10.1145/3361525.3361535,10.1145/3361525.3361535,"Serverless computing is an emerging paradigm that greatly simplifies the usage of cloud resources and suits well to many tasks. Most notably, Function-as-a-Service (FaaS) enables programmers to develop cloud applications as individual functions that can run and scale independently. Yet, due to the disaggregation of storage and compute resources in FaaS, applications that require fine-grained support for mutable state and synchronization, such as machine learning and scientific computing, are hard to build.In this work, we present Crucial, a system to program highly-concurrent stateful applications with serverless architectures. Its programming model keeps the simplicity of FaaS and allows to port effortlessly multi-threaded algorithms to this new environment. Crucial is built upon the key insight that FaaS resembles to concurrent programming at the scale of a data center. As a consequence, a distributed shared memory layer is the right answer to the need for fine-grained state management and coordination in serverless. We validate our system with the help of micro-benchmarks and various applications. In particular, we implement two common machine learning algorithms: k-means clustering and logistic regression. For both cases, Crucial obtains superior or comparable performance to an equivalent Spark cluster.","in-memory, FaaS, stateful, Serverless, synchronization",Middleware '19,,,
Conference Paper,"Horschig S,Mattis T,Hirschfeld R",Do Java Programmers Write Better Python? Studying off-Language Code Quality on GitHub,,2018,,,127–134,Association for Computing Machinery,"New York, NY, USA","Conference Companion of the 2nd International Conference on Art, Science, and Engineering of Programming","Nice, France",2018,9781450355131.0,,https://doi.org/10.1145/3191697.3214341;http://dx.doi.org/10.1145/3191697.3214341,10.1145/3191697.3214341,"There are style guides and best practices for many programming languages. Their goal is to promote uniformity and readability of code, consequentially reducing the chance of errors. While programmers who are frequently using the same programming language tend to internalize most of its best practices eventually, little is known about what happens when they casually switch languages and write code in a less familiar language. Insights into the factors that lead to coding convention violations could help to improve tutorials for programmers switching languages, make teachers aware of mistakes they might expect depending on what language students have been using before, or influence the order in which programming languages are taught. To approach this question, we make use of a large-scale data set representing a major part of the open source development activity happening on GitHub. In this data set, we search for Java and C++ programmers that occasionally program Python and study their Python code quality using a lint tool. Comparing their defect rates to those from Python programmers reveals significant effects in both directions: We observe that some of Python's best practices have more widespread adoption among Java and C++ programmers than Python experts. At the same time, python-specific coding conventions, especially indentation, scoping, and the use of semicolons, are violated more frequently. We conclude that programming off-language is not generally associated with better or worse code quality, but individual coding conventions are violated more or less frequently depending on whether they are more universal or language-specific. We intend to motivate a discussion and more research on what causes these effects, how we can mitigate or use them for good, and which related effects can be studied using the presented data set.","code quality, explorative study, github, best practices, lint",Programming'18 Companion,,,
Conference Paper,"Trubiani C,Koziolek A",Detection and Solution of Software Performance Antipatterns in Palladio Architectural Models,,2011,,,19–30,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 2nd ACM/SPEC International Conference on Performance Engineering,"Karlsruhe, Germany",2011,9781450305198.0,,https://doi.org/10.1145/1958746.1958755;http://dx.doi.org/10.1145/1958746.1958755,10.1145/1958746.1958755,"Antipatterns are conceptually similar to patterns in that they document recurring solutions to common design problems. Performance Antipatterns document, from a performance perspective, common mistakes made during software development as well as their solutions. The definition of performance antipatterns concerns software properties that can include static, dynamic, and deployment aspects. Currently, such knowledge is only used by domain experts; the problem of automatically detecting and solving antipatterns within an architectural model had not yet been empirically addressed. In this paper we present an approach to automatically detect and solve software performance antipatterns within the Palladio architectural models: the detection of an antipattern provides a software performance feedback to designers, since it suggests the architectural alternatives to overcome specific performance problems. We implemented the approach and a case study is presented to demonstrate its validity. The system performance under study has been improved by 50% with the use of antipatterns' solutions.",,ICPE '11,,,
Conference Paper,"Xia X,Wan Z,Kochhar PS,Lo D",How Practitioners Perceive Coding Proficiency,,2019,,,924–935,IEEE Press,"Montreal, Quebec, Canada",Proceedings of the 41st International Conference on Software Engineering,,2019,,,https://doi.org/10.1109/ICSE.2019.00098;http://dx.doi.org/10.1109/ICSE.2019.00098,10.1109/ICSE.2019.00098,"Coding proficiency is essential to software practitioners. Unfortunately, our understanding on coding proficiency often translates to vague stereotypes, e.g., ""able to write good code"". The lack of specificity hinders employers from measuring a software engineer's coding proficiency, and software engineers from improving their coding proficiency skills. This raises an important question: what skills matter to improve one's coding proficiency. To answer this question, we perform an empirical study by surveying 340 software practitioners from 33 countries across 5 continents. We first identify 38 coding proficiency skills grouped into nine categories by interviewing 15 developers from three companies. We then ask our survey respondents to rate the level of importance for these skills, and provide rationales of their ratings. Our study highlights a total of 21 important skills that receive an average rating of 4.0 and above (important and very important), along with rationales given by proponents and dissenters. We discuss implications of our findings to researchers, educators, and practitioners.",,ICSE '19,,,
Conference Paper,"Lee JT,Liu SF",Using 3D Carbon Dioxide Slab Laser Printing Processing for Craft Teaching in Elementary School,,2018,,,347–351,Association for Computing Machinery,"New York, NY, USA","Proceedings of the 2018 VII International Conference on Network, Communication and Computing","Taipei City, Taiwan",2018,9781450365536.0,,https://doi.org/10.1145/3301326.3301346;http://dx.doi.org/10.1145/3301326.3301346,10.1145/3301326.3301346,"Purpose: To provide the laser processing data of ceramic craft materials in elementary school.Method: Manufacturing and measuring samples of ceramic processing.Finding: Quickly hold the characteristics of ceramics materialsValue: Providing CO2 laser data settings for teaching, and to reduce the difficulties of material processing data set.Range: Data collection of traditional glaze and laser processing","QFD (Quality function Deployment), craft ceramic, material processing, Carbon dioxide slab laser",ICNCC 2018,,,
Conference Paper,"Krinke J,Gold N,Jia Y,Binkley D",Distinguishing Copies from Originals in Software Clones,,2010,,,41–48,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 4th International Workshop on Software Clones,"Cape Town, South Africa",2010,9781605589800.0,,https://doi.org/10.1145/1808901.1808907;http://dx.doi.org/10.1145/1808901.1808907,10.1145/1808901.1808907,"Cloning is widespread in today's systems where automated assistance is required to locate cloned code. Although the evolution of clones has been studied for many years, no attempt has been made so far to automatically distinguish the original source code leading to cloned copies. This paper presents an approach to classify the clones of a clone pair based on the version information available in version control systems. This automatic classification attempts to distinguish the original from the copy. It allows for the fact that the clones may be modified and thus consist of lines coming from different versions. An evaluation, based on two case studies, shows that when comments are ignored and a small tolerance is accepted, for the majority of clone pairs the proposed approach can automatically distinguish between the original and the copy.","mining software archives, clone detection, software evolution",IWSC '10,,,
Conference Paper,"Palomba F,Bavota G,Di Penta M,Oliveto R,De Lucia A,Poshyvanyk D",Detecting Bad Smells in Source Code Using Change History Information,,2013,,,268–278,IEEE Press,"Silicon Valley, CA, USA",Proceedings of the 28th IEEE/ACM International Conference on Automated Software Engineering,,2013,9781479902156.0,,https://doi.org/10.1109/ASE.2013.6693086;http://dx.doi.org/10.1109/ASE.2013.6693086,10.1109/ASE.2013.6693086,"Code smells represent symptoms of poor implementation choices. Previous studies found that these smells make source code more difficult to maintain, possibly also increasing its fault-proneness. There are several approaches that identify smells based on code analysis techniques. However, we observe that many code smells are intrinsically characterized by how code elements change over time. Thus, relying solely on structural information may not be sufficient to detect all the smells accurately.We propose an approach to detect five different code smells, namely Divergent Change, Shotgun Surgery, Parallel Inheritance, Blob, and Feature Envy, by exploiting change history information mined from versioning systems. We applied approach, coined as HIST (Historical Information for Smell deTection), to eight software projects written in Java, and wherever possible compared with existing state-of-the-art smell detectors based on source code analysis. The results indicate that HIST's precision ranges between 61% and 80%, and its recall ranges between 61% and 100%. More importantly, the results confirm that HIST is able to identify code smells that cannot be identified through approaches solely based on code analysis.","change history information, code smells",ASE'13,,,
Conference Paper,"Martínez-Fernández S,Jovanovic P,Franch X,Jedlitschka A",Towards Automated Data Integration in Software Analytics,,2018,,,,Association for Computing Machinery,"New York, NY, USA",Proceedings of the International Workshop on Real-Time Business Intelligence and Analytics,"Rio de Janeiro, Brazil",2018,9781450366076.0,,https://doi.org/10.1145/3242153.3242159;http://dx.doi.org/10.1145/3242153.3242159,10.1145/3242153.3242159,"Software organizations want to be able to base their decisions on the latest set of available data and the real-time analytics derived from them. In order to support ""real-time enterprise"" for software organizations and provide information transparency for diverse stakeholders, we integrate heterogeneous data sources about software analytics, such as static code analysis, testing results, issue tracking systems, network monitoring systems, etc. To deal with the heterogeneity of the underlying data sources, we follow an ontology-based data integration approach in this paper and define an ontology that captures the semantics of relevant data for software analytics. Furthermore, we focus on the integration of such data sources by proposing two approaches: a static and a dynamic one. We first discuss the current static approach with a predefined set of analytic views representing software quality factors and further envision how this process could be automated in order to dynamically build custom user analysis using a semi-automatic platform for managing the lifecycle of analytics infrastructures.","real-time enterprise, software analytics, Data integration, ontology",BIRTE '18,,,
Conference Paper,"Lehrig S,Becker S","The CloudScale Method for Software Scalability, Elasticity, and Efficiency Engineering: A Tutorial",,2015,,,329–331,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 6th ACM/SPEC International Conference on Performance Engineering,"Austin, Texas, USA",2015,9781450332484.0,,https://doi.org/10.1145/2668930.2688818;http://dx.doi.org/10.1145/2668930.2688818,10.1145/2668930.2688818,"In cloud computing, software engineers design systems for virtually unlimited resources that cloud providers account on a pay-per-use basis. Elasticity management systems provision these resource autonomously to deal with changing workloads. Such workloads call for new objective metrics allowing engineers to quantify quality properties like scalability, elasticity, and efficiency. However, software engineers currently lack engineering methods that aid them in engineering their software regarding such properties. Therefore, the CloudScale project developed tools for such engineering tasks. These tools cover reverse engineering of architectural models from source code, editors for manual design/adaption of such models, as well as tools for the analysis of modeled and operating software regarding scalability, elasticity, and efficiency. All tools are interconnected via ScaleDL, a common architectural language, and the CloudScale Method that leads through the engineering process. In this tutorial, we execute our method step-by-step such that every tool and ScaleDL are briefly introduced.","scalability, efficiency, metrics, tutorial, cloudscale, software analysis, method, cloud computing, elasticity, engineering",ICPE '15,,,
Conference Paper,"Guimaraes E,Garcia A,Figueiredo E,Cai Y",Prioritizing Software Anomalies with Software Metrics and Architecture Blueprints: A Controlled Experiment,,2013,,,82–88,IEEE Press,"San Francisco, California",Proceedings of the 5th International Workshop on Modeling in Software Engineering,,2013,9781467364478.0,,,,"According to recent studies, architecture degradation is to a large extent a consequence of the introduction of code anomalies as the system evolves. Many approaches have been proposed for detecting code anomalies, but none of them has been efficient on prioritizing code anomalies that represent real problems in the architecture design. In this sense, our work aims to investigate whether the prioritization of instances of three types of classical code anomalies, Divergent Change, God Class and Shotgun Surgery, can be improved when supported by architecture blueprints. These blueprints are informal models often available in software projects, and they are used to capture key architecture decisions. Moreover, we are also investigating what information may be useful in the design blueprints to help developers on prioritizing the most critical software anomalies. In many cases, developers indicated that it would be interesting the insertion of additional information on the blueprints in order to detect architecturally-relevant anomalies.","architectural design, software metrics, design blueprint, code anomaly",MiSE '13,,,
Conference Paper,"Zazworka N,Spínola RO,Vetro' A,Shull F,Seaman C",A Case Study on Effectively Identifying Technical Debt,,2013,,,42–47,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 17th International Conference on Evaluation and Assessment in Software Engineering,"Porto de Galinhas, Brazil",2013,9781450318488.0,,https://doi.org/10.1145/2460999.2461005;http://dx.doi.org/10.1145/2460999.2461005,10.1145/2460999.2461005,"Context: The technical debt (TD) concept describes a tradeoff between short-term and long-term goals in software development. While it is highly useful as a metaphor, it has utility beyond the facilitation of discussion, to inspire a useful set of methods and tools that support the identification, measurement, monitoring, management, and payment of TD. Objective: This study focuses on the identification of TD. We evaluate human elicitation of TD and compare it to automated identification. Method: We asked a development team to identify TD items in artifacts from a software project on which they were working. We provided the participants with a TD template and a short questionnaire. In addition, we also collected the output of three tools to automatically identify TD and compared it to the results of human elicitation. Results: There is little overlap between the TD reported by different developers, so aggregation, rather than consensus, is an appropriate way to combine TD reported by multiple developers. The tools used are especially useful for identifying defect debt but cannot help in identifying many other types of debt, so involving humans in the identification process is necessary. Conclusion: We have conducted a case study that focuses on the practical identification of TD, one area that could be facilitated by tools and techniques. It contributes to the TD landscape, which depicts an understanding of relationships between different types of debt and how they are best discovered.","technical debt, software maintenance, code smells, automatic static analysis",EASE '13,,,
Conference Paper,"Mannan UA,Ahmed I,Jensen C,Sarma A",On the Relationship between Design Discussions and Design Quality: A Case Study of Apache Projects,,2020,,,543–555,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering,"Virtual Event, USA",2020,9781450370431.0,,https://doi.org/10.1145/3368089.3409707;http://dx.doi.org/10.1145/3368089.3409707,10.1145/3368089.3409707,"Open design discussion is a primary mechanism through which open source projects debate, make and document design decisions. However, there are open questions regarding how design discussions are conducted and what effect they have on the design quality of projects. Recent work has begun to investigate design discussions, but has thus far focused on a single communication channel, whereas many projects use multiple channels. In this study, we examine 37 Apache projects and their design discussions, the project’s design quality evolution, and the relationship between design discussion and design quality. A mixed method empirical analysis (data mining and a survey of 130 developers) shows that: I) 89.51% of all design discussions occur in project mailing list, II) both core and non-core developers participate in design discussions, but core developers implement more design related changes (67.06%), and III) the correlation between design discussions and design quality is small. We conclude the paper with several observations that form the foundation for future research and development.","Code smells, Design quality, Design discussion, Empirical Analysis",ESEC/FSE 2020,,,
Conference Paper,"Goldsmith SF,O'Callahan R,Aiken A",Relational Queries over Program Traces,,2005,,,385–402,Association for Computing Machinery,"New York, NY, USA","Proceedings of the 20th Annual ACM SIGPLAN Conference on Object-Oriented Programming, Systems, Languages, and Applications","San Diego, CA, USA",2005,9781595930316.0,,https://doi.org/10.1145/1094811.1094841;http://dx.doi.org/10.1145/1094811.1094841,10.1145/1094811.1094841,"Instrumenting programs with code to monitor runtime behavior is a common technique for profiling and debugging. In practice, instrumentation is either inserted manually by programmers, or automatically by specialized tools that monitor particular properties. We propose Program Trace Query Language (PTQL), a language based on relational queries over program traces, in which programmers can write expressive, declarative queries about program behavior. We also describe our compiler, Partiqle. Given a PTQL query and a Java program, Partiqle instruments the program to execute the query on-line. We apply several PTQL queries to a set of benchmark programs, including the Apache Tomcat Web server. Our queries reveal significant performance bugs in the jack SpecJVM98 benchmark, in Tomcat, and in the IBM Java class library, as well as some correct though uncomfortably subtle code in the Xerces XML parser. We present performance measurements demonstrating that our prototype system has usable performance.","relational, program trace query language, PTQL, partiqle",OOPSLA '05,,,
Journal Article,"Goldsmith SF,O'Callahan R,Aiken A",Relational Queries over Program Traces,SIGPLAN Not.,2005,40.0,10.0,385–402,Association for Computing Machinery,"New York, NY, USA",,,2005-10,,0362-1340,https://doi.org/10.1145/1103845.1094841;http://dx.doi.org/10.1145/1103845.1094841,10.1145/1103845.1094841,"Instrumenting programs with code to monitor runtime behavior is a common technique for profiling and debugging. In practice, instrumentation is either inserted manually by programmers, or automatically by specialized tools that monitor particular properties. We propose Program Trace Query Language (PTQL), a language based on relational queries over program traces, in which programmers can write expressive, declarative queries about program behavior. We also describe our compiler, Partiqle. Given a PTQL query and a Java program, Partiqle instruments the program to execute the query on-line. We apply several PTQL queries to a set of benchmark programs, including the Apache Tomcat Web server. Our queries reveal significant performance bugs in the jack SpecJVM98 benchmark, in Tomcat, and in the IBM Java class library, as well as some correct though uncomfortably subtle code in the Xerces XML parser. We present performance measurements demonstrating that our prototype system has usable performance.","relational, PTQL, partiqle, program trace query language",,,,
Conference Paper,"Al-omari F,Roy CK",Is Code Cloning in Games Really Different?,,2016,,,1512–1519,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 31st Annual ACM Symposium on Applied Computing,"Pisa, Italy",2016,9781450337397.0,,https://doi.org/10.1145/2851613.2851792;http://dx.doi.org/10.1145/2851613.2851792,10.1145/2851613.2851792,"Since there are a tremendous number of similar functionalities related to images, 3D graphics, sounds, and script in games software, there is a common wisdom that there might be more cloned code in games compared to traditional software. Also, there might be more cloned code across games since many of these games share similar strategies and libraries. In this study, we attempt to investigate whether such statements are true by conducting a large empirical study using 32 games and 9 non-games software, written in three different programming languages C, Java, and C#, for the case of both exact and near-miss clones. Using a hybrid clone detection tool NiCad and a visualization tool VisCad, we examine and compare the cloning status in them and compare it to the non-games, and examine the cloned methods across game engines. The results show that code reuse in open source games is much different from that of other software systems. Specifically, in contrast to the common wisdom, there are fewer function clones in game open source comparing to non-game open source software systems. Similar to non-games open source, we observed that cloning status changes between different programming languages of the games. In addition, there are very fewer clones across games and mostly no clones (no code reuse) across different game engines. But clones exist heavily across recreated (cloned) games.","open source games, software clones, game clones",SAC '16,,,
Conference Paper,"Nagappan M,Murphy B,Vouk M",Which Code Construct Metrics Are Symptoms of Post Release Failures?,,2011,,,65–68,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 2nd International Workshop on Emerging Trends in Software Metrics,"Waikiki, Honolulu, HI, USA",2011,9781450305938.0,,https://doi.org/10.1145/1985374.1985389;http://dx.doi.org/10.1145/1985374.1985389,10.1145/1985374.1985389,"Software metrics, such as code complexity metrics and code churn metrics, are used to predict failures. In this paper we study a specific set of metrics called code construct metrics and relate them to post release failures. We use the values of the code construct metrics for each file to characterize that file. We analyze the code construct metrics along with the post release failure data on the files (that splits the files into two classes: files with post release failures and files without post release failures). In our analysis we compare a file with post release failure to a set of files without post release failures, that have similar characteristics. In our comparison we identify which code construct metric, more often than the others, differs the most between these two classes of files. The goal of our research is to find out which code construct metrics can perhaps be used as symptoms of post release failures. In this paper we analyzed the code construct metrics of Eclipse 2.0, 2.1, and 3.0. Our results indicate that MethodInvocation, QualifiedName, and SimpleName, are the code constructs that differentiates the two classes of files the most and hence are the key symptoms/indicators of a file with post release failures in these versions of Eclipse.","empirical analysis, code construct metrics, post release failures",WETSoM '11,,,
Conference Paper,"Johnson B,Song Y,Murphy-Hill E,Bowdidge R",Why Don't Software Developers Use Static Analysis Tools to Find Bugs?,,2013,,,672–681,IEEE Press,"San Francisco, CA, USA",Proceedings of the 2013 International Conference on Software Engineering,,2013,9781467330763.0,,,,"Using static analysis tools for automating code inspections can be beneficial for software engineers. Such tools can make finding bugs, or software defects, faster and cheaper than manual inspections. Despite the benefits of using static analysis tools to find bugs, research suggests that these tools are underused. In this paper, we investigate why developers are not widely using static analysis tools and how current tools could potentially be improved. We conducted interviews with 20 developers and found that although all of our participants felt that use is beneficial, false positives and the way in which the warnings are presented, among other things, are barriers to use. We discuss several implications of these results, such as the need for an interactive mechanism to help developers fix defects.",,ICSE '13,,,
Conference Paper,"Miura K,McIntosh S,Kamei Y,Hassan AE,Ubayashi N",The Impact of Task Granularity on Co-Evolution Analyses,,2016,,,,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 10th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement,"Ciudad Real, Spain",2016,9781450344272.0,,https://doi.org/10.1145/2961111.2962607;http://dx.doi.org/10.1145/2961111.2962607,10.1145/2961111.2962607,"Background: Substantial research in the software evolution field aims to recover knowledge about development from the project history that is archived in repositories, such as a Version Control System (VCS). However, the data that is archived in these repositories can be analyzed at different levels of granularity. Although software evolution is a well-studied phenomenon at the revision-level, revisions may be too fine-grained to accurately represent development tasks.Aim: In this paper, we set out to understand the impact that the revision granularity has on co-change analyses.Method: We conduct an empirical study of 14 open source systems that are developed by the Apache Software Foundation. To understand the impact that the revision granularity may have on co-change activity, we study work items, i.e., logical groups of revisions that address a single issue.Results: We find that work item grouping has the potential to impact co-change activity, since 29% of work items consist of 2 or more revisions in 7 of the 14 studied systems. Deeper quantitative analysis shows that, in 7 of the 14 studied systems: (1) 11% of largest work items are entirely composed of small revisions, and would be missed by traditional approaches to filter or analyze large changes, (2) 83% of revisions that co-change under a single work item cannot be grouped using the typical configuration of the sliding time window technique and (3) 48% of work items that involve multiple developers cannot be grouped at the revision-level.Conclusions: Since the work item granularity is the natural means that practitioners use to separate development tasks, future software evolution studies, especially co-change analyses, should be conducted at the work item level.","Task granularity, software evolution, co-change analysis",ESEM '16,,,
Journal Article,"Sharma V,Rizvi SA,Sharma A",Software Licenses - A Tool to Control Distribution of Software,SIGSOFT Softw. Eng. Notes,2013,38.0,1.0,49–51,Association for Computing Machinery,"New York, NY, USA",,,2013-01,,0163-5948,https://doi.org/10.1145/2413038.2382769;http://dx.doi.org/10.1145/2413038.2382769,10.1145/2413038.2382769,"The International Workshop on Developing Tools as Plug-Ins (TOPI) is a venue for researchers and practitioners interested in plug-in development. The main interest is understanding the opportunities and challenges of developing tools as plug-ins, and thus, we seek for discussions regarding the characteristics of good plug-ins, interoperability requirements to making tools available across platforms, recent successful tools as plug-ins as well as foreseen medium and long term challenges of tools as plug-ins. The second edition of this workshop, TOPI 2012 was co-located with the International Conference on Software Engineering (ICSE 2012). TOPI 2012 received a total of 32 submissions. Among them, 14 were accepted as full papers and 4 as short papers. The audience during the whole workshop ranged from 25 to 30 participants. The final program comprised position papers including new proposals for plug-in architectures as well as their interaction with development environments and run-times, and papers discussing the implementation of different kind of tools as plug-ins. This report describes the main results of TOPI 2012.",,,,,
Journal Article,"Sharma V,Rizvi SA,Sharma A",Software Licenses - A Tool to Control Distribution of Software,SIGSOFT Softw. Eng. Notes,2013,37.0,6.0,1–4,Association for Computing Machinery,"New York, NY, USA",,,2013-01,,0163-5948,https://doi.org/10.1145/2382756.2382769;http://dx.doi.org/10.1145/2382756.2382769,10.1145/2382756.2382769,"The International Workshop on Developing Tools as Plug-Ins (TOPI) is a venue for researchers and practitioners interested in plug-in development. The main interest is understanding the opportunities and challenges of developing tools as plug-ins, and thus, we seek for discussions regarding the characteristics of good plug-ins, interoperability requirements to making tools available across platforms, recent successful tools as plug-ins as well as foreseen medium and long term challenges of tools as plug-ins. The second edition of this workshop, TOPI 2012 was co-located with the International Conference on Software Engineering (ICSE 2012). TOPI 2012 received a total of 32 submissions. Among them, 14 were accepted as full papers and 4 as short papers. The audience during the whole workshop ranged from 25 to 30 participants. The final program comprised position papers including new proposals for plug-in architectures as well as their interaction with development environments and run-times, and papers discussing the implementation of different kind of tools as plug-ins. This report describes the main results of TOPI 2012.",,,,,
Conference Paper,"Chondamrongkul N,Sun J,Warren I,Lee SU",Semantic-Based Architecture Smell Analysis,,2020,,,109–118,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 8th International Conference on Formal Methods in Software Engineering,"Seoul, Republic of Korea",2020,9781450370714.0,,https://doi.org/10.1145/3372020.3391564;http://dx.doi.org/10.1145/3372020.3391564,10.1145/3372020.3391564,"Software smells have negative impacts on the reliability and modifiability of software systems. The smells in architecture design can be cascaded down to the implementation level and cause issues that require much effort to fix. Therefore, early detection of the architecture smells can benefit the overall quality of the software system. This paper presents an integration of methods that formally define the software architecture design towards architecture smell detection. Our approach serves as a framework that allows the architectural structures and behaviours to be formally analysed based on a coherent technique. We evaluated the accuracy and performance of our approach with the models generated from open source projects. The results show that our approach is effective and functions well.","Software Architecture, Ontology Web Language, Smell Detection, Architecture Smells, Model Checking",FormaliSE '20,,,
Conference Paper,"Peruma A,Newman CD,Mkaouer MW,Ouni A,Palomba F",An Exploratory Study on the Refactoring of Unit Test Files in Android Applications,,2020,,,350–357,Association for Computing Machinery,"New York, NY, USA",Proceedings of the IEEE/ACM 42nd International Conference on Software Engineering Workshops,"Seoul, Republic of Korea",2020,9781450379632.0,,https://doi.org/10.1145/3387940.3392189;http://dx.doi.org/10.1145/3387940.3392189,10.1145/3387940.3392189,"An essential activity of software maintenance is the refactoring of source code. Refactoring operations enable developers to take necessary actions to correct bad programming practices (i.e., smells) in the source code of both production and test files. With unit testing being a vital and fundamental part of ensuring the quality of a system, developers must address smelly test code. In this paper, we empirically explore the impact and relationship between refactoring operations and test smells in 250 open-source Android applications (apps). Our experiments showed that the type of refactoring operations performed by developers on test files differ from those performed on non-test files. Further, results around test smells show a co-occurrence between certain smell types and refactorings, and how refactorings are utilized to eliminate smells. Findings from this study will not only further our knowledge of refactoring operations on test files, but will also help developers in understanding the possible ways on how to maintain their apps.","Software maintenance and evolution, Unit testing, Android applications, Test smells, Refactoring",ICSEW'20,,,
Conference Paper,"Boccuzzo S,Gall HC",Automated Comprehension Tasks in Software Exploration,,2009,,,570–574,IEEE Computer Society,USA,Proceedings of the 2009 IEEE/ACM International Conference on Automated Software Engineering,,2009,9780769538914.0,,https://doi.org/10.1109/ASE.2009.47;http://dx.doi.org/10.1109/ASE.2009.47,10.1109/ASE.2009.47,"Finding issues in software usually requires a serie of comprehension tasks. After every task, an engineer explores the results and decides whether further tasks are required. Software comprehension therefore is a combination of tasks and a supported exploration of the results typically in an adequate visualization. In this paper, we describe how we simplify the combination of existing automated procedures to sequentially solve common software comprehension tasks. Beyond that we improve the understanding of the outcomes with interactive and explorative visualization concepts in a time efficient workflow. We validate the presented concept with basic comprehension tasks in an extended CocoViz tool implementation.","Exploration, Comprehension, Visualization",ASE '09,,,
Conference Paper,"Wang H,Kessentini M,Grosky W,Meddeb H",On the Use of Time Series and Search Based Software Engineering for Refactoring Recommendation,,2015,,,35–42,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 7th International Conference on Management of Computational and Collective IntElligence in Digital EcoSystems,"Caraguatatuba, Brazil",2015,9781450334808.0,,https://doi.org/10.1145/2857218.2857224;http://dx.doi.org/10.1145/2857218.2857224,10.1145/2857218.2857224,"To improve the quality of software systems, one of the widely used techniques is refactoring, defined as the process of improving the design of an existing system by changing its internal structure without altering the external behavior. The majority of existing refactoring works do not consider the impact of recommended refactorings on the quality of future releases of a system. In this paper, we propose to combine the use of search-based software engineering with time series to recommend good refactoring strategies in order to manage technical debt. We used a multi-objective algorithm to generate refactoring solutions that maximize the correction of important quality issues and minimize the effort. For these two fitness functions, we adapted time series forecasting to estimate the impact of the generated refactorings solution on future next releases of the system by predicting the evolution of the remaining code smells in the system, after refactoring, using different quality metrics. We evaluated our approach on one industrial project and a benchmark of 4 open source systems. The results confirm the efficiency of our technique to provide better refactoring management comparing to several existing refactoring techniques.","heuristic search, refactoring, data-mining",MEDES '15,,,
Conference Paper,"Camilo F,Meneely A,Nagappan M",Do Bugs Foreshadow Vulnerabilities? A Study of the Chromium Project,,2015,,,269–279,IEEE Press,"Florence, Italy",Proceedings of the 12th Working Conference on Mining Software Repositories,,2015,9780769555942.0,,,,"As developers face ever-increasing pressure to engineer secure software, researchers are building an understanding of security-sensitive bugs (i.e. vulnerabilities). Research into mining software repositories has greatly increased our understanding of software quality via empirical study of bugs. However, conceptually vulnerabilities are different from bugs: they represent abusive functionality as opposed to wrong or insufficient functionality commonly associated with traditional, non-security bugs. In this study, we performed an in-depth analysis of the Chromium project to empirically examine the relationship between bugs and vulnerabilities. We mined 374,686 bugs and 703 post-release vulnerabilities over five Chromium releases that span six years of development. Using logistic regression analysis, we examined how various categories of pre-release bugs (e.g. stability, compatibility, etc.) are associated with post-release vulnerabilities. While we found statistically significant correlations between pre-release bugs and post-release vulnerabilities, we also found the association to be weak. Number of features, SLOC, and number of pre-release security bugs are, in general, more closely associated with post-release vulnerabilities than any of our non-security bug categories. In a separate analysis, we found that the files with highest defect density did not intersect with the files of highest vulnerability density. These results indicate that bugs and vulnerabilities are empirically dissimilar groups, warranting the need for more research targeting vulnerabilities specifically.",,MSR '15,,,
Conference Paper,"Assunção E,Souza R",Incidence of Code Smells in the Application of Design Patterns: A Method-Level Analysis,,2019,,,73–82,Association for Computing Machinery,"New York, NY, USA","Proceedings of the XIII Brazilian Symposium on Software Components, Architectures, and Reuse","Salvador, Brazil",2019,9781450376372.0,,https://doi.org/10.1145/3357141.3357143;http://dx.doi.org/10.1145/3357141.3357143,10.1145/3357141.3357143,"Design patterns are reusable solutions that can be applied to solve specific problems in software design. Such patterns can be misapplied, though, and give rise to code smells, i.e., fragments in the code that indicate possible design flaws. In this study, we aim to understand how often code smells co-occur with design patterns, as well as to determine the most common co-occurrences. To this end, we identified instances of code smells and design patterns in methods of 25 open source Java projects, by using automated detection tools. We also manually inspected fragments of the projects' source code to gather insight on the relationship between specific pairs of smells and patterns. Among other findings, we found that methods that are part of the Adapter pattern are more likely to contain code smells, especially the Feature Envy smell, although it can be argued that the detection of this smell in this context is a false positive.","Design Patterns, Code Smells, Software Design",SBCARS '19,,,
Conference Paper,"Gupta M,Sureka A,Padmanabhuni S",Process Mining Multiple Repositories for Software Defect Resolution from Control and Organizational Perspective,,2014,,,122–131,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 11th Working Conference on Mining Software Repositories,"Hyderabad, India",2014,9781450328630.0,,https://doi.org/10.1145/2597073.2597081;http://dx.doi.org/10.1145/2597073.2597081,10.1145/2597073.2597081,"Issue reporting and resolution is a software engineering process supported by tools such as Issue Tracking System (ITS), Peer Code Review (PCR) system and Version Control System (VCS). Several open source software projects such as Google Chromium and Android follow process in which a defect or feature enhancement request is reported to an issue tracker followed by source-code change or patch review and patch commit using a version control system. We present an application of process mining three software repositories (ITS, PCR and VCS) from control flow and organizational perspective for effective process management. ITS, PCR and VCS are not explicitly linked so we implement regular expression based heuristics to integrate data from three repositories for Google Chromium project. We define activities such as bug reporting, bug fixing, bug verification, patch submission, patch review, and source code commit and create an event log of the bug resolution process. The extracted event log contains audit trail data such as caseID, timestamp, activity name and performer. We discover runtime process model for bug resolution process spanning three repositories using process mining tool, Disco, and conduct process performance and efficiency analysis. We identify bottlenecks, define and detect basic and composite anti-patterns. In addition to control flow analysis, we mine event log to perform organizational analysis and discover metrics such as handover of work, subcontracting, joint cases and joint activities.","Empirical Software Engineering and Measurements, Software Maintenance, Social Network Analysis, Peer Code Review System, Issue Tracking System, Process Mining",MSR 2014,,,
Conference Paper,"Ivers J,Ozkaya I,Nord RL,Seifried C",Next Generation Automated Software Evolution Refactoring at Scale,,2020,,,1521–1524,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering,"Virtual Event, USA",2020,9781450370431.0,,https://doi.org/10.1145/3368089.3417042;http://dx.doi.org/10.1145/3368089.3417042,10.1145/3368089.3417042,"Despite progress in providing software engineers with tools that automate an increasing number of development tasks, complex activities like redesigning and reengineering existing software remain resource intensive or are supported by tools that are error prone. Complex, but common tasks in industry, like evolving large codebases (1M+ SLOC) to meet changing needs, still rely on costly manual efforts and incur significant technical risk. In one example, an organization that we work with estimated 14,000 hours of development work alone (excluding integration and testing) to isolate a feature from the underlying hardware platform. These examples are pervasive in industry. Software engineering research has taken providing effective tools for software evolution for granted for far too long. The time is right for research to take advantage of advances in search-based software engineering and create the next generation of industry-relevant automated software evolution tools. This paper lays out a vision for automated refactoring at scale towards this goal.","refactoring at scale, feature isolation, search-based software engineering, automated software engineering",ESEC/FSE 2020,,,
Journal Article,"Badler NI,Erignac CA,Liu Y",Virtual Humans for Validating Maintenance Procedures,Commun. ACM,2002,45.0,7.0,56–63,Association for Computing Machinery,"New York, NY, USA",,,2002-07,,0001-0782,https://doi.org/10.1145/514236.514264;http://dx.doi.org/10.1145/514236.514264,10.1145/514236.514264,"They can be sent to check the human aspects of complex physical systems by simulating assembly, repair, and maintenance tasks in a 3D virtual environment.",,,,,
Conference Paper,"Gruska N,Wasylkowski A,Zeller A","Learning from 6,000 Projects: Lightweight Cross-Project Anomaly Detection",,2010,,,119–130,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 19th International Symposium on Software Testing and Analysis,"Trento, Italy",2010,9781605588230.0,,https://doi.org/10.1145/1831708.1831723;http://dx.doi.org/10.1145/1831708.1831723,10.1145/1831708.1831723,"Real production code contains lots of knowledge - on the domain, on the architecture, and on the environment. How can we leverage this knowledge in new projects? Using a novel lightweight source code parser, we have mined more than 6,000 open source Linux projects (totaling 200,000,000 lines of code) to obtain 16,000,000 temporal properties reflecting normal interface usage. New projects can be checked against these rules to detect anomalies - that is, code that deviates from the wisdom of the crowds. In a sample of 20 projects, 25% of the top-ranked anomalies uncovered actual code smells or defects.","mining specifications, lightweight parsing, language independent parsing, formal concept analysis, temporal properties",ISSTA '10,,,
Conference Paper,"Liu X,Woo G",Applying Code Quality Detection in Online Programming Judge,,2020,,,56–60,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 2020 5th International Conference on Intelligent Information Technology,"Hanoi, Viet Nam",2020,9781450376594.0,,https://doi.org/10.1145/3385209.3385226;http://dx.doi.org/10.1145/3385209.3385226,10.1145/3385209.3385226,"This article presents an enhanced programming online judge system that not only evaluates the correctness of the submitted program code but also detects its code quality. Both results of the correctness and quality detection are responded through web once the compilation, the execution and the quality detection of the submitted source code have been finished. We take advantage of SonarQube to provide code quality detection in our online judge system named neoESPA. Comparing with other online judges, our proposed work has significant advantages in helping both the instructor to discover the weaknesses in the lecture and the students to locate their mistakes efficiently.","Code quality detection, Online judge, Programming code assessment",ICIIT 2020,,,
Conference Paper,"Sko T,Gardner HJ,Martin M",Non-Parametric Decision Trees and Online HCI,,2013,,,2103–2106,Association for Computing Machinery,"New York, NY, USA",Proceedings of the SIGCHI Conference on Human Factors in Computing Systems,"Paris, France",2013,9781450318990.0,,https://doi.org/10.1145/2470654.2481288;http://dx.doi.org/10.1145/2470654.2481288,10.1145/2470654.2481288,This paper proposes that online HCI studies (such as web-surveys and remotely monitored usability tests) can benefit from statistical data analysis using modern statistical learning methods such as classification and regression trees (CARTs). Applying CARTs to the often large amount of data yielded by online studies can easily provide clarity concerning the most important effects underlying experimental data in situations where myriad possible factors are under consideration. The feedback provided by such an analysis can also provide valuable reflection on the experimental methodology. We discuss these matters with reference to a study of 1300 participants in a structured experiment concerned with head-interaction techniques for first-person-shooter games.,"decision trees, games, classification, parametric, non-parametric, regression, online studies",CHI '13,,,
Conference Paper,"Li YF,Zhang H",Integrating Software Engineering Data Using Semantic Web Technologies,,2011,,,211–214,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 8th Working Conference on Mining Software Repositories,"Waikiki, Honolulu, HI, USA",2011,9781450305747.0,,https://doi.org/10.1145/1985441.1985473;http://dx.doi.org/10.1145/1985441.1985473,10.1145/1985441.1985473,"A plethora of software engineering data have been produced by different organizations and tools over time. These data may come from different sources, and are often disparate and distributed. The integration of these data may open up the possibility of conducting systemic, holistic study of software projects in ways previously unexplored. Semantic Web technologies have been used successfully in a wide array of domains such as health care and life sciences as a platform for information integration and knowledge management. The success is largely due to the open and extensible nature of ontology languages as well as growing tool support. We believe that Semantic Web technologies represent an ideal platform for the integration of software engineering data in a semantic repository. By querying and analyzing such a repository, researchers and practitioners can better understand and control software engineering activities and processes. In this paper, we describe how we apply Semantic Web techniques to integrate object-oriented software engineering data from different sources. We also show how the integrated data can help us answer complex queries about large-scale software projects through a case study on the Eclipse system.","data integration, semantic web, software engineering data",MSR '11,,,
Conference Paper,"Frädrich C,Obermüller F,Körber N,Heuer U,Fraser G",Common Bugs in Scratch Programs,,2020,,,89–95,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 2020 ACM Conference on Innovation and Technology in Computer Science Education,"Trondheim, Norway",2020,9781450368742.0,,https://doi.org/10.1145/3341525.3387389;http://dx.doi.org/10.1145/3341525.3387389,10.1145/3341525.3387389,"Bugs in SCRATCH programs can spoil the fun and inhibit learning success. Many common bugs are the result of recurring patterns of bad code. In this paper we present a collection of common code patterns that typically hint at bugs in SCRATCH programs, and the LitterBox tool which can automatically detect them. We empirically evaluate how frequently these patterns occur, and how severe their consequences usually are. While fixing bugs inevitably is part of learning, the possibility to identify the bugs automatically provides the potential to support learners.","block-based programming, scratch, code quality",ITiCSE '20,,,
Journal Article,"Rehman S,Mustafa K",Research on Software Design Level Security Vulnerabilities,SIGSOFT Softw. Eng. Notes,2009,34.0,6.0,1–5,Association for Computing Machinery,"New York, NY, USA",,,2009-12,,0163-5948,https://doi.org/10.1145/1640162.1640171;http://dx.doi.org/10.1145/1640162.1640171,10.1145/1640162.1640171,"One of the major problems in software security is the lack of knowledge about security among software developers. Even if a developer has good knowledge about current software vulnerabilities, they generally have little or no idea about the causes and measures that can avoid those vulnerabilities. Now it is established fact that most of the vulnerabilities arise in design phase of the software development lifecycle. Keeping in view the importance of software design level security, a study of current software design level vulnerabilities and their cause is conducted. In this paper, we discuss current practices in specific software design tasks, vulnerabilities and mitigation mechanism. On the basis of the critical review, areas of research are identified that warrant further investigation.","vulnerabilities, mitigation mechanisms, software design, security, research",,,,
Conference Paper,"Fenske W,Thüm T,Saake G",A Taxonomy of Software Product Line Reengineering,,2014,,,,Association for Computing Machinery,"New York, NY, USA",Proceedings of the Eighth International Workshop on Variability Modelling of Software-Intensive Systems,"Sophia Antipolis, France",2014,9781450325561.0,,https://doi.org/10.1145/2556624.2556643;http://dx.doi.org/10.1145/2556624.2556643,10.1145/2556624.2556643,"In the context of single software systems, refactoring is commonly accepted to be the process of restructuring an existing body of code in order to improve its internal structure without changing its external behavior. This process is vital to the maintenance and evolution of software systems.Software product line engineering is a paradigm for the construction and customization of large-scale software systems. As systems grow in complexity and size, maintaining a clean structure becomes arguably more important. However, product line literature uses the term ""refactoring"" for such a wide range of reengineering activities that it has become difficult to see how these activities pertain to maintenance and evolution and how they are related.We improve this situation in the following way: i) We identify the dimensions along which product line reengineering occurs. ii) We derive a taxonomy that distinguishes and relates these reengineering activities. iii) We propose definitions for the three main branches of this taxonomy. iv) We classify a corpus of existing work.","taxonomy, reengineering, refactoring, software product lines",VaMoS '14,,,
Conference Paper,"Marcilio D,Bonifácio R,Monteiro E,Canedo E,Luz W,Pinto G",Are Static Analysis Violations Really Fixed? A Closer Look at Realistic Usage of SonarQube,,2019,,,209–219,IEEE Press,"Montreal, Quebec, Canada",Proceedings of the 27th International Conference on Program Comprehension,,2019,,,https://doi.org/10.1109/ICPC.2019.00040;http://dx.doi.org/10.1109/ICPC.2019.00040,10.1109/ICPC.2019.00040,"The use of automatic static analysis tools (ASATs) has gained increasing attention in the last few years. Even though available research have already explored ASATs issues and how they are fixed, these studies rely on revisions of the software, instead of mining real usage of these tools and real issue reports. In this paper we contribute with a comprehensive, multi-method study about the usage of SonarQube (a popular static analysis tool), mining 421,976 issues from 246 projects in four different instance of SonarQube: two hosted in open-source communities (Eclipse and Apache) and two hosted in Brazilian government institutions (Brazilian Court of Account (TCU) and Brazilian Federal Police (PF)). We first surveyed team leaders of the analyzed projects and found that they mostly consider ASATs warning messages as relevant for overall software improvement. Second, we found that both Eclipse and TCU employ highly customized instance of SonarQube, with more than one thousand distinct checkers-though just a subset of these checkers actually led to issues' reports. Surprisingly, we found a low resolution rate per project in all organizations-on average, 13% of the issues have been solved in the systems. We conjecture that just a subset of the checkers reveal real design and coding flaws, and this might artificially increase the technical debt of the systems. Nevertheless, considering all systems, there is a central tendency (median) of fixing issues after 18.99 days they had been reported, faster than the period for fixing bugs as reported in previous studies.",,ICPC '19,,,
Conference Paper,"Schimmel J,Molitorisz K,Jannesari A,Tichy WF",Combining Unit Tests for Data Race Detection,,2015,,,43–47,IEEE Press,"Florence, Italy",Proceedings of the 10th International Workshop on Automation of Software Test,,2015,,,,,"Multithreaded programs are subject to data races. Data race detectors find such defects by static or dynamic inspection of the program. Current race detectors suffer from high numbers of false positives, slowdown, and false negatives. Because of these disadvantages, recent approaches reduce the false positive rate and the runtime overhead by applying race detection only on a subset of the whole program. To achieve this, they make use of parallel test cases, but this has other disadvantages: Parallel test cases have to be engineered manually, cover code regions that are affected by data races, and execute with input data that provoke the data races.This paper introduces an approach that does not need additional parallel use cases to be engineered. Instead, we take conventional unit tests as input and automatically generate parallel test cases, execution contexts and input data. As can be observed, most real-world software projects nowadays have high test coverages, so a large information base as input for our approach is already available. We analyze and reuse input data, initialization code, and mock objects that conventional unit tests already contain. With this information, no further oracles are necessary for generating parallel test cases. Instead, we reuse the knowledge that is already implicitly available in conventional unit tests.We implemented our parallel test case generation strategy in a tool called TestMerge. To evaluate these test cases we used them as input for the dynamic race detector CHESS that evokes all possible thread interleavings for a given program. We evaluated TestMerge using six sample programs and one industrial application with a high test case coverage of over 94%. For this benchmark, TestMerge identified all previously known data races and even revealed previously unknown ones.","unit testing, multicore software engineering, data races",AST '15,,,
Conference Paper,"Li Y,Xu W",PrivPy: General and Scalable Privacy-Preserving Data Mining,,2019,,,1299–1307,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining,"Anchorage, AK, USA",2019,9781450362016.0,,https://doi.org/10.1145/3292500.3330920;http://dx.doi.org/10.1145/3292500.3330920,10.1145/3292500.3330920,"Privacy is a big hurdle for collaborative data mining across multiple parties. We present multi-party computation (MPC) framework designed for large-scale data mining tasks. PrivPy combines an easy-to-use and highly flexible Python programming interface with state-of-the-art secret-sharing-based MPC backend. With essential data types and operations (such as NumPy arrays and broadcasting), as well as automatic code-rewriting, programmers can write modern data mining algorithms conveniently in familiar Python. We demonstrate that we can support many real-world machine learning algorithms (e.g. logistic regression and convolutional neural networks) and large datasets (e.g. 5000-by-1-million matrix) with minimal algorithm porting effort.","python, privacy-preserving, numpy, data mining",KDD '19,,,
Conference Paper,"Awad A,Barnawi A,Elgammal A,Elshawi R,Almalaise A,Sakr S",Runtime Detection of Business Process Compliance Violations: An Approach Based on Anti Patterns,,2015,,,1203–1210,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 30th Annual ACM Symposium on Applied Computing,"Salamanca, Spain",2015,9781450331968.0,,https://doi.org/10.1145/2695664.2699488;http://dx.doi.org/10.1145/2695664.2699488,10.1145/2695664.2699488,"Today's enterprises demand a high degree of compliance in their business processes to meet diverse regulations and legislations. Several industrial studies have shown that compliance management is a daunting task, and organizations are still struggling and spending billions of dollars annually to ensure and prove their compliance. Theoretically, design-time compliance checking could provide a preliminary assurance that corresponding running instances would be compliant to relevant laws and regulations; however, due to the existence of human and machine related errors and the absence of necessary contextual information during design-time, runtime compliance monitoring becomes a must. In this paper, we present a generic proactive runtime Business Process (BP) compliance monitoring framework:BP-MaaS, which incorporates a wide range of expressive high-level compliance patterns for the abstract specification of runtime constraints. Compliance monitoring is achieved by means of anti-patterns, a novel evaluation approach that is independent of any underlying technology and could be applied to the checking of compliance in the different phases of the BP lifecycle. As a proof-of-concept, complex event processing (CEP) technology is adopted as one of the possible realizations of the framework.",,SAC '15,,,
Conference Paper,"Hammer C,Dolby J,Vaziri M,Tip F",Dynamic Detection of Atomic-Set-Serializability Violations,,2008,,,231–240,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 30th International Conference on Software Engineering,"Leipzig, Germany",2008,9781605580791.0,,https://doi.org/10.1145/1368088.1368120;http://dx.doi.org/10.1145/1368088.1368120,10.1145/1368088.1368120,"Previously we presented atomic sets, memory locations that share some consistency property, and units of work, code fragments that preserve consistency of atomic sets on which they are declared. We also proposed atomic-set serializability as a correctness criterion for concurrent programs, stating that units of work must be serializable for each atomic set. We showed that a set of problematic data access patterns characterize executions that are not atomic-set serializable. Our criterion subsumes data races (single-location atomic sets) and serializability (all locations in one set).In this paper, we present a dynamic analysis for detecting violations of atomic-set serializability. The analysis can be implemented efficiently, and does not depend on any specific synchronization mechanism. We implemented the analysis and evaluated it on a suite of real programs and benchmarks. We found a number of known errors as well as several problems not previously reported.","data races, dynamic analysis, atomicity, concurrent object-oriented programming, serializability",ICSE '08,,,
Conference Paper,"Brown N,Cai Y,Guo Y,Kazman R,Kim M,Kruchten P,Lim E,MacCormack A,Nord R,Ozkaya I,Sangwan R,Seaman C,Sullivan K,Zazworka N",Managing Technical Debt in Software-Reliant Systems,,2010,,,47–52,Association for Computing Machinery,"New York, NY, USA",Proceedings of the FSE/SDP Workshop on Future of Software Engineering Research,"Santa Fe, New Mexico, USA",2010,9781450304276.0,,https://doi.org/10.1145/1882362.1882373;http://dx.doi.org/10.1145/1882362.1882373,10.1145/1882362.1882373,"Delivering increasingly complex software-reliant systems demands better ways to manage the long-term effects of short-term expedients. The technical debt metaphor is gaining significant traction in the agile development community as a way to understand and communicate such issues. The idea is that developers sometimes accept compromises in a system in one dimension (e.g., modularity) to meet an urgent demand in some other dimension (e.g., a deadline), and that such compromises incur a ""debt"": on which ""interest"" has to be paid and which the ""principal"" should be repaid at some point for the long-term health of the project. We argue that the software engineering research community has an opportunity to study and improve this concept. We can offer software engineers a foundation for managing such trade-offs based on models of their economic impacts. Therefore, we propose managing technical debt as a part of the future research agenda for the software engineering field.","design decision trade-off, large-scale system development, software metrics, technical debt, cost-benefit analysis",FoSER '10,,,
Conference Paper,"Cheung SC,Chen W,Liu Y,Xu C",CUSTODES: Automatic Spreadsheet Cell Clustering and Smell Detection Using Strong and Weak Features,,2016,,,464–475,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 38th International Conference on Software Engineering,"Austin, Texas",2016,9781450339001.0,,https://doi.org/10.1145/2884781.2884796;http://dx.doi.org/10.1145/2884781.2884796,10.1145/2884781.2884796,"Various techniques have been proposed to detect smells in spreadsheets, which are susceptible to errors. These techniques typically detect spreadsheet smells through a mechanism based on a fixed set of patterns or metric thresholds. Unlike conventional programs, tabulation styles vary greatly across spreadsheets. Smell detection based on fixed patterns or metric thresholds, which are insensitive to the varying tabulation styles, can miss many smells in one spreadsheet while reporting many spurious smells in another. In this paper, we propose CUSTODES to effectively cluster spreadsheet cells and detect smells in these clusters. The clustering mechanism can automatically adapt to the tabulation styles of each spreadsheet using strong and weak features. These strong and weak features capture the invariant and variant parts of tabulation styles, respectively. As smelly cells in a spreadsheet normally occur in minority, they can be mechanically detected as clusters' outliers in feature spaces. We implemented and applied CUSTODES to 70 spreadsheets files randomly sampled from the EUSES corpus. These spreadsheets contain 1,610 formula cell clusters. Experimental results confirmed that CUSTODES is effective. It successfully detected harmful smells that can induce computation anomalies in spreadsheets with an F-measure of 0.72, outperforming state-of-the-art techniques.","smell detection, cell clustering, end-user programming, spreadsheets, feature modeling",ICSE '16,,,
Conference Paper,"Mkaouer MW,Kessentini M,Bechikh S,Deb K,Ó Cinnéide M",Recommendation System for Software Refactoring Using Innovization and Interactive Dynamic Optimization,,2014,,,331–336,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 29th ACM/IEEE International Conference on Automated Software Engineering,"Vasteras, Sweden",2014,9781450330138.0,,https://doi.org/10.1145/2642937.2642965;http://dx.doi.org/10.1145/2642937.2642965,10.1145/2642937.2642965,"We propose a novel recommendation tool for software refactoring that dynamically adapts and suggests refactorings to developers interactively based on their feedback and introduced code changes. Our approach starts by finding upfront a set of non-dominated refactoring solutions using NSGA-II to improve software quality, reduce the number of refactorings and increase semantic coherence. The generated non-dominated refactoring solutions are analyzed using our innovization component to extract some interesting common features between them. Based on this analysis, the suggested refactorings are ranked and suggested to the developer one by one. The developer can approve, modify or reject each suggested refactoring, and this feedback is used to update the ranking of the suggested refactorings. After a number of introduced code changes, a local search is performed to update and adapt the set of refactoring solutions suggested by NSGA-II. We evaluated this tool on four large open source systems and one industrial project provided by our partner. Statistical analysis of our experiments over 31 runs shows that the dynamic refactoring approach performed significantly better than three other search-based refactoring techniques, manual refactorings, and one refactoring tool not based on heuristic search.","software quality, refactoring, search based software engineering",ASE '14,,,
Conference Paper,"Martens A,Koziolek H,Becker S,Reussner R","Automatically Improve Software Architecture Models for Performance, Reliability, and Cost Using Evolutionary Algorithms",,2010,,,105–116,Association for Computing Machinery,"New York, NY, USA",Proceedings of the First Joint WOSP/SIPEW International Conference on Performance Engineering,"San Jose, California, USA",2010,9781605585635.0,,https://doi.org/10.1145/1712605.1712624;http://dx.doi.org/10.1145/1712605.1712624,10.1145/1712605.1712624,"Quantitative prediction of quality properties (i.e. extra-functional properties such as performance, reliability, and cost) of software architectures during design supports a systematic software engineering approach. Designing architectures that exhibit a good trade-off between multiple quality criteria is hard, because even after a functional design has been created, many remaining degrees of freedom in the software architecture span a large, discontinuous design space. In current practice, software architects try to find solutions manually, which is time-consuming, can be error-prone and can lead to suboptimal designs. We propose an automated approach to search the design space for good solutions. Starting with a given initial architectural model, the approach iteratively modifies and evaluates architectural models. Our approach applies a multi-criteria genetic algorithm to software architectures modelled with the Palladio Component Model. It supports quantitative performance, reliability, and cost prediction and can be extended to other quantitative quality criteria of software architectures. We validate the applicability of our approach by applying it to an architecture model of a component-based business information system and analyse its quality criteria trade-offs by automatically investigating more than 1200 alternative design candidates.","cost, optimisation, quality attribute prediction, performance, software architecture, reliability",WOSP/SIPEW '10,,,
Journal Article,Kamsky A,Adapting TPC-C Benchmark to Measure Performance of Multi-Document Transactions in MongoDB,Proc. VLDB Endow.,2019,12.0,12.0,2254–2262,VLDB Endowment,,,,2019-08,,2150-8097,https://doi.org/10.14778/3352063.3352140;http://dx.doi.org/10.14778/3352063.3352140,10.14778/3352063.3352140,"MongoDB is a popular distributed database that supports replication, horizontal partitioning (sharding), a flexible document schema and ACID guarantees on the document level. While it is generally grouped with ""NoSQL"" databases, MongoDB provides many features similar to those of traditional RDBMS such as secondary indexes, an ad hoc query language, support for complex aggregations, and new as of version 4.0 multi-statement, multi-document ACID transactions.We looked for a well understood OLTP workload benchmark to use in our own system performance test suite to establish a baseline of transaction performance to enable flagging performance regressions, as well as improvements as we continue to add new functionality. While there exist many published and widely used benchmarks for RDBMS OLTP workloads, there are none specifically for document databases.This paper describes the process of adapting an existing traditional RDBMS benchmark to MongoDB query language and transaction semantics to allow measuring transaction performance. We chose to adapt the TPC-C benchmark even though it assumes a relational database schema and SQL, hence extensive changes had to be made to stay consistent with MongoDB best practices. Our goal did not include creating official TPC-C certifiable results, however, every attempt was made to stay consistent with the spirit of the original benchmark specification as well as to be compliant to all specification requirements where possible.We discovered that following best practices for document schema design achieves better performance than using required normalized schema. All the source code used and validation scripts are published in github to allow the reader to recreate and verify our results.",,,,,
Conference Paper,"Saarimäki N,Lenarduzzi V,Vegas S,Juristo N,Taibi D",Cohort Studies in Software Engineering: A Vision of the Future,,2020,,,,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 14th ACM / IEEE International Symposium on Empirical Software Engineering and Measurement (ESEM),"Bari, Italy",2020,9781450375801.0,,https://doi.org/10.1145/3382494.3422160;http://dx.doi.org/10.1145/3382494.3422160,10.1145/3382494.3422160,Background. Most Mining Software Repositories (MSR) studies cannot obtain causal relations because they are not controlled experiments. The use of cohort studies as defined in epidemiology could help to overcome this shortcoming.Objective. Propose the adoption of cohort studies in MSR research in particular and empirical Software Engineering (SE) in general. Method. We run a preliminary literature review to show the current state of the practice of cohort studies in SE. We explore how cohort studies overcome the issues that prevent the identification of causality in this type of non-experimental designs.Results. The basic mechanism used by cohort studies to try to obtain causality consists of controlling potentially confounding variables. This is articulated by means of different techniques.Conclusion. Cohort studies seem to be a promising approach to be used in MSR in particular and SE in general.,"Cohort Study, Empirical Software Engineering, Empirical Methods",ESEM '20,,,
Conference Paper,"Alizadeh V,Ouali MA,Kessentini M,Chater M",RefBot: Intelligent Software Refactoring Bot,,2020,,,823–834,IEEE Press,"San Diego, California",Proceedings of the 34th IEEE/ACM International Conference on Automated Software Engineering,,2020,9781728125084.0,,https://doi.org/10.1109/ASE.2019.00081;http://dx.doi.org/10.1109/ASE.2019.00081,10.1109/ASE.2019.00081,"The adoption of refactoring techniques for continuous integration received much less attention from the research community comparing to root-canal refactoring to fix the quality issues in the whole system. Several recent empirical studies show that developers, in practice, are applying refactoring incrementally when they are fixing bugs or adding new features. There is an urgent need for refactoring tools that can support continuous integration and some recent development processes such as DevOps that are based on rapid releases. Furthermore, several studies show that manual refactoring is expensive and existing automated refactoring tools are challenging to configure and integrate into the development pipelines with significant disruption cost.In this paper, we propose, for the first time, an intelligent software refactoring bot, called RefBot. Integrated into the version control system (e.g. GitHub), our bot continuously monitors the software repository, and it is triggered by any ""open"" or ""merge"" action on pull requests. The bot analyzes the files changed during that pull request to identify refactoring opportunities using a set of quality attributes then it will find the best sequence of refactorings to fix the quality issues if any. The bot recommends all these refactorings through an automatically generated pull-request. The developer can review the recommendations and their impacts in a detailed report and select the code changes that he wants to keep or ignore. After this review, the developer can close and approve the merge of the bot's pull request. We quantitatively and qualitatively evaluated the performance and effectiveness of RefBot by a survey conducted with experienced developers who used the bot on both open source and industry projects.","refactoring, quality, Software bot",ASE '19,,,
Conference Paper,"Babur Ö,Stephan M",MoCoP: Towards a Model Clone Portal,,2019,,,78–81,IEEE Press,"Montreal, Quebec, Canada",Proceedings of the 11th International Workshop on Modelling in Software Engineerings,,2019,,,https://doi.org/10.1109/MiSE.2019.00019;http://dx.doi.org/10.1109/MiSE.2019.00019,10.1109/MiSE.2019.00019,"Widespread and mature practice of model-driven engineering is leading to a growing number of modeling artifacts and challenges in their management. Model clone detection (MCD) is an important approach for managing and maintaining modeling artifacts. While its counterpart in traditional source code development, code clone detection, is enjoying popularity and more than two decades of development, MCD is still in its infancy in terms of research and tooling. We aim to develop a portal for model clone detection, MoCoP, as a central hub to mitigate adoption barriers and foster MCD research. In this short paper, we present our vision for MoCoP and its features and goals. We discuss MoCoP's key components that we plan on realizing in the short term including public tooling, curated data sets, and a body of MCD knowledge. Our longer term goals include a dedicated service-oriented infrastructure, contests, and forums. We believe MoCoP will strengthen MCD research, tooling, and the community, which in turn will lead to better quality, maintenance, and scalability for model-driven engineering practices.","model-driven engineering, model repositories, software maintenance, model management, model analytics, model clone detection",MiSE '19,,,
Conference Paper,"Eismann S,Grohmann J,van Eyk E,Herbst N,Kounev S",Predicting the Costs of Serverless Workflows,,2020,,,265–276,Association for Computing Machinery,"New York, NY, USA",Proceedings of the ACM/SPEC International Conference on Performance Engineering,"Edmonton AB, Canada",2020,9781450369916.0,,https://doi.org/10.1145/3358960.3379133;http://dx.doi.org/10.1145/3358960.3379133,10.1145/3358960.3379133,"Function-as-a-Service (FaaS) platforms enable users to run arbitrary functions without being concerned about operational issues, while only paying for the consumed resources. Individual functions are often composed into workflows for complex tasks. However, the pay-per-use model and nontransparent reporting by cloud providers make it challenging to estimate the expected cost of a workflow, which prevents informed business decisions. Existing cost-estimation approaches assume a static response time for the serverless functions, without taking input parameters into account. In this paper, we propose a methodology for the cost prediction of serverless workflows consisting of input-parameter sensitive function models and a monte-carlo simulation of an abstract workflow model. Our approach enables workflow designers to predict, compare, and optimize the expected costs and performance of a planned workflow, which currently requires time-intensive experimentation. In our evaluation, we show that our approach can predict the response time and output parameters of a function based on its input parameters with an accuracy of 96.1%. In a case study with two audio-processing workflows, our approach predicts the costs of the two workflows with an accuracy of 96.2%.","prediction, serverless, workflows, cost, performance",ICPE '20,,,
Conference Paper,"Yan W,Gao J,Wu Z,Li Y,Guan Z,Li Q,Chen Z",EShield: Protect Smart Contracts against Reverse Engineering,,2020,,,553–556,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 29th ACM SIGSOFT International Symposium on Software Testing and Analysis,"Virtual Event, USA",2020,9781450380089.0,,https://doi.org/10.1145/3395363.3404365;http://dx.doi.org/10.1145/3395363.3404365,10.1145/3395363.3404365,"Smart contracts are the back-end programs of blockchain-based applications and the execution results are deterministic and publicly visible. Developers are unwilling to release source code of some smart contracts to generate randomness or for security reasons, however, attackers still can use reverse engineering tools to decompile and analyze the code. In this paper, we propose EShield, an automated security enhancement tool for protecting smart contracts against reverse engineering. EShield replaces original instructions of operating jump addresses with anti-patterns to interfere with control flow recovery from bytecode. We have implemented four methods in EShield and conducted an experiment on over 20k smart contracts. The evaluation results show that all the protected smart contracts are resistant to three different reverse engineering tools with little extra gas cost.","Program Analysis, Blockchain, Reverse Engineering, Smart Contract, Ethereum",ISSTA 2020,,,
Conference Paper,"Murphy-Hill E,Jiresal R,Murphy GC",Improving Software Developers' Fluency by Recommending Development Environment Commands,,2012,,,,Association for Computing Machinery,"New York, NY, USA",Proceedings of the ACM SIGSOFT 20th International Symposium on the Foundations of Software Engineering,"Cary, North Carolina",2012,9781450316149.0,,https://doi.org/10.1145/2393596.2393645;http://dx.doi.org/10.1145/2393596.2393645,10.1145/2393596.2393645,"Software developers interact with the development environments they use by issuing commands that execute various programming tools, from source code formatters to build tools. However, developers often only use a small subset of the commands offered by modern development environments, reducing their overall development fluency. In this paper, we use several existing command recommender algorithms to suggest new commands to developers based on their existing command usage history, and also introduce several new algorithms. By running these algorithms on data submitted by several thousand Eclipse users, we describe two studies that explore the feasibility of automatically recommending commands to software developers. The results suggest that, while recommendation is more difficult in development environments than in other domains, it is still feasible to automatically recommend commands to developers based on their usage history, and that using patterns of past discovery is a useful way to do so.","commands, IDEs, software developers, discovery",FSE '12,,,
Journal Article,"C. M,Chandrasekaran K,Chimalakonda S",Energy Diagnosis of Android Applications: A Thematic Taxonomy and Survey,ACM Comput. Surv.,2020,53.0,6.0,,Association for Computing Machinery,"New York, NY, USA",,,2020-12,,0360-0300,https://doi.org/10.1145/3417986;http://dx.doi.org/10.1145/3417986,10.1145/3417986,"The abnormal energy consumption of Android applications is a significant problem faced by developers and users. In recent years, researchers have invested their efforts to develop energy diagnosis tools that pinpoint and fix the energy bugs from source code automatically. These tools use traditional software engineering methods such as program analysis, refactoring, software repair, and bug localization to diagnose energy inefficiencies. Existing surveys focus only on energy measurement techniques and profiling tools and do not consider automated energy diagnosis tools. Therefore, this article organizes state of the art by surveying 25 relevant studies on Android applications’ automatic energy diagnosis. Further, this survey presents a systematic thematic taxonomy of existing approaches from a software engineering perspective. The taxonomy presented in this article would serve as a body of knowledge and help researchers and developers to understand the state of the field better. The future research directions discussed in this article might help prospective researchers to identify suitable topics to improve the current research work in this field.","android application, thematic taxonomy, Automated energy diagnosis, state of the art",,,,
Conference Paper,"Chen J,Yu D,Hu H,Li Z,Hu H",Analyzing Performance-Aware Code Changes in Software Development Process,,2019,,,300–310,IEEE Press,"Montreal, Quebec, Canada",Proceedings of the 27th International Conference on Program Comprehension,,2019,,,https://doi.org/10.1109/ICPC.2019.00049;http://dx.doi.org/10.1109/ICPC.2019.00049,10.1109/ICPC.2019.00049,"With the continuous expansion of software market and the updating of the maturity of the software development process, the performance requirements of software users have gradually become prominent. Performance issues are closely related to the source code. Thus, with the increasing complex of the software product, its performance changed during the evolution. Performance optimization related work has always been an important goal for developers who usually coding at a low-level. However, performance problems are well studied on architecture level. During the development process, some developers are ignorant of how their code modifications may affect performance. All too often, developers would like to do optimization until performance drops to a point that is unacceptable to the business side.As software developers did a lot of daily work at code level, we think code level performance awareness can help developers in sight of the performance of the code that they are working with. To deal with this, we firstly build performance-aware code change model to identify the performance changes and its related code changes at the granularity of function between each two reversions of a program. Then, we analyzed the evolution history of the code performance and mined the frequent code change patterns that used to improve performance. We have build related tool to implement the proposed approach and applied it to 8 open source projects.","code changes, development process",ICPC '19,,,
Conference Paper,Eloranta VP,Towards a Pattern Language for Software Start-Ups,,2014,,,,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 19th European Conference on Pattern Languages of Programs,"Irsee, Germany",2014,9781450334167.0,,https://doi.org/10.1145/2721956.2721965;http://dx.doi.org/10.1145/2721956.2721965,10.1145/2721956.2721965,"A growing trend in industrial software engineering is that new software products and information services are developed under conditions of notable uncertainty. This is especially visible in startup enterprises which aim at new kinds of products and services in rapidly changing social web, where potential customers can quickly adopt new behavior. Special characteristics of the startups are lack of resources and funds, and start-ups may need to change direction fast. All these affect the software engineering practices used in the start-ups.Unfortunately almost 90 percent of all start-ups fail. There are probably indefinite numbers of reasons why start-ups fail. Failure might be caused by wrongly chosen software engineering practices or inconsiderate decision making. While there is no recipe for success, we argue that good practices that can help on the way to success can be identified from successful start-ups. In this paper, we present three central patterns that could help start-ups to be successful and grow. The three patters presented in the paper are a part of larger set of patterns which was mined from successful start-ups in Finland and Switzerland.","lean start-up, patterns, start-up, organizational patterns, software engineering",EuroPLoP '14,,,
Conference Paper,"Kim H,Jung Y,Kim S,Yi K",MeCC: Memory Comparison-Based Clone Detector,,2011,,,301–310,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 33rd International Conference on Software Engineering,"Waikiki, Honolulu, HI, USA",2011,9781450304450.0,,https://doi.org/10.1145/1985793.1985835;http://dx.doi.org/10.1145/1985793.1985835,10.1145/1985793.1985835,"In this paper, we propose a new semantic clone detection technique by comparing programs' abstract memory states, which are computed by a semantic-based static analyzer.Our experimental study using three large-scale open source projects shows that our technique can detect semantic clones that existing syntactic- or semantic-based clone detectors miss. Our technique can help developers identify inconsistent clone changes, find refactoring candidates, and understand software evolution related to semantic clones.","abstract interpretation, static analysis, software maintenance, clone detection",ICSE '11,,,
Conference Paper,"Baddreddin O,Rahad K",The Impact of Design and UML Modeling on Codebase Quality and Sustainability,,2018,,,236–244,IBM Corp.,USA,Proceedings of the 28th Annual International Conference on Computer Science and Software Engineering,"Markham, Ontario, Canada",2018,,,,,"The general consensus of researchers and practitioners is that up-front and continuous software design using modeling languages such as UML improve code quality and reliability particularly as the software evolves over time. Software designs and models help in managing the underlying code complexities which are crucial for sustainability. Recently, there has been increasing evidence suggesting broader adoption of modeling languages such as UML. However, our understanding of the impact of using such modeling and design languages remains limited. This paper reports on a study that aims to characterize this impact on code quality and sustainability. We identify a sample of open source software repositories with extensive use of designs and modeling and compare their code qualities with similar code-centric repositories. Our evaluation focuses on various code quality attributes such as code smells and technical debt. We also conduct code evolution analysis over five-year period and collect additional data from questionnaires and interviews with active repository contributors. This study finds that repositories with significant use of models and design activities are associated with reduced critical code smells but are also associated with increase in non-critical code smells. The study also finds that modeling and design activities are associated with significant reduction in measures of technical debt. Analyzing code evolution over five year period reveals that UML repositories start with significantly lower technical debt density measures but tend to decline over time.","technical debt, sustainability, software maintenance, software design, empirical investigation, code-centric development, UML, open source repositories, code smells, model driven software development",CASCON '18,,,
Conference Paper,"Mkaouer MW,Kessentini M,Bechikh S,Deb K,Ó Cinnéide M",High Dimensional Search-Based Software Engineering: Finding Tradeoffs among 15 Objectives for Automating Software Refactoring Using NSGA-III,,2014,,,1263–1270,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 2014 Annual Conference on Genetic and Evolutionary Computation,"Vancouver, BC, Canada",2014,9781450326629.0,,https://doi.org/10.1145/2576768.2598366;http://dx.doi.org/10.1145/2576768.2598366,10.1145/2576768.2598366,"There is a growing need for scalable search-based software engineering approaches that address software engineering problems where a large number of objectives are to be optimized. Software refactoring is one of these problems where a refactoring sequence is sought that optimizes several software metrics. Most of the existing refactoring work uses a large set of quality metrics to evaluate the software design after applying refactoring operations, but current search-based software engineering approaches are limited to using a maximum of five metrics. We propose for the first time a scalable search-based software engineering approach based on a newly proposed evolutionary optimization method NSGA-III where there are 15 different objectives to be optimized. In our approach, automated refactoring solutions are evaluated using a set of 15 distinct quality metrics. We evaluated this approach on seven large open source systems and found that, on average, more than 92% of code smells were corrected. Statistical analysis of our experiments over 31 runs shows that NSGA-III performed significantly better than two other many-objective techniques (IBEA and MOEA/D), a multi-objective algorithm (NSGA-II) and two mono-objective approaches, hence demonstrating that our NSGA-III approach represents the new state of the art in fully-automated refactoring.","search-based software engineering, code-smells, refactroing",GECCO '14,,,
Conference Paper,"Bodden E,Havelund K",Racer: Effective Race Detection Using Aspectj,,2008,,,155–166,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 2008 International Symposium on Software Testing and Analysis,"Seattle, WA, USA",2008,9781605580500.0,,https://doi.org/10.1145/1390630.1390650;http://dx.doi.org/10.1145/1390630.1390650,10.1145/1390630.1390650,"Programming errors occur frequently in large software systems, and even more so if these systems are concurrent. In the past researchers have developed specialized programs to aid programmers detecting concurrent programming errors such as deadlocks, livelocks, starvation and data races.In this work we propose a language extension to the aspect-oriented programming language AspectJ, in the form of three new pointcuts, lock(), unlock() and maybeShared(). These pointcuts allow programmers to monitor program events where locks are granted or handed back, and where values are accessed that may be shared amongst multiple Java threads. We decide thread-locality using a static thread-local objects analysis developed by others. Using the three new primitive pointcuts, researchers can directly implement efficient monitoring algorithms to detect concurrent programming errors online. As an example, we expose a new algorithm which we call Racer, an adoption of the well-known Eraser algorithm to the memory model of Java.We implemented the new pointcuts as an extension to the AspectBench Compiler, implemented the Racer algorithm using this language extension and then applied the algorithm to the NASA K9 Rover Executive. Our experiments proved our implementation very effective. In the Rover Executive Racer finds 70 data races. Only one of these races was previously known. We further applied the algorithm to two other multi-threaded programs written by Computer Science researchers, in which we found races as well.","race detection, semantic pointcuts, aspect-oriented programming, runtime verification, static analysis",ISSTA '08,,,
Conference Paper,"He X,Avgeriou P,Liang P,Li Z",Technical Debt in MDE: A Case Study on GMF/EMF-Based Projects,,2016,,,162–172,Association for Computing Machinery,"New York, NY, USA",Proceedings of the ACM/IEEE 19th International Conference on Model Driven Engineering Languages and Systems,"Saint-malo, France",2016,9781450343213.0,,https://doi.org/10.1145/2976767.2976806;http://dx.doi.org/10.1145/2976767.2976806,10.1145/2976767.2976806,"Technical Debt (TD) is a metaphor referring to immature software artifacts that can hurt the long-term maintenance of a system. Model-Driven Engineering (MDE) is a model-centric software development approach, which promises better maintainability. However, there is a lack of empirical evidence on the existence and influence of TD in the context of MDE. This paper investigates the code-level TD in MDE projects, which is incurred during code generation. We evaluated 16 open-source and non-trivial GMF/EMF-based MDE projects using bad smells, which are widely-accepted TD indicators. The results demonstrate that MDE is not TD-free, and code generators also incur TD, similarly to developers. In fact, the generated code usually contains more TD than handwritten code, which influences significantly the maintenance of MDE projects.",,MODELS '16,,,
Conference Paper,"Salger F,Engels G,Hofmann A",Assessments in Global Software Development: A Tailorable Framework for Industrial Projects,,2010,,,29–38,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 32nd ACM/IEEE International Conference on Software Engineering - Volume 2,"Cape Town, South Africa",2010,9781605587196.0,,https://doi.org/10.1145/1810295.1810301;http://dx.doi.org/10.1145/1810295.1810301,10.1145/1810295.1810301,"Assessments are an effective technique for software quality assurance. As global software development (GSD) becomes the standard, an assessment framework must be flexible to support different sourcing and shoring models. Although much work exists on inspections and reviews, an assessment framework which addresses these challenges is missing. We present a systematic yet flexible assessment framework. The paper contributes: i) The description of our assessment framework which addresses four challenges: Appropriateness of a software requirements specification (SRS), viability of software architectures and SRS, wholeness of work packages, and compliance of results with predefined quality objectives. ii) A detailed explanation how the assessment framework can be tailored to support offshore and outsourcing scenarios. This paper describes the result of a two years research initiative at Capgemini sd&m and serves the practitioner to implement assessment frameworks according to his needs. We also discuss open research questions of high relevance for the software industry.","assessment, global software development",ICSE '10,,,
Conference Paper,"Smith J,Simons CL",A Comparison of Two Memetic Algorithms for Software Class Modelling,,2013,,,1485–1492,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 15th Annual Conference on Genetic and Evolutionary Computation,"Amsterdam, The Netherlands",2013,9781450319638.0,,https://doi.org/10.1145/2463372.2463552;http://dx.doi.org/10.1145/2463372.2463552,10.1145/2463372.2463552,"Recent research has demonstrated that the problem of class modelling within early cycle object orientated software engineering can be successfully tackled by posing it as a search problem to be tackled with meta-heuristics. This ""Search Based Software Engineering"" approach has been illustrated using both Evolutionary Algorithms and Ant Colony Optimisation to perform the underlying search. Each has been shown to display strengths and weaknesses - both in terms of how easily ""standard"" algorithms can be applied to the domain, and of optimisation performance. This paper extends that work by considering the effect of incorporating Local Search. Specifically we examine the hypothesis that within a memetic framework the choice of global search heuristic does not significantly affect search performance, freeing the decision to be made on other more subjective factors.Results show that in fact the use of local search is not always beneficial to the Ant Colony Algorithm, whereas for the Evolutionary Algorithm with order based recombination it is highly effective at improving both the quality and speed of optimisation. Across a range of parameter settings ACO found its best solutions earlier than EAs, but those solutions were of lower quality than those found by EAs. For both algorithms we demonstrated that the number of constraints present, which relates to the number of classes created, has a far bigger impact on solution quality and time than the size of the problem in terms of numbers of attributes and methods.","memetic algorithms, evolutionary algorithms, ant colony optimisation, search-based software engineering",GECCO '13,,,
Conference Paper,"Sharma T,Mishra P,Tiwari R",Designite: A Software Design Quality Assessment Tool,,2016,,,1–4,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 1st International Workshop on Bringing Architectural Design Thinking into Developers' Daily Activities,"Austin, Texas",2016,9781450341530.0,,https://doi.org/10.1145/2896935.2896938;http://dx.doi.org/10.1145/2896935.2896938,10.1145/2896935.2896938,"Poor design quality and huge technical debt are common issues perceived in real-life software projects. Design smells are indicators of poor design quality and the volume of design smells found could be treated as the design debt of the software system. The existing smell detection tools focus largely on implementation smells and do not reveal a comprehensive set of smells that arise at design level. In this paper, we present Designite - a software design quality assessment tool. It not only supports comprehensive design smells detection but also provides a detailed metrics analysis. Further, it offers various features to help identify issues contributing to design debt and improve the design quality of the analyzed software system.","design smells, technical debt, design debt, DSM, refactoring",BRIDGE '16,,,
Conference Paper,"Ouni A,Gaikovina Kula R,Kessentini M,Inoue K",Web Service Antipatterns Detection Using Genetic Programming,,2015,,,1351–1358,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 2015 Annual Conference on Genetic and Evolutionary Computation,"Madrid, Spain",2015,9781450334723.0,,https://doi.org/10.1145/2739480.2754724;http://dx.doi.org/10.1145/2739480.2754724,10.1145/2739480.2754724,"Service-Oriented Architecture (SOA) is an emerging paradigm that has radically changed the way software applications are architected, designed and implemented. SOA allows developers to structure their systems as a set of ready-made, reusable and compostable services. The leading technology used today for implementing SOA is Web Services. Indeed, like all software, Web services are prone to change constantly to add new user requirements or to adapt to environment changes. Poorly planned changes may risk introducing antipatterns into the system. Consequently, this may ultimately leads to a degradation of software quality, evident by poor quality of service (QoS). In this paper, we introduce an automated approach to detect Web service antipatterns using genetic programming. Our approach consists of using knowledge from real-world examples of Web service antipatterns to generate detection rules based on combinations of metrics and threshold values. We evaluate our approach on a benchmark of 310 Web services and a variety of five types of Web service antipatterns. The statistical analysis of the obtained results provides evidence that our approach is efficient to detect most of the existing antipatterns with a score of 85% of precision and 87% of recall.","web services, antipatterns, search-based software engineering",GECCO '15,,,
Conference Paper,"Tudu A,Bainbridge D,Rogers B",Finding a Safe Port: Cyber-Security Analysis for Open Source Digital Library Software,,2020,,,349–352,Association for Computing Machinery,"New York, NY, USA",Proceedings of the ACM/IEEE Joint Conference on Digital Libraries in 2020,"Virtual Event, China",2020,9781450375856.0,,https://doi.org/10.1145/3383583.3398612;http://dx.doi.org/10.1145/3383583.3398612,10.1145/3383583.3398612,"This article presents the results of an investigation into how safe, from a cyber-security standpoint, our Open Source Digital Library (DL) systems are. The fact that these systems use open source software presents particular challenges in terms of securely running a web-based digital repository, as a malicious user has the added advantage that they can study the source code to the system to establish new vectors of attack, in addition to the many well documented black-box forms of web hacking. To scope the work reported we focused on two widely used digital library systems: DSpace and Greenstone, undertaking both Static Application Security Testing (SAST) and Dynamic Application Security Testing (DAST), in addition to more traditional port scans. We summarize the deficiencies found and detail how to make improvements to both systems to make them more secure. We conclude by reflecting more broadly on the forms of security concerns found, to help inform future development of DL software architectures.","open source software, cyber-security, static application security testing (SAST), dynamic application security testing (DAST), digital library software",JCDL '20,,,
Journal Article,"Rech J,Ras E",Aggregation of Experiences in Experience Factories into Software Patterns,SIGSOFT Softw. Eng. Notes,2011,36.0,2.0,1–4,Association for Computing Machinery,"New York, NY, USA",,,2011-03,,0163-5948,https://doi.org/10.1145/1943371.1943390;http://dx.doi.org/10.1145/1943371.1943390,10.1145/1943371.1943390,"In software engineering Experience Factories have been in use for a long time to store and manage experiences from software projects, typically in large organizations. Beside the preservation of quantitative or numerical experiences, e.g., in form of project effort data or data from empirical studies, many experience facto-ries also preserve subjective or qualitative experiences, e.g., in form of observations or lessons learned from the projects. A key issue of experience management is to aggregate these documented experiences into more valuable software patterns. In this article we report about the aggregation (i.e., formalization and generalization) of documented experiences in an experience factory to software patterns. Observations from real-world projects are formalized (i.e., structurally contextualized) into semi-formal experiences and, over time, several similar of these experiences are generalized (i.e., systematically de-contextualized) into software patterns.","design patterns, experience management, experience factory",,,,
Conference Paper,"Sousa L,Oizumi W,Garcia A,Oliveira A,Cedrim D,Lucena C",When Are Smells Indicators of Architectural Refactoring Opportunities: A Study of 50 Software Projects,,2020,,,354–365,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 28th International Conference on Program Comprehension,"Seoul, Republic of Korea",2020,9781450379588.0,,https://doi.org/10.1145/3387904.3389276;http://dx.doi.org/10.1145/3387904.3389276,10.1145/3387904.3389276,"Refactoring is a widely adopted practice for improving code comprehension and for removing severe structural problems in a project. When refactorings affect the system architecture, they are called architectural refactorings. Unfortunately, developers usually do not know when and how they should apply refactorings to remove architectural problems. Nevertheless, they might be more susceptible to applying architectural refactoring if they rely on code smells and code refactoring -- two concepts that they usually deal with through their routine programming activities. To investigate if smells can serve as indicators of architectural refactoring opportunities, we conducted a retrospective study over the commit history of 50 software projects. We analyzed 52,667 refactored elements to investigate if they had architectural problems that could have been indicated by automatically-detected smells. We considered purely structural refactorings to identify elements that were likely to have architectural problems. We found that the proportion of refactored elements without smells is much lower than those refactored with smells. By analyzing the latter, we concluded that smells can be used as indicators of architectural refactoring opportunities when the affected source code is deteriorated, i.e., the code hosting two or more smells. For example, when God Class or Complex Class appear together with other smells, they are indicators of architectural refactoring opportunities. In general, smells that often co-occurred with other smells (67.53%) are indicators of architectural refactoring opportunities in most cases (88.53% of refactored elements). Our study also enables us to derive a catalog with patterns of smells that indicate refactoring opportunities to remove specific types of architectural problems. These patterns can guide developers and make them more susceptible to apply architectural refactorings.",,ICPC '20,,,
Conference Paper,"Gonzalez D,Rath M,Mirakhorli M",Did You Remember To Test Your Tokens?,,2020,,,232–242,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 17th International Conference on Mining Software Repositories,"Seoul, Republic of Korea",2020,9781450375177.0,,https://doi.org/10.1145/3379597.3387471;http://dx.doi.org/10.1145/3379597.3387471,10.1145/3379597.3387471,"Authentication is a critical security feature for confirming the identity of a system's users, typically implemented with help from frameworks like Spring Security. It is a complex feature which should be robustly tested at all stages of development. Unit testing is an effective technique for fine-grained verification of feature behaviors that is not widely-used to test authentication. Part of the problem is that resources to help developers unit test security features are limited. Most security testing guides recommend test cases in a ""black box"" or penetration testing perspective. These resources are not easily applicable to developers writing new unit tests, or who want a security-focused perspective on coverage.In this paper, we address these issues by applying a grounded theory-based approach to identify common (unit) test cases for token authentication through analysis of 481 JUnit tests exercising Spring Security-based authentication implementations from 53 open source Java projects. The outcome of this study is a developer-friendly unit testing guide organized as a catalog of 53 test cases for token authentication, representing unique combinations of 17 scenarios, 40 conditions, and 30 expected outcomes learned from the data set in our analysis. We supplement the test guide with common test smells to avoid. To verify the accuracy and usefulness of our testing guide, we sought feedback from selected developers, some of whom authored unit tests in our dataset.","Unit Test, Security Test, Repository Mining, Java, Authentication",MSR '20,,,
Conference Paper,Rahman A,Characteristics of Defective Infrastructure as Code Scripts in DevOps,,2018,,,476–479,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 40th International Conference on Software Engineering: Companion Proceeedings,"Gothenburg, Sweden",2018,9781450356633.0,,https://doi.org/10.1145/3183440.3183452;http://dx.doi.org/10.1145/3183440.3183452,10.1145/3183440.3183452,"Defects in infrastructure as code (IaC) scripts can have serious consequences for organizations who adopt DevOps. By identifying which characteristics of IaC scripts correlate with defects, we can identify anti-patterns, and help software practitioners make informed decisions on better development and maintenance of IaC scripts, and increase quality of IaC scripts. The goal of this paper is to help practitioners increase the quality of IaC scripts by identifying characteristics of IaC scripts and IaC development process that correlate with defects, and violate security and privacy objectives. We focus on characteristics of IaC scripts and IaC development that (i) correlate with IaC defects, and (ii) violate security and privacy-related objectives namely, confidentiality, availability, and integrity. For our initial studies, we mined open source version control systems from three organizations: Mozilla, Openstack, and Wikimedia, to identify the defect-related characteristics and conduct our case studies. From our empirical analysis, we identify (i) 14 IaC code and four churn characteristics that correlate with defects; and (ii) 12 process characteristics such as, frequency of changes, and ownership of IaC scripts that correlate with defects. We propose the following studies: (i) identify structural characteristics that correlate with defects; (ii) with respect to prediction performance, compare which characteristics of IaC scripts are more correlated with defects; and (iii) identify characteristics that violate security and privacy objectives.","infrastructure as code, devops, metrics, defects",ICSE '18,,,
Journal Article,"Zhou J,Yin K",Automated Web Testing Based on Textual-Visual UI Patterns: The UTF Approach,SIGSOFT Softw. Eng. Notes,2014,39.0,5.0,1–6,Association for Computing Machinery,"New York, NY, USA",,,2014-09,,0163-5948,https://doi.org/10.1145/2659118.2659136;http://dx.doi.org/10.1145/2659118.2659136,10.1145/2659118.2659136,"Automated software testing is the only resort for delivering quality software, since there are usually large test suites to be executed, especially for regression testing. Though many automated testing tools and techniques have been developed, they still do not solve all problems like cost and maintenance, and they can even be brittle in some situations, thus confining their adoption. To address these issues, we develop a pattern-based automated testing framework, called UTF (User-oriented Testing Framework), for Web applications. UTF encodes textual-visual information about and relationships between widgets into a domain specific language for test scripts based on the underlying invariant structural patterns in the DOM, which allows test scripts to be easily created and maintained. In addition, UTF provides flexible extension and customization capabilities to make it adaptable for various Web-application scenarios. Our experiences show UTF can greatly reduce the cost of adopting automated testing and facilitate its institutionalization.","automated testing, user-interface pattern, domain-specific language, web application",,,,
Conference Paper,"Gabel M,Yang J,Yu Y,Goldszmidt M,Su Z",Scalable and Systematic Detection of Buggy Inconsistencies in Source Code,,2010,,,175–190,Association for Computing Machinery,"New York, NY, USA",Proceedings of the ACM International Conference on Object Oriented Programming Systems Languages and Applications,"Reno/Tahoe, Nevada, USA",2010,9781450302036.0,,https://doi.org/10.1145/1869459.1869475;http://dx.doi.org/10.1145/1869459.1869475,10.1145/1869459.1869475,"Software developers often duplicate source code to replicate functionality. This practice can hinder the maintenance of a software project: bugs may arise when two identical code segments are edited inconsistently. This paper presents DejaVu, a highly scalable system for detecting these general syntactic inconsistency bugs.DejaVu operates in two phases. Given a target code base, a parallel /inconsistent clone analysis/ first enumerates all groups of source code fragments that are similar but not identical. Next, an extensible /buggy change analysis/ framework refines these results, separating each group of inconsistent fragments into a fine-grained set of inconsistent changes and classifying each as benign or buggy.On a 75+ million line pre-production commercial code base, DejaVu executed in under five hours and produced a report of over 8,000 potential bugs. Our analysis of a sizable random sample suggests with high likelihood that at this report contains at least 2,000 true bugs and 1,000 code smells. These bugs draw from a diverse class of software defects and are often simple to correct: syntactic inconsistencies both indicate problems and suggest solutions.","clone detection, static analysis, bug detection",OOPSLA '10,,,
Journal Article,"Gabel M,Yang J,Yu Y,Goldszmidt M,Su Z",Scalable and Systematic Detection of Buggy Inconsistencies in Source Code,SIGPLAN Not.,2010,45.0,10.0,175–190,Association for Computing Machinery,"New York, NY, USA",,,2010-10,,0362-1340,https://doi.org/10.1145/1932682.1869475;http://dx.doi.org/10.1145/1932682.1869475,10.1145/1932682.1869475,"Software developers often duplicate source code to replicate functionality. This practice can hinder the maintenance of a software project: bugs may arise when two identical code segments are edited inconsistently. This paper presents DejaVu, a highly scalable system for detecting these general syntactic inconsistency bugs.DejaVu operates in two phases. Given a target code base, a parallel /inconsistent clone analysis/ first enumerates all groups of source code fragments that are similar but not identical. Next, an extensible /buggy change analysis/ framework refines these results, separating each group of inconsistent fragments into a fine-grained set of inconsistent changes and classifying each as benign or buggy.On a 75+ million line pre-production commercial code base, DejaVu executed in under five hours and produced a report of over 8,000 potential bugs. Our analysis of a sizable random sample suggests with high likelihood that at this report contains at least 2,000 true bugs and 1,000 code smells. These bugs draw from a diverse class of software defects and are often simple to correct: syntactic inconsistencies both indicate problems and suggest solutions.","bug detection, static analysis, clone detection",,,,
Journal Article,"Filieri A,Maggio M,Angelopoulos K,D'ippolito N,Gerostathopoulos I,Hempel AB,Hoffmann H,Jamshidi P,Kalyvianaki E,Klein C,Krikava F,Misailovic S,Papadopoulos AV,Ray S,Sharifloo AM,Shevtsov S,Ujma M,Vogel T",Control Strategies for Self-Adaptive Software Systems,ACM Trans. Auton. Adapt. Syst.,2017,11.0,4.0,,Association for Computing Machinery,"New York, NY, USA",,,2017-02,,1556-4665,https://doi.org/10.1145/3024188;http://dx.doi.org/10.1145/3024188,10.1145/3024188,"The pervasiveness and growing complexity of software systems are challenging software engineering to design systems that can adapt their behavior to withstand unpredictable, uncertain, and continuously changing execution environments. Control theoretical adaptation mechanisms have received growing interest from the software engineering community in the last few years for their mathematical grounding, allowing formal guarantees on the behavior of the controlled systems. However, most of these mechanisms are tailored to specific applications and can hardly be generalized into broadly applicable software design and development processes.This article discusses a reference control design process, from goal identification to the verification and validation of the controlled system. A taxonomy of the main control strategies is introduced, analyzing their applicability to software adaptation for both functional and nonfunctional goals. A brief extract on how to deal with uncertainty complements the discussion. Finally, the article highlights a set of open challenges, both for the software engineering and the control theory research communities.","non-functional properties, Self-adaptive software, control theory, formal methods",,,,
Conference Paper,"Couto M,Maia D,Saraiva J,Pereira R",On Energy Debt: Managing Consumption on Evolving Software,,2020,,,62–66,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 3rd International Conference on Technical Debt,"Seoul, Republic of Korea",2020,9781450379601.0,,https://doi.org/10.1145/3387906.3388628;http://dx.doi.org/10.1145/3387906.3388628,10.1145/3387906.3388628,"This paper introduces the concept of energy debt: a new metric, reflecting the implied cost in terms of energy consumption over time, of choosing a flawed implementation of a software system rather than a more robust, yet possibly time consuming, approach. A flawed implementation is considered to contain code smells, known to have a negative influence on the energy consumption.Similar to technical debt, if energy debt is not properly addressed, it can accumulate an energy ""interest"". This interest will keep increasing as new versions of the software are released, and eventually reach a point where the interest will be higher than the initial energy debt. Addressing the issues/smells at such a point can remove energy debt, at the cost of having already consumed a significant amount of energy which can translate into high costs. We present all underlying concepts of energy debt, bridging the connection with the existing concept of technical debt and show how to compute the energy debt through a motivational example.","code analysis, energy debt, green software",TechDebt '20,,,
Conference Paper,"Tamrawi A,Nguyen HA,Nguyen HV,Nguyen TN",Build Code Analysis with Symbolic Evaluation,,2012,,,650–660,IEEE Press,"Zurich, Switzerland",Proceedings of the 34th International Conference on Software Engineering,,2012,9781467310673.0,,,,"Build process is crucial in software development. However, the analysis support for build code is still limited. In this paper, we present SYMake, an infrastructure and tool for the analysis of build code in make. Due to the dynamic nature of make language, it is challenging to understand and maintain complex Makefiles. SYMake provides a symbolic evaluation algorithm that processes Makefiles and produces a symbolic dependency graph (SDG), which represents the build dependencies (i.e. rules) among files via commands. During the symbolic evaluation, for each resulting string value in an SDG that represents a part of a file name or a command in a rule, SYMake provides also an acyclic graph (called T-model) to represent its symbolic evaluation trace. We have used SYMake to develop algorithms and a tool 1) to detect several types of code smells and errors in Makefiles, and 2) to support build code refactoring, e.g. renaming a variable/target even if its name is fragmented and built from multiple substrings. Our empirical evaluation for SYMake's renaming on several real-world systems showed its high accuracy in entity renaming. Our controlled experiment showed that with SYMake, developers were able to understand Makefiles better and to detect more code smells as well as to perform refactoring more accurately.",,ICSE '12,,,
Journal Article,"Alimadadi S,Zhong D,Madsen M,Tip F",Finding Broken Promises in Asynchronous JavaScript Programs,Proc. ACM Program. Lang.,2018,2.0,OOPSLA,,Association for Computing Machinery,"New York, NY, USA",,,2018-10,,,https://doi.org/10.1145/3276532;http://dx.doi.org/10.1145/3276532,10.1145/3276532,"Recently, promises were added to ECMAScript 6, the JavaScript standard, in order to provide better support for the asynchrony that arises in user interfaces, network communication, and non-blocking I/O. Using promises, programmers can avoid common pitfalls of event-driven programming such as event races and the deeply nested counterintuitive control ow referred to as “callback hell”. Unfortunately, promises have complex semantics and the intricate control– and data- ow present in promise-based code hinders program comprehension and can easily lead to bugs. The promise graph was proposed as a graphical aid for understanding and debugging promise-based code. However, it did not cover all promise-related features in ECMAScript 6, and did not present or evaluate any technique for constructing the promise graphs. In this paper, we extend the notion of promise graphs to include all promise-related features in ECMAScript 6, including default reactions, exceptions, and the synchronization operations race and all. Furthermore, we report on the construction and evaluation of PromiseKeeper, which performs a dynamic analysis to create promise graphs and infer common promise anti-patterns. We evaluate PromiseKeeper by applying it to 12 open source promise-based Node.js applications. Our results suggest that the promise graphs constructed by PromiseKeeper can provide developers with valuable information about occurrences of common anti-patterns in their promise-based code, and that promise graphs can be constructed with acceptable run-time overhead.","Promises, PromiseKeeper, Promise Graph, JavaScript, Dynamic Analysis",,,,
Conference Paper,"Tang Y,Sui Y,Wang H,Luo X,Zhou H,Xu Z",All Your App Links Are Belong to Us: Understanding the Threats of Instant Apps Based Attacks,,2020,,,914–926,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering,"Virtual Event, USA",2020,9781450370431.0,,https://doi.org/10.1145/3368089.3409702;http://dx.doi.org/10.1145/3368089.3409702,10.1145/3368089.3409702,"Android deep link is a URL that takes users to a specific page of a mobile app, enabling seamless user experience from a webpage to an app. Android app link, a new type of deep link introduced in Android 6.0, is claimed to offer more benefits, such as supporting instant apps and providing more secure verification to protect against hijacking attacks that previous deep links can not. However, we find that the app link is not as secure as claimed, because the verification process can be bypassed by exploiting instant apps. In this paper, we explore the weakness of the existing app link mechanism and propose three feasible hijacking attacks. Our findings show that even popular apps are subject to these attacks, such as Twitter, Whatsapp, Facebook Message. Our observation is confirmed by Google. To measure the severity of these vulnerabilities, we develop an automatic tool to detect vulnerable apps, and perform a large-scale empirical study on 400,000 Android apps. Experiment results suggest that app link hijacking vulnerabilities are prevalent in the ecosystem. Specifically, 27.1% apps are vulnerable to link hijacking with smart text selection (STS); 30.0% apps are vulnerable to link hijacking without STS, and all instant apps are vulnerable to instant app attack. We provide an in-depth understanding of the mechanisms behind these types of attacks. Furthermore, we propose the corresponding detection and defense methods that can successfully prevent the proposed hijackings for all the evaluated apps, thus raising the bar against the attacks on Android app links. Our insights and findings demonstrate the urgency to identify and prevent app link hijacking attacks.","Instant app, Android, App Link, Deep Link",ESEC/FSE 2020,,,
Conference Paper,"Kushilevitz E,Lu S,Ostrovsky R",On the (in)Security of Hash-Based Oblivious RAM and a New Balancing Scheme,,2012,,,143–156,Society for Industrial and Applied Mathematics,USA,Proceedings of the Twenty-Third Annual ACM-SIAM Symposium on Discrete Algorithms,"Kyoto, Japan",2012,,,,,"With the gaining popularity of remote storage (e.g. in the Cloud), we consider the setting where a small, protected local machine wishes to access data on a large, untrusted remote machine. This setting was introduced in the RAM model in the context of software protection by Goldreich and Ostrovsky. A secure Oblivious RAM simulation allows for a client, with small (e.g., constant size) protected memory, to hide not only the data but also the sequence of locations it accesses (both reads and writes) in the unprotected memory of size n.Our main results are as follows:• We analyze several schemes from the literature, observing a repeated design flaw that leaks information on the memory access pattern. For some of these schemes, the leakage is actually non-negligible, while for others it is negligible.• On the positive side, we present a new secure oblivious RAM scheme, extending a recent scheme by Goodrich and Mitzenmacher. Our scheme uses only O(1) local memory, and its (amortized) overhead is O(log2 n/log log n), outperforming the previously-best O(log2 n) overhead (among schemes where the client only uses O(1) additional local memory).• We also present a transformation of our scheme above (whose amortized overhead is O(log2 n/log log n)) into a scheme with worst-case overhead of O(log2 n/log log n).","cuckoo hashing, oblivious RAM, secure computation",SODA '12,,,
Conference Paper,"Benni B,Mosser S,Collet P,Riveill M",Supporting Micro-Services Deployment in a Safer Way: A Static Analysis and Automated Rewriting Approach,,2018,,,1706–1715,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 33rd Annual ACM Symposium on Applied Computing,"Pau, France",2018,9781450351911.0,,https://doi.org/10.1145/3167132.3167314;http://dx.doi.org/10.1145/3167132.3167314,10.1145/3167132.3167314,"The SOA ecosystem has drastically evolved since its childhood in the early 2000s. From monolithic services, micro-services now cooperate together in ultra-large scale systems. In this context, there is a tremendous need to deploy frequently new services, or new version of existing services. Container-based technologies (e.g., Docker) emerged recently to tool such deployments, promoting a black-box reuse mechanism to support off-the-shelf deployments. Unfortunately, from the service deployment point of view, such form of black-box reuse prevent to ensure what is really shipped inside the container with the service to deploy. In this paper, we propose a formalism to model and statically analyze service deployment artifacts based on state of the art deployment platforms. The static analysis mechanism leverages the hierarchy of deployment descriptors to verify a given deployment, as well as rewrite it to automatically fix common errors. The approach is validated through the automation of the guidelines provided by the user community associated to the reference Docker engine, and the analysis of 20,000 real deployment descriptors (hosted on GitHub).","container, static analysis, docker, microservice",SAC '18,,,
Conference Paper,"Soares E,Ribeiro M,Amaral G,Gheyi R,Fernandes L,Garcia A,Fonseca B,Santos A",Refactoring Test Smells: A Perspective from Open-Source Developers,,2020,,,50–59,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 5th Brazilian Symposium on Systematic and Automated Software Testing,"Natal, Brazil",2020,9781450387552.0,,https://doi.org/10.1145/3425174.3425212;http://dx.doi.org/10.1145/3425174.3425212,10.1145/3425174.3425212,"Test smells are symptoms in the test code that indicate possible design or implementation problems. Their presence, along with their harmfulness, has already been demonstrated by previous researches. However, we do not know to what extent developers acknowledge the presence of test smells and how to refactor existing code to eliminate them in practice. This study aims to assess open-source developers' awareness about the existence of test smells and their refactoring strategies. We conducted a mixed-method study with two parts: (i) a survey with 73 experienced open-source developers to assess their preference and motivation to choose between 10 different smelly test code samples, found in 272 open-source projects, and their refactored versions; and (ii) the submission of 50 pull requests to assess developers' acceptance of the proposed refactorings. As a result, most surveyed developers preferred the refactored proposal for 78% of the investigated test smells, and the pull requests had an average acceptance of 75% among respondents. Additionally, we were able to provide empiric validation for literature-proposed refactoring strategies. This study demonstrates that although not always using the academic terminology, developers acknowledge both the negative impact of test smells presence and most of the literature's proposals for their removal.",,SAST 20,,,
Conference Paper,"Chen Z,Kang Y,Li L,Zhang X,Zhang H,Xu H,Zhou Y,Yang L,Sun J,Xu Z,Dang Y,Gao F,Zhao P,Qiao B,Lin Q,Zhang D,Lyu MR",Towards Intelligent Incident Management: Why We Need It and How We Make It,,2020,,,1487–1497,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering,"Virtual Event, USA",2020,9781450370431.0,,https://doi.org/10.1145/3368089.3417055;http://dx.doi.org/10.1145/3368089.3417055,10.1145/3368089.3417055,"The management of cloud service incidents (unplanned interruptions or outages of a service/product) greatly affects customer satisfaction and business revenue. After years of efforts, cloud enterprises are able to solve most incidents automatically and timely. However, in practice, we still observe critical service incidents that occurred in an unexpected manner and orchestrated diagnosis workflow failed to mitigate them. In order to accelerate the understanding of unprecedented incidents and provide actionable recommendations, modern incident management system employs the strategy of AIOps (Artificial Intelligence for IT Operations). In this paper, to provide a broad view of industrial incident management and understand the modern incident management system, we conduct a comprehensive empirical study spanning over two years of incident management practices at Microsoft. Particularly, we identify two critical challenges (namely, incomplete service/resource dependencies and imprecise resource health assessment) and investigate the underlying reasons from the perspective of cloud system design and operations. We also present IcM BRAIN, our AIOps framework towards intelligent incident management, and show its practical benefits conveyed to the cloud services of Microsoft.","Cloud Computing, AIOps, Incident Management",ESEC/FSE 2020,,,
Conference Paper,"Zarras AV,Mamalis G,Papamichail A,Kollias P,Vassiliadis P",And the Tool Created a GUI That Was Impure and Without Form: Anti-Patterns in Automatically Generated GUIs,,2018,,,,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 23rd European Conference on Pattern Languages of Programs,"Irsee, Germany",2018,9781450363877.0,,https://doi.org/10.1145/3282308.3282333;http://dx.doi.org/10.1145/3282308.3282333,10.1145/3282308.3282333,"A basic prerequisite for any daily development task is to understand the source code that we are working with. To this end, the source code should be clean. Usually, it is up to us, the developers, to keep the source code clean. However, often there are parts of the code that are automatically generated. A typical such case are Graphical User Interfaces (GUIs) created via a GUI builder, i.e., a tool that allows the developer to design the GUI by combining graphical control elements, offered in a palette. In this paper, we investigate the quality of the code that is generated by GUI builders. To assist tool-smiths in developing better GUI builders, we report anti-patterns concerning naming, documentation, design and implementation issues, observed in a study that involves four popular GUI builders for Java. The reported anti-patterns can further assist GUI developers/designers in selecting appropriate tools.","Refactoring, Responsibilities, Patterns, GUIs, Code Clones",EuroPLoP '18,,,
Conference Paper,"Shihab E,Hassan AE,Adams B,Jiang ZM",An Industrial Study on the Risk of Software Changes,,2012,,,,Association for Computing Machinery,"New York, NY, USA",Proceedings of the ACM SIGSOFT 20th International Symposium on the Foundations of Software Engineering,"Cary, North Carolina",2012,9781450316149.0,,https://doi.org/10.1145/2393596.2393670;http://dx.doi.org/10.1145/2393596.2393670,10.1145/2393596.2393670,"Modelling and understanding bugs has been the focus of much of the Software Engineering research today. However, organizations are interested in more than just bugs. In particular, they are more concerned about managing risk, i.e., the likelihood that a code or design change will cause a negative impact on their products and processes, regardless of whether or not it introduces a bug. In this paper, we conduct a year-long study involving more than 450 developers of a large enterprise, spanning more than 60 teams, to better understand risky changes, i.e., changes for which developers believe that additional attention is needed in the form of careful code or design reviewing and/or more testing. Our findings show that different developers and different teams have their own criteria for determining risky changes. Using factors extracted from the changes and the history of the files modified by the changes, we are able to accurately identify risky changes with a recall of more than 67%, and a precision improvement of 87% (using developer specific models) and 37% (using team specific models), over a random model. We find that the number of lines and chunks of code added by the change, the bugginess of the files being changed, the number of bug reports linked to a change and the developer experience are the best indicators of change risk. In addition, we find that when a change has many related changes, the reliability of developers in marking risky changes is negatively affected. Our findings and models are being used today in practice to manage the risk of software projects.","change risk, code metrics, bug inducing changes, change metrics",FSE '12,,,
Conference Paper,"Papadopoulos L,Marantos C,Digkas G,Ampatzoglou A,Chatzigeorgiou A,Soudris D","Interrelations between Software Quality Metrics, Performance and Energy Consumption in Embedded Applications",,2018,,,62–65,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 21st International Workshop on Software and Compilers for Embedded Systems,"Sankt Goar, Germany",2018,9781450357807.0,,https://doi.org/10.1145/3207719.3207736;http://dx.doi.org/10.1145/3207719.3207736,10.1145/3207719.3207736,"Source code refactorings and transformations are extensively used by embedded system developers to improve the quality of applications, often supported by various open source and proprietary tools. They either aim at improving the design time quality such as the maintainability and reusability of software artifacts, or the runtime quality such as performance and energy efficiency. However, an inherent trade-off between design- and run-time qualities is often present posing challenges to embedded software development. This work is a first step towards the investigation of the impact of transformations for improving the performance and the energy efficiency on software quality metrics and the impact of refactorings for increasing the design time quality on the execution time, the memory and the energy consumption. Based on a set of embedded applications from widely used benchmark suites and typical transformations and refactorings, we identify interrelations and trade-offs between the aforementioned metrics.",,SCOPES '18,,,
Journal Article,"Landwehr CE,Bull AR,McDermott JP,Choi WS",A Taxonomy of Computer Program Security Flaws,ACM Comput. Surv.,1994,26.0,3,211–254,Association for Computing Machinery,"New York, NY, USA",,,1994-09,,0360-0300,https://doi.org/10.1145/185403.185412;http://dx.doi.org/10.1145/185403.185412,10.1145/185403.185412,"An organized record of actual flaws can be useful to computer system designers, programmers, analysts, administrators, and users. This survey provides a taxonomy for computer program security flaws, with an Appendix that documents 50 actual security flaws. These flaws have all been described previously in the open literature, but in widely separated places. For those new to the field of computer security, they provide a good introduction to the characteristics of security flaws and how they can arise. Because these flaws were not randomly selected from a valid statistical sample of such flaws, we make no strong claims concerning the likely distribution of actual security flaws within the taxonomy. However, this method of organizing security flaw data can help those who have custody of more representative samples to organize them and to focus their efforts to remove and, eventually, to prevent the introduction of security flaws.","security flaw, error/defect classification, taxonomy",,,,
Conference Paper,"Verdecchia R,Malavolta I,Lago P",Architectural Technical Debt Identification: The Research Landscape,,2018,,,11–20,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 2018 International Conference on Technical Debt,"Gothenburg, Sweden",2018,9781450357135.0,,https://doi.org/10.1145/3194164.3194176;http://dx.doi.org/10.1145/3194164.3194176,10.1145/3194164.3194176,"Architectural Technical Debt (ATD) regards sub-optimal design decisions that bring short-term benefits to the cost of long-term gradual deterioration of the quality of the architecture of a software system. The identification of ATD strongly influences the technical and economic sustainability of software systems and is attracting growing interest in the scientific community. During the years several approaches for ATD identification have been conceived, each of them addressing ATD from different perspectives and with heterogeneous characteristics.In this paper we apply the systematic mapping study methodology for identifying, classifying, and evaluating the state of the art on ATD identification from the following three perspectives: publication trends, characteristics, and potential for industrial adoption. Specifically, starting from a set of 509 potentially relevant studies, we systematically selected 47 primary studies and analyzed them according to a rigorously-defined classification framework.The analysis of the obtained results supports both researchers and practitioners by providing (i) an assessment of current research trends and gaps in ATD identification, (ii) a solid foundation for understanding existing (and future) research on ATD identification, and (iii) a rigorous evaluation of its potential for industrial adoption.","technical debt, systematic mapping study, software architecture",TechDebt '18,,,
Conference Paper,"Cortellessa V,Martens A,Reussner R,Trubiani C","Towards the Identification of ""Guilty"" Performance Antipatterns",,2010,,,245–246,Association for Computing Machinery,"New York, NY, USA",Proceedings of the First Joint WOSP/SIPEW International Conference on Performance Engineering,"San Jose, California, USA",2010,9781605585635.0,,https://doi.org/10.1145/1712605.1712644;http://dx.doi.org/10.1145/1712605.1712644,10.1145/1712605.1712644,"The problem of interpreting the results of software performance analysis is very critical. Software developers expect feedback in terms of architectural design alternatives (e.g., re-deploy a component), whereas the results of performance analysis are pure numbers. Support to the interpretation of such results that helps to fill the gap between numbers and software alternatives is still lacking. Performance antipatterns can play a key role in the search of performance problems and in the formulation of their solutions. In this poster, we introduce a process to elaborate the analysis results and to score performance requirements, model entities and ""guilty"" performance antipatterns.","antipatterns, feedback, performance analysis, software performance engineering",WOSP/SIPEW '10,,,
Conference Paper,"Lou Y,Chen J,Zhang L,Hao D,Zhang L",History-Driven Build Failure Fixing: How Far Are We?,,2019,,,43–54,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 28th ACM SIGSOFT International Symposium on Software Testing and Analysis,,2019,9781450362245.0,,https://doi.org/10.1145/3293882.3330578;http://dx.doi.org/10.1145/3293882.3330578,10.1145/3293882.3330578,"Build systems are essential for modern software development and maintenance since they are widely used to transform source code artifacts into executable software. Previous work shows that build systems break frequently during software evolution. Therefore, automated build-fixing techniques are in huge demand. In this paper we target a mainstream build system, Gradle, which has become the most widely used build system for Java projects in the open-source community (e.g., GitHub). HireBuild, state-of-the-art build-fixing tool for Gradle, has been recently proposed to fix Gradle build failures via mining the history of prior fixes. Although HireBuild has been shown to be effective for fixing real-world Gradle build failures, it was evaluated on only a limited set of build failures, and largely depends on the quality/availability of historical fix information. To investigate the efficacy and limitations of the history-driven build fix, we first construct a new and large build failure dataset from Top-1000 GitHub projects. Then, we evaluate HireBuild on the extended dataset both quantitatively and qualitatively. Inspired by the findings of the study, we propose a simplistic new technique that generates potential patches via searching from the present project under test and external resources rather than the historical fix information. According to our experimental results, the simplistic approach based on present information successfully fixes 2X more reproducible build failures than the state-of-art HireBuild based on historical fix information. Furthermore, our results also reveal various findings/guidelines for future advanced build failure fixing.","Automated Program Repair, Build System, Build Failure Fixing, location = Beijing, China, series = ISSTA 2019",,,,
Conference Paper,"Griffioen H,Doerr C",Examining Mirai's Battle over the Internet of Things,,2020,,,743–756,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 2020 ACM SIGSAC Conference on Computer and Communications Security,"Virtual Event, USA",2020,9781450370899.0,,https://doi.org/10.1145/3372297.3417277;http://dx.doi.org/10.1145/3372297.3417277,10.1145/3372297.3417277,"Using hundreds of thousands of compromised IoT devices, the Mirai botnet emerged in late 2016 as a game changing threat actor, capable of temporarily taking down major Internet service providers and Internet infrastructure. Since then, dozens of variants of IoT-based botnets have sprung up, and in today's Internet distributed denial-of-service attacks from IoT devices have become a major attack vector. This proliferation was significantly driven by the public distribution of the Mirai source code, which other actors used to create their own, customized version of the original Mirai botnet. In this paper we provide a comprehensive view into the ongoing battle over the Internet of Things fought by Mirai and its many siblings. Using 7,500 IoT honeypots, we show that we can use 300,000,000 compromisation attempts from infected IoT devices as well as a design flaw in Mirai's random number generator to obtain insights into Mirai infections worldwide. We find that networks and the particular malware strains that plague them are tightly connected, and malware authors over time take over strategies from their competitors. The most surprising finding is that epidemiologically, IoT botnets are not self-sustaining: were it not for continuous pushes from bootstrapping, Mirai and its variants would die out.","botnet, mirai, cyber threat intelligence, iot",CCS '20,,,
Conference Paper,"Molnar AJ,Motogna S,Vlad C",Using Static Analysis Tools to Assist Student Project Evaluation,,2020,,,7–12,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 2nd ACM SIGSOFT International Workshop on Education through Advanced Software Engineering and Artificial Intelligence,"Virtual, USA",2020,9781450381024.0,,https://doi.org/10.1145/3412453.3423195;http://dx.doi.org/10.1145/3412453.3423195,10.1145/3412453.3423195,Code review and static analysis tools are acknowledged as important instruments in software quality control and are used in the industry on a daily basis. In this exploratory study we examine how a well-known static analysis tool can be employed to assess the quality of student solutions to coding assignments. We examine all student solutions submitted to fulfill coding assignments required as part of an introductory programming course taught using Python. We show how teaching staff can evaluate the progress of individual students and how coding mistakes common to many students can be highlighted. We also show how teaching staff can improve their own understanding of perceived assignment complexity by evaluating the aggregate quality of student submitted source code.,"Computer Science education, student evaluation, static analysis",EASEAI 2020,,,
Conference Paper,"Russo D,Ciancarini P,Falasconi T,Tomasi M",Software Quality Concerns in the Italian Bank Sector: The Emergence of a Meta-Quality Dimension,,2017,,,63–72,IEEE Press,"Buenos Aires, Argentina",Proceedings of the 39th International Conference on Software Engineering: Software Engineering in Practice Track,,2017,9781538627174.0,,https://doi.org/10.1109/ICSE-SEIP.2017.10;http://dx.doi.org/10.1109/ICSE-SEIP.2017.10,10.1109/ICSE-SEIP.2017.10,"This paper reports on a Delphi-like study about the Italian banking IT sector's greatest concerns. A new research framework was developed to pursue this vertical study: domain and country specific, using a Mixed Methods approach. Data collection was drawn in four phases starting with a high level randomly stratified panel of 13 senior managers and then a target-panel of 124 carefully selected and well-informed domain experts. We have identified and dealt with 15 concerns about the present situation; they were discussed in a framework inspired by the ISO 25010 standard. After having mapped the concerns within the ISO standard, we identified the emergence of a new meta quality dimension which impacts both on software quality and architectural description. Our inductive outcome lets this meta dimension emerge connecting both ISO 25010 and ISO 42010 standards.","delphi study, software quality, mixed methods, information systems",ICSE-SEIP '17,,,
Conference Paper,"Islam SS,Krinke J,Binkley D",Dependence Cluster Visualization,,2010,,,93–102,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 5th International Symposium on Software Visualization,"Salt Lake City, Utah, USA",2010,9781450300285.0,,https://doi.org/10.1145/1879211.1879227;http://dx.doi.org/10.1145/1879211.1879227,10.1145/1879211.1879227,"Large clusters of mutual dependence have long been regarded as a problem impeding comprehension, testing, maintenance, and reverse engineering. An effective visualization can aid an engineer in addressing the presence of large clusters. Such a visualization is presented. It allows a program's dependence clusters to be considered from an abstract high level down thru a concrete source-level. At the highest level of abstraction, the visualization uses a heat-map (a color scheme) to efficiently overview the clusters found in an entire system. Other levels include three source code views that allow a user to ""zoom"" in on the clusters starting from the high-level system view, down through a file view, and then onto the actual source code where each cluster can be studied in detail.Also presented are two case studies, the first is the open-source calculator bc and the second is the industrial program copia, which performs signal processing. The studies consider qualitative evaluations of the visualization. From the results, it is seen that the visualization reveals high-level structure of programs and interactions between its components. The results also show that the visualization highlights potential candidates (functions/files) for re-factoring in bc and finds dependence pollution in copia.","program slicing, clustering, program comprehension, dependence, visualization, re-engineering, reverse engineering",SOFTVIS '10,,,
Conference Paper,Mongiovi M,Scaling Testing of Refactoring Engines,,2016,,,674–676,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 38th International Conference on Software Engineering Companion,"Austin, Texas",2016,9781450342056.0,,https://doi.org/10.1145/2889160.2891038;http://dx.doi.org/10.1145/2889160.2891038,10.1145/2889160.2891038,"Researchers have proposed a number of automated techniques for testing refactoring engines. However, they may have limitations related to the program generator, time consumption, kinds of bugs, and debugging. We propose a technique to scale testing of refactoring engines. We improve expressiveness of a program generator, use a technique to skip some test inputs to improve performance, and propose new oracles to detect behavioral changes using change impact analysis, overly strong conditions using mutation testing, and transformation issues related to the refactoring definitions. We evaluate our technique in 24 refactoring implementations of Java (Eclipse and JRRT) and C (Eclipse) and found 119 bugs. The technique reduces the time in 96% using skips while misses only 7% of the bugs. Using the new oracle to identify overly strong conditions, it detects 37% of new bugs while misses 16% of the bugs comparing with a previous technique. Furthermore, the proposed oracle facilitates debugging by indicating the overly strong conditions.","refactoring, program generation, testing",ICSE '16,,,
Conference Paper,"Kessentini M,Vaucher S,Sahraoui H",Deviance from Perfection is a Better Criterion than Closeness to Evil When Identifying Risky Code,,2010,,,113–122,Association for Computing Machinery,"New York, NY, USA",Proceedings of the IEEE/ACM International Conference on Automated Software Engineering,"Antwerp, Belgium",2010,9781450301169.0,,https://doi.org/10.1145/1858996.1859015;http://dx.doi.org/10.1145/1858996.1859015,10.1145/1858996.1859015,"We propose an approach for the automatic detection of potential design defects in code. The detection is based on the notion that the more code deviates from good practices, the more likely it is bad. Taking inspiration from artificial immune systems, we generated a set of detectors that characterize different ways that a code can diverge from good practices. We then used these detectors to measure how far code in assessed systems deviates from normality.We evaluated our approach by finding potential defects in two open-source systems (Xerces-J and Gantt). We used the library JHotDraw as the code base representing good design/programming practices. In both systems, we found that 90% of the riskiest classes were defects, a precision far superiour to state of the art rule-based approaches.","artificial immune systems, maintenance, design defects",ASE '10,,,
Conference Paper,"Maneva N,Grozev N,Lilov D",A Framework for Source Code Metrics,,2010,,,113–118,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 11th International Conference on Computer Systems and Technologies and Workshop for PhD Students in Computing on International Conference on Computer Systems and Technologies,"Sofia, Bulgaria",2010,9781450302432.0,,https://doi.org/10.1145/1839379.1839400;http://dx.doi.org/10.1145/1839379.1839400,10.1145/1839379.1839400,"The paper presents our approach to the systematic and tool-supported source code measurement for quality analysis. First, we describe the results of the performed thorough study about the use of some static measurement tools. We classify them as reporting and combining and manage to elucidate the main requirements for a feasible framework, supporting a set of such metrics. Next the details about the design and implementation principles of the framework are given. In conclusion we share some ideas about future scientific and practice-oriented work in this area.","framework, program quality, software metrics, source code, static analysis",CompSysTech '10,,,
Conference Paper,"Bettini L,Di Ruscio D,Iovino L,Pierantonio A",Edelta 2.0: Supporting Live Metamodel Evolutions,,2020,,,,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 23rd ACM/IEEE International Conference on Model Driven Engineering Languages and Systems: Companion Proceedings,"Virtual Event, Canada",2020,9781450381352.0,,https://doi.org/10.1145/3417990.3419501;http://dx.doi.org/10.1145/3417990.3419501,10.1145/3417990.3419501,"Evolving metamodels is a delicate task, both from the programming effort's point of view and, more importantly, from the correctness point of view: the evolved version of a metamodel must be correct and must not contain invalid elements (e.g., dangling references). In this paper we present the new version of Edelta, which provides EMF modelers with linguistic constructs for specifying both basic and complex refactorings. Edelta 2.0 is supported by an Eclipse-based IDE, which provides in this new version a ""live"" development environment for evolving metamodels. The modelers receive an immediate feedback of the evolved versions of the metamodels in the IDE. Moreover, Edelta performs many static checks, also by means of an interpreter that keeps track on-the-fly of the evolved metamodel, enforcing the correctness of the evolution right in the IDE, based on the flow of the execution of the refactoring operations specified by the user. Finally, Edelta 2.0 allows the users to easily introduce additional validation checks in their own Edelta programs, which are taken into consideration by the Edelta compiler and the IDE.","refactoring, edelta, evolution, metamodels",MODELS '20,,,
Conference Paper,"Sharma T,Fragkoulis M,Rizou S,Bruntink M,Spinellis D",Smelly Relations: Measuring and Understanding Database Schema Quality,,2018,,,55–64,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 40th International Conference on Software Engineering: Software Engineering in Practice,"Gothenburg, Sweden",2018,9781450356596.0,,https://doi.org/10.1145/3183519.3183529;http://dx.doi.org/10.1145/3183519.3183529,10.1145/3183519.3183529,"Context: Databases are an integral element of enterprise applications. Similarly to code, database schemas are also prone to smells - best practice violations.Objective: We aim to explore database schema quality, associated characteristics and their relationships with other software artifacts.Method: We present a catalog of 13 database schema smells and elicit developers' perspective through a survey. We extract embedded sql statements and identify database schema smells by employing the DbDeo tool which we developed. We analyze 2925 production-quality systems (357 industrial and 2568 well-engineered open-source projects) and empirically study quality characteristics of their database schemas. In total, we analyze 629 million lines of code containing more than 393 thousand sql statements.Results: We find that the index abuse smell occurs most frequently in database code, that the use of an orm framework doesn't immune the application from database smells, and that some database smells, such as adjacency list, are more prone to occur in industrial projects compared to open-source projects. Our co-occurrence analysis shows that whenever the clone table smell in industrial projects and the values in attribute definition smell in open-source projects get spotted, it is very likely to find other database smells in the project.Conclusion: The awareness and knowledge of database smells are crucial for developing high-quality software systems and can be enhanced by the adoption of better tools helping developers to identify database smells early.","database schema smells, software quality, software maintenance, code smells, technical debt, antipatterns",ICSE-SEIP '18,,,
Conference Paper,"Murphy-Hill E,Black AP",An Interactive Ambient Visualization for Code Smells,,2010,,,5–14,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 5th International Symposium on Software Visualization,"Salt Lake City, Utah, USA",2010,9781450300285.0,,https://doi.org/10.1145/1879211.1879216;http://dx.doi.org/10.1145/1879211.1879216,10.1145/1879211.1879216,"Code smells are characteristics of software that indicate that code may have a design problem. Code smells have been proposed as a way for programmers to recognize the need for restructuring their software. Because code smells can go unnoticed while programmers are working, tools called smell detectors have been developed to alert programmers to the presence of smells in their code, and to help them understand the cause of those smells. In this paper, we propose a novel smell detector called Stench Blossom that provides an interactive ambient visualization designed to first give programmers a quick, high-level overview of the smells in their code, and then, if they wish, to help in understanding the sources of those code smells. We also describe a laboratory experiment with 12 programmers that tests several hypotheses about our tool. Our findings suggest that programmers can use our tool effectively to identify smells and to make refactoring judgements. This is partly because the tool serves as a memory aid, and partly because it is more reliable and easier to use than heuristics for analyzing smells.","code smells, usability, refactoring, software",SOFTVIS '10,,,
Conference Paper,"Silva A,Araújo T,Nunes J,Perkusich M,Dilorenzo E,Almeida H,Perkusich A",A Systematic Review on the Use of Definition of Done on Agile Software Development Projects,,2017,,,364–373,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 21st International Conference on Evaluation and Assessment in Software Engineering,"Karlskrona, Sweden",2017,9781450348041.0,,https://doi.org/10.1145/3084226.3084262;http://dx.doi.org/10.1145/3084226.3084262,10.1145/3084226.3084262,"Background: Definition of Done (DoD) is a Scrum practice that consists of a simple list of criteria that adds verifiable or demonstrable value to the product. It is one of the most popular agile practices and assures a balance between short-term delivery of features and long-term product quality, but little is known of its actual use in Agile teams.Objective: To identify possible gaps in the literature and define a starting point to define DoD for practitioners through the identification and synthesis of the DoD criteria used in agile projects as presented in the scientific literature.Method: We applied a Systematic Literature Review of studies published up to (and including) 2016 through database search and backward and forward snowballing.Results: In total, we evaluated 2326 papers, of which 8 included DoD criteria used in agile projects. We identified that some studies presented up to 4 levels of DoD, which include story, sprint, release or project. We identified 62 done criteria, which are related to software verification and validation, deploy, code inspection, test process quality, regulatory compliance, software architecture design, process management, configuration management and non-functional requirements.Conclusion: The main implication for research is a need for more and better empirical studies documenting and evaluating the use of the DoD in agile software development. For the industry, the review provides a map of how DoD is currently being used in the industry and can be used as a starting point to define or compare with their own DoD definition.","Agile Software Development, Systematic Literature Review, Definition of Done",EASE'17,,,
Conference Paper,Moha N,Detection and Correction of Design Defects in Object-Oriented Designs,,2007,,,949–950,Association for Computing Machinery,"New York, NY, USA",Companion to the 22nd ACM SIGPLAN Conference on Object-Oriented Programming Systems and Applications Companion,"Montreal, Quebec, Canada",2007,9781595938657.0,,https://doi.org/10.1145/1297846.1297960;http://dx.doi.org/10.1145/1297846.1297960,10.1145/1297846.1297960,"Design defects come from poor design choices and have the effect of degrading the quality of object-oriented designs.Therefore, they present opportunities for improvements.However, design defects have not been precisely specified and there are few appropriate tools that allow their detection as well as their correction. Our goal is to provide a systematic method to specify systematically design defects precisely and to generate automatically detection and correction algorithms from their specifications. The detection algorithms are based not only on metrics but also on semantical and structural properties whereas the correction algorithms are based on refactorings. We apply and validate these algorithms on open-source object-oriented programs to show that our method allows a systematic specification, a precise detection, and a suitable correction of design defects.","design defects, meta-modelling, detection, refactorings, correction, antipatterns, code smells, specification",OOPSLA '07,,,
Conference Paper,"Angerbauer K,Okanović D,van Hoorn A,Heger C",The Back End is Only One Part of the Picture: Mobile-Aware Application Performance Monitoring and Problem Diagnosis,,2017,,,82–89,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 11th EAI International Conference on Performance Evaluation Methodologies and Tools,"Venice, Italy",2017,9781450363464.0,,https://doi.org/10.1145/3150928.3150939;http://dx.doi.org/10.1145/3150928.3150939,10.1145/3150928.3150939,"The success of modern businesses relies on the quality of their supporting application systems. Continuous application performance management is mandatory to enable efficient problem detection, diagnosis, and resolution during production. In today's age of ubiquitous computing, large fractions of users access application systems from mobile devices, such as phones and tablets. For detecting, diagnosing, and resolving performance and availability problems, an end-to-end view, i.e., traceability of requests starting on the (mobile) clients' devices, is becoming increasingly important. In this paper, we propose an approach for end-to-end monitoring of applications from the users' mobile devices to the back end, and diagnosing root-causes of detected performance problems. We extend our previous work on diagnosing performance anti-patterns from execution traces by new metrics and rules. The evaluation of this work shows that our approach successfully detects and diagnoses performance anti-patterns in applications with iOS-based mobile clients. While there are threats to validity to our experiment, our research is a promising starting point for future work.","performance anti-patterns, application performance monitoring, iOS",VALUETOOLS 2017,,,
Conference Paper,"Inoue H,Nakatani T",Identifying the Sources of Cache Misses in Java Programs without Relying on Hardware Counters,,2012,,,133–142,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 2012 International Symposium on Memory Management,"Beijing, China",2012,9781450313506.0,,https://doi.org/10.1145/2258996.2259014;http://dx.doi.org/10.1145/2258996.2259014,10.1145/2258996.2259014,"Cache miss stalls are one of the major sources of performance bottlenecks for multicore processors. A Hardware Performance Monitor (HPM) in the processor is useful for locating the cache misses, but is rarely used in the real world for various reasons. It would be better to find a simple approach to locate the sources of cache misses and apply runtime optimizations without relying on an HPM. This paper shows that pointer dereferencing in hot loops is a major source of cache misses in Java programs. Based on this observation, we devised a new approach to identify the instructions and objects that cause frequent cache misses. Our heuristic technique effectively identifies the majority of the cache misses in typical Java programs by matching the hot loops to simple idiomatic code patterns. On average, our technique selected only 2.8% of the load and store instructions generated by the JIT compiler and these instructions accounted for 47% of the L1D cache misses and 49% of the L2 cache misses caused by the JIT-compiled code. To prove the effectiveness of our technique in compiler optimizations, we prototyped object placement optimizations, which align objects in cache lines or collocate paired objects in the same cache line to reduce cache misses. For comparison, we also implemented the same optimizations based on the accurate information obtained from the HPM. Our results showed that our heuristic approach was as effective as the HPM-based approach and achieved comparable performance improvements in the SPECjbb2005 and SPECpower_ssj2008 benchmark programs.","hardware performance monitor, object placement optimization",ISMM '12,,,
Journal Article,"Inoue H,Nakatani T",Identifying the Sources of Cache Misses in Java Programs without Relying on Hardware Counters,SIGPLAN Not.,2012,47.0,11,133–142,Association for Computing Machinery,"New York, NY, USA",,,2012-06,,0362-1340,https://doi.org/10.1145/2426642.2259014;http://dx.doi.org/10.1145/2426642.2259014,10.1145/2426642.2259014,"Cache miss stalls are one of the major sources of performance bottlenecks for multicore processors. A Hardware Performance Monitor (HPM) in the processor is useful for locating the cache misses, but is rarely used in the real world for various reasons. It would be better to find a simple approach to locate the sources of cache misses and apply runtime optimizations without relying on an HPM. This paper shows that pointer dereferencing in hot loops is a major source of cache misses in Java programs. Based on this observation, we devised a new approach to identify the instructions and objects that cause frequent cache misses. Our heuristic technique effectively identifies the majority of the cache misses in typical Java programs by matching the hot loops to simple idiomatic code patterns. On average, our technique selected only 2.8% of the load and store instructions generated by the JIT compiler and these instructions accounted for 47% of the L1D cache misses and 49% of the L2 cache misses caused by the JIT-compiled code. To prove the effectiveness of our technique in compiler optimizations, we prototyped object placement optimizations, which align objects in cache lines or collocate paired objects in the same cache line to reduce cache misses. For comparison, we also implemented the same optimizations based on the accurate information obtained from the HPM. Our results showed that our heuristic approach was as effective as the HPM-based approach and achieved comparable performance improvements in the SPECjbb2005 and SPECpower_ssj2008 benchmark programs.","object placement optimization, hardware performance monitor",,,,
Journal Article,"Zhao Y,Wang L,Li S,Zhou F,Lin X,Lu Q,Ren L",A Visual Analysis Approach for Understanding Durability Test Data of Automotive Products,ACM Trans. Intell. Syst. Technol.,2019,10.0,6,,Association for Computing Machinery,"New York, NY, USA",,,2019-12,,2157-6904,https://doi.org/10.1145/3345640;http://dx.doi.org/10.1145/3345640,10.1145/3345640,"People face data-rich manufacturing environments in Industry 4.0. As an important technology for explaining and understanding complex data, visual analytics has been increasingly introduced into industrial data analysis scenarios. With the durability test of automotive starters as background, this study proposes a visual analysis approach for understanding large-scale and long-term durability test data. Guided by detailed scenario and requirement analyses, we first propose a migration-adapted clustering algorithm that utilizes a segmentation strategy and a group of matching-updating operations to achieve an efficient and accurate clustering analysis of the data for starting mode identification and abnormal test detection. We then design and implement a visual analysis system that provides a set of user-friendly visual designs and lightweight interactions to help people gain data insights into the test process overview, test data patterns, and durability performance dynamics. Finally, we conduct a quantitative algorithm evaluation, case study, and user interview by using real-world starter durability test datasets. The results demonstrate the effectiveness of the approach and its possible inspiration for the durability test data analysis of other similar industrial products.","smart manufacturing, Industry 4.0, visual analysis, automotive starter, durability test",,,,
Conference Paper,"Edwards SH,Kandru N,Rajagopal MB",Investigating Static Analysis Errors in Student Java Programs,,2017,,,65–73,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 2017 ACM Conference on International Computing Education Research,"Tacoma, Washington, USA",2017,9781450349680.0,,https://doi.org/10.1145/3105726.3106182;http://dx.doi.org/10.1145/3105726.3106182,10.1145/3105726.3106182,"Research on students learning to program has produced studies on both compile-time errors (syntax errors) and run-time errors (exceptions). Both of these types of errors are natural targets, since detection is built into the programming language. In this paper, we present an empirical investigation of static analysis errors present in syntactically correct code. Static analysis errors can be revealed by tools that examine a program's source code, but this error detection is typically not built into common programming languages and instead requires separate tools. Static analysis can be used to check formatting or commenting expectations, but it also can be used to identify problematic code or to find some kinds of conceptual or logic errors. We study nearly 10 million static analysis errors found in over 500 thousand program submissions made by students over a five-semester period. The study includes data from four separate courses, including a non-majors introductory course as well as the CS1/CS2/CS3 sequence for CS majors. We examine the differences between the error rates of CS major and non-major beginners, and also examine how these patterns change over time as students progress through the CS major course sequence. Our investigation shows that while formatting and Javadoc issues are the most common, static checks that identify coding flaws that are likely to be errors are strongly correlated with producing correct programs, even when students eventually fix the problems. With experience, students produce fewer errors, but the errors that are most frequent are consistent between both computer science majors and non-majors, and across experience levels. These results can highlight student struggles or misunderstandings that have escaped past analyses focused on syntax or run-time errors.","web-cat, documentation, java, checkstyle, pmd, formatting, coding standard, static analysis, coding style",ICER '17,,,
Conference Paper,"Nam D,Horvath A,Macvean A,Myers B,Vasilescu B",MARBLE: Mining for Boilerplate Code to Identify API Usability Problems,,2020,,,615–627,IEEE Press,"San Diego, California",Proceedings of the 34th IEEE/ACM International Conference on Automated Software Engineering,,2020,9781728125084.0,,https://doi.org/10.1109/ASE.2019.00063;http://dx.doi.org/10.1109/ASE.2019.00063,10.1109/ASE.2019.00063,"Designing usable APIs is critical to developers' productivity and software quality, but is quite difficult. One of the challenges is that anticipating API usability barriers and real-world usage is difficult, due to a lack of automated approaches to mine usability data at scale. In this paper, we focus on one particular grievance that developers repeatedly express in online discussions about APIs: ""boilerplate code."" We investigate what properties make code count as boilerplate, the reasons for boilerplate, and how programmers can reduce the need for it. We then present MARBLE, a novel approach to automatically mine boilerplate code candidates from API client code repositories. MARBLE adapts existing techniques, including an API usage mining algorithm, an AST comparison algorithm, and a graph partitioning algorithm. We evaluate MARBLE with 13 Java APIs, and show that our approach successfully identifies both already-known and new API-related boilerplate code instances.",,ASE '19,,,
Conference Paper,"Tilevich E,Song M",Reusable Enterprise Metadata with Pattern-Based Structural Expressions,,2010,,,25–36,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 9th International Conference on Aspect-Oriented Software Development,"Rennes and Saint-Malo, France",2010,9781605589589.0,,https://doi.org/10.1145/1739230.1739234;http://dx.doi.org/10.1145/1739230.1739234,10.1145/1739230.1739234,"An essential part of modern enterprise software development is metadata. Mainstream metadata formats, including XML deployment descriptors and Java 5 annotations, suffer from a number of limitations that complicate the development and maintenance of enterprise applications. Their key problem is that they make it impossible to reuse metadata specifications not only across different applications but even across smaller program constructs such as classes or methods.To provide better enterprise metadata, we present pattern-based structural expressions (PBSE), a novel metadata representation that offers conciseness and maintainability advantages and is reusable. To apply PBSE to enterprise applications, we translate PBSE specifications to Java annotations, with annotating classes automatically as an intermediate build step. We demonstrate the advantages of the new metadata format by assessing its conciseness and reusability, as compared to XML and annotations, in the task of expressing metadata of J2EE reference applications and a mid-size, commercial, enterprise application.","configuration, frameworks, Eclipse, metadata, annotations",AOSD '10,,,
Conference Paper,"Niu N,Easterbrook S",Concept Analysis for Product Line Requirements,,2009,,,137–148,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 8th ACM International Conference on Aspect-Oriented Software Development,"Charlottesville, Virginia, USA",2009,9781605584423.0,,https://doi.org/10.1145/1509239.1509259;http://dx.doi.org/10.1145/1509239.1509259,10.1145/1509239.1509259,"Traditional methods characterize a software product line's requirements using either functional or quality criteria. This appears to be inadequate to assess modularity, detect interferences, and analyze trade-offs. We take advantage of both symmetric and asymmetric views of aspects, and perform formal concept analysis to examine the functional and quality requirements of an evolving product line. The resulting concept lattice provides a rich notion which allows remarkable insights into the modularity and interactions of requirements. We formulate a number of problems that aspect-oriented product line requirements engineering should address, and present our solutions according to the concept lattice. We describe a case study applying our approach to analyze a mobile game product line's requirements, and review lessons learned.","functional requirements profiles, formal concept analysis, product line engineering, quality attribute scenarios",AOSD '09,,,
Conference Paper,"Piveta E,Pimenta M,Araújo J,Moreira A,Guerreiro P,Price RT",Representing Refactoring Opportunities,,2009,,,1867–1872,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 2009 ACM Symposium on Applied Computing,"Honolulu, Hawaii",2009,9781605581668.0,,https://doi.org/10.1145/1529282.1529701;http://dx.doi.org/10.1145/1529282.1529701,10.1145/1529282.1529701,"Approaches for the representation of refactoring opportunities and their association with refactorings are usually described in an informal basis. This informality can hamper the creation of catalogues and tools to represent and search for refactoring opportunities. We propose an unified way to represent both the conditions in which the application of a refactoring can be advantageous and the mechanisms to associate these conditions with refactorings. The resulting representation mechanisms can be used to express search criteria based on software metrics, structural problems, heuristics or improvements on the software quality.","refactoring, representation",SAC '09,,,
Conference Paper,"De Bleser J,Di Nucci D,De Roover C",SoCRATES: Scala Radar for Test Smells,,2019,,,22–26,Association for Computing Machinery,"New York, NY, USA",Proceedings of the Tenth ACM SIGPLAN Symposium on Scala,"London, United Kingdom",2019,9781450368247.0,,https://doi.org/10.1145/3337932.3338815;http://dx.doi.org/10.1145/3337932.3338815,10.1145/3337932.3338815,"Test smells are indications of poorly designed unit tests. Previous studies have demonstrated their negative impact on test understanding and maintenance. Moreover, surveys show that developers are not able to identify test smells, hindering optimal software quality. Automated tools can aid developers to handle these issues and detect test smells in the early stage of software development. However, few tools are publicly available and all of them target JUnit -- the most popular testing framework in Java. To overcome these limitations, we propose SoCRATES. This fully automated tool is able to identify six test smells in Scala Test which is the most prevalent testing framework in Scala. An empirical investigation on 164 Scala projects shows that our tool is able to reach a high precision without sacrificing recall. Moreover, the results show that Scala projects have a lower diffusion than Java projects. We make SoCRATES publicly available as an IntelliJ IDEA plugin, as well as an open-source project in order to facilitate the detection of test smells.","Scala, test smells, software quality, tool",Scala '19,,,
Conference Paper,"Trese T,Tilley S",Documenting Software Systems with Views V: Towards Visual Documentation of Design Patterns as an Aid to Program Understanding,,2007,,,103–112,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 25th Annual ACM International Conference on Design of Communication,"El Paso, Texas, USA",2007,9781595935885.0,,https://doi.org/10.1145/1297144.1297167;http://dx.doi.org/10.1145/1297144.1297167,10.1145/1297144.1297167,"Cognitive science research indicates that a system is more readily understood when it is presented at progressive levels of decomposition, exposing increasing amounts of detail. One logical level of detail would present a system in terms of its implemented design patterns. However, to date, no entirely satisfactory method of documentation has been devised for explicating a software system as a set of design patterns. This paper discusses the challenges inherent in visualizing a software system as a set of design patterns, reviews the progress of another current effort, and describes a UML-compliant enhanced class-participation diagram as one possible solution.","program understanding, design patterns, documentation",SIGDOC '07,,,
Conference Paper,"Owens CC,Murali TM,Ramakrishnan N",Capturing Truthiness: Mining Truth Tables in Binary Datasets,,2009,,,1467–1474,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 2009 ACM Symposium on Applied Computing,"Honolulu, Hawaii",2009,9781605581668.0,,https://doi.org/10.1145/1529282.1529609;http://dx.doi.org/10.1145/1529282.1529609,10.1145/1529282.1529609,"We introduce a new data mining problem: mining truth tables in binary datasets. Given a matrix of objects and the properties they satisfy, a truth table identifies a subset of properties that exhibit maximal variability (and hence, complete independence) in occurrence patterns over the underlying objects. This problem is relevant in many domains, e.g., in bioinformatics where we seek to identify and model independent components of combinatorial regulatory pathways, and in social/economic demographics where we desire to determine independent behavioral attributes of populations. We outline a family of levelwise approaches adapted to mining truth tables, algorithmic optimizations, and applications to bioinformatics and political datasets.","levelwise algorithms, truth tables, independence models",SAC '09,,,
Conference Paper,"van Zyl P,Kourie DG,Coetzee L,Boake A",The Influence of Optimisations on the Performance of an Object Relational Mapping Tool,,2009,,,150–159,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 2009 Annual Research Conference of the South African Institute of Computer Scientists and Information Technologists,"Vanderbijlpark, Emfuleni, South Africa",2009,9781605586434.0,,https://doi.org/10.1145/1632149.1632169;http://dx.doi.org/10.1145/1632149.1632169,10.1145/1632149.1632169,"Object Relational Mapping (ORM or also known as O-R) tools provide a mapping between the object model and the relational model, acting as an intermediary between an object oriented code base, and a relational database. Over the last few years the use of object relational mapping tools have grown. Today their exist many ORM tools for programming languages, like C++ (LiteSQL), Java (Hibernate, Apache OJB), Python (SQLAlchemy), Ruby, PHP, .Net (iBATIS) and Perl to name a few. ORM tools provide an extra layer between the business logic layer and the data layer. It is important that this layer is not a bottleneck. The aim of the study was to investigate the influence of this extra layer against the use of object databases that remove the need for this extra mapping layer. During this investigation the impact of certain optimisation techniques on performance was investigated. In this paper we only report on how vendor-recommended optimisation techniques influenced the performance of Hibernate. The OO7 benchmark, tailored to investigate Java based object persistence, was used to create a database; to traverse it; to query it; and to delete and update data. An initial experiment used ""out of the box"" Hibernate settings with regard to the use of indexes, lazy and eager loading, and caching. The times taken were submitted to Hibernate for comment. The experiment was then re-run, using optimised settings as recommended by the vendor. The extent of improvement is reported.","optimisation techniques, cache, object relation mapping (O-R or ORM), benchmark, eager loading, lazy loading, hibernate, relational database management systems (RDBMS), persistence, performance",SAICSIT '09,,,
Journal Article,"Boyen P,Van Dyck D,Neven F,van Ham RC,van Dijk AD",SLIDER: A Generic Metaheuristic for the Discovery of Correlated Motifs in Protein-Protein Interaction Networks,IEEE/ACM Trans. Comput. Biol. Bioinformatics,2011,8.0,5,1344–1357,IEEE Computer Society Press,"Washington, DC, USA",,,2011-09,,1545-5963,https://doi.org/10.1109/TCBB.2011.17;http://dx.doi.org/10.1109/TCBB.2011.17,10.1109/TCBB.2011.17,"Correlated motif mining (cmm) is the problem of finding overrepresented pairs of patterns, called motifs, in sequences of interacting proteins. Algorithmic solutions for cmm thereby provide a computational method for predicting binding sites for protein interaction. In this paper, we adopt a motif-driven approach where the support of candidate motif pairs is evaluated in the network. We experimentally establish the superiority of the Chi-square-based support measure over other support measures. Furthermore, we obtain that cmm is an np-hard problem for a large class of support measures (including Chi-square) and reformulate the search for correlated motifs as a combinatorial optimization problem. We then present the generic metaheuristic slider which uses steepest ascent with a neighborhood function based on sliding motifs and employs the Chi-square-based support measure. We show that slider outperforms existing motif-driven cmm methods and scales to large protein-protein interaction networks. The slider-implementation and the data used in the experiments are available on http://bioinformatics.uhasselt.be.","Graphs and networks, biology and genetics.",,,,
Conference Paper,"Yang J,Sethi U,Yan C,Cheung A,Lu S",Managing Data Constraints in Database-Backed Web Applications,,2020,,,1098–1109,Association for Computing Machinery,"New York, NY, USA",Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering,"Seoul, South Korea",2020,9781450371216.0,,https://doi.org/10.1145/3377811.3380375;http://dx.doi.org/10.1145/3377811.3380375,10.1145/3377811.3380375,"Database-backed web applications manipulate large amounts of persistent data, and such applications often contain constraints that restrict data length, data value, and other data properties. Such constraints are critical in ensuring the reliability and usability of these applications. In this paper, we present a comprehensive study on where data constraints are expressed, what they are about, how often they evolve, and how their violations are handled. The results show that developers struggle with maintaining consistent data constraints and checking them across different components and versions of their web applications, leading to various problems. Guided by our study, we developed checking tools and API enhancements that can automatically detect such problems and improve the quality of such applications.",,ICSE '20,,,
Journal Article,"Andreasen E,Gong L,Møller A,Pradel M,Selakovic M,Sen K,Staicu CA",A Survey of Dynamic Analysis and Test Generation for JavaScript,ACM Comput. Surv.,2017,50.0,5,,Association for Computing Machinery,"New York, NY, USA",,,2017-09,,0360-0300,https://doi.org/10.1145/3106739;http://dx.doi.org/10.1145/3106739,10.1145/3106739,"JavaScript has become one of the most prevalent programming languages. Unfortunately, some of the unique properties that contribute to this popularity also make JavaScript programs prone to errors and difficult for program analyses to reason about. These properties include the highly dynamic nature of the language, a set of unusual language features, a lack of encapsulation mechanisms, and the “no crash” philosophy. This article surveys dynamic program analysis and test generation techniques for JavaScript targeted at improving the correctness, reliability, performance, security, and privacy of JavaScript-based software.","dynamic languages, Program analysis, test generation",,,,
Conference Paper,"Márquez G,Astudillo H",Identifying Availability Tactics to Support Security Architectural Design of Microservice-Based Systems,,2019,,,123–129,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 13th European Conference on Software Architecture - Volume 2,"Paris, France",2019,9781450371421.0,,https://doi.org/10.1145/3344948.3344996;http://dx.doi.org/10.1145/3344948.3344996,10.1145/3344948.3344996,"Microservices is an architectural style that considers systems as modular, customer-centric, independent, and scalable suite of services. In order to address security requirements in microservices-based systems, architects often must focus on critical quality attributes, such as availability, aiming at employing architectural solutions that provide design decisions that address key security concerns (also known as architectural tactics). Although current architectural tactics for availability offer an extensive catalog of alternatives to improve availability and security factors, new availability concerns (emerging from security microservices requirements) demand new or improved architectural tactics. In this article, we examined the source code and documentation of 17 open source microservices-based systems, identified 5 uses of availability tactics, and characterized them using a newly introduced descriptive template. We found that almost all (4 out of 5) tactics did focus on preventing faults rather than detecting, mitigating or recovering from them (which are the traditional tactics taxonomies' branches). This approach can be further used to systematically identify and characterize architectural tactics in existing microservices-based systems in other critical quality attributes concerning security, such as confidentiality and integrity.","microservices, patterns, availability, architectural tactics, frameworks",ECSA '19,,,
Conference Paper,"Ko AJ,Dosono B,Duriseti N",Thirty Years of Software Problems in the News,,2014,,,32–39,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 7th International Workshop on Cooperative and Human Aspects of Software Engineering,"Hyderabad, India",2014,9781450328609.0,,https://doi.org/10.1145/2593702.2593719;http://dx.doi.org/10.1145/2593702.2593719,10.1145/2593702.2593719,"How have the consequences of software problems changed over the past 30 years? To begin to answer this question, we analyzed 386,381 news articles reporting on software problems published between 1980 and 2012, spanning widely circulated newspapers to small trade magazines. Our results show that after an increase in reporting just prior to Y2K, news on software problems has declined in North America, but increased in the rest of the world. Most articles only report minor consequences such as frustration, confusion, anger, or at worst, having to delay some activity for a few hours, usually due to service outages in government, transportation, finance, and information services. However, about once per month, the news reports at least one death, injury, or threatened access to food or shelter due to software problems. Reports of these severe consequences are also increasing, due primarily to stories about transportation and government software.","news clustering, Software failures, news",CHASE 2014,,,
Journal Article,Olbert AG,Extended Control Program Support: VM/370: A Hardware Assist for the IBM Virtual Machine Facility/370,SIGMICRO Newsl.,1978,9.0,3,8–25,Association for Computing Machinery,"New York, NY, USA",,,1978-09,,1050-916X,https://doi.org/10.1145/1096532.1096534;http://dx.doi.org/10.1145/1096532.1096534,10.1145/1096532.1096534,ECPS:VM/370 is a hardware assist of the VM/370 software control program. ECPS:VM/370 provides improved performance for the software system through a combination of hardware assist technologies. The assist provides hardware support for virtual machine execution of certain System/370 real machine functions. The assist also introduces instructions intended for use by the VM/370 control program. ECPS:VM/370 is defined to simplify maintenance and service of the assisted system. This paper describes the general functions of the assist and presents an explanation of the control and service capabilities provided with the assist.,,,,,
Conference Paper,"Rentschler A,Werle D,Noorshams Q,Happe L,Reussner R",Designing Information Hiding Modularity for Model Transformation Languages,,2014,,,217–228,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 13th International Conference on Modularity,"Lugano, Switzerland",2014,9781450327725.0,,https://doi.org/10.1145/2577080.2577094;http://dx.doi.org/10.1145/2577080.2577094,10.1145/2577080.2577094,"Development and maintenance of model transformations make up a substantial share of the lifecycle costs of software products that rely on model-driven techniques. In particular large and heterogeneous models lead to poorly understandable transformation code due to missing language concepts to master complexity. At the present time, there exists no module concept for model transformation languages that allows programmers to control information hiding and strictly declare model and code dependencies at module interfaces. Yet only then can we break down transformation logic into smaller parts, so that each part owns a clear interface for separating concerns. In this paper, we propose a module concept suitable for model transformation engineering. We formalize our concept based on cQVTom, a compact subset of the transformation language QVT-Operational. To meet the special demands of transformations, module interfaces give control over both model and code accessibility. We also implemented the approach for validation. In a case study, we examined the effort required to carry out two typical maintenance tasks on a real-world transformation. We are able to attest a significant reduction of effort, thereby demonstrating the practical effects of a thorough interface concept on the maintainability of model transformations.","model-driven software engineering, transformation languages, modularity, model transformations, maintenance",MODULARITY '14,,,
Conference Paper,"Matinnejad R,Nejati S,Briand LC",Automated Testing of Hybrid Simulink/Stateflow Controllers: Industrial Case Studies,,2017,,,938–943,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 2017 11th Joint Meeting on Foundations of Software Engineering,"Paderborn, Germany",2017,9781450351058.0,,https://doi.org/10.1145/3106237.3117770;http://dx.doi.org/10.1145/3106237.3117770,10.1145/3106237.3117770,"We present the results of applying our approach for testing Simulink controllers to one public and one proprietary model, both industrial. Our approach combines explorative and exploitative search algorithms to visualize the controller behavior over its input space and to identify test scenarios in the controller input space that violate or are likely to violate the controller requirements. The engineers' feedback shows that our approach is easy to use in practice and gives them confidence about the behavior of their models.","Matlab/Simulink, testing, Automotive software systems",ESEC/FSE 2017,,,
Conference Paper,Mongiovi M,Scaling Testing of Refactoring Engines,,2016,,,15–17,Association for Computing Machinery,"New York, NY, USA","Companion Proceedings of the 2016 ACM SIGPLAN International Conference on Systems, Programming, Languages and Applications: Software for Humanity","Amsterdam, Netherlands",2016,9781450344371.0,,https://doi.org/10.1145/2984043.2984048;http://dx.doi.org/10.1145/2984043.2984048,10.1145/2984043.2984048,"Refactoring engines may have overly weak conditions, overly strong conditions, and transformation issues related to the refactoring definitions. We find that 86% of the test suites of Eclipse and JRRT are concerned to those kinds of bugs. However, the engines still have them. Researchers have proposed a number of techniques for testing refactoring engines. Nevertheless, they may have limitations related to the program generator, time consumption, kinds of bugs, automation, and debugging. We propose and implement a technique to scale testing of refactoring engines. We improve expressiveness of a program generator and use a technique to skip some test inputs to improve performance. Moreover, we propose new oracles to detect behavioral changes using change impact analysis, overly strong conditions using mutation testing, and transformation issues. We evaluate our technique in 28 refactoring implementations of Java (Eclipse and JRRT) and C (Eclipse) and find 119 bugs. The technique reduces the time in 96% using skips while missing only 6% of the bugs. Using the new oracle to identify overly strong conditions, it detects more bugs and facilitates the debugging activity different from previous works. Finally, we evaluate refactoring implementations of Eclipse and JRRT using the input programs of their refactoring test suites and find a number of bugs not detected by the developers.","Refactoring, Program Synthesis, Testing",SPLASH Companion 2016,,,
Conference Paper,Bodden E,"The Secret Sauce in Efficient and Precise Static Analysis: The Beauty of Distributive, Summary-Based Static Analyses (and How to Master Them)",,2018,,,85–93,Association for Computing Machinery,"New York, NY, USA",Companion Proceedings for the ISSTA/ECOOP 2018 Workshops,"Amsterdam, Netherlands",2018,9781450359399.0,,https://doi.org/10.1145/3236454.3236500;http://dx.doi.org/10.1145/3236454.3236500,10.1145/3236454.3236500,"In this paper I report on experiences gained from more than five years of extensively designing static code analysis tools- in particular such ones with a focus on security- to scale to real-world projects within an industrial context. Within this time frame, my team and I were able to design static-analysis algorithms that yield both largely improved precision and performance compared to previous approaches. I will give a number of insights regarding important design decisions that made this possible.In particular, I argue that summary-based static-analysis techniques for distributive problems, such as IFDS, IDE and WPDS have been unduly under-appreciated. As my experience shows, those techniques can tremendously benefit both precision and performance, if one uses them in a well-informed way, using carefully designed abstract domains. As one example, I will explain how in previous work on BOOMERANG we were able to decompose pointer analysis, a static analysis problem that is actually not distributive, into sub-problems that are distributive. This yields an implementation that is both highly precise and efficient.This breakthrough, along with the use of a demand-driven program-analysis design, has recently allowed us to implement practical static analysis tools such as the crypto-misuse checker CogniCrypt, which can analyze the entire Maven-Central repository with more than 200.000 binaries in under five days, although its analysis is flow-sensitive, field-sensitive, and fully context-sensitive.","abstract domains, precision, summarization, performance, static analysis",ISSTA '18,,,
Journal Article,"Feijs L,De Jong R",3D Visualization of Software Architectures,Commun. ACM,1998,41.0,12,73–78,Association for Computing Machinery,"New York, NY, USA",,,1998-12,,0001-0782,https://doi.org/10.1145/290133.290151;http://dx.doi.org/10.1145/290133.290151,10.1145/290133.290151,,,,,,
Conference Paper,"Mukelabai M,Nešić D,Maro S,Berger T,Steghöfer JP",Tackling Combinatorial Explosion: A Study of Industrial Needs and Practices for Analyzing Highly Configurable Systems,,2018,,,155–166,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 33rd ACM/IEEE International Conference on Automated Software Engineering,"Montpellier, France",2018,9781450359375.0,,https://doi.org/10.1145/3238147.3238201;http://dx.doi.org/10.1145/3238147.3238201,10.1145/3238147.3238201,"Highly configurable systems are complex pieces of software. To tackle this complexity, hundreds of dedicated analysis techniques have been conceived, many of which able to analyze system properties for all possible system configurations, as opposed to traditional, single-system analyses. Unfortunately, it is largely unknown whether these techniques are adopted in practice, whether they address actual needs, or what strategies practitioners actually apply to analyze highly configurable systems. We present a study of analysis practices and needs in industry. It relied on a survey with 27 practitioners engineering highly configurable systems and follow-up interviews with 15 of them, covering 18 different companies from eight countries. We confirm that typical properties considered in the literature (e.g., reliability) are relevant, that consistency between variability models and artifacts is critical, but that the majority of analyses for specifications of configuration options (a.k.a., variability model analysis) is not perceived as needed. We identified rather pragmatic analysis strategies, including practices to avoid the need for analysis. For instance, testing with experience-based sampling is the most commonly applied strategy, while systematic sampling is rarely applicable. We discuss analyses that are missing and synthesize our insights into suggestions for future research.","Product Lines, Analysis, Highly Configurable Systems",ASE 2018,,,
Conference Paper,"Lin Y,Peng X,Cai Y,Dig D,Zheng D,Zhao W",Interactive and Guided Architectural Refactoring with Search-Based Recommendation,,2016,,,535–546,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering,"Seattle, WA, USA",2016,9781450342186.0,,https://doi.org/10.1145/2950290.2950317;http://dx.doi.org/10.1145/2950290.2950317,10.1145/2950290.2950317,"Architectural refactorings can contain hundreds of steps and experienced developers could carry them out over several weeks. Moreover, developers need to explore a correct sequence of refactorings steps among many more incorrect alternatives. Thus, carrying out architectural refactorings is costly, risky, and challenging. In this paper, we present Refactoring Navigator: a tool-supported and interactive recommendation approach for aiding architectural refactoring. Our approach takes a given implementation as the starting point, a desired high-level design as the target, and iteratively recommends a series of refactoring steps. Moreover, our approach allows the user to accept, reject, or ignore a recommended refactoring step, and uses the user's feedback in further refactoring recommendations. We evaluated the effectiveness of our approach and tool using a controlled experiment and an industrial case study. The controlled experiment shows that the participants who used Refactoring Navigator accomplished their tasks in 77.4% less time and manually edited 98.3% fewer lines than the control group. The industrial case study suggests that Refactoring Navigator has the potential to help with architectural refactorings in practice.","reflexion model, user feedback, interactive, high-level design, automatic refactoring",FSE 2016,,,
Conference Paper,"Dietrich J,McCartin C",Scalable Motif Detection and Aggregation,,2012,,,31–40,"Australian Computer Society, Inc.",AUS,Proceedings of the Twenty-Third Australasian Database Conference - Volume 124,"Melbourne, Australia",2012,9781921770050.0,,,,"Motif search in graphs has become a popular field of research in recent years, mainly motivated by applications in bioinformatics. Existing work has focused on simple motifs: small sets of vertices directly connected by edges. However, there are applications that require a more general concept of motif, where vertices are only indirectly connected by paths. The size of the solution space is a major limiting factor when dealing with this kind of motif. We try to address this challenge through motif instance aggregation. It turns out that effective, parallel algorithms can be found to compute instances of generalised motifs in large graphs.To expedite the process, we have developed GUERY, a tool that can be used to define motifs and find motif instances, in graphs represented using the popular JUNG graph library [10]. GUERY consists of two parts - a simple domain specific language that can be used to define motifs, and a solver. The main strengths of GUERY are 1. support for motif instance aggregation, 2. generation of query result streams, as opposed to (very large) static sets of matching instances, 3. support for effective parallelisation in the evaluation of queries.The examples used for validation originate from problems encountered when analysing the dependency graphs of object-oriented programs for instances of architectural antipatterns.",,ADC '12,,,
Conference Paper,"Wasylkowski A,Zeller A,Lindig C",Detecting Object Usage Anomalies,,2007,,,35–44,Association for Computing Machinery,"New York, NY, USA",Proceedings of the the 6th Joint Meeting of the European Software Engineering Conference and the ACM SIGSOFT Symposium on The Foundations of Software Engineering,"Dubrovnik, Croatia",2007,9781595938114.0,,https://doi.org/10.1145/1287624.1287632;http://dx.doi.org/10.1145/1287624.1287632,10.1145/1287624.1287632,"Interacting with objects often requires following a protocol---for instance, a specific sequence of method calls. These protocols are not always documented, and violations can lead to subtle problems. Our approach takes code examples to automatically infer legal sequences of method calls. The resulting patterns can then be used to detect anomalies such as ""Before calling next, one normally calls hasNext"". To our knowledge, this is the first fully automatic defect detection approach that learns and checks methodcall sequences. Our JADET prototype has detected yet undiscovered defects and code smells in five popular open-source programs, including two new defects in AspectJ.","programming rules, automated defect detection, object usage anomalies, static analysis, automated specification generation, pattern recognition, data mining for software engineering",ESEC-FSE '07,,,
Conference Paper,"Kessel M,Atkinson C","Integrating Reuse into the Rapid, Continuous Software Engineering Cycle through Test-Driven Search",,2018,,,8–11,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 4th International Workshop on Rapid Continuous Software Engineering,"Gothenburg, Sweden",2018,9781450357456.0,,https://doi.org/10.1145/3194760.3194761;http://dx.doi.org/10.1145/3194760.3194761,10.1145/3194760.3194761,"Today's advanced agile practices such as Continuous Integration and Test-Driven Development support a wide range of software development activities to facilitate the rapid delivery of high-quality software. However, the reuse of pre-existing, third-party software components is not one of them. Software reuse is still primarily perceived as a time-consuming, unsystematic and ultimately, ""discontinuous"" activity even though it aims to deliver the same basic benefits as continuous software engineering - namely, a reduction in the time and effort taken to deliver quality software. However, the increasingly central role of testing in continuous software engineering offers a way of addressing this problem by exploiting the new generation of test-driven search engines that can harvest components based on tests. This search technology not only exploits artifacts that have already been created as part of the continuous testing process to harvest components, it returns results that have a high likelihood of being fit for purpose and thus of being worth reusing. In this paper, we propose to augment continuous software engineering with the rapid, continuous reuse of software code units by integrating the test-driven mining of software artifact repositories into the continuous integration process. More specifically, we propose to use tests written as part of the Test-First Development approach to perform test-driven searches for matching functionality while developers are working on their normal development activities. We discuss the idea of rapid, continuous code reuse based on recent advances in our test-driven search platform and elaborate on scenarios for its application in the future.","test-driven reuse, test-driven development, rapid continuous integration, test-driven search, rapid continuous code reuse",RCoSE '18,,,
Conference Paper,"Ferreira M,Barbosa E,Macia I,Arcoverde R,Garcia A",Detecting Architecturally-Relevant Code Anomalies: A Case Study of Effectiveness and Effort,,2014,,,1158–1163,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 29th Annual ACM Symposium on Applied Computing,"Gyeongju, Republic of Korea",2014,9781450324694.0,,https://doi.org/10.1145/2554850.2555036;http://dx.doi.org/10.1145/2554850.2555036,10.1145/2554850.2555036,"Code anomalies are structural problems in the program. Even though they might represent symptoms of architecture degradation, several code anomalies do not contribute to this process. Source code inspection by developers might not support time-effective detection of architecturally-relevant anomalies in a program. Hence, they usually rely on multiple software metrics known to effectively detect code anomalies. However, there is still no empirical knowledge about the time effectiveness of metric-based strategies to detect architecturally-relevant anomalies. Given the longitudinal nature of this activity, we performed a first exploratory case study to address this gap. We compare metrics-based strategies with manual inspections made by the actual software developers. The study was conducted in the context of a legacy software system with 30K lines of code, 415 architectural elements, 210 versions, and embracing reengineering effort for almost 3 years. Effectiveness was assessed in terms of several quantitative and qualitative indicators. To measure the effort, we computed the amount of time used in several activities required to identify architecturally-relevant code anomalies. The results of our study shed light on potential effort reduction and effectiveness improvements of metrics-based strategies.","effectiveness, effort, code anomaly, architectural degeneration",SAC '14,,,
Conference Paper,"Gopalakrishnan G,Pillai S,Dhanju N",Collaboration in Offshore Software Projects: Practices and Challenges,,2009,,,225–228,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 2009 International Workshop on Intercultural Collaboration,"Palo Alto, California, USA",2009,9781605585024.0,,https://doi.org/10.1145/1499224.1499262;http://dx.doi.org/10.1145/1499224.1499262,10.1145/1499224.1499262,This paper presents the findings of a study that sought to explore key practices and challenges in offshore software project collaborations.,"distributed work, offshore software project, collaboration",IWIC '09,,,
Conference Paper,"Garousi V,Felderer M",Experience-Based Guidelines for Effective and Efficient Data Extraction in Systematic Reviews in Software Engineering,,2017,,,170–179,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 21st International Conference on Evaluation and Assessment in Software Engineering,"Karlskrona, Sweden",2017,9781450348041.0,,https://doi.org/10.1145/3084226.3084238;http://dx.doi.org/10.1145/3084226.3084238,10.1145/3084226.3084238,"To systematically collect evidence and to structure a given area in software engineering (SE), Systematic Literature Reviews (SLR) and Systematic Mapping (SM) studies have become common. Data extraction is one of the main phases (activities) when conducting an SM or an SLR, whose objective is to extract required data from the primary studies and to accurately record the information researchers need to answer the questions of the SM/SLR study. Based on experience in a large number of SM/SLR studies, we and many other researchers have found the data extraction in SLRs to be time consuming and error-prone, thus raising the real need for heuristics and guidelines for effective and efficient data extraction in these studies, especially to be learnt by junior and young researchers. As a 'guideline' paper, this paper contributes a synthesized list of challenges usually faced during SLRs' data extraction phase and the corresponding solutions (guidelines). For our synthesis, we consider two data sources: (1) the pool of 16 SLR studies in which the authors have been involved in, as well as (2) a review of challenges and guidelines in the existing literature. Our experience in utilizing the presented guidelines in the near past have helped our junior colleagues to conduct data extractions more effectively and efficiently.","research methodology, data extraction, SLR, systematic literature reviews, SM, empirical software engineering, Systematic mapping studies",EASE'17,,,
Conference Paper,"Fang Z,Han W,Li D,Guo Z,Guo D,Wang XS,Qian Z,Chen H",RevDroid: Code Analysis of the Side Effects after Dynamic Permission Revocation of Android Apps,,2016,,,747–758,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 11th ACM on Asia Conference on Computer and Communications Security,"Xi'an, China",2016,9781450342339.0,,https://doi.org/10.1145/2897845.2897914;http://dx.doi.org/10.1145/2897845.2897914,10.1145/2897845.2897914,"Dynamic revocation of permissions of installed Android applications has been gaining popularity, because of the increasing concern of security and privacy in the Android platform. However, applications often crash or misbehave when their permissions are revoked, rendering applications completely unusable. Even though Google has officially introduced the new permission mechanism in Android 6.0 to explicitly support dynamic permission revocation, the issue still exists. In this paper, we conduct an empirical study to understand the latest application practice post Android 6.0. Specifically, we design a practical tool, referred to as revDroid, to help us to empirically analyze how often the undesirable side effects, especially application crash, can occur in off-the-shelf Android applications. From the analysis of 248 popular applications from Google Play Store, revDroid finds out that 70% applications and 46% permission-relevant calls do not appropriately catch exceptions caused by permission revocation, while third-party libraries pay much more attention to permission revocation. We also use revDroid to analyze 132 recent malware samples. The result shows that only 27% malwares and 36% permission-relevant API calls of malwares fail to consider the permission revocation. In fact, many of them perform specialized handling of permission revocation to keep the core malicious logic running. Finally, revDroid can be used to help developers uncover the unhandled permission revocations during development time and greatly improve the application quality.","android security, revdroid, permission over-claim, permission revocation",ASIA CCS '16,,,
Conference Paper,"Bernius JP,Kovaleva A,Krusche S,Bruegge B",Towards the Automation of Grading Textual Student Submissions to Open-Ended Questions,,2020,,,61–70,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 4th European Conference on Software Engineering Education,"Seeon/Bavaria, Germany",2020,9781450377522.0,,https://doi.org/10.1145/3396802.3396805;http://dx.doi.org/10.1145/3396802.3396805,10.1145/3396802.3396805,"Growing student numbers at universities worldwide pose new challenges for instructors. Providing feedback to textual exercises is a challenge in large courses while being important for student's learning success. Exercise submissions and their grading are a primary and individual communication channel between instructors and students. The pure amount of submissions makes it impossible for a single instructor to provide regular feedback to large student bodies. Employing tutors in the process introduces new challenges. Feedback should be consistent and fair for all students. Additionally, interactive teaching models strive for real-time feedback and multiple submissions.We propose a support system for grading textual exercises using an automatic segment-based assessment concept. The system aims at providing suggestions to instructors by reusing previous comments as well as scores. The goal is to reduce the workload for instructors, while at the same time creating timely and consistent feedback to the students. We present the design and a prototypical implementation of an algorithm using topic modeling for segmenting the submissions into smaller blocks. Thereby, the system derives smaller units for assessment and allowing the creation of reusable and structured feedback.We have evaluated the algorithm qualitatively by comparing automatically produced segments with manually produced segments created by humans. The results show that the system can produce topically coherent segments. The segmentation algorithm based on topic modeling is superior to approaches purely based on syntax and punctuation.","Automatic Assessment, Assessment Support Systems, Software Engineering Education, Textual Exercise",ECSEE '20,,,
Conference Paper,"Yan J,Randell B",A Systematic Classification of Cheating in Online Games,,2005,,,1–9,Association for Computing Machinery,"New York, NY, USA",Proceedings of 4th ACM SIGCOMM Workshop on Network and System Support for Games,"Hawthorne, NY",2005,9781595931566.0,,https://doi.org/10.1145/1103599.1103606;http://dx.doi.org/10.1145/1103599.1103606,10.1145/1103599.1103606,"Cheating is rampant in current game play on the Internet. However, it is not as well understood as one might expect. In this paper, we summarize the various known methods of cheating, and we define a taxonomy of online game cheating with respect to the underlying vulnerability (what is exploited?), consequence (what type of failure can be achieved?) and the cheating principal (who is cheating?). This taxonomy provides a systematic introduction to the characteristics of cheats in online games and how they can arise. It is intended to be comprehensible and useful not only to security specialists, but also to game developers, operators and players who are less knowledgeable and experienced in security. One of our findings is that although cheating in online games is largely due to various security failures, the four traditional aspects of security -- confidentiality, integrity, availability and authenticity -- are insufficient to explain it. Instead, fairness becomes a vital additional aspect, and its enforcement provides a convincing perspective for understanding the role of security techniques in developing and operating online games.","security, cheating, taxonomy, online computer games",NetGames '05,,,
Journal Article,"Pollock L,Andrews J",ICSE 2005 Workshop Summary Third International Workshop on Dynamic Analysis (WODA 2005),SIGSOFT Softw. Eng. Notes,2005,30.0,4.0,1–2,Association for Computing Machinery,"New York, NY, USA",,,2005-07,,0163-5948,https://doi.org/10.1145/1082983.1082999;http://dx.doi.org/10.1145/1082983.1082999,10.1145/1082983.1082999,"Dynamic analysis techniques collect and examine information about program executions. Particularly combined with static analysis, results from dynamic analysis can have many uses in regard to developing robust and reliable large-scale software systems.",,,,,
Conference Paper,"Karmaker Santu SK,Sondhi P,Zhai C",Generative Feature Language Models for Mining Implicit Features from Customer Reviews,,2016,,,929–938,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 25th ACM International on Conference on Information and Knowledge Management,"Indianapolis, Indiana, USA",2016,9781450340731.0,,https://doi.org/10.1145/2983323.2983729;http://dx.doi.org/10.1145/2983323.2983729,10.1145/2983323.2983729,"Online customer reviews are very useful for both helping consumers make buying decisions on products or services and providing business intelligence. However, it is a challenge for people to manually digest all the opinions buried in large amounts of review data, raising the need for automatic opinion summarization and analysis. One fundamental challenge in automatic opinion summarization and analysis is to mine implicit features, i.e., recognizing the features implicitly mentioned (referred to) in a review sentence. Existing approaches require many ad hoc manual parameter tuning, and are thus hard to optimize or generalize; their evaluation has only been done with Chinese review data. In this paper, we propose a new approach based on generative feature language models that can mine the implicit features more effectively through unsupervised statistical learning. The parameters are optimized automatically using an Expectation-Maximization algorithm. We also created eight new data sets to facilitate evaluation of this task in English. Experimental results show that our proposed approach is very effective for assigning features to sentences that do not explicitly mention the features, and outperforms the existing algorithms by a large margin.","opinion analysis, review summarization, implicit feature mining, language models",CIKM '16,,,
Conference Paper,"Markiegi U,Arrieta A,Etxeberria L,Sagardui G",White-Box and Black-Box Test Quality Metrics for Configurable Simulation Models,,2019,,,211–214,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 23rd International Systems and Software Product Line Conference - Volume B,"Paris, France",2019,9781450366687.0,,https://doi.org/10.1145/3307630.3342396;http://dx.doi.org/10.1145/3307630.3342396,10.1145/3307630.3342396,"Simulation models are widely employed to model and simulate complex systems from different domains, such as automotive. These systems are becoming highly configurable to support different users' demands. Testing all of them is impracticable, and thus, cost-effective techniques are mandatory. Costs are usually attributed either to the time it takes to test a configurable system or to its monetary value. Nevertheless, for the case of test effectiveness several quality metrics can be found in the literature. This paper aims at proposing both black-box and white-box test quality metrics for configurable simulation models relying on 150% variability modeling approaches.","test quality metrics, product lines, simulation models",SPLC '19,,,
Conference Paper,"Stijlaart M,Zaytsev V",Towards a Taxonomy of Grammar Smells,,2017,,,43–54,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 10th ACM SIGPLAN International Conference on Software Language Engineering,"Vancouver, BC, Canada",2017,9781450355254.0,,https://doi.org/10.1145/3136014.3136035;http://dx.doi.org/10.1145/3136014.3136035,10.1145/3136014.3136035,"Any grammar engineer can tell a good grammar from a bad one, but there is no commonly accepted taxonomy of indicators of required grammar refactorings. One of the consequences of this lack of general smell taxonomy is the scarcity of tools to assess and improve the quality of grammars. By combining two lines of research — on smell detection and on grammar transformation — we have assembled a taxonomy of smells in grammars. As a pilot case, the detectors for identified smells were implemented for grammars in a broad sense and applied to the 641 grammars of the Grammar Zoo.","grammar engineering, smell detection",SLE 2017,,,
Conference Paper,"Fernández-Sánchez C,Garbajosa J,Vidal C,Yagüe A",An Analysis of Techniques and Methods for Technical Debt Management: A Reflection from the Architecture Perspective,,2015,,,22–28,IEEE Press,"Florence, Italy",Proceedings of the Second International Workshop on Software Architecture and Metrics,,2015,,,,,"Technical debt is a metaphor referring to the consequences of weak software development. Managing technical debt is necessary in order to keep it under control, and several techniques have been developed with the goal of accomplishing this. However, available techniques have grown disperse and managers lack guidance. This paper covers this gap by providing a systematic mapping of available techniques and methods for technical debt management, covering architectural debt, and identifying existing gaps that prevent to manage technical debt efficiently.",,SAM '15,,,
Conference Paper,"Smith CU,Williams LG",Software Performance Antipatterns,,2000,,,127–136,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 2nd International Workshop on Software and Performance,"Ottawa, Ontario, Canada",2000,9781581131956.0,,https://doi.org/10.1145/350391.350420;http://dx.doi.org/10.1145/350391.350420,10.1145/350391.350420,,,WOSP '00,,,
Journal Article,"Farahat A,Ebnenasir A",A Lightweight Method for Automated Design of Convergence in Network Protocols,ACM Trans. Auton. Adapt. Syst.,2012,7.0,4.0,,Association for Computing Machinery,"New York, NY, USA",,,2012-12,,1556-4665,https://doi.org/10.1145/2382570.2382574;http://dx.doi.org/10.1145/2382570.2382574,10.1145/2382570.2382574,"Design and verification of Self-Stabilizing (SS) network protocols are difficult tasks in part because of the convergence property that requires an SS protocol to recover to a set of legitimate states from any state in its state space. Once an SS protocol reaches a legitimate state, it remains in the set of legitimate states as long as there are no faults, called the closure property. Distribution issues exacerbate the design complexity of SS protocols as processes should collaborate and take local actions that result in global convergence. Most existing design techniques are manual, and mainly focus on protocols whose global state can be corrected if the local states of all processes are corrected, called the locally correctable protocols. After manual design, an SS protocol has to be verified for closure and convergence. Previous work observes that verifying SS protocols is a harder problem than designing them as developers have to ensure the correctness of closure and convergence functionalities and their noninterference. An algorithmic method for the design of convergence generates protocols that are correct by construction, thereby eliminating the need for verification.In order to facilitate the design of SS protocols, this article presents a lightweight method for algorithmic addition of convergence to finite-state nonstabilizing protocols, including nonlocally correctable protocols. The proposed method enables the reuse of design efforts in the development of different self-stabilizing protocols. Moreover, for the first time (to the best of our knowledge), this article presents an algorithmic method for the addition of convergence to symmetric protocols that consist of structurally similar processes. The proposed approach is supported by a software tool that automatically adds convergence to nonstabilizing protocols. We have used the proposed method/tool to automatically generate several self-stabilizing protocols with up to 40 processes (and 340 states) in a few minutes on a regular PC. Surprisingly, our tool has synthesized both protocols that are the same as their manually designed versions as well as alternative solutions for well-known problems in the literature (e.g., Dijkstra’s token ring, maximal matching, graph coloring, agreement and leader election in a ring). Moreover, the proposed method has helped us detect a design flaw in a manually designed self-stabilizing protocol.","self-stabilization, automated design, Fault tolerance",,,,
Conference Paper,"Weichbrodt N,Aublin PL,Kapitza R",Sgx-Perf: A Performance Analysis Tool for Intel SGX Enclaves,,2018,,,201–213,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 19th International Middleware Conference,"Rennes, France",2018,9781450357029.0,,https://doi.org/10.1145/3274808.3274824;http://dx.doi.org/10.1145/3274808.3274824,10.1145/3274808.3274824,"Novel trusted execution technologies such as Intel's Software Guard Extensions (SGX) are considered a cure to many security risks in clouds. This is achieved by offering trusted execution contexts, so called enclaves, that enable confidentiality and integrity protection of code and data even from privileged software and physical attacks. To utilise this new abstraction, Intel offers a dedicated Software Development Kit (SDK). While it is already used to build numerous applications, understanding the performance implications of SGX and the offered programming support is still in its infancy. This inevitably leads to time-consuming trial-and-error testing and poses the risk of poor performance.To enable the development of well-performing SGX-based applications, this paper makes the following three contributions: First, it summarises identified performance critical factors of SGX. Second, it presents sgx-perf, a collection of tools for high-level dynamic performance analysis of SGX-based applications. In particular, sgx-perf performs not only fined-grained profiling of performance critical events in enclaves but also offers recommendations on how to improve enclave performance. Third, it demonstrates how we used sgx-perf in four non-trivial SGX workloads to increase their performance by up to 2.16x.","Trusted Execution, Performance Profiling, Intel Software Guard Extensions",Middleware '18,,,
Conference Paper,"Cassee N,Pinto G,Castor F,Serebrenik A",How Swift Developers Handle Errors,,2018,,,292–302,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 15th International Conference on Mining Software Repositories,"Gothenburg, Sweden",2018,9781450357166.0,,https://doi.org/10.1145/3196398.3196428;http://dx.doi.org/10.1145/3196398.3196428,10.1145/3196398.3196428,"Swift is a new programming language developed by Apple as a replacement to Objective-C. It features a sophisticated error handling (EH) mechanism that provides the kind of separation of concerns afforded by exception handling mechanisms in other languages, while also including constructs to improve safety and maintainability. However, Swift also inherits a software development culture stemming from Objective-C being the de-facto standard programming language for Apple platforms for the last 15 years. It is, therefore, a priori unclear whether Swift developers embrace the novel EH mechanisms of the programming language or still rely on the old EH culture of Objective-C even working in Swift.In this paper, we study to what extent developers adhere to good practices exemplified by EH guidelines and tutorials, and what are the common bad EH practices particularly relevant to Swift code. Furthermore, we investigate whether perception of these practices differs between novices and experienced Swift developers.To answer these questions we employ a mixed-methods approach and combine 10 semi-structured interviews with Swift developers and quantitative analysis of 78,760 Swift 4 files extracted from 2,733 open-source GitHub repositories. Our findings indicate that there is ample opportunity to improve the way Swift developers use error handling mechanisms. For instance, some recommendations derived in this work are not well spread in the corpus of studied Swift projects. For example, generic catch handlers are common in Swift (even though it is not uncommon for them to share space with their counterparts: non empty catch handlers), custom, developerdefined error types are rare, and developers are mostly reactive when it comes to error handling, using Swift's constructs mostly to handle errors thrown by libraries, instead of throwing and handling application-specific errors.","swift, language feature usage, error handling",MSR '18,,,
Conference Paper,Rushby J,New Challenges in Certification for Aircraft Software,,2011,,,211–218,Association for Computing Machinery,"New York, NY, USA",Proceedings of the Ninth ACM International Conference on Embedded Software,"Taipei, Taiwan",2011,9781450307147.0,,https://doi.org/10.1145/2038642.2038675;http://dx.doi.org/10.1145/2038642.2038675,10.1145/2038642.2038675,"We outline the current approach to certification of aircraft software, and the role of DO-178B. We consider evidence for its effectiveness and discuss possible explanations for this. We then describe how changes in aircraft systems and in the air traffic system pose new challenges for certification, chiefly by increasing the extent of interaction and integration.","certification, do-178b, formal methods",EMSOFT '11,,,
Conference Paper,"Filieri A,Grunske L,Leva A",Lightweight Adaptive Filtering for Efficient Learning and Updating of Probabilistic Models,,2015,,,200–211,IEEE Press,"Florence, Italy",Proceedings of the 37th International Conference on Software Engineering - Volume 1,,2015,9781479919345.0,,,,"Adaptive software systems are designed to cope with unpredictable and evolving usage behaviors and environmental conditions. For these systems reasoning mechanisms are needed to drive evolution, which are usually based on models capturing relevant aspects of the running software. The continuous update of these models in evolving environments requires efficient learning procedures, having low overhead and being robust to changes. Most of the available approaches achieve one of these goals at the price of the other. In this paper we propose a lightweight adaptive filter to accurately learn time-varying transition probabilities of discrete time Markov models, which provides robustness to noise and fast adaptation to changes with a very low overhead. A formal stability, unbiasedness and consistency assessment of the learning approach is provided, as well as an experimental comparison with state-of-the-art alternatives.",,ICSE '15,,,
Conference Paper,"Sousa L,Cedrim D,Garcia A,Oizumi W,Bibiano AC,Oliveira D,Kim M,Oliveira A","Characterizing and Identifying Composite Refactorings: Concepts, Heuristics and Patterns",,2020,,,186–197,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 17th International Conference on Mining Software Repositories,"Seoul, Republic of Korea",2020,9781450375177.0,,https://doi.org/10.1145/3379597.3387477;http://dx.doi.org/10.1145/3379597.3387477,10.1145/3379597.3387477,"Refactoring consists of a transformation applied to improve the program internal structure, for instance, by contributing to remove code smells. Developers often apply multiple interrelated refactorings called composite refactoring. Even though composite refactoring is a common practice, an investigation from different points of view on how composite refactoring manifests in practice is missing. Previous empirical studies also neglect how different kinds of composite refactorings affect the removal, prevalence or introduction of smells. To address these matters, we provide a conceptual framework and two heuristics to respectively characterize and identify composite refactorings within and across commits. Then, we mined the commit history of 48 GitHub software projects. We identified and analyzed 24,911 composite refactorings involving 104,505 single refactorings. Amongst several findings, we observed that most composite refactorings occur in the same commit and have the same refactoring type. We found that several refactorings are semantically related to each other, which occur in different parts of the system but are still related to the same task. Our study is the first to reveal that many smells are introduced in a program due to ""incomplete"" composite refactorings. Our study is also the first to reveal 111 patterns of composite refactorings that frequently introduce or remove certain smell types. These patterns can be used as guidelines for developers to improve their refactoring practices as well as for designers of recommender systems.",,MSR '20,,,
Conference Paper,"Gaber I,Kirsh A",The Effect of Reporting Known Issues on Students' Work,,2018,,,74–79,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 49th ACM Technical Symposium on Computer Science Education,"Baltimore, Maryland, USA",2018,9781450351034.0,,https://doi.org/10.1145/3159450.3159456;http://dx.doi.org/10.1145/3159450.3159456,10.1145/3159450.3159456,"Students in Computer Science programming courses often have difficulty with coding, which results in flawed exercises. We asked students working on programming exercises to report known defects in their submission. We distinguish between three types of defects: bugs in the program, missing features, and poorly written code. Results show that students detect and report missing features and bugs quite accurately (59% of the bugs and 61% of the missing features were reported), but they are much less aware of the quality of their code (only 28% of the code issues were reported). After comparing their grades to the grades of the previous year we argue that the request to report defects helps student in submitting exercises with fewer bugs. Finally, the students affirmed that the request to report defects helped them in detecting problems and improved their time management.","students' exercises, software defects, code quality, students perception, known issues, object-oriented course",SIGCSE '18,,,
Journal Article,"Seaman C,Nord RL,Kruchten P,Ozkaya I",Technical Debt: Beyond Definition to Understanding Report on the Sixth International Workshop on Managing Technical Debt,SIGSOFT Softw. Eng. Notes,2015,40.0,2.0,32–34,Association for Computing Machinery,"New York, NY, USA",,,2015-04,,0163-5948,https://doi.org/10.1145/2735399.2735419;http://dx.doi.org/10.1145/2735399.2735419,10.1145/2735399.2735419,"We report here on the Sixth International Workshop on Managing Technical Debt, collocated with the International Conference on Software Maintenance and Evolution (ICSME 2014). The major themes of discussion this year indicate a growing maturity in this area of investigation and the desire to move beyond definitional issues to impact.","software quality, software evolution, software economics, technical debt",,,,
Conference Paper,"Brasileiro F,Almeida JP,Carvalho VA,Guizzardi G",Applying a Multi-Level Modeling Theory to Assess Taxonomic Hierarchies in Wikidata,,2016,,,975–980,International World Wide Web Conferences Steering Committee,"Republic and Canton of Geneva, CHE",Proceedings of the 25th International Conference Companion on World Wide Web,"Montréal, Québec, Canada",2016,9781450341448.0,,https://doi.org/10.1145/2872518.2891117;http://dx.doi.org/10.1145/2872518.2891117,10.1145/2872518.2891117,"Wikidata captures structured data on a number of subject domains, managing, among others, the information underlying Wikipedia and other Wikimedia projects. Wikidata serves as a repository of structured data, whose purpose is to support the consistent sharing and linking of data on the Web. To support these purposes, it is key that Wikidata is built on consistent data models and representation schemas, which are constructed and managed in a collaborative platform. In this paper, we address the quality of taxonomic hierarchies in Wikidata. We focus on taxonomic hierarchies with entities at different classification levels (particular individuals, types of individuals, types of types of individuals, etc.). We use an axiomatic theory for multi-level modeling to analyze current Wikidata content, and identify a significant number of problematic classification and taxonomic statements. The problems seem to arise from an inadequate use of instantiation and subclassing in certain Wikidata hierarchies.","metamodeling, multi-level modeling, wikidata, taxonomies",WWW '16 Companion,,,
Conference Paper,"Fontana FA,Mangiacavalli M,Pochiero D,Zanoni M",On Experimenting Refactoring Tools to Remove Code Smells,,2015,,,,Association for Computing Machinery,"New York, NY, USA",Scientific Workshop Proceedings of the XP2015,"Helsinki, Finland",2015,9781450334099.0,,https://doi.org/10.1145/2764979.2764986;http://dx.doi.org/10.1145/2764979.2764986,10.1145/2764979.2764986,"When we develop a software project of a certain complexity, source code maintainability could become a problem, in particular if developers do not use a consolidate development process that simplifies the management of the entire project. When source code becomes very complex, it is difficult for developers to share and modify it. We can improve internal software qualities such as reusability, maintainability and readability through refactoring. Refactoring can be applied to remove possible problems in the code, as code smells. Identifying code smells and removing them through refactoring results in better code maintainability, but it can be an overwhelming task. In this paper, we describe our experimentation on using four refactoring tools to remove code smells in four systems, with the aim to outline advantages and disadvantages of the tools with respect to the accomplishment of this task, and to identify the smells easier to be removed among the ones we considered in this paper.","refactoring, code smells",XP '15 workshops,,,
Conference Paper,Catolino G,Does Source Code Quality Reflect the Ratings of Apps?,,2018,,,43–44,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 5th International Conference on Mobile Software Engineering and Systems,"Gothenburg, Sweden",2018,9781450357128.0,,https://doi.org/10.1145/3197231.3198447;http://dx.doi.org/10.1145/3197231.3198447,10.1145/3197231.3198447,"In the past, bad code quality has been associated with higher bugproneness. At the same time, the main reason why mobile users negatively rate an app is due to the presence of bugs leading to crashes. In this paper, we preliminarily investigate the extent to which code quality metrics can be exploited to predict the commercial success of mobile apps. Key results suggest the existence of a relation between code quality and commercial success; We found that inheritance and information hiding metrics represent important indicators and therefore should be carefully monitored by developers.","mobile applications, metrics, software quality",MOBILESoft '18,,,
Conference Paper,"Islam JF,Mondal M,Roy CK,Schneider KA",Comparing Bug Replication in Regular and Micro Code Clones,,2019,,,81–92,IEEE Press,"Montreal, Quebec, Canada",Proceedings of the 27th International Conference on Program Comprehension,,2019,,,https://doi.org/10.1109/ICPC.2019.00022;http://dx.doi.org/10.1109/ICPC.2019.00022,10.1109/ICPC.2019.00022,"Copying and pasting source code during software development is known as code cloning. Clone fragments with a minimum size of 5 LOC were usually considered in previous studies. In recent studies, clone fragments which are less than 5 LOC are referred as micro-clones. It has been established by the literature that code clones are closely related with software bugs as well as bug replication. None of the previous studies have been conducted on bug-replication of micro-clones. In this paper we investigate and compare bug-replication in between regular and micro-clones. For the purpose of our investigation, we analyze the evolutionary history of our subject systems and identify occurrences of similarity preserving co-changes (SPCOs) in both regular and micro-clones where they experienced bug-fixes. From our experiment on thousands of revisions of six diverse subject systems written in three different programming languages, C, C# and Java we find that the percentage of clone fragments that take part in bug-replication is often higher in micro-clones than in regular code clones. The percentage of bugs that get replicated in micro-clones is almost the same as the percentage in regular clones. Finally, both regular and micro-clones have similar tendencies of replicating severe bugs according to our experiment. Thus, micro-clones in a code-base should not be ignored. We should rather consider these equally important as of the regular clones when making clone management decisions.","severe bugs, micro-clones, replicated bugs, code clones",ICPC '19,,,
Journal Article,"Afanasov M,Mottola L,Ghezzi C",Software Adaptation in Wireless Sensor Networks,ACM Trans. Auton. Adapt. Syst.,2018,12.0,4.0,,Association for Computing Machinery,"New York, NY, USA",,,2018-01,,1556-4665,https://doi.org/10.1145/3145453;http://dx.doi.org/10.1145/3145453,10.1145/3145453,"We present design concepts, programming constructs, and automatic verification techniques to support the development of adaptive Wireless Sensor Network (WSN) software. WSNs operate at the interface between the physical world and the computing machine and are hence exposed to unpredictable environment dynamics. WSN software must adapt to these dynamics to maintain dependable and efficient operation. However, developers are left without proper support to develop adaptive functionality in WSN software. Our work fills this gap with three key contributions: (i) design concepts help developers organize the necessary adaptive functionality and understand their relations, (ii) dedicated programming constructs simplify the implementations, (iii) custom verification techniques allow developers to check the correctness of their design before deployment. We implement dedicated tool support to tie the three contributions, facilitating their practical application. Our evaluation considers representative WSN applications to analyze code metrics, synthetic simulations, and cycle-accurate emulation of popular WSN platforms. The results indicate that our work is effective in simplifying the development of adaptive WSN software; for example, implementations are provably easier to test and to maintain, the run-time overhead of our dedicated programming constructs is negligible, and our verification techniques return results in a matter of seconds.","context-oriented programming, Wireless sensor networks, software adaptation",,,,
Conference Paper,"Serebryany K,Iskhodzhanov T",ThreadSanitizer: Data Race Detection in Practice,,2009,,,62–71,Association for Computing Machinery,"New York, NY, USA",Proceedings of the Workshop on Binary Instrumentation and Applications,"New York, New York, USA",2009,9781605587936.0,,https://doi.org/10.1145/1791194.1791203;http://dx.doi.org/10.1145/1791194.1791203,10.1145/1791194.1791203,Data races are a particularly unpleasant kind of threading bugs. They are hard to find and reproduce -- you may not observe a bug during the entire testing cycle and will only see it in production as rare unexplainable failures. This paper presents ThreadSanitizer -- a dynamic detector of data races. We describe the hybrid algorithm (based on happens-before and locksets) used in the detector. We introduce what we call dynamic annotations -- a sort of race detection API that allows a user to inform the detector about any tricky synchronization in the user program. Various practical aspects of using ThreadSanitizer for testing multithreaded C++ code at Google are also discussed.,"Valgrind, testing, dynamic data race detection, concurrency bugs",WBIA '09,,,
Conference Paper,"Favato D,Ishitani D,Oliveira J,Figueiredo E",Linus's Law: More Eyes Fewer Flaws in Open Source Projects,,2019,,,69–78,Association for Computing Machinery,"New York, NY, USA",Proceedings of the XVIII Brazilian Symposium on Software Quality,"Fortaleza, Brazil",2019,9781450372824.0,,https://doi.org/10.1145/3364641.3364650;http://dx.doi.org/10.1145/3364641.3364650,10.1145/3364641.3364650,"Linus's Law states that ""given enough eyeballs, all bugs are shallow"". In other words, given a large enough number of developers, almost every programming flaw is characterized and fixed quickly. Although there is much debate about this subject, we still lack empirical evidence to support this law. Given that this theme has, and still is, motivating business decisions in software development, we investigate the implications of Linus's Law in two empirical studies on open source projects mined from GitHub. In the first pilot study, we mined seven popular Java projects from GitHub and investigated the correlation between committers and programming flaws in source code files. Results of this pilot study suggest a positive correlation between the number of developers and programming flaws. We cross-validate these results in a second study with almost one hundred Python projects from GitHub. In this second study, we analyzed the correlation between the number of forks - i.e., a proxy for number of developers - and programming flaws identified in projects. In both studies, programming flaws were detected by using static code analysis tools. As a result of the second study, we could not observe a correlation between the number of developers and the number of programming flaws in Python projects. From both studies we conclude that we were unable to find evidence to support the Linus's Law.","python, software quality, Linus's law, java",SBQS'19,,,
Conference Paper,Xu D,Software Security Testing of an Online Banking System: A Unique Research Experience for Undergraduates and Computer Teachers,,2013,,,705–710,Association for Computing Machinery,"New York, NY, USA",Proceeding of the 44th ACM Technical Symposium on Computer Science Education,"Denver, Colorado, USA",2013,9781450318686.0,,https://doi.org/10.1145/2445196.2445400;http://dx.doi.org/10.1145/2445196.2445400,10.1145/2445196.2445400,"This paper presents a unique summer project for a group of undergraduate students and high school computer teachers to gain research experiences in the area of cybersecurity. The students and teachers were selected from the participants in the NSF REU and RET programs at the host institution. Through the research on security testing of a real-world online banking system, the students and teachers have not only learned about the cutting-edge security testing techniques, but also made publishable contributions to the research base. The two collaborating graduate assistants served as an immediate role model for the undergraduates and an indirect role model for high school students through the teachers. With the help from the graduate assistants, the students and teachers were able to work effectively toward achieving their research objectives. The internal competition helped the participants get a better sense of achievement and satisfaction. The research experiences also prepared the teachers with the necessary knowledge for introducing cybersecurity topics (e.g., secure programming) into future classroom activity. As such, the project described in this paper provides a model summer program for undergraduate and/or K-12 teachers to gain research experiences.","mutation analysis, security testing, security attacks, software testing, access control, cybersecurity",SIGCSE '13,,,
Conference Paper,Trubiani C,A Model-Based Framework for Software Performance Feedback,,2010,,,19–34,Springer-Verlag,"Berlin, Heidelberg",Proceedings of the 2010 International Conference on Models in Software Engineering,"Oslo, Norway",2010,9783642212093.0,,,,"The problem of interpreting the results of performance analysis is quite critical in the software performance domain: mean values, variances, and probability distributions are hard to interpret for providing feedback to software architects. Support to the interpretation of such results that helps to fill the gap between numbers and architectural alternatives is still lacking.The goal of my PhD thesis is to develop a model-based framework addressing the results interpretation and the feedback generation problems by means of performance antipatterns, that are recurring solutions to common mistakes (i.e. bad practices) in the software development. Such antipatterns can play a key role in the software performance domain, since they can be used in the search of performance problems as well as in the formulation of their solutions in terms of architectural alternatives.","architectural alternatives, antipatterns, performance evaluation, feedback generation, software architecture",MODELS'10,,,
Conference Paper,"Dig D,Batory D",Fourth Workshop on Refactoring Tools (WRT 2011),,2011,,,1202–1203,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 33rd International Conference on Software Engineering,"Waikiki, Honolulu, HI, USA",2011,9781450304450.0,,https://doi.org/10.1145/1985793.1986046;http://dx.doi.org/10.1145/1985793.1986046,10.1145/1985793.1986046,"Refactoring is the process of applying behavior-preserving transformations to a program with the objective of improving the program's design. A specific refactoring is identified by a name (e.g., Extract Method), a set of preconditions, and a set of transformations that need to be performed.Tool support for refactoring is essential because checking the preconditions of refactoring often requires nontrivial program analysis, and applying transformations may affect many locations throughout a program. In recent years, the emergence of light-weight programming methodologies such as Extreme Programming has generated a great amount of interest in refactoring, and refactoring support has become a required feature in today's IDEs.This workshop is a continuation of a series of previous workshops (ECOOP 2007, OOPSLA 2008 and 2009 - see http://refactoring.info/WRT) where researchers and developers of refactoring tools can meet and discuss recent ideas and work, and view tool demonstrations.","program analysis, transformation, refactoring, program manipulation",ICSE '11,,,
Conference Paper,"Weckesser M,Lochau M,Ries M,Schürr A",Mathematical Programming for Anomaly Analysis of Clafer Models,,2018,,,34–44,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 21th ACM/IEEE International Conference on Model Driven Engineering Languages and Systems,"Copenhagen, Denmark",2018,9781450349499.0,,https://doi.org/10.1145/3239372.3239398;http://dx.doi.org/10.1145/3239372.3239398,10.1145/3239372.3239398,"Clafer combines UML-like class- and meta-modeling with feature-oriented variability-modeling and first-order logic constraints. The considerable expressiveness of Clafer mainly stems from its built-in variability constructs, multiplicity annotations and recursive model structures which yield a potentially unbounded number of valid model instances. As a result, automated reasoning about semantic properties like model consistency (i.e., existence of valid model instances) and anomalies (e.g., false cardinality bounds) is very challenging. Recent analysis techniques are inherently incomplete as they impose an a-priori finite search space with either manually or heuristically adjusted bounds. In this paper, we present a novel approach for automated search-space restriction for a considerably rich, yet decidable fragment of the Clafer language that guarantees sound and complete detection results for a wide range of semantic anomalies. Our approach employs principles from mathematical programming by encoding Clafer models as Mixed Integer Linear Programs (MILP). Our experimental evaluation shows remarkable improvements of runtime efficiency as well as effectiveness of anomaly detection as compared to existing techniques.","Consistency Checks, Anomaly Detection, Software Product Lines",MODELS '18,,,
Conference Paper,"Wang T,Harman M,Jia Y,Krinke J",Searching for Better Configurations: A Rigorous Approach to Clone Evaluation,,2013,,,455–465,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 2013 9th Joint Meeting on Foundations of Software Engineering,"Saint Petersburg, Russia",2013,9781450322379.0,,https://doi.org/10.1145/2491411.2491420;http://dx.doi.org/10.1145/2491411.2491420,10.1145/2491411.2491420,"Clone detection finds application in many software engineering activities such as comprehension and refactoring. However, the confounding configuration choice problem poses a widely-acknowledged threat to the validity of previous empirical analyses. We introduce desktop and parallelised cloud-deployed versions of a search based solution that finds suitable configurations for empirical studies. We evaluate our approach on 6 widely used clone detection tools applied to the Bellon suite of 8 subject systems. Our evaluation reports the results of 9.3 million total executions of a clone tool; the largest study yet reported. Our approach finds significantly better configurations (p < 0.05) than those currently used, providing evidence that our approach can ameliorate the confounding configuration choice problem.","SBSE, Clone Detection, Genetic Algorithms",ESEC/FSE 2013,,,
Conference Paper,"Heinze TS,Møller A,Strocco F",Type Safety Analysis for Dart,,2016,,,1–12,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 12th Symposium on Dynamic Languages,"Amsterdam, Netherlands",2016,9781450344456.0,,https://doi.org/10.1145/2989225.2989226;http://dx.doi.org/10.1145/2989225.2989226,10.1145/2989225.2989226,"Optional typing is traditionally viewed as a compromise between static and dynamic type checking, where code without type annotations is not checked until runtime. We demonstrate that optional type annotations in Dart programs can be integrated into a flow analysis to provide static type safety guarantees both for annotated and non-annotated parts of the code. We explore two approaches: one that uses type annotations for filtering, and one that uses them as specifications. What makes this particularly challenging for Dart is that its type system is unsound even for fully annotated code. Experimental results show that the technique is remarkably effective, even without context sensitivity: 99.3% of all property lookup operations are reported type safe in a collection of benchmark programs.","optional types, static analysis, type systems",DLS 2016,,,
Journal Article,"Heinze TS,Møller A,Strocco F",Type Safety Analysis for Dart,SIGPLAN Not.,2016,52.0,2.0,1–12,Association for Computing Machinery,"New York, NY, USA",,,2016-11,,0362-1340,https://doi.org/10.1145/3093334.2989226;http://dx.doi.org/10.1145/3093334.2989226,10.1145/3093334.2989226,"Optional typing is traditionally viewed as a compromise between static and dynamic type checking, where code without type annotations is not checked until runtime. We demonstrate that optional type annotations in Dart programs can be integrated into a flow analysis to provide static type safety guarantees both for annotated and non-annotated parts of the code. We explore two approaches: one that uses type annotations for filtering, and one that uses them as specifications. What makes this particularly challenging for Dart is that its type system is unsound even for fully annotated code. Experimental results show that the technique is remarkably effective, even without context sensitivity: 99.3% of all property lookup operations are reported type safe in a collection of benchmark programs.","static analysis, type systems, optional types",,,,
Conference Paper,"Fioravanti S,Patara F,Vicario E",Engineering the Performance of a Meta-Modeling Architecture,,2017,,,203–208,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 8th ACM/SPEC on International Conference on Performance Engineering Companion,"L'Aquila, Italy",2017,9781450348997.0,,https://doi.org/10.1145/3053600.3053647;http://dx.doi.org/10.1145/3053600.3053647,10.1145/3053600.3053647,"The Reflection architectural pattern is an elegant reusable solution to design software applications based on a meta-model that provides a self-representation of the types used in the domain model. This provides significant benefits in terms of adaptability, maintainability, self-awareness, and direct involvement of domain experts in the configuration stage. However, while virtuous in the perspective of object-oriented development, the meta-model adds a level of indirection that may result in poor performance. The complexity is further exacerbated when the object-oriented domain model is mapped to a relational database. We identify four performance anti-patterns that may naturally occur in the design of a meta-modeling architecture, and for each of them we propose a refactoring intervention on the object model and on the database mapping strategy. Experimental results are reported to characterize the gain obtained applying the proposed refactoring techniques to a real case of data management system, in order to provide a roadmap for engineering the performance of meta-modeling architectures.","reflection pattern, performance, anti-patterns, dynamic architectures, mapping, relational database, meta-modeling architecture, electronic health record (ehr) systems",ICPE '17 Companion,,,
Conference Paper,"Kouros P,Chaikalis T,Arvanitou EM,Chatzigeorgiou A,Ampatzoglou A,Amanatidis T",JCaliper: Search-Based Technical Debt Management,,2019,,,1721–1730,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 34th ACM/SIGAPP Symposium on Applied Computing,"Limassol, Cyprus",2019,9781450359337.0,,https://doi.org/10.1145/3297280.3297448;http://dx.doi.org/10.1145/3297280.3297448,10.1145/3297280.3297448,"Technical Debt (TD) reflects problems in software maintainability along evolution. TD principal is defined as the effort required for refactoring an existing system to an ideal one (a.k.a. optimal) that suffers from no maintainability problems. One of the open problems in the TD community is that ideal versions of systems do not exist, and there are no methods in the literature for approaching them, even theoretically. To alleviate this problem, in this paper we propose an efficient TD management strategy, by applying Search-Based Software Engineering techniques. In particular, we focus on one specific aspect of TD, namely inefficient software modularity, by properly assigning behavior and state to classes through search space exploration. At the same time, in the context of TD, we: (a) investigate the use of local search algorithms to obtain a near-optimum solution and propose TD repayment actions (i.e., refactorings), and (b) calculate the distance of a design to the corresponding optimal (i.e., a proxy of TD principal). The approach has been implemented in the JCaliper Eclipse plugin enabling a case study, which validates the approach and contrasts it to existing measure of software evolution.","refactoring, software quality, object-oriented design",SAC '19,,,
Conference Paper,"Arrieta A,Wang S,Arruabarrena A,Markiegi U,Sagardui G,Etxeberria L",Multi-Objective Black-Box Test Case Selection for Cost-Effectively Testing Simulation Models,,2018,,,1411–1418,Association for Computing Machinery,"New York, NY, USA",Proceedings of the Genetic and Evolutionary Computation Conference,"Kyoto, Japan",2018,9781450356183.0,,https://doi.org/10.1145/3205455.3205490;http://dx.doi.org/10.1145/3205455.3205490,10.1145/3205455.3205490,"In many domains, engineers build simulation models (e.g., Simulink) before developing code to simulate the behavior of complex systems (e.g., Cyber-Physical Systems). Those models are commonly heavy to simulate which makes it difficult to execute the entire test suite. Furthermore, it is often difficult to measure white-box coverage of test cases when employing such models. In addition, the historical data related to failures might not be available. This paper proposes a cost-effective approach for test case selection that relies on black-box data related to inputs and outputs of the system. The approach defines in total five effectiveness measures and one cost measure followed by deriving in total 15 objective combinations and integrating them within Non-Dominated Sorting Genetic Algorithm-II (NSGA-II). We empirically evaluated our approach with all these 15 combinations using four case studies by employing mutation testing to assess the fault revealing capability. The results demonstrated that our approach managed to improve Random Search by 26% on average in terms of the Hypervolume quality indicator.","simulation models, search-based testing, test selection",GECCO '18,,,
Conference Paper,"Vidal S,Abait ES,Marcos C,Casas S,Díaz Pace JA",Aspect Mining Meets Rule-Based Refactoring,,2009,,,23–27,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 1st Workshop on Linking Aspect Technology and Evolution,"Charlottesville, Virginia, USA",2009,9781605584539.0,,https://doi.org/10.1145/1509847.1509852;http://dx.doi.org/10.1145/1509847.1509852,10.1145/1509847.1509852,"Aspect-oriented software development allows the encapsulation of crosscutting concerns, achieving a better system modularization and, therefore, improving its maintenance. One important challenge is how to evolve an object-oriented system into an aspect-oriented one in such a way the system structure gets gradually improved. This paper describes a process to assist developers in the refactoring of object-oriented systems to aspects. To do so, we propose a tool approach that combines aspect mining techniques with a rule-base engine to apply refactorings.","software maintenance, aspect mining, aspect refactoring",PLATE '09,,,
Conference Paper,"Decker MJ,Newman CD,Dragan N,Collard ML,Maletic JI,Kraft NA",A Taxonomy of How Method Stereotypes Change,,2018,,,337–338,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 40th International Conference on Software Engineering: Companion Proceeedings,"Gothenburg, Sweden",2018,9781450356633.0,,https://doi.org/10.1145/3183440.3194998;http://dx.doi.org/10.1145/3183440.3194998,10.1145/3183440.3194998,"The role of a well-designed method should not change frequently or significantly over its lifetime. As such, changes to the role of a method can be an indicator of design improvement or degradation. To measure this, we use method stereotypes. Method stereotypes provide a high-level description of a method's behavior and role; giving insight into how a method interacts with its environment and carries out tasks. When a method's stereotype changes, so has its role. This work presents a taxonomy of how method stereotypes change and why the categories of changes are significant.","software change, method stereotypes, software evolution",ICSE '18,,,
Conference Paper,Smith CU,Software Performance Antipatterns in Cyber-Physical Systems,,2020,,,173–180,Association for Computing Machinery,"New York, NY, USA",Proceedings of the ACM/SPEC International Conference on Performance Engineering,"Edmonton AB, Canada",2020,9781450369916.0,,https://doi.org/10.1145/3358960.3379138;http://dx.doi.org/10.1145/3358960.3379138,10.1145/3358960.3379138,"Software performance antipatterns (SPAs) document common performance problems in software architecture and design and how to fix them. They differ from software antipatterns in their focus on the performance of the software. This paper addresses performance antipatterns that are common in today's Cyber-Physical Systems (CPS). We describe the characteristics of today's CPS that cause performance problems that have been uncommon in real-time embedded systems of the past. Three new performance antipatterns are defined and their impact on CPS is described. Six previously defined performance antipatterns are described that are particularly relevant to today's CPS. The paper concludes with observations on how this work is useful in the design, implementation, and operation of CPS.","performance antipatterns, cyber-physical systems design, software performance engineering (SPE), software architecture",ICPE '20,,,
Conference Paper,"Tekin U,Erdemir U,Buzluca F",Mining Object-Oriented Design Models for Detecting Identical Design Structures,,2012,,,43–49,IEEE Press,"Zurich, Switzerland",Proceedings of the 6th International Workshop on Software Clones,,2012,9781467317955.0,,,,"The object-oriented design is the most popular design methodology of the last twenty-five years. Several design patterns and principles are defined to improve the design quality of object-oriented software systems. In addition, designers can use unique design motifs which are particular for the specific application domain. Another common habit is cloning and modifying some parts of the software while creating new modules. Therefore, object-oriented programs can include many identical design structures. This work proposes a sub-graph mining based approach to detect identical design structures in object-oriented systems. By identifying and analyzing these structures, we can obtain useful information about the design, such as commonly-used design patterns, most frequent design defects, domain-specific patterns, and design clones, which may help developers to improve their knowledge about the software architecture. Furthermore, problematic parts of frequent identical design structures are the appropriate refactoring opportunities because they affect multiple areas of the architecture. Experiments with several open-source projects show that we can successfully find many identical design structures in each project. We observe that usually most of the identical structures are an implementation of common design patterns; however we also detect various anti-patterns, domain-specific patterns, and design-level clones.","software design models, clones, identical design structures, graph mining, pattern extraction, software motifs",IWSC '12,,,
Conference Paper,"Ghorbani N,Garcia J,Malek S",Detection and Repair of Architectural Inconsistencies in Java,,2019,,,560–571,IEEE Press,"Montreal, Quebec, Canada",Proceedings of the 41st International Conference on Software Engineering,,2019,,,https://doi.org/10.1109/ICSE.2019.00067;http://dx.doi.org/10.1109/ICSE.2019.00067,10.1109/ICSE.2019.00067,"Java is one of the most widely used programming languages. However, the absence of explicit support for architectural constructs, such as software components, in the programming language itself has prevented software developers from achieving the many benefits that come with architecture-based development. To address this issue, Java 9 has introduced the Java Platform Module System (JPMS), resulting in the first instance of encapsulation of modules with rich software architectural interfaces added to a mainstream programming language. The primary goal of JPMS is to construct and maintain large applications efficiently---as well as improve the encapsulation, security, and maintainability of Java applications in general and the JDK itself. A challenge, however, is that module declarations do not necessarily reflect actual usage of modules in an application, allowing developers to mistakenly specify inconsistent dependencies among the modules. In this paper, we formally define 8 inconsistent modular dependencies that may arise in Java-9 applications. We also present Darcy, an approach that leverages these definitions and static program analyses to automatically (1) detect the specified inconsistent dependencies within Java applications and (2) repair those identified inconsistencies. The results of our experiments, conducted over 38 open-source Java-9 applications, indicate that architectural inconsistencies are widespread and demonstrate the benefits of Darcy in automated detection and repair of these inconsistencies.",,ICSE '19,,,
Conference Paper,"Fioravanti S,Mattolini S,Patara F,Vicario E",Experimental Performance Evaluation of Different Data Models for a Reflection Software Architecture over NoSQL Persistence Layers,,2016,,,297–308,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 7th ACM/SPEC on International Conference on Performance Engineering,"Delft, The Netherlands",2016,9781450340809.0,,https://doi.org/10.1145/2851553.2851561;http://dx.doi.org/10.1145/2851553.2851561,10.1145/2851553.2851561,"The recent rise of the NoSQL movement motivates investigation on the performance impact that new persistence approaches can bring in the model-driven re-engineering of a consolidated object-oriented software architecture. We report comparative experimental performance results attained by combining a pattern-based domain logic with a persistence layer based on different paradigms and we describe how data model is persisted in various implementation based on MySQL, Neo4j, and MongoDB.","model-driven performance engineering, mongoDB, reflection pattern, relational databases, noSQL databases, electronic health record (EHR) systems, mySQL, neo4j",ICPE '16,,,
Conference Paper,"Riebisch M,Bode S,Brcina R",Problem-Solution Mapping for Forward and Reengineering on Architectural Level,,2011,,,106–115,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 12th International Workshop on Principles of Software Evolution and the 7th Annual ERCIM Workshop on Software Evolution,"Szeged, Hungary",2011,9781450308489.0,,https://doi.org/10.1145/2024445.2024466;http://dx.doi.org/10.1145/2024445.2024466,10.1145/2024445.2024466,"Software architectures play a key role for the development and evolution of software systems because they have to enable their quality properties such as scalability, flexibility, and security. Software architectural decisions represent a transition from problem space with quality goals and requirements on one side to solution space with technical solutions on the other side. Technical solutions are reusable elements for the work of the architect as for example patterns, styles, frameworks and building blocks. For long-term evolution of the systems, an explicit mapping between goals and solutions is helpful for expressing design knowledge and fundamental decisions. Such a mapping has to bridge between the fields of requirements engineering, software architectural design, and software quality thus enabling reuse. In this paper the Goal Solution Scheme is discussed, which maps quality goals and goal refinements to architectural principles and solutions. The paper extends the approach from the previously discussed forward engineering to re-engineering activities thus covering evolutionary development processes. The evaluation of the approach has been performed in several case studies and projects including a large industrial one.","quality goals, reuse, decision support, reengineering, traceability, software architecture, software evolution",IWPSE-EVOL '11,,,
Conference Paper,"Silva RB,Bezerra CI",Analyzing Continuous Integration Bad Practices in Closed-Source Projects: An Initial Study,,2020,,,642–647,Association for Computing Machinery,"New York, NY, USA",Proceedings of the XXXIV Brazilian Symposium on Software Engineering,"Natal, Brazil",2020,9781450387538.0,,https://doi.org/10.1145/3422392.3422474;http://dx.doi.org/10.1145/3422392.3422474,10.1145/3422392.3422474,"Continuous integration (CI) is a process widely used in projects of different types and sizes, generally aimed at improving development and also ensuring the quality of the software. However, some bad practices can mitigate or invalidate the beneficial effects of CI. In this context, we present an initial study to analyze CI bad practices in two closed-source projects. The study had as main research questions: (i) the frequency of CI bad practices in projects, (ii) the time that builds failures remain unresolved in projects, and (iii) the effects of CI bad practices on software health from the perspective of developers and project managers. For this, we carried out a mapping of the existence of bad practices through a questionnaire directed to the projects team. Also, it was realized the analysis of the pipeline history of the projects. Our findings indicate that the most common CI bad practices are related to improper management of the CI repository, and defective builds usually take a long time to fix. Besides, we found that the difficulties resulting from CI bad practices promote an increase in the development cost and reduction of software quality assurance.","bad practices, continuous integration, software quality",SBES '20,,,
Conference Paper,Bondi AB,Best Practices for Writing and Managing Performance Requirements: A Tutorial,,2012,,,1–8,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 3rd ACM/SPEC International Conference on Performance Engineering,"Boston, Massachusetts, USA",2012,9781450312028.0,,https://doi.org/10.1145/2188286.2188288;http://dx.doi.org/10.1145/2188286.2188288,10.1145/2188286.2188288,"Performance requirements are one of the main drivers of architectural decisions. Because many performance problems have their roots in architectural decisions, and since poor performance is a principal cause of software project risk, it is essential that performance requirements be developed early in the software lifecycle, and that they be clearly formulated. In this tutorial, we shall look at criteria for high-quality performance requirements, including algebraic consistency, measurability, testability, and linkage to business and engineering needs. While focus of this tutorial is on practice, we shall show how the drafting of performance requirements can be aided by performance modeling. We shall show methods for presenting and managing performance requirements that will improve their chances of being accepted by architects, developers, testers, contract negotiators, and purchasers; and of their being successfully implemented and tested.","performance requirements, system performance",ICPE '12,,,
Conference Paper,Li Z,Towards Providing Automated Supports to Developers on Writing Logging Statements,,2020,,,198–201,Association for Computing Machinery,"New York, NY, USA",Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering: Companion Proceedings,"Seoul, South Korea",2020,9781450371223.0,,https://doi.org/10.1145/3377812.3381385;http://dx.doi.org/10.1145/3377812.3381385,10.1145/3377812.3381385,"Developers write logging statements to generate logs and record system execution behaviors. Such logs are widely used for a variety of tasks, such as debugging, testing, program comprehension, and performance analysis. However, there exists no practical guidelines on how to write logging statements; hence, making the logging decision a very challenging task. There are two main challenges that developers are facing while making logging decisions: 1) Difficult to accurately and succinctly record execution behaviors; and 2) Hard to decide where to write logging statements. This thesis proposes a series of approaches to address the problems and help developers make logging decisions in two aspects: assist in making decisions on logging contents and on logging locations. Through case studies on large-scale open source and commercial systems, we anticipate that our study will provide useful suggestions and supports to developers for writing better logging statements.",,ICSE '20,,,
Conference Paper,Grammenos D,Game over: Learning by Dying,,2008,,,1443–1452,Association for Computing Machinery,"New York, NY, USA",Proceedings of the SIGCHI Conference on Human Factors in Computing Systems,"Florence, Italy",2008,9781605580111.0,,https://doi.org/10.1145/1357054.1357281;http://dx.doi.org/10.1145/1357054.1357281,10.1145/1357054.1357281,"This paper presents the design and evaluation of ""Game Over!"", which is the world's first universally inaccessible game (i.e., a game that can be played by no one). The game is meant to be used as an educational tool for disseminating and teaching game accessibility guidelines. This is achieved by providing game developers a first-hand (frustrating) experience of how it feels interacting with a game that is not accessible, due to the fact that important design rules were not considered or applied during its design. Both the overall concept and the approach followed were evaluated and validated through: (a) an on-line survey; (b) ""live"" feedback from players and developers; and (c) public opinions and critique collected from numerous Web sites and blogs where ""Game Over!"" was presented and discussed. The evaluation outcomes strongly suggest that computer games and humor constitute a perfect match for reaching out, motivating and educating the game developers' community in the subject of game accessibility.","game-based learning, design guidelines, game accessibility",CHI '08,,,
Conference Paper,"Spadini D,Schvarcbacher M,Oprescu AM,Bruntink M,Bacchelli A",Investigating Severity Thresholds for Test Smells,,2020,,,311–321,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 17th International Conference on Mining Software Repositories,"Seoul, Republic of Korea",2020,9781450375177.0,,https://doi.org/10.1145/3379597.3387453;http://dx.doi.org/10.1145/3379597.3387453,10.1145/3379597.3387453,"Test smells are poor design decisions implemented in test code, which can have an impact on the effectiveness and maintainability of unit tests. Even though test smell detection tools exist, how to rank the severity of the detected smells is an open research topic. In this work, we aim at investigating the severity rating for four test smells and investigate their perceived impact on test suite maintainability by the developers. To accomplish this, we first analyzed some 1,500 open-source projects to elicit severity thresholds for commonly found test smells. Then, we conducted a study with developers to evaluate our thresholds. We found that (1) current detection rules for certain test smells are considered as too strict by the developers and (2) our newly defined severity thresholds are in line with the participants' perception of how test smells have an impact on the maintainability of a test suite. Preprint [https://doi.org/10.5281/zenodo.3744281], data and material [https://doi.org/10.5281/zenodo.3611111].","Software Testing, Test Smells, Empirical Software Engineering",MSR '20,,,
Conference Paper,"Connolly Bree D,Cinnéide MÓ",Inheritance versus Delegation: Which is More Energy Efficient?,,2020,,,323–329,Association for Computing Machinery,"New York, NY, USA",Proceedings of the IEEE/ACM 42nd International Conference on Software Engineering Workshops,"Seoul, Republic of Korea",2020,9781450379632.0,,https://doi.org/10.1145/3387940.3392192;http://dx.doi.org/10.1145/3387940.3392192,10.1145/3387940.3392192,"Energy consumption of software is receiving more attention as concerns regarding climate change increase. One factor that significantly impacts how much energy is expended by a software application is the design of the software itself. Existing studies find few consistent results regarding the impact of common refactorings on energy consumption, nor do they define a concrete set of metrics that measure the energy efficiency of software. In this paper, we present the results of preliminary experiments that explore the Replace Inheritance with Delegation refactoring, and its inverse, to assess the impact these design-level refactorings have on energy consumption in the Java programming language. In the tested programs, inheritance proved to be more energy efficient than delegation, with a reduction in run time of 77% and a reduction in average power consumption of 4%. We subsequently propose a research plan to further explore this problem and observe a number of specific challenges in this area. The primary goals of this research are threefold: (i) to investigate how redundancy in an object-oriented design can contribute to unnecessary energy consumption, (ii) to determine how refactoring of the software can remove this redundancy, and (iii) to develop a general-purpose automated tool to perform this refactoring.","Electricity Consumption, Energy-Aware Software, Software Refactoring, Code Smells, Design Patterns",ICSEW'20,,,
Conference Paper,"Milzner K,Klinke R",Synthesis of Analog Circuits Using a Blackboard Approach,,1990,,,114–122,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 3rd International Conference on Industrial and Engineering Applications of Artificial Intelligence and Expert Systems - Volume 1,"Charleston, South Carolina, USA",1990,9780897913720.0,,https://doi.org/10.1145/98784.98805;http://dx.doi.org/10.1145/98784.98805,10.1145/98784.98805,"A novel approach to the design automation of analog circuits is presented. The prototype implementation -OASE- has been realized as a set of cooperating expert systems with blackboard architectures. The circuit specific knowledge bases use hybrid representation schemes and are strictly separated from the execution engine. This alleviates the knowledge acquisition process as well as the extension and maintenance of existing knowledge. OASE has been developed as a design assistant, providing a hierarchical design style and fully embedded standard algorithmic tools.",,IEA/AIE '90,,,
Journal Article,"Billingsley W,Torbay R,Fletcher PR,Thomas RN,Steel JR,Süß JG",Taking a Studio Course in Distributed Software Engineering from a Large Local Cohort to a Small Global Cohort,ACM Trans. Comput. Educ.,2019,19.0,2,,Association for Computing Machinery,"New York, NY, USA",,,2019-01,,,https://doi.org/10.1145/3218284;http://dx.doi.org/10.1145/3218284,10.1145/3218284,"One of the challenges of global software engineering courses is to bring the practices and experience of large geographically distributed teams into the local and time-limited environment of a classroom. Over the last 6 years, an on-campus studio course for software engineering has been developed at the University of Queensland (UQ) that places small teams of students on different features of a common product. This creates two layers of collaboration, as students work within their teams on individual features, and the teams must interoperate with many other teams on the common product. The class uses continuous integration practices and predominantly asynchronous communication channels (Slack and GitHub) to facilitate this collaboration. The original goal of this design was to ensure that students would authentically experience issues associated with realistically sized software projects, and learn to apply appropriate software engineering and collaboration practices to overcome them, in a course without significant extra staffing. Data from the development logs showed that most commits take place outside synchronous class hours, and the project operates as a temporally distributed team even though the students are geographically co-located. Since 2015, a course adapted from this format has also been taught at the University of New England (UNE), an Australian regional university that is also a longstanding provider of distance education. In this course, most students study online, and the class has to be able to work globally, because as well as students taking part from around Australia, there are also typically a small number of students taking part from overseas. Transferring the course to a smaller but predominantly online institution has allowed us to evaluate the distributed nature of the course, by considering what aspects of the course needed to change to support students who are geographically distributed, and comparing how the two cohorts behave. This has produced an overall course design, to teach professional distributed software engineering practices, that is adaptable from large classes to small, and from local to global.","studio pedagogies, Global software engineering",,,,
Conference Paper,"Tobias E,Maquil V,Latour T",TULIP: A Widget-Based Software Framework for Tangible Tabletop Interfaces,,2015,,,216–221,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 7th ACM SIGCHI Symposium on Engineering Interactive Computing Systems,"Duisburg, Germany",2015,9781450336468.0,,https://doi.org/10.1145/2774225.2775080;http://dx.doi.org/10.1145/2774225.2775080,10.1145/2774225.2775080,"In this paper, we describe a new software framework for tangible tabletop interfaces: TULIP. The framework uses an abstraction layer to receive information from computer vision frameworks, such as reacTIVision, and a widget model based on MCRit to enable rapid application development. TULIP applies Software Engineering principles such as Separation of Concerns to remain extensible and simple while providing support for tangible interaction. TULIP implements a widget model and defines a program flow. This paper will show the different considerations during the conception and design of TULIP before illustrating its use with a tangible application developed for a national project.","tangible user interfaces, interactive tabletops, widget, rapid application development, software framework",EICS '15,,,
Conference Paper,"Butler RW,Finelli GB",The Infeasibility of Experimental Quantification of Life-Critical Software Reliability,,1991,,,66–76,Association for Computing Machinery,"New York, NY, USA",Proceedings of the Conference on Software for Citical Systems,"New Orleans, Louisiana, USA",1991,9780897914550.0,,https://doi.org/10.1145/125083.123054;http://dx.doi.org/10.1145/125083.123054,10.1145/125083.123054,,,SIGSOFT '91,,,
Journal Article,"Butler RW,Finelli GB",The Infeasibility of Experimental Quantification of Life-Critical Software Reliability,SIGSOFT Softw. Eng. Notes,1991,16.0,5,66–76,Association for Computing Machinery,"New York, NY, USA",,,1991-09,,0163-5948,https://doi.org/10.1145/123041.123054;http://dx.doi.org/10.1145/123041.123054,10.1145/123041.123054,,,,,,
Conference Paper,Vassallo C,Enabling Continuous Improvement of a Continuous Integration Process,,2020,,,1246–1249,IEEE Press,"San Diego, California",Proceedings of the 34th IEEE/ACM International Conference on Automated Software Engineering,,2020,9781728125084.0,,https://doi.org/10.1109/ASE.2019.00151;http://dx.doi.org/10.1109/ASE.2019.00151,10.1109/ASE.2019.00151,"Continuous Integration (CI) is a widely-adopted software engineering practice. Despite its undisputed benefits, like higher software quality and improved developer productivity, mastering CI is not easy. Among the several barriers when transitioning to CI, developers need to face a new type of software failures (i.e., build failures) that requires them to understand complex build logs. Even when a team has successfully introduced a CI culture, living up to its principles and improving the CI practice are also challenging. In my research, I want to provide developers with the right support for establishing CI and the proper recommendations for continuously improving their CI process.","best practices, build failures, anti-patterns, continuous integration",ASE '19,,,
Conference Paper,"Virgínio T,Santana R,Martins LA,Soares LR,Costa H,Machado I",On the Influence of Test Smells on Test Coverage,,2019,,,467–471,Association for Computing Machinery,"New York, NY, USA",Proceedings of the XXXIII Brazilian Symposium on Software Engineering,"Salvador, Brazil",2019,9781450376518.0,,https://doi.org/10.1145/3350768.3350775;http://dx.doi.org/10.1145/3350768.3350775,10.1145/3350768.3350775,"Software testing is a key practice in the software quality assurance process. Usually, the quality of a test is not analyzed before its execution, i.e., there are no tests to check the tests. When the quality of tests is not guaranteed, it may impair the quality of the software. Test Smells are an alternative to indicate problems in the test code that can affect test maintainability, more specifically readability and comprehension. This study investigates correlations between test coverage and test smells types. We also introduce the JNose Test, a tool to automate test smells detection. We analyzed 11 open source projects and detected 21 types of smells and 10 different test coverage metrics to each test class. We identified 63 out of 210 calculated correlations. Our results show that there is a relationship between test smells and test coverage, in which test smells may influence code coverage. Our findings might support software testers and help them understand the behavior and consequences of poorly written and designed tests.",,SBES '19,,,
Conference Paper,"Krisper M,Dobaj J,Macher G",Patterns for Communicating Numerical Uncertainty,,2019,,,,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 24th European Conference on Pattern Languages of Programs,"Irsee, Germany",2019,9781450362061.0,,https://doi.org/10.1145/3361149.3361160;http://dx.doi.org/10.1145/3361149.3361160,10.1145/3361149.3361160,"Uncertainty is an inherent property of all measurements, statistics, or generally all communication involving numbers. Whenever numerical data is communicated, the uncertainty or confidence in this data should also be included. Neglecting it, or communicating it in an ambiguous way, leads to misinterpretation and misunderstandings. There are some well-known and proven patterns to avoid such problems. In this paper we present a collection of patterns for the communication of numerical uncertainty. These patterns revolve around three areas of applications: textual, numerical, and graphical. For numerical representations the pattern Numbers with Uncertainties is shown. For textual descriptions Words of Estimative Probability, Numeric Hedge Words and Quantitative Comparisons are explained, and for graphical visualization Error Indicator and Distribution Plots are described. The paper is targeted towards communicators, visualizers, reporters, as well as developers, engineers, and researchers of solutions for problems which involve uncertainty.","communication, visualization, probability, confidence, ambiguity, error, uncertainty",EuroPLop '19,,,
Conference Paper,"Mondal M,Roy CK,Schneider KA",Identifying Code Clones Having High Possibilities of Containing Bugs,,2017,,,99–109,IEEE Press,"Buenos Aires, Argentina",Proceedings of the 25th International Conference on Program Comprehension,,2017,9781538605356.0,,https://doi.org/10.1109/ICPC.2017.31;http://dx.doi.org/10.1109/ICPC.2017.31,10.1109/ICPC.2017.31,"Code cloning has emerged as a controversial term in software engineering research and practice because of its positive and negative impacts on software evolution and maintenance. Researchers suggest managing code clones through refactoring and tracking. Given the huge number of code clones in a software system's code-base, it is essential to identify the most important ones to manage. In our research, we investigate which clone fragments have high possibilities of containing bugs so that such clones can be prioritized for refactoring and tracking to help minimize future bug-fixing tasks. Existing studies on clone bug-proneness cannot pinpoint code clones that are likely to experience bug-fixes in the future.According to our analysis on thousands of revisions of four diverse subject systems written in Java, change frequency of code clones does not indicate their bug-proneness (i.e., does not indicate their tendencies of experiencing bug-fixes in future). Bug-proneness is mainly related with change recency of code clones. In other words, more recently changed code clones have a higher possibility of containing bugs. Moreover, for the code clones that were not changed previously we observed that clones that were created more recently have higher possibilities of experiencing bug-fixes. Thus, our research reveals the fact that bug-proneness of code clones mainly depends on how recently they were changed or created (for the ones that were not changed before). It invalidates the common intuition regarding the relatedness between high change frequency and bug-proneness. We believe that code clones should be prioritized for management considering their change recency or recency of creation (for the unchanged ones).",,ICPC '17,,,
Journal Article,"Fleming SD,Scaffidi C,Piorkowski D,Burnett M,Bellamy R,Lawrance J,Kwan I","An Information Foraging Theory Perspective on Tools for Debugging, Refactoring, and Reuse Tasks",ACM Trans. Softw. Eng. Methodol.,2013,22.0,2,,Association for Computing Machinery,"New York, NY, USA",,,2013-03,,1049-331X,https://doi.org/10.1145/2430545.2430551;http://dx.doi.org/10.1145/2430545.2430551,10.1145/2430545.2430551,"Theories of human behavior are an important but largely untapped resource for software engineering research. They facilitate understanding of human developers’ needs and activities, and thus can serve as a valuable resource to researchers designing software engineering tools. Furthermore, theories abstract beyond specific methods and tools to fundamental principles that can be applied to new situations. Toward filling this gap, we investigate the applicability and utility of Information Foraging Theory (IFT) for understanding information-intensive software engineering tasks, drawing upon literature in three areas: debugging, refactoring, and reuse. In particular, we focus on software engineering tools that aim to support information-intensive activities, that is, activities in which developers spend time seeking information. Regarding applicability, we consider whether and how the mathematical equations within IFT can be used to explain why certain existing tools have proven empirically successful at helping software engineers. Regarding utility, we applied an IFT perspective to identify recurring design patterns in these successful tools, and consider what opportunities for future research are revealed by our IFT perspective.","Information foraging, software maintenance",,,,
Conference Paper,"Xiong Y,Wang B,Fu G,Zang L",Learning to Synthesize,,2018,,,37–44,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 4th International Workshop on Genetic Improvement Workshop,"Gothenburg, Sweden",2018,9781450357531.0,,https://doi.org/10.1145/3194810.3194816;http://dx.doi.org/10.1145/3194810.3194816,10.1145/3194810.3194816,"In many scenarios we need to find the most likely program under a local context, where the local context can be an incomplete program, a partial specification, natural language description, etc. We call such problem program estimations. In this paper we propose an abstract framework, learning to synthesis, or L2S in short, to address this problem. L2S combines four tools to achieve this: rewriting rules are used to define the search space and search steps, constraint solving is used to prune off invalid candidates at each search step, machine learning is used to estimate conditional probabilities for the candidates at each search step, and search algorithms are used to find the best possible solution. The main goal of L2S is to lay out the design space to motivate the research on program estimation.We have performed a preliminary evaluation by instantiating this framework for synthesizing conditions of an automated program repair (APR) system. The training data are from the project itself and related JDK packages. Compared to ACS, a state-of-the-art condition synthesis system for program repair, our approach could deal with a larger search space such that we fixed 4 additional bugs outside the search space of ACS, and relies only the source code of the current projects.",,GI '18,,,
Conference Paper,"Dai T,Dean D,Wang P,Gu X,Lu S",Hytrace: A Hybrid Approach to Performance Bug Diagnosis in Production Cloud Infrastructures,,2017,,,641,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 2017 Symposium on Cloud Computing,"Santa Clara, California",2017,9781450350280.0,,https://doi.org/10.1145/3127479.3132562;http://dx.doi.org/10.1145/3127479.3132562,10.1145/3127479.3132562,"Server applications running inside production cloud infrastructures are prone to various performance problems (e.g., software hang, performance slow down). When those problems occur, developers often have little clue to diagnose those problems. We present HyTrace, a novel hybrid approach to diagnosing performance problems in production cloud infrastructures. HyTrace combines rule-based static analysis and runtime inference techniques to achieve higher bug localization accuracy than pure-static and pure-dynamic approaches for performance bugs. HyTrace does not require source code and can be applied to both compiled and interpreted programs such as C/C++ and Java. We conduct experiments using real performance bugs from seven commonly used server applications. The results show that our approach can significantly improve the performance bug diagnosis accuracy compared to existing diagnosis techniques.","hybrid analysis, performance bug diagnosis",SoCC '17,,,
Conference Paper,"Maddox J,Long Y,Rajan H",Large-Scale Study of Substitutability in the Presence of Effects,,2018,,,528–538,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 2018 26th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering,"Lake Buena Vista, FL, USA",2018,9781450355735.0,,https://doi.org/10.1145/3236024.3236075;http://dx.doi.org/10.1145/3236024.3236075,10.1145/3236024.3236075,"A majority of modern software is constructed using languages that compute by producing side-effects such as reading/writing from/to files, throwing exceptions, acquiring locks, etc. To understand a piece of software, e.g. a class, it is important for a developer to understand its side-effects. Similarly, to replace a class with another, it is important to understand whether the replacement is a safe substitution for the former in terms of its behavior, a property known as substitutability, because mismatch may lead to bugs. The problem is especially severe for superclass-subclass pairs since at runtime an instance of the subclass may be used in the client code where a superclass is mentioned. Despite the importance of this property, we do not yet know whether substitutability w.r.t. effects between subclass and superclass is preserved in the wild, and if not what sorts of substitutability violations are common and what is the impact of such violations. This paper conducts a large scale study on over 20 million Java classes, in order to compare the effects of the methods of subclasses and superclasses in practice. Our comprehensive study considers the exception, synchronization, I/O, and method call effects. It reveals that in pairs with effects, only 8-24% have the same effects, and 31-56% of submethods have more effects, and the effects of a large percentage of submethods cannot be inferred from the supermethod.","Substitutability, Object-oriented Software Engineering, Side-effects",ESEC/FSE 2018,,,
Conference Paper,"De Ruvo G,Tempero E,Luxton-Reilly A,Rowe GB,Giacaman N",Understanding Semantic Style by Analysing Student Code,,2018,,,73–82,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 20th Australasian Computing Education Conference,"Brisbane, Queensland, Australia",2018,9781450363402.0,,https://doi.org/10.1145/3160489.3160500;http://dx.doi.org/10.1145/3160489.3160500,10.1145/3160489.3160500,"Good coding style is recognised by the software engineering profession as being important, and this is reflected in the standard computing curricula. Feedback on some aspects of coding style is now commonly provided by IDEs and by tools such as Checkstyle, but this feedback focuses on coding standards that are largely based on syntax. However, some aspects of coding style relate to the semantics of code --- of the many ways to achieve some functionality, some are preferred because they are simpler, yet students struggle to create them. In this paper, we introduce the concept of semantic style, and in particular semantic style indicators that may be manifestations of poor knowledge of some programming concepts. We describe 16 semantic style indicators and demonstrate their prevalence in almost 19,000 code samples submitted by over 900 novice students. Half the students submitted code exhibiting two or more of these indicators, demonstrating the potential value to learn by providing feedback on semantic style. We also find many indicators are present in the code of students attending their fourth year of a highly competitive Software Engineering programme, demonstrating the need for more attention to teaching of semantic style issues.","automated feedback, coding standards, semantic style, formative feedback, software quality, novice programmers",ACE '18,,,
Conference Paper,"Vercammen S,Ghafari M,Demeyer S,Borg M",Goal-Oriented Mutation Testing with Focal Methods,,2018,,,23–30,Association for Computing Machinery,"New York, NY, USA","Proceedings of the 9th ACM SIGSOFT International Workshop on Automating TEST Case Design, Selection, and Evaluation","Lake Buena Vista, FL, USA",2018,9781450360531.0,,https://doi.org/10.1145/3278186.3278190;http://dx.doi.org/10.1145/3278186.3278190,10.1145/3278186.3278190,"Mutation testing is the state-of-the-art technique for assessing the fault-detection capacity of a test suite. Unfortunately, mutation testing consumes enormous computing resources because it runs the whole test suite for each and every injected mutant. In this paper we explore fine-grained traceability links at method level (named focal methods), to reduce the execution time of mutation testing and to verify the quality of the test cases for each individual method, instead of the usually verified overall test suite quality. Validation of our approach on the open source Apache Ant project shows a speed-up of 573.5x for the mutants located in focal methods with a quality score of 80%.","Feasibility study, Focal methods, Mutation testing, Software testing",A-TEST 2018,,,
Conference Paper,"Kazman R,Cai Y,Mo R,Feng Q,Xiao L,Haziyev S,Fedak V,Shapochka A",A Case Study in Locating the Architectural Roots of Technical Debt,,2015,,,179–188,IEEE Press,"Florence, Italy",Proceedings of the 37th International Conference on Software Engineering - Volume 2,,2015,,,,,"Our recent research has shown that, in large-scale software systems, defective files seldom exist alone. They are usually architecturally connected, and their architectural structures exhibit significant design flaws which propagate bugginess among files. We call these flawed structures the architecture roots, a type of technical debt that incurs high maintenance penalties. Removing the architecture roots of bugginess requires refactoring, but the benefits of refactoring have historically been difficult for architects to quantify or justify. In this paper, we present a case study of identifying and quantifying such architecture debts in a large-scale industrial software project. Our approach is to model and analyze software architecture as a set of design rule spaces (DRSpaces). Using data extracted from the project's development artifacts, we were able to identify the files implicated in architecture flaws and suggest refactorings based on removing these flaws. Then we built economic models of the before and (predicted) after states, which gave the organization confidence that doing the refactorings made business sense, in terms of a handsome return on investment.",,ICSE '15,,,
Conference Paper,"Habchi S,Blanc X,Rouvoy R",On Adopting Linters to Deal with Performance Concerns in Android Apps,,2018,,,6–16,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 33rd ACM/IEEE International Conference on Automated Software Engineering,"Montpellier, France",2018,9781450359375.0,,https://doi.org/10.1145/3238147.3238197;http://dx.doi.org/10.1145/3238147.3238197,10.1145/3238147.3238197,"With millions of applications (apps) distributed through mobile markets, engaging and retaining end-users challenge Android developers to deliver a nearly perfect user experience. As mobile apps run in resource-limited devices, performance is a critical criterion for the quality of experience. Therefore, developers are expected to pay much attention to limit performance bad practices. On the one hand, many studies already identified such performance bad practices and showed that they can heavily impact app performance. Hence, many static analysers, a.k.a. linters, have been proposed to detect and fix these bad practices. On the other hand, other studies have shown that Android developers tend to deal with performance reactively and they rarely build on linters to detect and fix performance bad practices. In this paper, we therefore perform a qualitative study to investigate this gap between research and development community. In particular, we performed interviews with 14 experienced Android developers to identify the perceived benefits and constraints of using linters to identify performance bad practices in Android apps. Our observations can have a direct impact on developers and the research community. Specifically, we describe why and how developers leverage static source code analysers to improve the performance of their apps. On top of that, we bring to light important challenges faced by developers when it comes to adopting static analysis for performance purposes.","performance, Android, static analysis, linters",ASE 2018,,,
Conference Paper,"Trubiani C,Di Marco A,Cortellessa V,Mani N,Petriu D",Exploring Synergies between Bottleneck Analysis and Performance Antipatterns,,2014,,,75–86,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 5th ACM/SPEC International Conference on Performance Engineering,"Dublin, Ireland",2014,9781450327336.0,,https://doi.org/10.1145/2568088.2568092;http://dx.doi.org/10.1145/2568088.2568092,10.1145/2568088.2568092,"The problem of interpreting the results of performance analysis is quite critical, mostly because the analysis results (i.e. mean values, variances, and probability distributions) are hard to transform into feedback for software engineers that allows to remove performance problems. Approaches aimed at identifying and removing the causes of poor performance in software systems commonly fall in two categories: (i) bottleneck analysis, aimed at identifying overloaded software components and/or hardware resources that affect the whole system performance, and (ii) performance antipatterns, aimed at detecting and removing common design mistakes that notably induce performance degradation.In this paper, we look for possible synergies between these two categories of approaches in order to empower the performance investigation capabilities. In particular, we aim at showing that the approach combination allows to provide software engineers with broader sets of alternative solutions leading to better performance results. We have explored this research direction in the context of Layered Queueing Network models, and we have considered a case study in the e-commerce domain. After comparing the results achievable with each approach separately, we quantitatively show the benefits of merging bottleneck analysis and performance antipatterns.","performance antipatterns, bottleneck analysis, software performance, model-based performance analysis, software performance feedback",ICPE '14,,,
Conference Paper,"Fakhoury S,Roy D,Hassan SA,Arnaoudova V",Improving Source Code Readability: Theory and Practice,,2019,,,2–12,IEEE Press,"Montreal, Quebec, Canada",Proceedings of the 27th International Conference on Program Comprehension,,2019,,,https://doi.org/10.1109/ICPC.2019.00014;http://dx.doi.org/10.1109/ICPC.2019.00014,10.1109/ICPC.2019.00014,"There are several widely accepted metrics to measure code quality that are currently being used in both research and practice to detect code smells and to find opportunities for code improvement. Although these metrics have been proposed as a proxy of code quality, recent research suggests that more often than not, state-of-the-art code quality metrics do not successfully capture quality improvements in the source code as perceived by developers. More specifically, results show that there may be inconsistencies between, on the one hand, the results from metrics for cohesion, coupling, complexity, and readability, and, on the other hand, the interpretation of these metrics in practice. As code improvement tools rely on these metrics, there is a clear need to identify and resolve the aforementioned inconsistencies. This will allow for the creation of tools that are more aligned with developers' perception of quality, and can more effectively help source code improvement efforts.In this study, we investigate 548 instances of source code readability improvements, as explicitly stated by internal developers in practice, from 63 engineered software projects. We show that current readability models fail to capture readability improvements. We also show that tools to calculate additional metrics, to detect refactorings, and to detect style problems are able to capture characteristics that are specific to readability changes and thus should be considered by future readability models.","readability, developers'perception, code quality metrics",ICPC '19,,,
Conference Paper,"Nagy C,Cleve A",SQLInspect: A Static Analyzer to Inspect Database Usage in Java Applications,,2018,,,93–96,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 40th International Conference on Software Engineering: Companion Proceeedings,"Gothenburg, Sweden",2018,9781450356633.0,,https://doi.org/10.1145/3183440.3183496;http://dx.doi.org/10.1145/3183440.3183496,10.1145/3183440.3183496,"We present SQLInspect, a tool intended to assist developers who deal with SQL code embedded in Java applications. It is integrated into Eclipse as a plug-in that is able to extract SQL queries from Java code through static string analysis. It parses the extracted queries and performs various analyses on them. As a result, one can readily explore the source code which accesses a given part of the database, or which is responsible for the construction of a given SQL query. SQL-related metrics and common coding mistakes are also used to spot inefficiently or defectively performing SQL statements and to identify poorly designed classes, like those that construct many queries via complex control-flow paths. SQLInspect is a novel tool that relies on recent query extraction approaches. It currently supports Java applications working with JDBC and SQL code written for MySQL or Apache Impala. Check out the live demo of SQLInspect at http://perso.unamur.be/ cnagy/sqlinspect.","metrics, JDBC, concept location, Java, apache impala, MySQL, static analysis, embedded SQL, eclipse, bad smells",ICSE '18,,,
Conference Paper,"Alquraan A,Takruri H,Alfatafta M,Al-Kiswany S",An Analysis of Network-Partitioning Failures in Cloud Systems,,2018,,,51–68,USENIX Association,USA,Proceedings of the 13th USENIX Conference on Operating Systems Design and Implementation,"Carlsbad, CA, USA",2018,9781931971478.0,,,,"We present a comprehensive study of 136 system failures attributed to network-partitioning faults from 25 widely used distributed systems. We found that the majority of the failures led to catastrophic effects, such as data loss, reappearance of deleted data, broken locks, and system crashes. The majority of the failures can easily manifest once a network partition occurs: They require little to no client input, can be triggered by isolating a single node, and are deterministic. However, the number of test cases that one must consider is extremely large. Fortunately, we identify ordering, timing, and network fault characteristics that significantly simplify testing. Furthermore, we found that a significant number of the failures are due to design flaws in core system mechanisms.We found that the majority of the failures could have been avoided by design reviews, and could have been discovered by testing with network-partitioning fault injection. We built NEAT, a testing framework that simplifies the coordination of multiple clients and can inject different types of network-partitioning faults. We used NEAT to test seven popular systems and found and reported 32 failures.",,OSDI'18,,,
Conference Paper,"Jiang L,Rewcastle R,Denny P,Tempero E",CompareCFG: Providing Visual Feedback on Code Quality Using Control Flow Graphs,,2020,,,493–499,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 2020 ACM Conference on Innovation and Technology in Computer Science Education,"Trondheim, Norway",2020,9781450368742.0,,https://doi.org/10.1145/3341525.3387362;http://dx.doi.org/10.1145/3341525.3387362,10.1145/3341525.3387362,"The quality of the code impacts the cost of its maintenance, yet ""code quality"" is often not given attention in introductory programming courses, perhaps due to the difficulty of providing automated code quality feedback. We have been exploring how to provide automated feedback on complexity, one aspect of code quality. We have developed CompareCFG that provides feedback based on control flow graphs (CFGs). It generates visualisations of students' submissions and provides the means for a student to compare the CFG of their own code with CFGs of less complex submissions, helping to support their understanding of code complexity. CompareCFG also provides actionable feedback by indicating specific issues in a submission that can reduce its complexity. We evaluated CompareCFG in a pilot study. We found it provides useful feedback to participants that helped them reduce the complexity of their code. CompareCFG offers a convenient way to provide programming students with automated visual feedback on code quality.","software maintenance, automatic assessment tools, CFG, control flow graph, code quality",ITiCSE '20,,,
Conference Paper,"Kapova L,Buhnova B,Martens A,Happe J,Reussner R",State Dependence in Performance Evaluation of Component-Based Software Systems,,2010,,,37–48,Association for Computing Machinery,"New York, NY, USA",Proceedings of the First Joint WOSP/SIPEW International Conference on Performance Engineering,"San Jose, California, USA",2010,9781605585635.0,,https://doi.org/10.1145/1712605.1712613;http://dx.doi.org/10.1145/1712605.1712613,10.1145/1712605.1712613,"Integrating rising variability of software systems in performance prediction models is crucial to allow widespread industrial use of performance prediction. One of such variabilities is the dependency of system performance on the context and history-dependent internal state of the system (or its components). The questions that rise for current prediction models are (i) how to include the state properties in a prediction model, and (ii) how to balance the expressiveness and complexity of created models.Only a few performance prediction approaches deal with modelling states in component-based systems. Currently, there is neither a consensus in the definition, nor in the method to include the state in prediction models. For these reasons, we have conducted a state-of-the-art survey of existing approaches addressing their expressiveness to model stateful components. Based on the results, we introduce a classification scheme and present the state-defining and state-dependent model parameters. We extend the Palladio Component Model (PCM), a model-based performance prediction approach, with state-modelling capabilities, and study the performance impact of modelled state. A practical influences of the internal state on software performance is evaluated on a realistic case study.","state dependency, performance, design-time prediction",WOSP/SIPEW '10,,,
Journal Article,"Bouwers E,Visser J,Van Deursen A",Getting What You Measure: Four Common Pitfalls in Using Software Metrics for Project Management,Queue,2012,10.0,5,50–56,Association for Computing Machinery,"New York, NY, USA",,,2012-05,,1542-7730,https://doi.org/10.1145/2208917.2229115;http://dx.doi.org/10.1145/2208917.2229115,10.1145/2208917.2229115,"Software metrics - helpful tools or a waste of time? For every developer who treasures these mathematical abstractions of software systems there is a developer who thinks software metrics are invented just to keep project managers busy. Software metrics can be very powerful tools that help achieve your goals but it is important to use them correctly, as they also have the power to demotivate project teams and steer development in the wrong direction.",,,,,
Conference Paper,"Gil Jyossi,Maman I",Micro Patterns in Java Code,,2005,,,97–116,Association for Computing Machinery,"New York, NY, USA","Proceedings of the 20th Annual ACM SIGPLAN Conference on Object-Oriented Programming, Systems, Languages, and Applications","San Diego, CA, USA",2005,9781595930316.0,,https://doi.org/10.1145/1094811.1094819;http://dx.doi.org/10.1145/1094811.1094819,10.1145/1094811.1094819,"Micro patterns are similar to design patterns, except that micro patterns are stand at a lower, closer to the implementation, level of abstraction. Micro patterns are also unique in that they are mechanically recognizable, since each such pattern can be expressed as a formal condition on the structure of a class.This paper presents a catalog of 27 micro-patterns defined on Java classes and interfaces. The catalog captures a wide spectrum of common programming practices, including a particular and (intentionally restricted) use of inheritance, immutability, data management and wrapping, restricted creation, and emulation of procedural-, modular-, and even functional- programming paradigms with object oriented constructs. Together, the patterns present a set of prototypes after which a large portion of all Java classes and interfaces are modeled. We provide empirical indication that this portion is as high as 75%.A statistical analysis of occurrences of micro patterns in a large software corpus, spanning some 70,000 Java classes drawn from a rich set of application domains, shows, with high confidence level that the use of these patterns is not random. These results indicate consciousness and discernible design decisions, which are sustained in the software evolution. With high confidence level, we can also show that the use of these patterns is tied to the specification, or the purpose, that the software realizes.The traceability, abundance and the statistical significance of micro pattern occurrence raise the hope of using the classification of software into these patterns for a more founded appreciation of its design and code quality.","program analysis, object-oriented programming, design patterns, implementation patterns",OOPSLA '05,,,
Journal Article,"Gil Jyossi,Maman I",Micro Patterns in Java Code,SIGPLAN Not.,2005,40.0,10,97–116,Association for Computing Machinery,"New York, NY, USA",,,2005-10,,0362-1340,https://doi.org/10.1145/1103845.1094819;http://dx.doi.org/10.1145/1103845.1094819,10.1145/1103845.1094819,"Micro patterns are similar to design patterns, except that micro patterns are stand at a lower, closer to the implementation, level of abstraction. Micro patterns are also unique in that they are mechanically recognizable, since each such pattern can be expressed as a formal condition on the structure of a class.This paper presents a catalog of 27 micro-patterns defined on Java classes and interfaces. The catalog captures a wide spectrum of common programming practices, including a particular and (intentionally restricted) use of inheritance, immutability, data management and wrapping, restricted creation, and emulation of procedural-, modular-, and even functional- programming paradigms with object oriented constructs. Together, the patterns present a set of prototypes after which a large portion of all Java classes and interfaces are modeled. We provide empirical indication that this portion is as high as 75%.A statistical analysis of occurrences of micro patterns in a large software corpus, spanning some 70,000 Java classes drawn from a rich set of application domains, shows, with high confidence level that the use of these patterns is not random. These results indicate consciousness and discernible design decisions, which are sustained in the software evolution. With high confidence level, we can also show that the use of these patterns is tied to the specification, or the purpose, that the software realizes.The traceability, abundance and the statistical significance of micro pattern occurrence raise the hope of using the classification of software into these patterns for a more founded appreciation of its design and code quality.","program analysis, design patterns, implementation patterns, object-oriented programming",,,,
Conference Paper,"Vakilian M,Sauciuc R,Morgenthaler JD,Mirrokni V",Automated Decomposition of Build Targets,,2015,,,123–133,IEEE Press,"Florence, Italy",Proceedings of the 37th International Conference on Software Engineering - Volume 1,,2015,9781479919345.0,,,,"A (build) target specifies the information that is needed to automatically build a software artifact. This paper focuses on underutilized targets---an important dependency problem that we identified at Google. An underutilized target is one with files not needed by some of its dependents. Underutilized targets result in less modular code, overly large artifacts, slow builds, and unnecessary build and test triggers. To mitigate these problems, programmers decompose underutilized targets into smaller targets. However, manually decomposing a target is tedious and error-prone. Although we prove that finding the best target decomposition is NP-hard, we introduce a greedy algorithm that proposes a decomposition through iterative unification of the strongly connected components of the target. Our tool found that 19,994 of 40,000 Java library targets at Google can be decomposed to at least two targets. The results show that our tool is (1) efficient because it analyzes a target in two minutes on average and (2) effective because for each of 1,010 targets, it would save at least 50% of the total execution time of the tests triggered by the target.",,ICSE '15,,,
Conference Paper,"Mileva YM,Zeller A",Project-Specific Deletion Patterns,,2008,,,41–42,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 2008 International Workshop on Recommendation Systems for Software Engineering,"Atlanta, Georgia",2008,9781605582283.0,,https://doi.org/10.1145/1454247.1454262;http://dx.doi.org/10.1145/1454247.1454262,10.1145/1454247.1454262,"We apply data mining to version control data in order to detect project-specific deletion patterns---subcomponents or features of the software that were deleted on purpose. We believe that locations that are similar to earlier deletions are likely to be code smells. Future recommendation tools can warn against such smells: ""People who used gets() in the past now use fgets(). Consider a change, too.""",,RSSE '08,,,
Conference Paper,"Gopstein D,Zhou HH,Frankl P,Cappos J",Prevalence of Confusing Code in Software Projects: Atoms of Confusion in the Wild,,2018,,,281–291,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 15th International Conference on Mining Software Repositories,"Gothenburg, Sweden",2018,9781450357166.0,,https://doi.org/10.1145/3196398.3196432;http://dx.doi.org/10.1145/3196398.3196432,10.1145/3196398.3196432,"Prior work has shown that extremely small code patterns, such as the conditional operator and implicit type conversion, can cause considerable misunderstanding in programmers. Until now, the real world impact of these patterns - known as 'atoms of confusion' - was only speculative. This work uses a corpus of 14 of the most popular and influential open source C and C++ projects to measure the prevalence and significance of these small confusing patterns. Our results show that the 15 known types of confusing micro patterns occur millions of times in programs like the Linux kernel and GCC, appearing on average once every 23 lines. We show there is a strong correlation between these confusing patterns and bug-fix commits as well as a tendency for confusing patterns to be commented. We also explore patterns at the project level showing the rate of security vulnerabilities is higher in projects with more atoms. Finally, we examine real code examples containing these atoms, including ones that were used to find and fix bugs in our corpus. In total this work demonstrates that beyond simple misunderstanding in the lab setting, atoms of confusion are both prevalent - occurring often in real projects, and meaningful - being removed by bug-fix commits at an elevated rate.","programming languages, program understanding",MSR '18,,,
Journal Article,"Bornholt J,Torlak E",Finding Code That Explodes under Symbolic Evaluation,Proc. ACM Program. Lang.,2018,2.0,OOPSLA,,Association for Computing Machinery,"New York, NY, USA",,,2018-10,,,https://doi.org/10.1145/3276519;http://dx.doi.org/10.1145/3276519,10.1145/3276519,"Solver-aided tools rely on symbolic evaluation to reduce programming tasks, such as verification and synthesis, to satisfiability queries. Many reusable symbolic evaluation engines are now available as part of solver-aided languages and frameworks, which have made it possible for a broad population of programmers to create and apply solver-aided tools to new domains. But to achieve results for real-world problems, programmers still need to write code that makes effective use of the underlying engine, and understand where their code needs careful design to elicit the best performance. This task is made difficult by the all-paths execution model of symbolic evaluators, which defies both human intuition and standard profiling techniques.This paper presents symbolic profiling, a new approach to identifying and diagnosing performance bottlenecks in programs under symbolic evaluation. To help with diagnosis, we develop a catalog of common performance anti-patterns in solver-aided code. To locate these bottlenecks, we develop SymPro, a new profiling technique for symbolic evaluation. SymPro identifies bottlenecks by analyzing two implicit resources at the core of every symbolic evaluation engine: the symbolic heap and symbolic evaluation graph. These resources form a novel performance model of symbolic evaluation that is general (encompassing all forms of symbolic evaluation), explainable (providing programmers with a conceptual framework for understanding symbolic evaluation), and actionable (enabling precise localization of bottlenecks). Performant solver-aided code carefully manages the shape of these implicit structures; SymPro makes their evolution explicit to the programmer.To evaluate SymPro, we implement profilers for the Rosette solver-aided language and the Jalangi program analysis framework. Applying SymPro to 15 published solver-aided tools, we discover 8 previously undiagnosed performance issues. Repairing these issues improves performance by orders of magnitude, and our patches were accepted by the tools' developers. We also conduct a small user study with Rosette programmers, finding that SymPro helps them both understand what the symbolic evaluator is doing and identify performance issues they could not otherwise locate.","profiling, symbolic execution, solver-aided programming",,,,
Conference Paper,Techapalokul P,Sniffing Through Millions of Blocks for Bad Smells,,2017,,,781–782,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 2017 ACM SIGCSE Technical Symposium on Computer Science Education,"Seattle, Washington, USA",2017,9781450346986.0,,https://doi.org/10.1145/3017680.3022450;http://dx.doi.org/10.1145/3017680.3022450,10.1145/3017680.3022450,"Code smells codify poor coding patterns known to degrade software quality. Block-based languages have proven to be a viable educational and end-user programming paradigm with increasing adoption across a broad spectrum of users and domains. This rising popularity of this programming paradigm calls for a serious look at the program quality written in block-based languages. While code smells in the context of text-based languages have been studied extensively, the research community lacks a comprehensive understanding of code smells in block-based software. To address this problem, we present the results of a large-scale study of code smells prevalent in programs written in the highly popular Scratch programming language. We analyzed programs submitted to the public Scratch repository in 2016, considering a million programs altogether. We discovered interesting relationships between the prevalence of certain smells and the levels of proficiency of the programmers commonly introducing them. Our findings not only can help block-based programmers improve the quality of their software, but also establish the requirements for refactoring support in this programming domain.","code smells, software quality, scratch, introductory computing, CS education, block-based programming",SIGCSE '17,,,
Journal Article,"Counsell S,Swift S",Issues Arising from Refactoring Studies: An Experience Report,SIGSOFT Softw. Eng. Notes,2012,37.0,3,1–5,Association for Computing Machinery,"New York, NY, USA",,,2012-05,,0163-5948,https://doi.org/10.1145/2180921.2180922;http://dx.doi.org/10.1145/2180921.2180922,10.1145/2180921.2180922,"In theory, refactoring should reverse the trend in code decay and many studies have explored the different facets of refactoring (both its trends and characteristics). While much progress has been made in this area, a number of observations about refactoring studies have become evident to us over the past seven years in the time during which we have been undertaking empirical studies in this area. This paper outlines our experiences of the issues that arise with refactoring studies. We outline six of those issues, together forming the set of challenges that are still prevalent in this area. The purpose of the paper is thus to put under the spotlight the real potential benefits of refactoring, but more importantly the challenges that our experiences have raised","refactoring, empirical studies, OO",,,,
Conference Paper,"Varma P,Anand A,Pazel DP,Tibbitts BR",NextGen EXtreme Porting: Structured by Automation,,2005,,,1511–1517,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 2005 ACM Symposium on Applied Computing,"Santa Fe, New Mexico",2005,9781581139648.0,,https://doi.org/10.1145/1066677.1067018;http://dx.doi.org/10.1145/1066677.1067018,10.1145/1066677.1067018,"""Maintenance is really the normal state of an XP project"" - Beck. Thus porting is a natural candidate for eXtreme Programming and we present a novel tool-based XP methodology for porting C/C++ programs. The structure provided by our tooling is designed for scalability, to enable XP on large projects porting enterprise-scale codebases. Overall planning and iteration planning of the methodology are assisted by a novel, first-of-its-kind migration orchestrator tool. Automated test, debugging, and audit function are provided as unified support by our refactoring tool framework. We focus on the orchestrator tool and offer preliminary benchmarks with encouraging results.","XP, orchestration, dialects, analyze, software process model, re-factor, debug, test, fix, port planning, eXtreme programming",SAC '05,,,
Conference Paper,"Luo J,Lu F,Wang T",A Multi-Dimensional Assessment Model and Its Application in E-Learning Courses of Computer Science,,2020,,,187–193,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 21st Annual Conference on Information Technology Education,"Virtual Event, USA",2020,9781450370455.0,,https://doi.org/10.1145/3368308.3415388;http://dx.doi.org/10.1145/3368308.3415388,10.1145/3368308.3415388,"Computer science is a practical discipline. It is always a great challenge to evaluate students' computer practice using computer-aided means for large scale students. We always need to address problems such as suspected plagiarism and deviation of the overall difficulty factor. In this paper, a multi-dimensional assessment model is designed for CS courses based on the detailed practice processing data in an E-learning system. The model comprehensively evaluates the students' learning process and results in three aspects of correctness, originality, and quality detection. Besides, the teacher can easily participate in the assessment according to their needs. The correctness is an essential requirement, and the originality is based on the clustering results of students' behaviors after clone detection to curb homework plagiarism. SonarQube is used to detect code quality and put forward higher requirements for codes. Manual participation intelligence has improved the flexibility and applicability of the model to a certain extent. We applied this model on the EduCoder online education platform and carried out a comprehensive analysis of 485 students in the Parallel Programming Principles and Practice Class of Huazhong University of Science and Technology. Experiment results confirm the distinction, rationality, and fairness of the model in assessing student performance. It not only gives students a credible, comprehensive score in large-scale online practical programming courses but also gives teachers and students corresponding suggestions based on the evaluation results. Furthermore, the model can be extended to other online education platforms.","massive online open practice, multi-dimensional intelligent scoring model, student assessment, student behavior analysis",SIGITE '20,,,
Conference Paper,"Syeed MM,Lokhman A,Mikkonen T,Hammouda I",Pluggable Systems as Architectural Pattern: An Ecosystemability Perspective,,2015,,,,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 2015 European Conference on Software Architecture Workshops,"Dubrovnik, Cavtat, Croatia",2015,9781450333931.0,,https://doi.org/10.1145/2797433.2797477;http://dx.doi.org/10.1145/2797433.2797477,10.1145/2797433.2797477,"In this paper we review the use of plug-in architectures as a technological platform for software ecosystems. Our observation is that the software community has viewed and used plug-ins as powerful extension mechanisms offering a wide range of quality properties. Looking beyond such low-level technical interpretation, we argue that pluggable systems should be perceived and treated as a higher level architectural pattern. In order to back our perspective we present the pattern following widely adopted documentation scheme, we show example usage of the pattern in the Eclipse ecosystem, and we discuss different implementation options of the pattern when building new technical solutions for ecosystems.","Plug-in systems, architectural patterns, software ecosystems",ECSAW '15,,,
Conference Paper,"Aivaloglou E,Hermans F",How Kids Code and How We Know: An Exploratory Study on the Scratch Repository,,2016,,,53–61,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 2016 ACM Conference on International Computing Education Research,"Melbourne, VIC, Australia",2016,9781450344494.0,,https://doi.org/10.1145/2960310.2960325;http://dx.doi.org/10.1145/2960310.2960325,10.1145/2960310.2960325,"Block-based programming languages like Scratch, Alice and Blockly are becoming increasingly common as introductory languages in programming education. There is substantial research showing that these visual programming environments are suitable for teaching programming concepts. But, what do people do when they use Scratch? In this paper we explore the characteristics of Scratch programs. To this end we have scraped the Scratch public repository and retrieved 250,000 projects. We present an analysis of these projects in three different dimensions. Initially, we look at the types of blocks used and the size of the projects. We then investigate complexity, used abstractions and programming concepts. Finally we detect code smells such as large scripts, dead code and duplicated code blocks. Our results show that 1) most Scratch programs are small, however Scratch programs consisting of over 100 sprites exist, 2) programming abstraction concepts like procedures are not commonly used and 3) Scratch programs do suffer from code smells including large scripts and unmatched broadcast signals.","scratch, static analysis, programming practices, code smells, block-based languages",ICER '16,,,
Conference Paper,"Plösch R,Bräuer J,Saft M,Körner C",Design Debt Prioritization: A Design Best Practice-Based Approach,,2018,,,95–104,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 2018 International Conference on Technical Debt,"Gothenburg, Sweden",2018,9781450357135.0,,https://doi.org/10.1145/3194164.3194172;http://dx.doi.org/10.1145/3194164.3194172,10.1145/3194164.3194172,"Technical debt (TD) in a software system is a metaphor that tries to illustrate the remediation effort of the already introduced quality deficit and the impact thereof to the business value of the system. To address TD, various management activities are proposed, each addressing a particular purpose. Whereas the activities of debt identification and measurement are broadly considered in literature, the activities of debt prioritization and communication lack appropriate approaches with an economic perspective. This work proposes a TD prioritization approach. Therefore, it narrows down the focus of TD to design debt and relies on the quantification of design best practices. Further, the non-conformance of these practices is assessed by applying a benchmarking technique. As a result, the gained information is transferred into a portfolio-matrix to support the prioritization and communication of design remediation actions. The applicability and suitability of the approach are demonstrated by using the source code of the open source project GeoGebra.","design quality, design debt, debt prioritization, technical debt",TechDebt '18,,,
Conference Paper,"Wettel R,Lanza M",Visually Localizing Design Problems with Disharmony Maps,,2008,,,155–164,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 4th ACM Symposium on Software Visualization,"Ammersee, Germany",2008,9781605581125.0,,https://doi.org/10.1145/1409720.1409745;http://dx.doi.org/10.1145/1409720.1409745,10.1145/1409720.1409745,"Assessing the quality of software design is difficult, as ""design"" is expressed through guidelines and heuristics, not rigorous rules. One successful approach to assess design quality is based on detection strategies, which are metrics-based composed logical conditions, by which design fragments with specific properties are detected in the source code. Such detection strategies, when executed on large software systems usually return large sets of artifacts, which potentially exhibit one or more ""design disharmonies"", which are then inspected manually, a cumbersome activity.In this article we present disharmony maps, a visualization-based approach to locate such flawed software artifacts in large systems. We display the whole system using a 3D visualization technique based on a city metaphor. We enrich such visualizations with the results returned by a number of detection strategies, and thus render both the static structure and the design problems that affect a subject system. We evaluate our approach on a number of open-source Java systems and report on our findings.","software visualization, software design anomalies",SoftVis '08,,,
Conference Paper,"De Bleser J,Di Nucci D,De Roover C",Assessing Diffusion and Perception of Test Smells in Scala Projects,,2019,,,457–467,IEEE Press,"Montreal, Quebec, Canada",Proceedings of the 16th International Conference on Mining Software Repositories,,2019,,,https://doi.org/10.1109/MSR.2019.00072;http://dx.doi.org/10.1109/MSR.2019.00072,10.1109/MSR.2019.00072,"Test smells are, analogously to code smells, defined as the characteristics exhibited by poorly designed unit tests. Their negative impact on test effectiveness, understanding, and maintenance has been demonstrated by several empirical studies.However, the scope of these studies has been limited mostly to JAVA in combination with the JUNIT testing framework. Results for other language and framework combinations are ---despite their prevalence in practice--- few and far between, which might skew our understanding of test smells. The combination of Scala and ScalaTest, for instance, offers more comprehensive means for defining and reusing test fixtures, thereby possibly reducing the diffusion and perception of fixture-related test smells.This paper therefore reports on two empirical studies conducted for this combination. In the first study, we analyse the tests of 164 open-source Scala projects hosted on GitHub for the diffusion of test smells. This required the transposition of their original definition to this new context, and the implementation of a tool (SoCRATES) for their automated detection. In the second study, we assess the perception and the ability of 14 Scala developers to identify test smells. For this context, our results show (i) that test smells have a low diffusion across test classes, (ii) that the most frequently occurring test smells are Lazy Test, Eager Test, and Assertion Roulette, and (iii) that many developers were able to perceive but not to identify the smells.","test smells, scala language, test quality",MSR '19,,,
Conference Paper,"Durelli RS,Viana MC,de S. Landi A,Durelli VH,Delamaro ME,de Camargo VV",Improving the Structure of KDM Instances via Refactorings: An Experimental Study Using KDM-RE,,2017,,,174–183,Association for Computing Machinery,"New York, NY, USA",Proceedings of the XXXI Brazilian Symposium on Software Engineering,"Fortaleza, CE, Brazil",2017,9781450353267.0,,https://doi.org/10.1145/3131151.3131153;http://dx.doi.org/10.1145/3131151.3131153,10.1145/3131151.3131153,"Architecture-Driven Modernization (ADM) is an initiative of the Object Management Group (OMG) whose main purpose is to provide standard metamodels for software modernization activities. The most important metamodel is the Knowledge Discovery Metamodel (KDM), which represents software artifacts in a language-agnostic fashion. A fundamental step in software modernization is refactoring. However, there is a lack of tools that address how refactoring can be applied in conjunction with ADM. We developed a tool, called KDM-RE, that supports refactorings in KDM instances through: (i) a set of wizards that aid the software modernization engineer during refactoring activities; (ii) a change propagation module that keeps the internal metamodels synchronized; and (iii) the selection and application of refactorings available in its repository. This paper evaluates the application of refactorings to KDM instances in an experiment involving seven systems implemented in Java. We compared the pre-refactoring versions of these systems with the refactored ones using the Quality Model for Object-Oriented Design (QMOOD) metric set. The results from this evaluation suggest that KDM-RE provides advantages to software modernization engineers refactoring systems represented as KDMs.","Model-Driven Development, Architecture-Driven Modernization, Refactoring, Knowledge-Discovery Metamodel",SBES '17,,,
Conference Paper,"Sadeghi A,Jabbarvand R,Ghorbani N,Bagheri H,Malek S",A Temporal Permission Analysis and Enforcement Framework for Android,,2018,,,846–857,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 40th International Conference on Software Engineering,"Gothenburg, Sweden",2018,9781450356381.0,,https://doi.org/10.1145/3180155.3180172;http://dx.doi.org/10.1145/3180155.3180172,10.1145/3180155.3180172,"Permission-induced attacks, i.e., security breaches enabled by permission misuse, are among the most critical and frequent issues threatening the security of Android devices. By ignoring the temporal aspects of an attack during the analysis and enforcement, the state-of-the-art approaches aimed at protecting the users against such attacks are prone to have low-coverage in detection and high-disruption in prevention of permission-induced attacks. To address this shortcomings, we present Terminator, a temporal permission analysis and enforcement framework for Android. Leveraging temporal logic model checking,Terminator's analyzer identifies permission-induced threats with respect to dynamic permission states of the apps. At runtime, Terminator's enforcer selectively leases (i.e., temporarily grants) permissions to apps when the system is in a safe state, and revokes the permissions when the system moves to an unsafe state realizing the identified threats. The results of our experiments, conducted over thousands of apps, indicate that Terminator is able to provide an effective, yet non-disruptive defense against permission-induced attacks. We also show that our approach, which does not require modification to the Android framework or apps' implementation logic, is highly reliable and widely applicable.","Android, temporal logic, access control (permission)",ICSE '18,,,
Conference Paper,"Yantzi DJ,Andrews JH",Industrial Evaluation of a Log File Analysis Methodology,,2007,,,4,IEEE Computer Society,USA,Proceedings of the 5th International Workshop on Dynamic Analysis,,2007,9780769529639.0,,https://doi.org/10.1109/WODA.2007.7;http://dx.doi.org/10.1109/WODA.2007.7,10.1109/WODA.2007.7,"Test result evaluation programs often take the form of log file analyzers, which analyze text logs of events that have happened during testing. Previously, we proposed a methodology for deriving logging instrumentation and state-based log file analyzer programs from requirements. In this paper, we report on an industrial evaluation in which the methodology was carried out on two pieces of commercial software in a commercial setting. We report on our quantitative and qualitative observations and on recommendations for improving the methodology and the tools used.",,WODA '07,,,
Journal Article,"Jiau HC,Yang FP",Facing up to the Inequality of Crowdsourced API Documentation,SIGSOFT Softw. Eng. Notes,2012,37.0,1,1–9,Association for Computing Machinery,"New York, NY, USA",,,2012-01,,0163-5948,https://doi.org/10.1145/2088883.2088892;http://dx.doi.org/10.1145/2088883.2088892,10.1145/2088883.2088892,"API usability is a crucial issue in software development. One bottleneck of API usability is insufficient documentation. This study empirically confirmed the inequality of crowdsourced API documentation, which is one of the main sources of API documentation. To manage the inequality, a method for documentation reuse is proposed based on the nature of object-oriented programming language, inheritance. A case study was conducted in Stackoverflow, which is a widely used Q&A site, to study the feasibility of the documentation reuse. Results of the case study indicate that documentation reuse is feasible in improving both the coverage and quality of crowdsourced API documentations.","stackoverflow, inheritance, power law, inequality, jQuery, forum, crowdsourcing, GWT, API, wxPython, SWT, swing",,,,
Conference Paper,"Müller SC,Fritz T",Stuck and Frustrated or in Flow and Happy: Sensing Developers' Emotions and Progress,,2015,,,688–699,IEEE Press,"Florence, Italy",Proceedings of the 37th International Conference on Software Engineering - Volume 1,,2015,9781479919345.0,,,,"Software developers working on change tasks commonly experience a broad range of emotions, ranging from happiness all the way to frustration and anger. Research, primarily in psychology, has shown that for certain kinds of tasks, emotions correlate with progress and that biometric measures, such as electro-dermal activity and electroencephalography data, might be used to distinguish between emotions. In our research, we are building on this work and investigate developers' emotions, progress and the use of biometric measures to classify them in the context of software change tasks. We conducted a lab study with 17 participants working on two change tasks each. Participants were wearing three biometric sensors and had to periodically assess their emotions and progress. The results show that the wide range of emotions experienced by developers is correlated with their perceived progress on the change tasks. Our analysis also shows that we can build a classifier to distinguish between positive and negative emotions in 71.36% and between low and high progress in 67.70% of all cases. These results open up opportunities for improving a developer's productivity. For instance, one could use such a classifier for providing recommendations at opportune moments when a developer is stuck and making no progress.",,ICSE '15,,,
Conference Paper,"Belkhir A,Abdellatif M,Tighilt R,Moha N,Guéhéneuc YG,Beaudry É",An Observational Study on the State of REST API Uses in Android Mobile Applications,,2019,,,66–75,IEEE Press,"Montreal, Quebec, Canada",Proceedings of the 6th International Conference on Mobile Software Engineering and Systems,,2019,,,,,"REST is by far the most commonly-used style for designing APIs, especially for mobile platforms. Indeed, REST APIs are well suited for providing content to apps running on small devices, like smart-phones and tablets. Several research works studied REST APIs development practices for mobile apps. However, little is known about how Android apps use/consume these APIs in practice through HTTP client libraries. Consequently, we propose an observational study on the state of the practice of REST APIs use in Android mobile apps. We (1) build a catalogue of Android REST mobile clients practices; (2) define each of these practices through a number of heuristics based on their potential implementations in Android apps, and (3) propose an automatic approach to detect these practices. We analyze 1,595 REST mobile apps downloaded from the Google Play Store and mine thousands of StackOverflow posts to study REST APIs uses in Android apps. We observe that developers have always used HttpURLConnection class for REST APIs implementation in Android apps. However, since the apparition of REST third-party libraries such as Okhttp, Retrofit and Google Volley, Android REST clients have been increasingly relying on the facilities offered by these libraries. Also, we observe that developers used to ignore some good practices of REST APIs uses in Android apps. Such practices are the use of HTTP third-party libraries, caching responses, timeout management, and error handling. Moreover, we report that only two good practices are widely considered by Android developers when implementing their mobile apps. These practices are network connectivity awareness and JSON vs. XML response parsing. We also find that Retrofit is the most targeted third-party HTTP client library by Android developers because of its ease of use and provided features. Thus, we conclude that service providers must strive to make their libraries as simple as possible while mobile-service consumers should consider existing libraries to benefit from their features, such as asynchronous requests, awareness to connectivity, timeout management, and cached responses.",,MOBILESoft '19,,,
Conference Paper,"Nistor A,Song L,Marinov D,Lu S",Toddler: Detecting Performance Problems via Similar Memory-Access Patterns,,2013,,,562–571,IEEE Press,"San Francisco, CA, USA",Proceedings of the 2013 International Conference on Software Engineering,,2013,9781467330763.0,,,,"Performance bugs are programming errors that create significant performance degradation. While developers often use automated oracles for detecting functional bugs, detecting performance bugs usually requires time-consuming, manual analysis of execution profiles. The human effort for performance analysis limits the number of performance tests analyzed and enables performance bugs to easily escape to production. Unfortunately, while profilers can successfully localize slow executing code, profilers cannot be effectively used as automated oracles. This paper presents TODDLER, a novel automated oracle for performance bugs, which enables testing for performance bugs to use the well established and automated process of testing for functional bugs. TODDLER reports code loops whose computation has repetitive and partially similar memory-access patterns across loop iterations. Such repetitive work is likely unnecessary and can be done faster. We implement TODDLER for Java and evaluate it on 9 popular Java codebases. Our experiments with 11 previously known, real-world performance bugs show that TODDLER finds these bugs with a higher accuracy than the standard Java profiler. Using TODDLER, we also found 42 new bugs in six Java projects: Ant, Google Core Libraries, JUnit, Apache Collections, JDK, and JFreeChart. Based on our bug reports, developers so far fixed 10 bugs and confirmed 6 more as real bugs.",,ICSE '13,,,
Conference Paper,Bures M,Metrics for Automated Testability of Web Applications,,2015,,,83–89,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 16th International Conference on Computer Systems and Technologies,"Dublin, Ireland",2015,9781450333573.0,,https://doi.org/10.1145/2812428.2812458;http://dx.doi.org/10.1145/2812428.2812458,10.1145/2812428.2812458,"The paper presents the set of metrics for automated testability of web application and categorizes them by principal areas of automated testability assessment. The presented set of metrics focuses on areas as stability of the system under test, quality of front-end pages from the automated testability point of view, occurrence of elements which are difficult to automate and other related areas. The presented set of metrics are aiding the test analyst to set a scope suitable to automate and estimate potential development and maintenance costs.","automated testing, economics of testing, metrics, web applications, automated testability",CompSysTech '15,,,
Conference Paper,"Karam GM,Buhr RJ",Experience with the Automatic Temporal Analysis of Multitasking Ada Designs,,1987,,,36–44,Association for Computing Machinery,"New York, NY, USA",Proceedings of the 1987 Annual ACM SIGAda International Conference on Ada,"Boston, Massachusetts, USA",1987,9780897912433.0,,https://doi.org/10.1145/317500.317505;http://dx.doi.org/10.1145/317500.317505,10.1145/317500.317505,"In this paper, we report on experience gained in the temporal analysis of multitasking Ada designs. The analysis tool-set, developed as part of the CAEDE project, includes an operational specification language, and deadlock, starvation and critical race analyzers. We identify design parameters that lead to costly analysis, and then describe analysis heuristics that can lead to less costly analysis. Several design examples in which we apply our heuristics, are described. In one of the examples, a 50-fold reduction in analysis cost was obtained with the application of one of the heuristics. Finally, we make recommendations for design environment features that would support the application of analysis heuristics.",,SIGAda '87,,,
Conference Paper,"McCarthy T,Rümmer P,Schäf M",Bixie: Finding and Understanding Inconsistent Code,,2015,,,645–648,IEEE Press,"Florence, Italy",Proceedings of the 37th International Conference on Software Engineering - Volume 2,,2015,,,,,"We present Bixie, a tool to detect inconsistencies in Java code. Bixie detects inconsistent code at a higher precision than previous tools and provides novel fault localization techniques to explain why code is inconsistent. We demonstrate the usefulness of Bixie on over one million lines of code, show that it can detect inconsistencies at a low false alarm rate, and fix a number of inconsistencies in popular open-source projects. Watch our Demo at http://youtu.be/QpsoUBJMxhk.",,ICSE '15,,,
